{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c211ab9c",
   "metadata": {},
   "source": [
    "# Fine-tuning a model for Sentence Similarity and Information Retrieval\n",
    "\n",
    "In this notebook, we will fine-tune a sentence transformer model for document similarity tasks, specifically for the RepositoriUM collection. We'll use a pre-trained model and fine-tune it on pairs of document abstracts with similarity scores.\n",
    "\n",
    "The completed system will allow us to:\n",
    "1. Process document collections from RepositoriUM\n",
    "2. Train a similarity model on document pairs\n",
    "3. Retrieve relevant documents based on a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b235e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/tomas/miniconda3/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /home/tomas/miniconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: sentence-transformers in /home/tomas/miniconda3/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: pandas in /home/tomas/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/tomas/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/tomas/miniconda3/lib/python3.12/site-packages (4.66.4)\n",
      "Requirement already satisfied: evaluate in /home/tomas/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: huggingface_hub in /home/tomas/miniconda3/lib/python3.12/site-packages (0.32.4)\n",
      "Requirement already satisfied: torch in /home/tomas/miniconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Requirement already satisfied: packaging in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Requirement already satisfied: packaging in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: Pillow in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/tomas/.local/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/tomas/.local/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install datasets transformers sentence-transformers pandas numpy tqdm evaluate huggingface_hub torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3929228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.2.0)\n",
      "Requirement already satisfied: requests in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.2.0)\n",
      "Requirement already satisfied: requests in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/tomas/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/tomas/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install accelerate package for PyTorch training support\n",
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac5a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomas/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import sentence_transformers\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe6bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"sentence_similarity_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5a700",
   "metadata": {},
   "source": [
    "## Configuring the Model\n",
    "\n",
    "We'll set the parameters for our model training. For best results in sentence similarity tasks, we should use a pre-trained sentence-transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f536d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Training parameters: 4 epochs, batch size 16, learning rate 2e-05\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "model_checkpoint = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "batch_size = 16  # Adjust based on your GPU memory\n",
    "max_length = 512  # Maximum sequence length\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 4  # Number of training epochs (adjust as needed)\n",
    "warmup_ratio = 0.1  # Percentage of steps for warmup\n",
    "learning_rate = 2e-5  # Learning rate for training\n",
    "\n",
    "print(f\"Selected model: {model_checkpoint}\")\n",
    "print(f\"Training parameters: {num_epochs} epochs, batch size {batch_size}, learning rate {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa71fa",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "First, we need to load the training data that was created by our `process_data.py` script.\n",
    "This data consists of pairs of document abstracts with similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d392025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 document pairs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the training data\n",
    "data_dir = Path(\"data\")\n",
    "train_file = data_dir / \"training_data.json\"\n",
    "\n",
    "try:\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        training_data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(training_data)} document pairs\")\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    # Make sure we handle the data consistently as lists\n",
    "    train_df = pd.DataFrame([\n",
    "        {\"abstract1\": item[0], \"abstract2\": item[1], \"similarity\": float(item[2])}\n",
    "        for item in training_data\n",
    "    ])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading training data: {e}\")\n",
    "    print(\"Please run process_data.py first to create the training data, or check the file path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe6dd8",
   "metadata": {},
   "source": [
    "Let's examine the distribution of similarity scores to understand our data better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7dbd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity: 0.0\n",
      "Max similarity: 0.3\n",
      "Mean similarity: 0.1371096928071928\n",
      "Median similarity: 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if we have enough data\n",
    "if len(train_df) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train_df['similarity'], bins=20)\n",
    "    plt.title('Distribution of Similarity Scores')\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Min similarity: {train_df['similarity'].min()}\")\n",
    "    print(f\"Max similarity: {train_df['similarity'].max()}\")\n",
    "    print(f\"Mean similarity: {train_df['similarity'].mean()}\")\n",
    "    print(f\"Median similarity: {train_df['similarity'].median()}\")\n",
    "else:\n",
    "    print(\"Not enough data to display similarity distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d6a63",
   "metadata": {},
   "source": [
    "Now, let's prepare the data for training by splitting it into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad61ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 400 pairs\n",
      "Validation data: 100 pairs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if we have enough data for a valid split\n",
    "if len(train_df) < 10:\n",
    "    print(\"Warning: Not enough data for a meaningful split. Consider generating more data.\")\n",
    "    # Create a simple split for demonstration\n",
    "    train_data = train_df.iloc[:int(len(train_df)*0.8)]\n",
    "    val_data = train_df.iloc[int(len(train_df)*0.8):]\n",
    "else:\n",
    "    # Split data into train and validation sets\n",
    "    train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data: {len(train_data)} pairs\")\n",
    "print(f\"Validation data: {len(val_data)} pairs\")\n",
    "\n",
    "# Check if we have a reasonable amount of training data\n",
    "if len(train_data) < 100:\n",
    "    print(\"\\nWARNING: Training with a small dataset may lead to poor model performance.\")\n",
    "    print(\"Consider collecting more data for better results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5d629",
   "metadata": {},
   "source": [
    "## Preparing the Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d43bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Model successfully loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "Model is on device: cuda:0\n",
      "Prepared 400 training examples and 100 validation examples\n",
      "Model successfully loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "Model is on device: cuda:0\n",
      "Prepared 400 training examples and 100 validation examples\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Set the device for the model\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on: {device_str}\")\n",
    "\n",
    "try:\n",
    "    # Load pretrained sentence-transformer model\n",
    "    model = SentenceTransformer(model_checkpoint, device=device_str)\n",
    "    print(f\"Model successfully loaded: {model}\")\n",
    "    \n",
    "    # Check model's current device\n",
    "    print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "    # Prepare training examples\n",
    "    train_examples = []\n",
    "    for _, row in train_data.iterrows():\n",
    "        # Convert to string if not already\n",
    "        abstract1 = str(row['abstract1']) if not isinstance(row['abstract1'], str) else row['abstract1']\n",
    "        abstract2 = str(row['abstract2']) if not isinstance(row['abstract2'], str) else row['abstract2']\n",
    "        \n",
    "        # Create input example with properly formatted texts\n",
    "        train_examples.append(InputExample(\n",
    "            texts=[abstract1, abstract2],\n",
    "            label=float(row['similarity'])\n",
    "        ))\n",
    "\n",
    "    # Prepare validation examples\n",
    "    val_examples = []\n",
    "    for _, row in val_data.iterrows():\n",
    "        # Convert to string if not already\n",
    "        abstract1 = str(row['abstract1']) if not isinstance(row['abstract1'], str) else row['abstract1']\n",
    "        abstract2 = str(row['abstract2']) if not isinstance(row['abstract2'], str) else row['abstract2']\n",
    "        \n",
    "        val_examples.append(InputExample(\n",
    "            texts=[abstract1, abstract2],\n",
    "            label=float(row['similarity'])\n",
    "        ))\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    # Create evaluator\n",
    "    evaluator = EmbeddingSimilarityEvaluator(\n",
    "        sentences1=[ex.texts[0] for ex in val_examples],\n",
    "        sentences2=[ex.texts[1] for ex in val_examples],\n",
    "        scores=[ex.label for ex in val_examples]\n",
    "    )\n",
    "    \n",
    "    print(f\"Prepared {len(train_examples)} training examples and {len(val_examples)} validation examples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93629c06",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we'll train our model using the CosineSimilarityLoss which is appropriate for similarity tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aef7d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 4 epochs with 25 batches per epoch\n",
      "Warmup steps: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson Cosine</th>\n",
       "      <th>Spearman Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.073507</td>\n",
       "      <td>0.079409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146122</td>\n",
       "      <td>0.145021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.140694</td>\n",
       "      <td>0.134405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152562</td>\n",
       "      <td>0.144129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 16:00:30 - Saving model checkpoint to output/repositorium-similarity-model/checkpoint-25\n",
      "2025-06-06 16:00:30 - Save model to output/repositorium-similarity-model/checkpoint-25\n",
      "2025-06-06 16:00:30 - Save model to output/repositorium-similarity-model/checkpoint-25\n",
      "2025-06-06 16:00:34 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 1.0 after 25 steps:\n",
      "2025-06-06 16:00:34 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 1.0 after 25 steps:\n",
      "2025-06-06 16:00:37 - Cosine-Similarity :\tPearson: 0.0735\tSpearman: 0.0794\n",
      "2025-06-06 16:00:37 - Save model to output/repositorium-similarity-model\n",
      "2025-06-06 16:00:37 - Cosine-Similarity :\tPearson: 0.0735\tSpearman: 0.0794\n",
      "2025-06-06 16:00:37 - Save model to output/repositorium-similarity-model\n",
      "2025-06-06 16:01:26 - Saving model checkpoint to output/repositorium-similarity-model/checkpoint-50\n",
      "2025-06-06 16:01:26 - Save model to output/repositorium-similarity-model/checkpoint-50\n",
      "2025-06-06 16:01:32 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 2.0 after 50 steps:\n",
      "2025-06-06 16:01:35 - Cosine-Similarity :\tPearson: 0.1461\tSpearman: 0.1450\n",
      "2025-06-06 16:01:35 - Save model to output/repositorium-similarity-model\n",
      "2025-06-06 16:02:24 - Saving model checkpoint to output/repositorium-similarity-model/checkpoint-75\n",
      "2025-06-06 16:02:24 - Save model to output/repositorium-similarity-model/checkpoint-75\n",
      "2025-06-06 16:02:27 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 3.0 after 75 steps:\n",
      "2025-06-06 16:02:31 - Cosine-Similarity :\tPearson: 0.1407\tSpearman: 0.1344\n",
      "2025-06-06 16:03:15 - Saving model checkpoint to output/repositorium-similarity-model/checkpoint-100\n",
      "2025-06-06 16:03:15 - Save model to output/repositorium-similarity-model/checkpoint-100\n",
      "2025-06-06 16:03:20 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 4.0 after 100 steps:\n",
      "2025-06-06 16:03:23 - Cosine-Similarity :\tPearson: 0.1526\tSpearman: 0.1441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 219.29 seconds (3.65 minutes)\n",
      "Model saved to: output/repositorium-similarity-model\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import losses\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Enable logging to see the training progress\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Define loss function\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Set up training parameters\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * warmup_ratio)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_path = 'output/repositorium-similarity-model'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs with {len(train_dataloader)} batches per epoch\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")\n",
    "\n",
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the model with progress bar and proper logging\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=evaluator,\n",
    "        epochs=num_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=output_path,\n",
    "        show_progress_bar=True,\n",
    "        callback=None,\n",
    "        use_amp=True,\n",
    "        checkpoint_path=output_path,\n",
    "        checkpoint_save_steps=len(train_dataloader),  # Save checkpoint after each epoch\n",
    "        checkpoint_save_total_limit=1  # Keep only the latest checkpoint\n",
    "    )\n",
    "    \n",
    "    # Calculate and display training time\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    print(f\"Model saved to: {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    # Try to save the model anyway\n",
    "    try:\n",
    "        model.save(output_path)\n",
    "        print(f\"Partially trained model saved to: {output_path}\")\n",
    "    except:\n",
    "        print(\"Could not save model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1243e",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Let's evaluate our trained model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb885d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 16:03:23 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 16:03:27 - Cosine-Similarity :\tPearson: 0.1526\tSpearman: 0.1441\n",
      "2025-06-06 16:03:27 - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson score: 0.1526\n",
      "Validation Spearman score: 0.1441\n",
      "Loading baseline model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 16:03:30 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset:\n",
      "2025-06-06 16:03:32 - Cosine-Similarity :\tPearson: 0.1459\tSpearman: 0.1312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Pearson score: 0.1459\n",
      "Baseline Spearman score: 0.1312\n",
      "Pearson improvement: 0.0067\n",
      "Spearman improvement: 0.0129\n",
      "The fine-tuned model shows improvement over the baseline!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on validation set\n",
    "try:\n",
    "    print(\"Evaluating fine-tuned model...\")\n",
    "    val_score = evaluator(model)\n",
    "    print(f\"Validation Pearson score: {val_score['pearson_cosine']:.4f}\")\n",
    "    print(f\"Validation Spearman score: {val_score['spearman_cosine']:.4f}\")\n",
    "    \n",
    "    # Compare with baseline model\n",
    "    print(\"Loading baseline model for comparison...\")\n",
    "    baseline_model = SentenceTransformer(model_checkpoint, device=device_str)\n",
    "    baseline_score = evaluator(baseline_model)\n",
    "    print(f\"Baseline Pearson score: {baseline_score['pearson_cosine']:.4f}\")\n",
    "    print(f\"Baseline Spearman score: {baseline_score['spearman_cosine']:.4f}\")\n",
    "    \n",
    "    # Calculate improvement for each metric\n",
    "    pearson_improvement = val_score['pearson_cosine'] - baseline_score['pearson_cosine']\n",
    "    spearman_improvement = val_score['spearman_cosine'] - baseline_score['spearman_cosine']\n",
    "    \n",
    "    print(f\"Pearson improvement: {pearson_improvement:.4f}\")\n",
    "    print(f\"Spearman improvement: {spearman_improvement:.4f}\")\n",
    "    \n",
    "    if pearson_improvement > 0 or spearman_improvement > 0:\n",
    "        print(\"The fine-tuned model shows improvement over the baseline!\")\n",
    "    else:\n",
    "        print(\"The fine-tuned model doesn't show improvement. Consider adjusting parameters or collecting more training data.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e455bf3",
   "metadata": {},
   "source": [
    "## Information Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "718b88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, documents, top_k=5, model=model):\n",
    "    try:\n",
    "        # Check if documents list is empty\n",
    "        if not documents:\n",
    "            print(\"Warning: Empty document list provided\")\n",
    "            return []\n",
    "            \n",
    "        # Encode query\n",
    "        query_embedding = model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
    "        \n",
    "        # Get document abstracts, handling missing abstracts\n",
    "        doc_abstracts = []\n",
    "        for doc in documents:\n",
    "            abstract = doc.get('dc.description.abstract', '')\n",
    "            # Skip empty abstracts\n",
    "            if abstract:\n",
    "                doc_abstracts.append(abstract)\n",
    "            \n",
    "        # Skip processing if no valid abstracts\n",
    "        if not doc_abstracts:\n",
    "            print(\"Warning: No valid abstracts found in the documents\")\n",
    "            return []\n",
    "            \n",
    "        # Encode all documents\n",
    "        doc_embeddings = model.encode(doc_abstracts, \n",
    "                                     convert_to_tensor=True, \n",
    "                                     show_progress_bar=(len(doc_abstracts) > 10))\n",
    "        \n",
    "        # Calculate similarities\n",
    "        import torch.nn.functional as F\n",
    "        similarities = F.cosine_similarity(query_embedding.unsqueeze(0), doc_embeddings)\n",
    "        \n",
    "        # Sort by similarity\n",
    "        results = []\n",
    "        for i, sim in enumerate(similarities):\n",
    "            # Find the original document that corresponds to this abstract\n",
    "            for j, doc in enumerate(documents):\n",
    "                if doc.get('dc.description.abstract', '') == doc_abstracts[i]:\n",
    "                    results.append((doc, sim.item()))\n",
    "                    break\n",
    "        \n",
    "        # Return top k results\n",
    "        return sorted(results, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieve function: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ceac0",
   "metadata": {},
   "source": [
    "## Testing the Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205719ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load document collection\n",
    "collection_file = data_dir / \"col_1822_21316_processed.json\"\n",
    "\n",
    "try:\n",
    "    with open(collection_file, 'r', encoding='utf-8') as f:\n",
    "        documents = json.load(f)\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading document collection: {e}\")\n",
    "    print(\"Creating empty document list\")\n",
    "    documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5873c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query: 'processamento de linguagem natural em português'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:15<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 results:\n",
      "\n",
      "Document 1 - Similarity: 0.4935\n",
      "Title: entity recognition in archival descriptions\n",
      "Authors: cunha, luís filipe da costa\n",
      "Abstract: ['at the moment, there is a vast amount of archival data spread across the portuguese archives, which keeps information from our ancestors’ times to the present day. most of this information was already transcribed to digital format, and the public can access it through archives’ online repositories. despite that, some of these documents are structured with many plain text fields without any annotations, making their content analyses difficult. in this thesis, we implemented several named entity recognition solutions to perform a semantic interpretation of the archival finding aids by extracting named entities like person, place, date, profession, and organization. these entities translate into crucial information about the context in which they are inserted. they can be used for several purposes with high confidence results, such as creating smart browsing tools by using entity linking and record linking techniques. in this way, the main challenge of this work was the creation of powerful ner models capable of producing high confidence results. in order to achieve high result scores, we annotated several corpora to train our machine learning algorithms in the archival domain. we also used different ml architectures such as maxent, cnns, lstms, and bert models. during the model’s validation, we created different environments to test the effect of the context proximity in the training data. finally, during the model’s training, we noticed a lack of available portuguese annotated data, limiting the potential of several nlp tasks. in this way, we developed an intelligent corpus annotator that uses one of our ner models to assist and accelerate the annotation process.', 'de momento, existe uma vasta quantidade de dados arquivísticos espalhados pelos arquivos portugueses, que guardam informações desde os tempos dos nossos antepassados até aos dias de hoje. a maior parte desta informação já foi transcrita para o formato digital e encontra-se disponível ao público através de repositórios online dos arquivos. apesar disso, alguns destes documentos estão estruturados com muitos campos de texto livre, sem quaisquer anotações, o que pode dificultar a análise do seu conteúdo. nesta tese, implementamos várias soluções de reconhecimento de entidades mencionadas, a fim de se realizar uma interpretação semântica sobre descrições arquivísticas, extraindo entidades tais como pessoa, local, data, profissão e organização. estes tipos de entidades traduzem-se em informação crucial sobre o contexto em que estão inseridas. com métricas de confiança suficientemente elevadas, estas entidades podem ser utilizadas para diversos fins, como a criação de ferramentas de navegação inteligente por meio de técnicas de entity linking e record linking. desta forma, o principal desafio deste trabalho consistiu na criação de poderosos modelos ner que fossem capazes de produzir resultados de elevada confiança. para alcançar tais resultados, anotamos vários datasets para treinar os nossos próprios algoritmos de aprendizado de máquina no contexto arquivístico. para além disso, usamos diferentes arquiteturas de ml tais como maxent, cnns, lstms e bert. durante a validação do modelo, criamos diferentes ambientes de teste de modo a testar o efeito da proximidade de contexto nos dados de treino. por fim, durante o treino dos modelos verificamos que existe pouca quantidade de dados disponíveis anotados em português, o que pode limitar o potencial de várias tarefas de nlp. desta forma, desenvolvemos um anotador de datasets inteligente que utiliza um dos nossos modelos de ner para auxiliar e acelerar o processo de anotação.']\n",
      "Keywords: ['named entity recognition', 'archival finding aids', 'machine learning', 'deep learning', 'bert', 'data annotation', 'reconhecimento de entidades mencionadas', 'descrições arquivísticas', 'anotação de dados']\n",
      "\n",
      "Document 2 - Similarity: 0.4811\n",
      "Title: clav: planos de preservação digital\n",
      "Authors: freitas, josé duarte santos\n",
      "Abstract: ['não podendo deixar de aproveitar os benefícios da era tecnológica que vivemos a administração pública portuguesa caminha a passos largos para a digitalização de todo o seu processo organizacional. tal se explica através de diversos fatores que, apesar de díspares, se complementam e interligam entre si. assim, primeiramente se pensarmos no fator da proteção ambiental, a digitalização vai permitir uma redução da utilização de papel e simultaneamente uma redução de custos com o mesmo. por outro lado, esta garante uma maior agilização e otimização dos processos administrativos, assegurando, ainda, uma maior longevidade aos documentos. de forma a atingir estes objetivos nasceu a plataforma classificação e avaliação da informação pública (clav), plataforma essa que tem vindo a crescer ao longo dos últimos anos, que conta com vários colaboradores do departamento de informática da universidade do minho, sendo financiado pelo simplex, visando a classificação e avaliação de toda a documentação presente na administração pública portuguesa. como referido esta plataforma já está bem madura e como tal já conta com diversas funcionalidades para criação e manutenção dos instrumentos de classificação e avaliação, esta dissertação pretende acrescentar uma nova componente ao clav que permita não só a criar como também gerir os planos de preservação digital. para isso, foi necessário definir um modelo a seguir, tendo em conta todos os seus requisitos e invariantes, e adicionar as interfaces necessárias ao clav, com todas as funcionalidades e métodos necessários para a criação, importação e manutenção dos planos de preservação digital.', 'being able to take advantage of the benefits of the technological age that we are experiencing, the portuguese public administration is making great strides towards the digitization of its entire organizational process.this is explained by several factors that, although disparate, complement and interconnect with each other.so, first, if we think about the environmental protection factor, digitization will allow a reduction in the use of paper and simultaneously a cost reduction with the same.on the other hand, it ensures greater speed and optimization of administrative processes, while also ensuring greater longevity to documents.to achieve these objectives, the clav platform was born, a platform that has been growing over the past few years, with several employees from the it department of the university of minho, being financed by simplex, aiming at the classification and evaluation of all the documentation present in the portuguese public administration.as mentioned, this platform is already very mature and as such it already has several functionalities for creating and maintaining the classification and evaluation instruments, this dissertation intends to add a new component to the clav that allows not only to create but also to manage the digital preservation plans.therefore, it was necessary to define a model to follow, bearing in mind all its requirements and invariants, and add the necessary interfaces to the clav, with all the functionalities and methods necessary for the creation, import, and maintenance of the digital preservation plans.']\n",
      "Keywords: ['planos de preservação digital', 'clav', 'digitalização', 'classificação', 'avaliação', 'digital preservation plans', 'digitization', 'classification', 'evaluation']\n",
      "\n",
      "Document 3 - Similarity: 0.4734\n",
      "Title: applying attribute grammars to teach linguistic rules\n",
      "Authors: sousa, manuel gouveia carneiro de\n",
      "Abstract: ['this document presents the topic “applying attribute grammars to teach linguistic rules”, at universidade do minho in braga, portugal. this thesis is focused on using the formalisms of attribute grammars in order to create a tool to help linguistic students learn the different rules of a natural language. the system developed, named lyntax, consists in a processor for a domain specific language which intends to enable the user to specify different kinds of sentence structures, and afterwards, test various phrases against said structures. the processor validates and evaluates the input given, generating a grammar which is specific to a previously chosen sentence. lastly, using antlr, a parser is generated for that specific grammar referred above. the processor built by antlr also creates a syntax tree that is presented to the user for analysis purposes. an interface that supports the specification of the language (written in lyntax dsl) was built, also allowing the use of the processor and the generation of the specific grammar, exempting the user from knowing the details of the process. within this document, the focus will be primarly dedicated to the analysis of the system and how each block was built. different examples of the processor in action will be shown and explained.', 'este documento refere-se a uma dissertação sobre o tópico “aplicar gramáticas de atribu tos no ensino de regras de linguística”, e será concluída na universidade do minho em braga, portugal. esta dissertação pretende focar-se no uso dos formalismos das gramáticas de atributos de maneira a criar uma ferramenta que ajude os alunos de linguística a aprender as diversas regras da língua natural. o sistema desenvolvido, denominado de lyntax, consiste em um processor para uma linguagem de domínio específico cujo objetivo é o de permitir ao seu utilizador a possibili dade de especificar diversas estruturas de frases, e posteriormente, testar frases contra essas mesmas estruturas. o processador valida e avalia o input recebido, gerando uma gramática específica à frase previamente escolhida. por fim, usando uma ferramenta como o antlr, um parser é gerado para a gramática específica acima referida. o processador construído pelo antlr também gera a árvore de syntax que é apresentada ao utilizador com o intuito de ser analisada. foi também criada uma interface que suporta a especificação da linguagem, permitindo também o uso do processador e a geração da gramática específica, abstraindo assim o utilizador de quaisquer tipo de cálculos. neste documento, o focus primário será dedicado à análise do sistema e como cada bloco foi construído. diferentes exemplos de uso do processador serão apresentados e explicados.']\n",
      "Keywords: ['linguistic', 'natural language processing', 'attribute grammar', 'linguística', 'processamento de língua natural', 'gramáticas de atributo']\n",
      "\n",
      "================================================================================\n",
      "Query: 'web performance optimization'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:15<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 results:\n",
      "\n",
      "Document 1 - Similarity: 0.3971\n",
      "Title: on the performance of webassembly\n",
      "Authors: macedo, joão gonçalves de\n",
      "Abstract: ['the worldwide web has dramatically evolved in recent years. web pages are dynamic, expressed by pro grams written in common programming languages given rise to sophisticated web applications. thus, web browsers are almost operating systems, having to interpret/compile such programs and execute them. although javascript is widely used to express dynamic web pages, it has several shortcomings and performance inefficiencies. to overcome such limitations, major it powerhouses are developing a new portable and size/load efficient language: webassembly. in this dissertation, we conduct the first systematic study on the energy and run-time performance of webassembly and javascript on the web. we used micro-benchmarks and real applications to have more realistic results. the results show that webassembly, while still in its infancy, is starting to already outperform javascript, with much more room to grow. a statistical analysis indicates that webassembly produces significant performance differences compared to javascript. however, these differences differ between micro-benchmarks and real-world benchmarks. our results also show that webassembly improved energy efficiency by 30%, on average, and show how different webassembly behaviour is among three popular web browsers: google chrome, microsoft edge, and mozilla firefox. our findings indicate that webassembly is faster than javascript and even more energy-efficient. our benchmarking framework is also available to allow further research and replication.', 'a web evoluiu dramaticamente em todo o mundo nos últimos anos. as páginas web são dinâmicas, expressas por programas escritos em linguagens de programação comuns, dando origem a aplicativos web sofisticados. assim, os navegadores web são quase como sistemas operacionais, tendo que interpre tar/compilar tais programas e executá-los. embora o javascript seja amplamente usado para expressar páginas web dinâmicas, ele tem várias deficiências e ineficiências de desempenho. para superar tais limitações, as principais potências de ti estão a desenvolver uma nova linguagem portátil e eficiente em tamanho/carregamento: webassembly. nesta dissertação, conduzimos o primeiro estudo sistemático sobre o desempenho da energia e do tempo de execução do webassembly e javascript na web. usamos micro-benchmarks e aplicações reais para obter resultados mais realistas. os resultados mostram que webassembly, embora ainda esteja na sua infância, já está começa a superar o javascript, com muito mais espaço para crescer. uma análise estatística indica que webassembly produz diferenças de desempenho significativas em relação ao javascript. no entanto, essas diferenças diferem entre micro-benchmarks e benchmarks de aplicações reais. os nossos resultados também mostram que o webassembly melhorou a eficiência energética em 30%, em média, e mostram como o comportamento do webassembly é diferente entre três navegadores web populares: google chrome, microsoft edge e mozilla firefox. as nossas descobertas indicam que o webassembly é mais rápido que o javascript e ainda mais eficiente em termos de energia. a nossa benchmarking framework está disponível para permitir pesquisas adicionais e replicação.']\n",
      "Keywords: ['energy efficiency', 'green software', 'web browsers', 'webassembly', 'eficiência energética', 'navegadores web', 'software verde']\n",
      "\n",
      "Document 2 - Similarity: 0.3357\n",
      "Title: query optimizers based on machine learning techniques\n",
      "Authors: souto, rui pedro sousa rodrigues do\n",
      "Abstract: ['query optimizers are considered one of the most relevant and sophisticated components in a database management system. however, despite currently producing nearly optimal results, optimizers rely on statistical estimates and heuristics to reduce the search space of alternative execution plans for a single query. as a result, for more complex queries, errors may grow exponentially, often translating into sub-optimal plans resulting in less than ideal performance. recent advances in machine learning techniques have opened new opportunities for many of the existing problems related to system optimization. this document proposes a solution built on top of postgresql that learns to select the most efficient set of optimizer strategy settings for a particular query. instead of depending entirely on the optimizer’s estimates to compare different plans under different configurations, it relies on a greedy selection algorithm that supports several types of predictive modeling techniques, from more traditional modeling techniques to a deep learning approach. the system is evaluated experimentally with the standard tpc-h and join order ing benchmark workloads to measure the cost and benefits of adding machine learning capabilities to traditional query optimizers.', 'os otimizadores de queries são considerados um dos componentes de maior relevância e complexidade num sistema de gestão de bases de dados. no entanto, apesar de atualmente produzirem resultados quase ótimos, os otimizadores dependem do uso de estimativas estatísticas e de heurísticas para reduzir o espaço de procura de planos de execução alternativos para uma determinada query. como resultado, para queries mais complexas, os erros podem crescer exponencialmente, o que geralmente se traduz em planos sub-ótimos, resultando num desempenho inferior ao ideal. os recentes avanços nas técnicas de aprendizagem automática abriram novas oportunidades para muitos dos problemas existentes relacionados com otimização de sistemas. este documento propõe uma solução construída sobre o postgresql que aprende a selecionar o conjunto mais eficiente de configurações do otimizador para uma determinada query. em vez de depender inteiramente de estimativas do otimizador para comparar planos de configurações diferentes, a solução baseia-se num algoritmo de seleção greedy que suporta vários tipos de técnicas de modelagem preditiva, desde técnicas mais tradicionais a uma abordagem de deep learning. o sistema é avaliado experimentalmente com os workloads tpc-h e join ordering benchmark para medir o custo e os benefícios de adicionar aprendizagem automática a otimizadores de queries tradicionais.']\n",
      "Keywords: ['database tuning', 'machine learning', 'query optimization', 'aprendizagem automática', 'otimização de queries', 'tuning de base de dados']\n",
      "\n",
      "Document 3 - Similarity: 0.3296\n",
      "Title: estudo e implementação de interfaces web em html5\n",
      "Authors: rodrigues, samuel da costa\n",
      "Abstract: ['a web está em constante evolução. a evolução tecnológica fez com que as aplicações web estivessem cada vez mais presentes no mercado e surgissem novos padrões arquiteturais, novos dispositivos e novas experiências de utilização das aplicações. tudo isto com o propósito de satisfazer as exigências que o mercado e os utilizadores finais impõem. toda esta evolução potenciou o aparecimento de uma nova versão do html, o html5, a qual está em constante progresso. não se pode esperar que o html5 esteja totalmente concluído para implementar interfaces com esta tecnologia, podendo assim ser considerado um living standard, porque já existem funcionalidades suficientemente maduras e que inclusivamente já são utilizadas em produtos no mercado. devido ao facto do html ser uma linguagem de marcação, torna-se indispensável associar o html ao javascript, de modo a poder disponibilizar dinamismo e funcionalidade às páginas web. existiu por isso, tal como o html, uma grande evolução relativa à linguagem, mais propriamente às frameworks javascript de desenvolvimento web existentes. devido ao progresso destas tecnologias, esta dissertação pretende analisar as várias funcionalidades disponibilizadas pela tecnologia html5 e as frameworks javascript existentes no mercado. para tal, estas tecnologias serão usadas, de forma experimental, num projeto de desenvolvimento de software na empresa onde decorre este trabalho. os resultados focam-se em identificar as funcionalidades html5 implementadas e apresentar uma comparação entre as frameworks javascript em estudo, segundo um conjunto específico de critérios. é do interesse da empresa onde este trabalho foi realizado aplicar as funcionalidades html5 e frameworks javascript, de modo a identificar as vantagens e desvantagens de cada tecnologia para, num futuro próximo, as aplicar em aplicações web. o objetivo deste trabalho é assim medir quantitativa e qualitativamente, segundo os critérios considerados para análise, o impacto da introdução do html5 e das frameworks javascript em produtos de software, caso estas substituam as que estão hoje em dia em utilização.', \"the web is continuously evolving. this evolution increased the presence of web applications in the market, created new architectural patterns, new devices and new ways of user experience. all of this with the goal to meet the requirements that the market and the final users require. those developments have caused the creation of a new version of html, html5, which is in constant progress. no one can expect that html5 is a finished specification, to start creating interfaces only with this technology, but it can be considered a standard living because there are features mature enough to be developed in web applications available for the market. due to the fact html is a markup language, it is essential to link html to javascript, in order to provide dynamism and functionality to web pages. for this reason there was a major evolution on the language, more specifically in javascript frameworks existing for web development. the progress of these technologies has led this dissertation to analyze the multiple features provided by html5 technology and the existing javascript frameworks. for such, these technologies will be used, experimentally, in a software development project in the company where this work follows. the results are focused on identifying the features made with html5 and display a comparison of javascript frameworks studied, according to a specific set of criteria. it is the company's interest where this work was carried out, apply the html5 features and javascript frameworks, in order to identify the pros and cons of each technology, to in the near future, develop them in web applications. the goal of this work is to measure quantitatively and qualitatively, according to the criteria considered for analysis, the impact of the introduction of html5 and javascript frameworks in web software products, if they replace those in use today.\"]\n",
      "Keywords: ['html5', 'frameworks javascript mvc']\n",
      "\n",
      "================================================================================\n",
      "Query: 'machine learning applications'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 32/32 [00:15<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 results:\n",
      "\n",
      "Document 1 - Similarity: 0.5671\n",
      "Title: in-vehicle object detection with yolo algorithm\n",
      "Authors: farinha, joão simões\n",
      "Abstract: ['with the growing computational power that we have at our disposal and the ever-increasing amount of data available the field of machine learning has given rise to deep learning, a subset of machine learning algorithms that have shown extraordinary results in a variety of applications from natural language processing to computer vision. in the field of computer vision, these algorithms have greatly improved the state-of-the-art accuracy in tasks associated with object recognition such as detection. this thesis makes use of one of these algorithms, specifically the yolo algorithm, as a basis in the development of a system capable of detecting objects laying inside a car cockpit. to this end a dataset is collected for the purpose of training the yolo algorithm on this task. a comparative analysis of the detection performance of the yolov2 and yolov3 architectures is performed.several experiments are performed by modifying the yolov3 architecture to attempt to improve its accuracy. specifically tests are performed in regards to network size, and the multiple outputs present in this network. explorative experiments are done in order to test the effect that parallel network might have on detection performance. lastly tests are done to try to find an optimal learning rate and batch size for our dataset on the new architectures.', 'com o crescente poder computacional que temos à nossa disposição e o aumento da quantidade dados a que temos acesso o campo de machine learning deu origem ao deep learning um subconjunto de algoritmos de machine learning que têm demonstrado resultados extraordinários numa variedade de aplicações desde processamento de linguagens naturais a visão por computador. no campo de visão por computador estes algoritmos têm levado a enormes progressos na correção de sistemas de deteção de objetos. nesta tese usamos um destes algoritmos, especificament o yolo, como base para desenvolver um sistema capaz de detetar objetos dentro de um carro. dado isto um dataset é recolhido com o propósito de treinar o algoritmo yolo nesta tarefa. uma analise comparativa da correção dos algoritmos yolov2 e yolov3 ´e realizada. várias técnicas relacionadas com a modificação da arquitetura yolov3 são exploradas para otimizar o sistema para o problema especifico de deteção a bordo de veículos. especificamente testes são realizados no contexto de tamanho da rede e dos múltiplos outputs presentes nesta rede. experiencias exploratórias são realizadas de forma a testar o efeito que redes parallelas podem ter na correção dos algoritmos. por fim testes são feitos para tentar encontrar learning rates e batch sizes apropriados para o nosso dataset nas novas arquiteturas.']\n",
      "Keywords: None\n",
      "\n",
      "Document 2 - Similarity: 0.5592\n",
      "Title: fault tolerant decentralized deep neural networks\n",
      "Authors: padrão, joão carlos faria\n",
      "Abstract: ['machine learning is trending in computer science, especially deep learning. training algorithms that follow this approach to machine learning routinely deal with vast amounts of data. processing these enormous quantities of data requires complex computation tasks that can take a long time to produce results. distributing computation efforts across multiple machines makes sense in this context, as it allows conclusive results to be available in a shorter time frame. distributing the training of a deep neural network is not a trivial procedure. various architectures have been proposed, following two different paradigms. the most common one follows a centralized approach, where a centralized entity, broadly named parameter server, synchronizes and coordinates the updates generated by a number of workers. the alternative discards the centralized unit, assuming a decentralized architecture. the synchronization between the multiple workers is assured by communication techniques that average gradients between a node and its peers. high-end clusters are the ideal environment to deploy deep learning systems. low latency between nodes assures low idle times for workers, increasing the overall system performance. these setups, however, are expensive and are only available to a limited number of entities. on the other end, there is a continuous growth of edge devices with potentially vast amounts of available computational resources. in this dissertation, we aim to implement a fault tolerant decentralized deep neural net work training framework, capable of handling the high latency and unreliability characteristic of edge networks. to manage communication between nodes, we employ decentralized algorithms capable of estimating parameters globally', 'machine learning, mais especificamente deep learning, é um campo emergente nas ciências da computação. algoritmos de treino aplicados em deep learning lidam muito frequentemente com vastas quantidades de dados. processar estas enormes quantidades de dados requer operações computacionais complexas que demoram demasiado tempo para produzir resultados. distribuir o esforço computacional por múltiplas máquinas faz todo o sentido neste contexto e permite um aumento significativo de desempenho. distribuir o método de treino de uma rede neuronal não é um processo trivial. várias arquiteturas têm sido propostas, seguindo dois diferentes paradigmas. o mais comum segue uma abordagem centralizada, onde uma entidade central, normalmente denominada de parameter server, sincroniza e coordena todas as atualizações produzidas pelos workers. a alternativa passa por descartar a entidade centralizada, assumindo uma arquitetura descentralizada. a sincronização entre workers é assegurada através de estratégias de comunicação descentralizadas. clusters de alta performance são o ambiente ideal para a implementação de sistemas de deep learning. a baixa latência entre nodos assegura baixos períodos de inatividade nos workers, aumentando assim o rendimento do sistema. estas instalações, contudo, são muito custosas, estando apenas disponíveis para um pequeno número de entidades. por outro lado, o número de equipamentos nas extremidades da rede, com baixo aproveitamento de poder computacional, continua a crescer, o que torna o seu uso desejável. nesta dissertação, visamos implementar um ambiente de treino de redes neuronais descentralizado e tolerante a faltas, apto a lidar com alta latência nas comunicações e baixa estabilidade nos nodos, caraterística de redes na extremidade. para coordenar a comunicação entre os nodos, empregamos algoritmos de agregação, capazes de criar uma visão geral de parâmetros numa topologia.']\n",
      "Keywords: ['distributed systems', 'machine learning', 'artificial intelligence', 'fault tolerance', 'sistemas distribuídos', 'inteligência artificial', 'tolerância a faltas']\n",
      "\n",
      "Document 3 - Similarity: 0.5480\n",
      "Title: automation of machine learning models benchmarking\n",
      "Authors: sá, joão pedro barros\n",
      "Abstract: [\"na área de ciência de dados, o machine learning está-se a revelar uma ferramenta essencial para resolver problemas complexos. as empresas estão a investir em equipas de ciência de dados e machine learning para desenvolver modelos que apresentem valor para os clientes. no entanto, estes modelos são uma pequena percentagem de uma pipeline de projetos de machine learning (ml) e, para entregar um produto de ml completo, é necessário um número maior de componentes. devops é uma mentalidade de engenharia e um conjunto de práticas que visa unificar o processo de desenvolvimento e o processo de operações em um software, mlops é um conceito similar a devops mas aplicado ao desenvolvimento e entrega de soluções de ml. o nível de automatização das etapas em uma pipeline de ml define a maturidade do processo de ml, que reflete a velocidade de treino de novos modelos com novos dados ou de treino de novos modelos com diferentes implementações. um sistema de ml é um sistema de software, desenvolvimento e atualizações contínuas são necessárias para garantir um sistema que escale conforme as necessidades. o principal objetivo desta tese é apoiar a criação de um sistema integrado de ml com uma arquitetura que proporcione a capacidade de ser continuamente operada em um ambiente de produção. um conceito para avaliação de desempenho de algoritmos deve ser elaborado e implementado. o principal obetivo e melhorar e ace'erar o cicio de desenvolvimento de modelos de ml na empresa. para atingir este objetivo surge a necessidade de definir uma arquitetura com especificações e a implementação de processos automatizadas num pipeline de ml existente, este processo têm como objetivo alcançar uma ferramenta de benchmark de modelos, com capacidade de analisar o desempenho do modelo, um motor de inferência e um banco de dados para armazenar todas as métricas computadas. um sistema baseado em ia em desenvolvimento fornece o caso de estudo para desenvolver e validar a arquitetura. os avanços atuais na área da condução semiautomática introduz a necessidade de sistemas de monitoramento que podem localizar e detectar eventos especificas no veículo. os conjuntos de sensores são instalados dentro da cabine para alimentar sistemas inteligentes que visam analisar e sinalizar certos comportamentos que podem impactar a segurança e o conforto dos passageiros..\", 'in the field of data science, ml is proving to be a core feature to solve complex real-world problems. businesses are investing in data science and ml teams to develop ai based models that can deliver business value to their users. however, these models are only a small fraction of an ml project pipeline, and to deliver an end to end ml product, a greater number of components are needed. devops is an engineering mindset and a set of practices that aims to unify the development process and the operation process on software. mlops is a similar concept to devops but applicable to the development and delivery of ml based solutions. the automation of the steps in a ml pipeline defines the maturity of the ml process, reflecting the velocity of training new models given new data or training new models given new implementations. an ml system is a software system that can support development, provide continuous integration and continuous delivery apply to help guarantee that one can reliably build and operate ml systems at scale. the main objective of this thesis are to support the creation of an integrated ml system with an archi tecture that provides the ability to be continuously operated in a production-like environment. furthermore, a concept to evaluate the performance of algorithms shall be devised and implemented. the end goal is to improve and accelerate the ml development lifecycle. to achieve this goal surges the need to define an architecture alongside specifications and the implementation of several automated steps into an existing ml pipeline. to improve and accelerate model development an model engine benchmark tool is devised capable of several features, including the ability to have dashboards for model performance evaluation, an automatic inference engine, performance metrics for the model and a database to store all the computed metrics and metadata. an ai-based system under development provides the case study to develop and validate this architec ture. the current advances of semi-automated driving introduce the need for monitoring systems to scan and detect specific events in the vehicle. sensor clusters are installed inside the vehicle cabin to feed data to intelligent systems that aim to analyze and red flag certain behaviours that can potentially impact passengers safety and comfort while using the vehicle.']\n",
      "Keywords: ['engenharia software', 'aprendizagem máquina', 'ciência dados', 'devops', 'mlops', 'machine learning', 'software', 'data science', 'pipelines', 'automation']\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "queries = [\n",
    "    \"processamento de linguagem natural em português\",\n",
    "    \"web performance optimization\",\n",
    "    \"machine learning applications\"\n",
    "]\n",
    "\n",
    "# Test the retrieval function with each query\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Retrieve similar documents\n",
    "        results = retrieve(query, documents, top_k=3)\n",
    "        \n",
    "        # Display results\n",
    "        if results:\n",
    "            print(f\"Top {len(results)} results:\")\n",
    "            for i, (doc, sim) in enumerate(results, 1):\n",
    "                print(f\"\\nDocument {i} - Similarity: {sim:.4f}\")\n",
    "                print(f\"Title: {doc.get('dc.title', 'No title')}\")\n",
    "                print(f\"Authors: {doc.get('dc.contributor.author', 'Unknown')}\")\n",
    "                \n",
    "                abstract = doc.get('dc.description.abstract', 'No abstract')\n",
    "                if len(abstract) > 200:\n",
    "                    print(f\"Abstract: {abstract[:200]}...\")\n",
    "                else:\n",
    "                    print(f\"Abstract: {abstract}\")\n",
    "                    \n",
    "                print(f\"Keywords: {doc.get('dc.subject', 'None')}\")\n",
    "        else:\n",
    "            print(\"No results found for this query.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing retrieval: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
