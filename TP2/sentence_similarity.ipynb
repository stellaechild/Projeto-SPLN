{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c211ab9c",
   "metadata": {},
   "source": [
    "# Fine-tuning a model for Sentence Similarity and Information Retrieval\n",
    "\n",
    "In this notebook, we will fine-tune a sentence transformer model for document similarity tasks, specifically for the RepositoriUM collection. We'll use a pre-trained model and fine-tune it on pairs of document abstracts with similarity scores.\n",
    "\n",
    "The completed system will allow us to:\n",
    "1. Process document collections from RepositoriUM\n",
    "2. Train a similarity model on document pairs\n",
    "3. Retrieve relevant documents based on a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b235e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/tomas/miniconda3/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /home/tomas/miniconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: sentence-transformers in /home/tomas/miniconda3/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: pandas in /home/tomas/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/tomas/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/tomas/miniconda3/lib/python3.12/site-packages (4.66.4)\n",
      "Requirement already satisfied: evaluate in /home/tomas/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: huggingface_hub in /home/tomas/miniconda3/lib/python3.12/site-packages (0.32.4)\n",
      "Requirement already satisfied: torch in /home/tomas/miniconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Requirement already satisfied: packaging in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.2.0)\n",
      "Requirement already satisfied: packaging in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: Pillow in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tomas/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/tomas/.local/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/tomas/.local/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install datasets transformers sentence-transformers pandas numpy tqdm evaluate huggingface_hub torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3929228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.2.0)\n",
      "Requirement already satisfied: requests in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.2.0)\n",
      "Requirement already satisfied: requests in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/tomas/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/tomas/.local/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tomas/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomas/miniconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install accelerate package for PyTorch training support\n",
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac5a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomas/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import sentence_transformers\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe6bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"sentence_similarity_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5a700",
   "metadata": {},
   "source": [
    "## Configuring the Model\n",
    "\n",
    "We'll set the parameters for our model training. For best results in sentence similarity tasks, we should use a pre-trained sentence-transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f536d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "Training parameters: 4 epochs, batch size 16, learning rate 2e-05\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "model_checkpoint = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "batch_size = 16  # Adjust based on your GPU memory\n",
    "max_length = 512  # Maximum sequence length\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 4  # Number of training epochs (adjust as needed)\n",
    "warmup_ratio = 0.1  # Percentage of steps for warmup\n",
    "learning_rate = 2e-5  # Learning rate for training\n",
    "\n",
    "print(f\"Selected model: {model_checkpoint}\")\n",
    "print(f\"Training parameters: {num_epochs} epochs, batch size {batch_size}, learning rate {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa71fa",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "First, we need to load the training data that was created by our `process_data.py` script.\n",
    "This data consists of pairs of document abstracts with similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d392025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 document pairs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the training data\n",
    "data_dir = Path(\"data\")\n",
    "train_file = data_dir / \"training_data.json\"\n",
    "\n",
    "try:\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        training_data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(training_data)} document pairs\")\n",
    "    \n",
    "    # Convert to DataFrame for easier handling\n",
    "    # Make sure we handle the data consistently as lists\n",
    "    train_df = pd.DataFrame([\n",
    "        {\"abstract1\": item[0], \"abstract2\": item[1], \"similarity\": float(item[2])}\n",
    "        for item in training_data\n",
    "    ])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading training data: {e}\")\n",
    "    print(\"Please run process_data.py first to create the training data, or check the file path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe6dd8",
   "metadata": {},
   "source": [
    "Let's examine the distribution of similarity scores to understand our data better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7dbd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity: 0.0\n",
      "Max similarity: 0.3\n",
      "Mean similarity: 0.1371096928071928\n",
      "Median similarity: 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if we have enough data\n",
    "if len(train_df) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train_df['similarity'], bins=20)\n",
    "    plt.title('Distribution of Similarity Scores')\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Min similarity: {train_df['similarity'].min()}\")\n",
    "    print(f\"Max similarity: {train_df['similarity'].max()}\")\n",
    "    print(f\"Mean similarity: {train_df['similarity'].mean()}\")\n",
    "    print(f\"Median similarity: {train_df['similarity'].median()}\")\n",
    "else:\n",
    "    print(\"Not enough data to display similarity distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d6a63",
   "metadata": {},
   "source": [
    "Now, let's prepare the data for training by splitting it into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad61ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 400 pairs\n",
      "Validation data: 100 pairs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if we have enough data for a valid split\n",
    "if len(train_df) < 10:\n",
    "    print(\"Warning: Not enough data for a meaningful split. Consider generating more data.\")\n",
    "    # Create a simple split for demonstration\n",
    "    train_data = train_df.iloc[:int(len(train_df)*0.8)]\n",
    "    val_data = train_df.iloc[int(len(train_df)*0.8):]\n",
    "else:\n",
    "    # Split data into train and validation sets\n",
    "    train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data: {len(train_data)} pairs\")\n",
    "print(f\"Validation data: {len(val_data)} pairs\")\n",
    "\n",
    "# Check if we have a reasonable amount of training data\n",
    "if len(train_data) < 100:\n",
    "    print(\"\\nWARNING: Training with a small dataset may lead to poor model performance.\")\n",
    "    print(\"Consider collecting more data for better results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5d629",
   "metadata": {},
   "source": [
    "## Preparing the Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d43bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Model successfully loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "Model is on device: cuda:0\n",
      "Prepared 400 training examples and 100 validation examples\n",
      "Model successfully loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "Model is on device: cuda:0\n",
      "Prepared 400 training examples and 100 validation examples\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Set the device for the model\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on: {device_str}\")\n",
    "\n",
    "try:\n",
    "    # Load pretrained sentence-transformer model\n",
    "    model = SentenceTransformer(model_checkpoint, device=device_str)\n",
    "    print(f\"Model successfully loaded: {model}\")\n",
    "    \n",
    "    # Check model's current device\n",
    "    print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "    # Prepare training examples\n",
    "    train_examples = []\n",
    "    for _, row in train_data.iterrows():\n",
    "        # Convert to string if not already\n",
    "        abstract1 = str(row['abstract1']) if not isinstance(row['abstract1'], str) else row['abstract1']\n",
    "        abstract2 = str(row['abstract2']) if not isinstance(row['abstract2'], str) else row['abstract2']\n",
    "        \n",
    "        # Create input example with properly formatted texts\n",
    "        train_examples.append(InputExample(\n",
    "            texts=[abstract1, abstract2],\n",
    "            label=float(row['similarity'])\n",
    "        ))\n",
    "\n",
    "    # Prepare validation examples\n",
    "    val_examples = []\n",
    "    for _, row in val_data.iterrows():\n",
    "        # Convert to string if not already\n",
    "        abstract1 = str(row['abstract1']) if not isinstance(row['abstract1'], str) else row['abstract1']\n",
    "        abstract2 = str(row['abstract2']) if not isinstance(row['abstract2'], str) else row['abstract2']\n",
    "        \n",
    "        val_examples.append(InputExample(\n",
    "            texts=[abstract1, abstract2],\n",
    "            label=float(row['similarity'])\n",
    "        ))\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    # Create evaluator\n",
    "    evaluator = EmbeddingSimilarityEvaluator(\n",
    "        sentences1=[ex.texts[0] for ex in val_examples],\n",
    "        sentences2=[ex.texts[1] for ex in val_examples],\n",
    "        scores=[ex.label for ex in val_examples]\n",
    "    )\n",
    "    \n",
    "    print(f\"Prepared {len(train_examples)} training examples and {len(val_examples)} validation examples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93629c06",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we'll train our model using the CosineSimilarityLoss which is appropriate for similarity tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aef7d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 4 epochs with 25 batches per epoch\n",
      "Warmup steps: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson Cosine</th>\n",
       "      <th>Spearman Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.073507</td>\n",
       "      <td>0.079409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146122</td>\n",
       "      <td>0.145021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.140694</td>\n",
       "      <td>0.134405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152562</td>\n",
       "      <td>0.144129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 16:00:30 - Saving model checkpoint to output/repositorium-similarity-model/checkpoint-25\n",
      "2025-06-06 16:00:30 - Save model to output/repositorium-similarity-model/checkpoint-25\n",
      "2025-06-06 16:00:30 - Save model to output/repositorium-similarity-model/checkpoint-25\n",
      "2025-06-06 16:00:34 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 1.0 after 25 steps:\n",
      "2025-06-06 16:00:34 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 1.0 after 25 steps:\n",
      "2025-06-06 16:00:37 - Cosine-Similarity :\tPearson: 0.0735\tSpearman: 0.0794\n",
      "2025-06-06 16:00:37 - Save model to output/repositorium-similarity-model\n",
      "2025-06-06 16:00:37 - Cosine-Similarity :\tPearson: 0.0735\tSpearman: 0.0794\n",
      "2025-06-06 16:00:37 - Save model to output/repositorium-similarity-model\n",
      "2025-06-06 16:01:26 - Saving model checkpoint to output/repositorium-similarity-model/checkpoint-50\n",
      "2025-06-06 16:01:26 - Save model to output/repositorium-similarity-model/checkpoint-50\n",
      "2025-06-06 16:01:32 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 2.0 after 50 steps:\n",
      "2025-06-06 16:01:35 - Cosine-Similarity :\tPearson: 0.1461\tSpearman: 0.1450\n",
      "2025-06-06 16:01:35 - Save model to output/repositorium-similarity-model\n",
      "2025-06-06 16:02:24 - Saving model checkpoint to output/repositorium-similarity-model/checkpoint-75\n",
      "2025-06-06 16:02:24 - Save model to output/repositorium-similarity-model/checkpoint-75\n",
      "2025-06-06 16:02:27 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 3.0 after 75 steps:\n",
      "2025-06-06 16:02:31 - Cosine-Similarity :\tPearson: 0.1407\tSpearman: 0.1344\n",
      "2025-06-06 16:03:15 - Saving model checkpoint to output/repositorium-similarity-model/checkpoint-100\n",
      "2025-06-06 16:03:15 - Save model to output/repositorium-similarity-model/checkpoint-100\n",
      "2025-06-06 16:03:20 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset in epoch 4.0 after 100 steps:\n",
      "2025-06-06 16:03:23 - Cosine-Similarity :\tPearson: 0.1526\tSpearman: 0.1441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 219.29 seconds (3.65 minutes)\n",
      "Model saved to: output/repositorium-similarity-model\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import losses\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Enable logging to see the training progress\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Define loss function\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Set up training parameters\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * warmup_ratio)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_path = 'output/repositorium-similarity-model'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs with {len(train_dataloader)} batches per epoch\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")\n",
    "\n",
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the model with progress bar and proper logging\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=evaluator,\n",
    "        epochs=num_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=output_path,\n",
    "        show_progress_bar=True,\n",
    "        callback=None,\n",
    "        use_amp=True,\n",
    "        checkpoint_path=output_path,\n",
    "        checkpoint_save_steps=len(train_dataloader),  # Save checkpoint after each epoch\n",
    "        checkpoint_save_total_limit=1  # Keep only the latest checkpoint\n",
    "    )\n",
    "    \n",
    "    # Calculate and display training time\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    print(f\"Model saved to: {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    # Try to save the model anyway\n",
    "    try:\n",
    "        model.save(output_path)\n",
    "        print(f\"Partially trained model saved to: {output_path}\")\n",
    "    except:\n",
    "        print(\"Could not save model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1243e",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Let's evaluate our trained model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb885d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 16:03:23 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 16:03:27 - Cosine-Similarity :\tPearson: 0.1526\tSpearman: 0.1441\n",
      "2025-06-06 16:03:27 - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson score: 0.1526\n",
      "Validation Spearman score: 0.1441\n",
      "Loading baseline model for comparison...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 16:03:30 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset:\n",
      "2025-06-06 16:03:32 - Cosine-Similarity :\tPearson: 0.1459\tSpearman: 0.1312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Pearson score: 0.1459\n",
      "Baseline Spearman score: 0.1312\n",
      "Pearson improvement: 0.0067\n",
      "Spearman improvement: 0.0129\n",
      "The fine-tuned model shows improvement over the baseline!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on validation set\n",
    "try:\n",
    "    print(\"Evaluating fine-tuned model...\")\n",
    "    val_score = evaluator(model)\n",
    "    print(f\"Validation Pearson score: {val_score['pearson_cosine']:.4f}\")\n",
    "    print(f\"Validation Spearman score: {val_score['spearman_cosine']:.4f}\")\n",
    "    \n",
    "    # Compare with baseline model\n",
    "    print(\"Loading baseline model for comparison...\")\n",
    "    baseline_model = SentenceTransformer(model_checkpoint, device=device_str)\n",
    "    baseline_score = evaluator(baseline_model)\n",
    "    print(f\"Baseline Pearson score: {baseline_score['pearson_cosine']:.4f}\")\n",
    "    print(f\"Baseline Spearman score: {baseline_score['spearman_cosine']:.4f}\")\n",
    "    \n",
    "    # Calculate improvement for each metric\n",
    "    pearson_improvement = val_score['pearson_cosine'] - baseline_score['pearson_cosine']\n",
    "    spearman_improvement = val_score['spearman_cosine'] - baseline_score['spearman_cosine']\n",
    "    \n",
    "    print(f\"Pearson improvement: {pearson_improvement:.4f}\")\n",
    "    print(f\"Spearman improvement: {spearman_improvement:.4f}\")\n",
    "    \n",
    "    if pearson_improvement > 0 or spearman_improvement > 0:\n",
    "        print(\"The fine-tuned model shows improvement over the baseline!\")\n",
    "    else:\n",
    "        print(\"The fine-tuned model doesn't show improvement. Consider adjusting parameters or collecting more training data.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e455bf3",
   "metadata": {},
   "source": [
    "## Information Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "718b88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, documents, top_k=5, model=model):\n",
    "    try:\n",
    "        # Check if documents list is empty\n",
    "        if not documents:\n",
    "            print(\"Warning: Empty document list provided\")\n",
    "            return []\n",
    "            \n",
    "        # Encode query\n",
    "        query_embedding = model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
    "        \n",
    "        # Get document abstracts, handling missing abstracts\n",
    "        doc_abstracts = []\n",
    "        for doc in documents:\n",
    "            abstract = doc.get('dc.description.abstract', '')\n",
    "            # Skip empty abstracts\n",
    "            if abstract:\n",
    "                doc_abstracts.append(abstract)\n",
    "            \n",
    "        # Skip processing if no valid abstracts\n",
    "        if not doc_abstracts:\n",
    "            print(\"Warning: No valid abstracts found in the documents\")\n",
    "            return []\n",
    "            \n",
    "        # Encode all documents\n",
    "        doc_embeddings = model.encode(doc_abstracts, \n",
    "                                     convert_to_tensor=True, \n",
    "                                     show_progress_bar=(len(doc_abstracts) > 10))\n",
    "        \n",
    "        # Calculate similarities\n",
    "        import torch.nn.functional as F\n",
    "        similarities = F.cosine_similarity(query_embedding.unsqueeze(0), doc_embeddings)\n",
    "        \n",
    "        # Sort by similarity\n",
    "        results = []\n",
    "        for i, sim in enumerate(similarities):\n",
    "            # Find the original document that corresponds to this abstract\n",
    "            for j, doc in enumerate(documents):\n",
    "                if doc.get('dc.description.abstract', '') == doc_abstracts[i]:\n",
    "                    results.append((doc, sim.item()))\n",
    "                    break\n",
    "        \n",
    "        # Return top k results\n",
    "        return sorted(results, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieve function: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ceac0",
   "metadata": {},
   "source": [
    "## Testing the Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205719ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load document collection\n",
    "collection_file = data_dir / \"col_1822_21316_processed.json\"\n",
    "\n",
    "try:\n",
    "    with open(collection_file, 'r', encoding='utf-8') as f:\n",
    "        documents = json.load(f)\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading document collection: {e}\")\n",
    "    print(\"Creating empty document list\")\n",
    "    documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5873c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query: 'processamento de linguagem natural em portugus'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 32/32 [00:15<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 results:\n",
      "\n",
      "Document 1 - Similarity: 0.4935\n",
      "Title: entity recognition in archival descriptions\n",
      "Authors: cunha, lus filipe da costa\n",
      "Abstract: ['at the moment, there is a vast amount of archival data spread across the portuguese archives, which keeps information from our ancestors times to the present day. most of this information was already transcribed to digital format, and the public can access it through archives online repositories. despite that, some of these documents are structured with many plain text fields without any annotations, making their content analyses difficult. in this thesis, we implemented several named entity recognition solutions to perform a semantic interpretation of the archival finding aids by extracting named entities like person, place, date, profession, and organization. these entities translate into crucial information about the context in which they are inserted. they can be used for several purposes with high confidence results, such as creating smart browsing tools by using entity linking and record linking techniques. in this way, the main challenge of this work was the creation of powerful ner models capable of producing high confidence results. in order to achieve high result scores, we annotated several corpora to train our machine learning algorithms in the archival domain. we also used different ml architectures such as maxent, cnns, lstms, and bert models. during the models validation, we created different environments to test the effect of the context proximity in the training data. finally, during the models training, we noticed a lack of available portuguese annotated data, limiting the potential of several nlp tasks. in this way, we developed an intelligent corpus annotator that uses one of our ner models to assist and accelerate the annotation process.', 'de momento, existe uma vasta quantidade de dados arquivsticos espalhados pelos arquivos portugueses, que guardam informaes desde os tempos dos nossos antepassados at aos dias de hoje. a maior parte desta informao j foi transcrita para o formato digital e encontra-se disponvel ao pblico atravs de repositrios online dos arquivos. apesar disso, alguns destes documentos esto estruturados com muitos campos de texto livre, sem quaisquer anotaes, o que pode dificultar a anlise do seu contedo. nesta tese, implementamos vrias solues de reconhecimento de entidades mencionadas, a fim de se realizar uma interpretao semntica sobre descries arquivsticas, extraindo entidades tais como pessoa, local, data, profisso e organizao. estes tipos de entidades traduzem-se em informao crucial sobre o contexto em que esto inseridas. com mtricas de confiana suficientemente elevadas, estas entidades podem ser utilizadas para diversos fins, como a criao de ferramentas de navegao inteligente por meio de tcnicas de entity linking e record linking. desta forma, o principal desafio deste trabalho consistiu na criao de poderosos modelos ner que fossem capazes de produzir resultados de elevada confiana. para alcanar tais resultados, anotamos vrios datasets para treinar os nossos prprios algoritmos de aprendizado de mquina no contexto arquivstico. para alm disso, usamos diferentes arquiteturas de ml tais como maxent, cnns, lstms e bert. durante a validao do modelo, criamos diferentes ambientes de teste de modo a testar o efeito da proximidade de contexto nos dados de treino. por fim, durante o treino dos modelos verificamos que existe pouca quantidade de dados disponveis anotados em portugus, o que pode limitar o potencial de vrias tarefas de nlp. desta forma, desenvolvemos um anotador de datasets inteligente que utiliza um dos nossos modelos de ner para auxiliar e acelerar o processo de anotao.']\n",
      "Keywords: ['named entity recognition', 'archival finding aids', 'machine learning', 'deep learning', 'bert', 'data annotation', 'reconhecimento de entidades mencionadas', 'descries arquivsticas', 'anotao de dados']\n",
      "\n",
      "Document 2 - Similarity: 0.4811\n",
      "Title: clav: planos de preservao digital\n",
      "Authors: freitas, jos duarte santos\n",
      "Abstract: ['no podendo deixar de aproveitar os benefcios da era tecnolgica que vivemos a administrao pblica portuguesa caminha a passos largos para a digitalizao de todo o seu processo organizacional. tal se explica atravs de diversos fatores que, apesar de dspares, se complementam e interligam entre si. assim, primeiramente se pensarmos no fator da proteo ambiental, a digitalizao vai permitir uma reduo da utilizao de papel e simultaneamente uma reduo de custos com o mesmo. por outro lado, esta garante uma maior agilizao e otimizao dos processos administrativos, assegurando, ainda, uma maior longevidade aos documentos. de forma a atingir estes objetivos nasceu a plataforma classificao e avaliao da informao pblica (clav), plataforma essa que tem vindo a crescer ao longo dos ltimos anos, que conta com vrios colaboradores do departamento de informtica da universidade do minho, sendo financiado pelo simplex, visando a classificao e avaliao de toda a documentao presente na administrao pblica portuguesa. como referido esta plataforma j est bem madura e como tal j conta com diversas funcionalidades para criao e manuteno dos instrumentos de classificao e avaliao, esta dissertao pretende acrescentar uma nova componente ao clav que permita no s a criar como tambm gerir os planos de preservao digital. para isso, foi necessrio definir um modelo a seguir, tendo em conta todos os seus requisitos e invariantes, e adicionar as interfaces necessrias ao clav, com todas as funcionalidades e mtodos necessrios para a criao, importao e manuteno dos planos de preservao digital.', 'being able to take advantage of the benefits of the technological age that we are experiencing, the portuguese public administration is making great strides towards the digitization of its entire organizational process.this is explained by several factors that, although disparate, complement and interconnect with each other.so, first, if we think about the environmental protection factor, digitization will allow a reduction in the use of paper and simultaneously a cost reduction with the same.on the other hand, it ensures greater speed and optimization of administrative processes, while also ensuring greater longevity to documents.to achieve these objectives, the clav platform was born, a platform that has been growing over the past few years, with several employees from the it department of the university of minho, being financed by simplex, aiming at the classification and evaluation of all the documentation present in the portuguese public administration.as mentioned, this platform is already very mature and as such it already has several functionalities for creating and maintaining the classification and evaluation instruments, this dissertation intends to add a new component to the clav that allows not only to create but also to manage the digital preservation plans.therefore, it was necessary to define a model to follow, bearing in mind all its requirements and invariants, and add the necessary interfaces to the clav, with all the functionalities and methods necessary for the creation, import, and maintenance of the digital preservation plans.']\n",
      "Keywords: ['planos de preservao digital', 'clav', 'digitalizao', 'classificao', 'avaliao', 'digital preservation plans', 'digitization', 'classification', 'evaluation']\n",
      "\n",
      "Document 3 - Similarity: 0.4734\n",
      "Title: applying attribute grammars to teach linguistic rules\n",
      "Authors: sousa, manuel gouveia carneiro de\n",
      "Abstract: ['this document presents the topic applying attribute grammars to teach linguistic rules, at universidade do minho in braga, portugal. this thesis is focused on using the formalisms of attribute grammars in order to create a tool to help linguistic students learn the different rules of a natural language. the system developed, named lyntax, consists in a processor for a domain specific language which intends to enable the user to specify different kinds of sentence structures, and afterwards, test various phrases against said structures. the processor validates and evaluates the input given, generating a grammar which is specific to a previously chosen sentence. lastly, using antlr, a parser is generated for that specific grammar referred above. the processor built by antlr also creates a syntax tree that is presented to the user for analysis purposes. an interface that supports the specification of the language (written in lyntax dsl) was built, also allowing the use of the processor and the generation of the specific grammar, exempting the user from knowing the details of the process. within this document, the focus will be primarly dedicated to the analysis of the system and how each block was built. different examples of the processor in action will be shown and explained.', 'este documento refere-se a uma dissertao sobre o tpico aplicar gramticas de atribu tos no ensino de regras de lingustica, e ser concluda na universidade do minho em braga, portugal. esta dissertao pretende focar-se no uso dos formalismos das gramticas de atributos de maneira a criar uma ferramenta que ajude os alunos de lingustica a aprender as diversas regras da lngua natural. o sistema desenvolvido, denominado de lyntax, consiste em um processor para uma linguagem de domnio especfico cujo objetivo  o de permitir ao seu utilizador a possibili dade de especificar diversas estruturas de frases, e posteriormente, testar frases contra essas mesmas estruturas. o processador valida e avalia o input recebido, gerando uma gramtica especfica  frase previamente escolhida. por fim, usando uma ferramenta como o antlr, um parser  gerado para a gramtica especfica acima referida. o processador construdo pelo antlr tambm gera a rvore de syntax que  apresentada ao utilizador com o intuito de ser analisada. foi tambm criada uma interface que suporta a especificao da linguagem, permitindo tambm o uso do processador e a gerao da gramtica especfica, abstraindo assim o utilizador de quaisquer tipo de clculos. neste documento, o focus primrio ser dedicado  anlise do sistema e como cada bloco foi construdo. diferentes exemplos de uso do processador sero apresentados e explicados.']\n",
      "Keywords: ['linguistic', 'natural language processing', 'attribute grammar', 'lingustica', 'processamento de lngua natural', 'gramticas de atributo']\n",
      "\n",
      "================================================================================\n",
      "Query: 'web performance optimization'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 32/32 [00:15<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 results:\n",
      "\n",
      "Document 1 - Similarity: 0.3971\n",
      "Title: on the performance of webassembly\n",
      "Authors: macedo, joo gonalves de\n",
      "Abstract: ['the worldwide web has dramatically evolved in recent years. web pages are dynamic, expressed by pro grams written in common programming languages given rise to sophisticated web applications. thus, web browsers are almost operating systems, having to interpret/compile such programs and execute them. although javascript is widely used to express dynamic web pages, it has several shortcomings and performance inefficiencies. to overcome such limitations, major it powerhouses are developing a new portable and size/load efficient language: webassembly. in this dissertation, we conduct the first systematic study on the energy and run-time performance of webassembly and javascript on the web. we used micro-benchmarks and real applications to have more realistic results. the results show that webassembly, while still in its infancy, is starting to already outperform javascript, with much more room to grow. a statistical analysis indicates that webassembly produces significant performance differences compared to javascript. however, these differences differ between micro-benchmarks and real-world benchmarks. our results also show that webassembly improved energy efficiency by 30%, on average, and show how different webassembly behaviour is among three popular web browsers: google chrome, microsoft edge, and mozilla firefox. our findings indicate that webassembly is faster than javascript and even more energy-efficient. our benchmarking framework is also available to allow further research and replication.', 'a web evoluiu dramaticamente em todo o mundo nos ltimos anos. as pginas web so dinmicas, expressas por programas escritos em linguagens de programao comuns, dando origem a aplicativos web sofisticados. assim, os navegadores web so quase como sistemas operacionais, tendo que interpre tar/compilar tais programas e execut-los. embora o javascript seja amplamente usado para expressar pginas web dinmicas, ele tem vrias deficincias e ineficincias de desempenho. para superar tais limitaes, as principais potncias de ti esto a desenvolver uma nova linguagem porttil e eficiente em tamanho/carregamento: webassembly. nesta dissertao, conduzimos o primeiro estudo sistemtico sobre o desempenho da energia e do tempo de execuo do webassembly e javascript na web. usamos micro-benchmarks e aplicaes reais para obter resultados mais realistas. os resultados mostram que webassembly, embora ainda esteja na sua infncia, j est comea a superar o javascript, com muito mais espao para crescer. uma anlise estatstica indica que webassembly produz diferenas de desempenho significativas em relao ao javascript. no entanto, essas diferenas diferem entre micro-benchmarks e benchmarks de aplicaes reais. os nossos resultados tambm mostram que o webassembly melhorou a eficincia energtica em 30%, em mdia, e mostram como o comportamento do webassembly  diferente entre trs navegadores web populares: google chrome, microsoft edge e mozilla firefox. as nossas descobertas indicam que o webassembly  mais rpido que o javascript e ainda mais eficiente em termos de energia. a nossa benchmarking framework est disponvel para permitir pesquisas adicionais e replicao.']\n",
      "Keywords: ['energy efficiency', 'green software', 'web browsers', 'webassembly', 'eficincia energtica', 'navegadores web', 'software verde']\n",
      "\n",
      "Document 2 - Similarity: 0.3357\n",
      "Title: query optimizers based on machine learning techniques\n",
      "Authors: souto, rui pedro sousa rodrigues do\n",
      "Abstract: ['query optimizers are considered one of the most relevant and sophisticated components in a database management system. however, despite currently producing nearly optimal results, optimizers rely on statistical estimates and heuristics to reduce the search space of alternative execution plans for a single query. as a result, for more complex queries, errors may grow exponentially, often translating into sub-optimal plans resulting in less than ideal performance. recent advances in machine learning techniques have opened new opportunities for many of the existing problems related to system optimization. this document proposes a solution built on top of postgresql that learns to select the most efficient set of optimizer strategy settings for a particular query. instead of depending entirely on the optimizers estimates to compare different plans under different configurations, it relies on a greedy selection algorithm that supports several types of predictive modeling techniques, from more traditional modeling techniques to a deep learning approach. the system is evaluated experimentally with the standard tpc-h and join order ing benchmark workloads to measure the cost and benefits of adding machine learning capabilities to traditional query optimizers.', 'os otimizadores de queries so considerados um dos componentes de maior relevncia e complexidade num sistema de gesto de bases de dados. no entanto, apesar de atualmente produzirem resultados quase timos, os otimizadores dependem do uso de estimativas estatsticas e de heursticas para reduzir o espao de procura de planos de execuo alternativos para uma determinada query. como resultado, para queries mais complexas, os erros podem crescer exponencialmente, o que geralmente se traduz em planos sub-timos, resultando num desempenho inferior ao ideal. os recentes avanos nas tcnicas de aprendizagem automtica abriram novas oportunidades para muitos dos problemas existentes relacionados com otimizao de sistemas. este documento prope uma soluo construda sobre o postgresql que aprende a selecionar o conjunto mais eficiente de configuraes do otimizador para uma determinada query. em vez de depender inteiramente de estimativas do otimizador para comparar planos de configuraes diferentes, a soluo baseia-se num algoritmo de seleo greedy que suporta vrios tipos de tcnicas de modelagem preditiva, desde tcnicas mais tradicionais a uma abordagem de deep learning. o sistema  avaliado experimentalmente com os workloads tpc-h e join ordering benchmark para medir o custo e os benefcios de adicionar aprendizagem automtica a otimizadores de queries tradicionais.']\n",
      "Keywords: ['database tuning', 'machine learning', 'query optimization', 'aprendizagem automtica', 'otimizao de queries', 'tuning de base de dados']\n",
      "\n",
      "Document 3 - Similarity: 0.3296\n",
      "Title: estudo e implementao de interfaces web em html5\n",
      "Authors: rodrigues, samuel da costa\n",
      "Abstract: ['a web est em constante evoluo. a evoluo tecnolgica fez com que as aplicaes web estivessem cada vez mais presentes no mercado e surgissem novos padres arquiteturais, novos dispositivos e novas experincias de utilizao das aplicaes. tudo isto com o propsito de satisfazer as exigncias que o mercado e os utilizadores finais impem. toda esta evoluo potenciou o aparecimento de uma nova verso do html, o html5, a qual est em constante progresso. no se pode esperar que o html5 esteja totalmente concludo para implementar interfaces com esta tecnologia, podendo assim ser considerado um living standard, porque j existem funcionalidades suficientemente maduras e que inclusivamente j so utilizadas em produtos no mercado. devido ao facto do html ser uma linguagem de marcao, torna-se indispensvel associar o html ao javascript, de modo a poder disponibilizar dinamismo e funcionalidade s pginas web. existiu por isso, tal como o html, uma grande evoluo relativa  linguagem, mais propriamente s frameworks javascript de desenvolvimento web existentes. devido ao progresso destas tecnologias, esta dissertao pretende analisar as vrias funcionalidades disponibilizadas pela tecnologia html5 e as frameworks javascript existentes no mercado. para tal, estas tecnologias sero usadas, de forma experimental, num projeto de desenvolvimento de software na empresa onde decorre este trabalho. os resultados focam-se em identificar as funcionalidades html5 implementadas e apresentar uma comparao entre as frameworks javascript em estudo, segundo um conjunto especfico de critrios.  do interesse da empresa onde este trabalho foi realizado aplicar as funcionalidades html5 e frameworks javascript, de modo a identificar as vantagens e desvantagens de cada tecnologia para, num futuro prximo, as aplicar em aplicaes web. o objetivo deste trabalho  assim medir quantitativa e qualitativamente, segundo os critrios considerados para anlise, o impacto da introduo do html5 e das frameworks javascript em produtos de software, caso estas substituam as que esto hoje em dia em utilizao.', \"the web is continuously evolving. this evolution increased the presence of web applications in the market, created new architectural patterns, new devices and new ways of user experience. all of this with the goal to meet the requirements that the market and the final users require. those developments have caused the creation of a new version of html, html5, which is in constant progress. no one can expect that html5 is a finished specification, to start creating interfaces only with this technology, but it can be considered a standard living because there are features mature enough to be developed in web applications available for the market. due to the fact html is a markup language, it is essential to link html to javascript, in order to provide dynamism and functionality to web pages. for this reason there was a major evolution on the language, more specifically in javascript frameworks existing for web development. the progress of these technologies has led this dissertation to analyze the multiple features provided by html5 technology and the existing javascript frameworks. for such, these technologies will be used, experimentally, in a software development project in the company where this work follows. the results are focused on identifying the features made with html5 and display a comparison of javascript frameworks studied, according to a specific set of criteria. it is the company's interest where this work was carried out, apply the html5 features and javascript frameworks, in order to identify the pros and cons of each technology, to in the near future, develop them in web applications. the goal of this work is to measure quantitatively and qualitatively, according to the criteria considered for analysis, the impact of the introduction of html5 and javascript frameworks in web software products, if they replace those in use today.\"]\n",
      "Keywords: ['html5', 'frameworks javascript mvc']\n",
      "\n",
      "================================================================================\n",
      "Query: 'machine learning applications'\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 32/32 [00:15<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 results:\n",
      "\n",
      "Document 1 - Similarity: 0.5671\n",
      "Title: in-vehicle object detection with yolo algorithm\n",
      "Authors: farinha, joo simes\n",
      "Abstract: ['with the growing computational power that we have at our disposal and the ever-increasing amount of data available the field of machine learning has given rise to deep learning, a subset of machine learning algorithms that have shown extraordinary results in a variety of applications from natural language processing to computer vision. in the field of computer vision, these algorithms have greatly improved the state-of-the-art accuracy in tasks associated with object recognition such as detection. this thesis makes use of one of these algorithms, specifically the yolo algorithm, as a basis in the development of a system capable of detecting objects laying inside a car cockpit. to this end a dataset is collected for the purpose of training the yolo algorithm on this task. a comparative analysis of the detection performance of the yolov2 and yolov3 architectures is performed.several experiments are performed by modifying the yolov3 architecture to attempt to improve its accuracy. specifically tests are performed in regards to network size, and the multiple outputs present in this network. explorative experiments are done in order to test the effect that parallel network might have on detection performance. lastly tests are done to try to find an optimal learning rate and batch size for our dataset on the new architectures.', 'com o crescente poder computacional que temos  nossa disposio e o aumento da quantidade dados a que temos acesso o campo de machine learning deu origem ao deep learning um subconjunto de algoritmos de machine learning que tm demonstrado resultados extraordinrios numa variedade de aplicaes desde processamento de linguagens naturais a viso por computador. no campo de viso por computador estes algoritmos tm levado a enormes progressos na correo de sistemas de deteo de objetos. nesta tese usamos um destes algoritmos, especificament o yolo, como base para desenvolver um sistema capaz de detetar objetos dentro de um carro. dado isto um dataset  recolhido com o propsito de treinar o algoritmo yolo nesta tarefa. uma analise comparativa da correo dos algoritmos yolov2 e yolov3 e realizada. vrias tcnicas relacionadas com a modificao da arquitetura yolov3 so exploradas para otimizar o sistema para o problema especifico de deteo a bordo de veculos. especificamente testes so realizados no contexto de tamanho da rede e dos mltiplos outputs presentes nesta rede. experiencias exploratrias so realizadas de forma a testar o efeito que redes parallelas podem ter na correo dos algoritmos. por fim testes so feitos para tentar encontrar learning rates e batch sizes apropriados para o nosso dataset nas novas arquiteturas.']\n",
      "Keywords: None\n",
      "\n",
      "Document 2 - Similarity: 0.5592\n",
      "Title: fault tolerant decentralized deep neural networks\n",
      "Authors: padro, joo carlos faria\n",
      "Abstract: ['machine learning is trending in computer science, especially deep learning. training algorithms that follow this approach to machine learning routinely deal with vast amounts of data. processing these enormous quantities of data requires complex computation tasks that can take a long time to produce results. distributing computation efforts across multiple machines makes sense in this context, as it allows conclusive results to be available in a shorter time frame. distributing the training of a deep neural network is not a trivial procedure. various architectures have been proposed, following two different paradigms. the most common one follows a centralized approach, where a centralized entity, broadly named parameter server, synchronizes and coordinates the updates generated by a number of workers. the alternative discards the centralized unit, assuming a decentralized architecture. the synchronization between the multiple workers is assured by communication techniques that average gradients between a node and its peers. high-end clusters are the ideal environment to deploy deep learning systems. low latency between nodes assures low idle times for workers, increasing the overall system performance. these setups, however, are expensive and are only available to a limited number of entities. on the other end, there is a continuous growth of edge devices with potentially vast amounts of available computational resources. in this dissertation, we aim to implement a fault tolerant decentralized deep neural net work training framework, capable of handling the high latency and unreliability characteristic of edge networks. to manage communication between nodes, we employ decentralized algorithms capable of estimating parameters globally', 'machine learning, mais especificamente deep learning,  um campo emergente nas cincias da computao. algoritmos de treino aplicados em deep learning lidam muito frequentemente com vastas quantidades de dados. processar estas enormes quantidades de dados requer operaes computacionais complexas que demoram demasiado tempo para produzir resultados. distribuir o esforo computacional por mltiplas mquinas faz todo o sentido neste contexto e permite um aumento significativo de desempenho. distribuir o mtodo de treino de uma rede neuronal no  um processo trivial. vrias arquiteturas tm sido propostas, seguindo dois diferentes paradigmas. o mais comum segue uma abordagem centralizada, onde uma entidade central, normalmente denominada de parameter server, sincroniza e coordena todas as atualizaes produzidas pelos workers. a alternativa passa por descartar a entidade centralizada, assumindo uma arquitetura descentralizada. a sincronizao entre workers  assegurada atravs de estratgias de comunicao descentralizadas. clusters de alta performance so o ambiente ideal para a implementao de sistemas de deep learning. a baixa latncia entre nodos assegura baixos perodos de inatividade nos workers, aumentando assim o rendimento do sistema. estas instalaes, contudo, so muito custosas, estando apenas disponveis para um pequeno nmero de entidades. por outro lado, o nmero de equipamentos nas extremidades da rede, com baixo aproveitamento de poder computacional, continua a crescer, o que torna o seu uso desejvel. nesta dissertao, visamos implementar um ambiente de treino de redes neuronais descentralizado e tolerante a faltas, apto a lidar com alta latncia nas comunicaes e baixa estabilidade nos nodos, caraterstica de redes na extremidade. para coordenar a comunicao entre os nodos, empregamos algoritmos de agregao, capazes de criar uma viso geral de parmetros numa topologia.']\n",
      "Keywords: ['distributed systems', 'machine learning', 'artificial intelligence', 'fault tolerance', 'sistemas distribudos', 'inteligncia artificial', 'tolerncia a faltas']\n",
      "\n",
      "Document 3 - Similarity: 0.5480\n",
      "Title: automation of machine learning models benchmarking\n",
      "Authors: s, joo pedro barros\n",
      "Abstract: [\"na rea de cincia de dados, o machine learning est-se a revelar uma ferramenta essencial para resolver problemas complexos. as empresas esto a investir em equipas de cincia de dados e machine learning para desenvolver modelos que apresentem valor para os clientes. no entanto, estes modelos so uma pequena percentagem de uma pipeline de projetos de machine learning (ml) e, para entregar um produto de ml completo,  necessrio um nmero maior de componentes. devops  uma mentalidade de engenharia e um conjunto de prticas que visa unificar o processo de desenvolvimento e o processo de operaes em um software, mlops  um conceito similar a devops mas aplicado ao desenvolvimento e entrega de solues de ml. o nvel de automatizao das etapas em uma pipeline de ml define a maturidade do processo de ml, que reflete a velocidade de treino de novos modelos com novos dados ou de treino de novos modelos com diferentes implementaes. um sistema de ml  um sistema de software, desenvolvimento e atualizaes contnuas so necessrias para garantir um sistema que escale conforme as necessidades. o principal objetivo desta tese  apoiar a criao de um sistema integrado de ml com uma arquitetura que proporcione a capacidade de ser continuamente operada em um ambiente de produo. um conceito para avaliao de desempenho de algoritmos deve ser elaborado e implementado. o principal obetivo e melhorar e ace'erar o cicio de desenvolvimento de modelos de ml na empresa. para atingir este objetivo surge a necessidade de definir uma arquitetura com especificaes e a implementao de processos automatizadas num pipeline de ml existente, este processo tm como objetivo alcanar uma ferramenta de benchmark de modelos, com capacidade de analisar o desempenho do modelo, um motor de inferncia e um banco de dados para armazenar todas as mtricas computadas. um sistema baseado em ia em desenvolvimento fornece o caso de estudo para desenvolver e validar a arquitetura. os avanos atuais na rea da conduo semiautomtica introduz a necessidade de sistemas de monitoramento que podem localizar e detectar eventos especificas no veculo. os conjuntos de sensores so instalados dentro da cabine para alimentar sistemas inteligentes que visam analisar e sinalizar certos comportamentos que podem impactar a segurana e o conforto dos passageiros..\", 'in the field of data science, ml is proving to be a core feature to solve complex real-world problems. businesses are investing in data science and ml teams to develop ai based models that can deliver business value to their users. however, these models are only a small fraction of an ml project pipeline, and to deliver an end to end ml product, a greater number of components are needed. devops is an engineering mindset and a set of practices that aims to unify the development process and the operation process on software. mlops is a similar concept to devops but applicable to the development and delivery of ml based solutions. the automation of the steps in a ml pipeline defines the maturity of the ml process, reflecting the velocity of training new models given new data or training new models given new implementations. an ml system is a software system that can support development, provide continuous integration and continuous delivery apply to help guarantee that one can reliably build and operate ml systems at scale. the main objective of this thesis are to support the creation of an integrated ml system with an archi tecture that provides the ability to be continuously operated in a production-like environment. furthermore, a concept to evaluate the performance of algorithms shall be devised and implemented. the end goal is to improve and accelerate the ml development lifecycle. to achieve this goal surges the need to define an architecture alongside specifications and the implementation of several automated steps into an existing ml pipeline. to improve and accelerate model development an model engine benchmark tool is devised capable of several features, including the ability to have dashboards for model performance evaluation, an automatic inference engine, performance metrics for the model and a database to store all the computed metrics and metadata. an ai-based system under development provides the case study to develop and validate this architec ture. the current advances of semi-automated driving introduce the need for monitoring systems to scan and detect specific events in the vehicle. sensor clusters are installed inside the vehicle cabin to feed data to intelligent systems that aim to analyze and red flag certain behaviours that can potentially impact passengers safety and comfort while using the vehicle.']\n",
      "Keywords: ['engenharia software', 'aprendizagem mquina', 'cincia dados', 'devops', 'mlops', 'machine learning', 'software', 'data science', 'pipelines', 'automation']\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "queries = [\n",
    "    \"processamento de linguagem natural em portugus\",\n",
    "    \"web performance optimization\",\n",
    "    \"machine learning applications\"\n",
    "]\n",
    "\n",
    "# Test the retrieval function with each query\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Retrieve similar documents\n",
    "        results = retrieve(query, documents, top_k=3)\n",
    "        \n",
    "        # Display results\n",
    "        if results:\n",
    "            print(f\"Top {len(results)} results:\")\n",
    "            for i, (doc, sim) in enumerate(results, 1):\n",
    "                print(f\"\\nDocument {i} - Similarity: {sim:.4f}\")\n",
    "                print(f\"Title: {doc.get('dc.title', 'No title')}\")\n",
    "                print(f\"Authors: {doc.get('dc.contributor.author', 'Unknown')}\")\n",
    "                \n",
    "                abstract = doc.get('dc.description.abstract', 'No abstract')\n",
    "                if len(abstract) > 200:\n",
    "                    print(f\"Abstract: {abstract[:200]}...\")\n",
    "                else:\n",
    "                    print(f\"Abstract: {abstract}\")\n",
    "                    \n",
    "                print(f\"Keywords: {doc.get('dc.subject', 'None')}\")\n",
    "        else:\n",
    "            print(\"No results found for this query.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing retrieval: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
