[
  [
    [
      "the prototyping of ubiquitous environments has never been an easy task. under normal circumstances, this type of system can only be tested during deployment, resulting in complications in the development process. to solve this issue, developers at the department of informatics in the university of minho created the rapid prototyping for user experience (apex) framework, that relies on a 3d application server to create simulations for ubiquitous environments. the goal is to use these simulations as prototypes of the actual environments, allowing early assessment of how users will experience the proposed designs. the focus of this dissertation is in integrating a 3d application server with a cave automatic virtual environment (cave), for a more realistic approach when using the apex framework. 3d application servers provide the means to develop virtual worlds. the cave is a virtual reality (vr) theatre where a user can interact with an immersive environment, through sensors and mobile devices. this dissertation shows a set of solutions to achieve the desired integration, as well as a number of interaction devices that can be used for this purpose.",
      "a prototipagem de ambientes ubíquos nunca foi uma tarefa fácil. por norma, este tipo de sistema só pode ser testado durante o seu uso efetivo, o que acaba por resultar em complicações no processo de desenvolvimento. para resolver este problema, foi criada a framework rapid prototyping for user experience (apex), no departamento de informática da universidade do minho, que utiliza um servidor de aplicações 3d para criar simulações de ambientes ubíquos. o seu objetivo é utilizar estas simulações como protótipos dos ambientes, possibilitando desde cedo uma avaliação da sua experiência de utilização. o objetivo desta dissertação consiste na integração de um servidor de aplicações 3d com uma cave automatic virtual environment (cave), no sentido de se obter simulações mais realistas durante a utilização da framework apex. um servidor de aplicações 3d providencia meios para o desenvolvimento de mundos virtuais, enquanto que uma cave é um sistema de realidade virtual que permite ao utilizador interagir com um ambiente imersivo, por intermédio de sensores e dispositivos móveis. nesta dissertação, apresenta-se um conjunto de soluções para obter este tipo de integração, assim como alguns dispositivos de interação que podem ser utilizados para este propósito"
    ],
    [
      "metabolism acts a machinery by maintaining the functionality of the cell in response to several perturbations, keeping a balance in the levels of crucial metabolites and cell components and producing energy by breaking down certain compounds. a better understanding of these mechanisms cannot be restricted to the knowledge of the function of specific tissues or cell types, it also requires knowledge about their interactions. the human liver has a high number of physiological functions related to the metabolism, such as the production of the bile, hormones and vitamins. the hepatocytes have a major impact in human metabolism, being the most metabolically active cell types in humans. malfunction on the metabolism of this type of cells is related to several diseases, like hepatitis, cirrhosis or non-alcoholic fatty liver disease (nafld), where the last one is considered a manifestation of obesity. a particular pathway has been associated not only with obesity, but also with cancer and type 2 diabetes, the mechanistic tor (mtor) pathway. signalling of this pathway has an effect on most of cellular functions and regulates growth and proliferation. it has been shown that alterations in this pathway can lead to fat accumulation in the liver of obese people. a better understanding of this complex pathway may help researchers to unveil more information on how this pathway works and how it can help in the treatment of several diseases. the increase of high-throughput data, due to the advances in sequencing and other experimental techniques, allowed us to better understand the molecular characteristics of the cell. a useful tool to process all this information are genome-scale metabolic models (gsmms). a gsmm is a list of mass-balanced reactions, which can be related to cellular compartments, like the cytoplasm. given high-throughput data, gsmms can be utilized for the simulation of the metabolism of a certain cell type through a constraintbased modelling framework. there are several algorithms/ tools to create tissue-specific metabolic models (based on a generic human model, such as recon2) including tinit, mba or mcadre. although all these methods still face a number of issues, the generated models can simulate human tissues and can be a good starting point for a better understanding of complex diseases. an important limitation of these models is the fact that they only represent the metabolic layer of the cells, while for models to be able to support accurate simulations, a number of other important sub-systems (e.g. regulation, signalling) should also be taken into account. this models (integrative models) combine the information and material flow of the three previous mentioned sub-systems, delivering a more robust tool with more predictive strength.",
      "o metabolismo actua como uma máquina que mantém a funcionalidade da célula como resposta a várias perturbações, mantendo os níveis de metabolitos cruciais e componentes celulares e produzindo energia através da quebra de determinados compostos. uma melhor compreensão destes mecanismos não se pode restringir ao conhecimento das funções de tecidos ou tipos celulares, também requer um conhecimento sobre as suas interacções. o fígado humano tem um grande número de funções fisiológicas relacionadas com o metabolismo, como a produção de bile, hormonas e vitaminas. os hepatócitos têm um grande impacto no metabolismo humano, sendo as suas células as metabolicamente mais activas. um mau funcionamento do metabolismo destas células está associado com algumas doenças, como hepatite, cirrose ou doença hepática gordurosa não alcoólica, onde esta última se encontra associada a obesidade. uma via metabólica particular tem sido associada não só com a obesidade, mas também com cancro e diabetes tipo 2, a via metabólica mecânica tor (mtor). a sinalização desta via tem efeito na maior parte das funções celulares e regula o crescimento e proliferação. foi demonstrado que alterações nesta via pode levar a acumulação de gordura em pessoas obesas. uma melhor compreensão desta via complexa pode ajudar os investigadores para revelar mais informação sobre como esta via funciona e como pode ajudar no tratamento de diversas doenças. o aumento de dados provenientes de alto débito, devido aos avanços na sequenciação e outras técnicas experimentais, permitiram-nos ter um melhor conhecimento sobre as características moleculares da célula. uma ferramenta útil para processar toda esta informação são os modelos metabólicos à escala genómica (mmeg). um mmeg é uma lista de reacções balanceadas pela massa, que pode ser relacionada com compartimentos celulares, como o citoplasma. dados os dados de alto rendimento, mmeg podem ser utilizados para a simulação do metabolismo de um certo tipo celular através de modelação baseados em restrições. existem vários algoritmos/ferramentas para criar modelos metabólicos específicos para um tecido (baseado em modelos metabólicos humanos, como o recon2) incluindo o tinit, mba ou mcadre. apesar de todos estes métodos ainda apresentarem algumas limitações, os modelos gerados pode simular tecidos humanos e ser um bom ponto de partida para uma melhor compreensão de doenças complexas. uma limitação importante destes modelos é o facto de apenas representarem a camada metabólica da célula, enquanto para os modelos serem capazes de suportar simulações precisas, outros sub-sistemas (ex: regulação, sinalização) devem ser também tidos em consideração. estes modelos (modelos integrados) combinam a informação e fluxo de material dos três sistemas previamente descritos, fornecendo assim uma ferramenta robusta com maior poder preditivo."
    ],
    0.0
  ],
  [
    [
      "one of the main factors for achieving school success is related to the level of attention and interest that students manifest in the classroom. when in the classroom students perform tasks on electronic devices, and if the classes have a high number of students, the teacher does not have the idea if a student is really attentive and focused on the tasks defined (pimenta et al. (2015)). usually the teacher only realizes this situation when he evaluates the students, which is in most of cases too late. however, if the teacher receives information about the attention and interest of each user (student) of a class in real time, he/she can adopt a set of teaching strategies in order to maximize the results of his/her students. hence it is possible to avoid and prevent some negative behaviors in the classroom and increase the level of attention and consequently, school success. in terms of school success, it is common sense that a high level of attention, allows to acquire better results. this thesis intends to develop a framework which allows the teacher to visualize, in real time, the attention level of each student, allowing him to adopt strategies for the students with abnormal behaviors (carneiro et al. (2015); duraes et al. (2016b)).",
      "um dos principais fatores para a obtenção do sucesso escolar está relacionado com o nível de atenção e de interesse que o aluno manifesta em sala de aula. quando os alunos, na sala de aula, realizam tarefas em dispositivos eletrónicos, e se as turmas tiverem um elevado número de alunos, o professor não tem a verdadeira noção dos alunos que realmente estão atentos e focados nas tarefas definidas (pimenta et al. (2015). muitas vezes o professor só se apercebe desta situação aquando da realização da avaliação, o que pode ser, em muitos casos, demasiado tarde. se o professor receber informações sobre os níveis de atenção e de interesse de cada utilizador (aluno) de uma turma em tempo real, poder´a adotar um conjunto de estratégias de ensino por forma a maximizar os resultados dos seus alunos. desta forma é possível evitar e prevenir alguns comportamentos negativos em sala de aula e aumentar o nível de atenção e, consequentemente, o sucesso escolar. em termos de sucesso escolar, é do senso comum que um elevado nível de atenção e interesse, permite obter melhores resultados. este projeto pretende desenvolver uma ferramenta que permite ao professor visualizar, em tempo-real, o nível de atenção de cada aluno, permitindo-lhe adotar estratégias para os alunos com comportamentos desviantes (carneiro et al. (2015); durães et al. (2016b))."
    ],
    [
      "in this work, the optimization of two atmospheric pressure chemical vapor deposition systems was carried out in order to grow two different bidimensional materials, namely hbn and mose2. the first is an insulator with a structure similar to graphene and it is seen as an optimal candidate for several applications, in particular photonics and optoelectronics. the latter is a prominent semiconductor belonging to the family of two-dimensional transition metal dichalcogenides which demonstrated outstanding optoelectronic properties, such as thickness-dependent photoluminescence, combined with lightweight and flexibility. several deposition parameters were investigated in parallel with an extensive characterization methodology carried out by optical microscopy, raman spectroscopy, atomic-force microscopy, scanning electron microscopy, x-ray photoelectron spectroscopy, energy dispersive x-ray spectroscopy and transmission electron microscopy. as a final result, reliable experimental procedures have been established which lead to the growth of few-layer polycrystalline hbn films (up to 20cm2 ) and μm-sized single crystals of monolayer mose2. selected samples were tested in experimental devices. the fluorescent properties of 2d hbn films were probed to quantify its performance as a single-photon emission source at room temperature. a sensing device based on 2d mose2 was assembled to investigate the optical response of the material to various degree of tensile strain.",
      "neste trabalho, a otimização de dois sistemas de deposição química de vapores, operados à pressão atmosférica, foi levada a cabo a fim de depositar dois materiais bidimensionais diferentes: hbn e mose2. o primeiro é um isolante com uma estrutura semelhante ao grafeno e é visto como um ótimo candidato para várias aplicações, em particular na área da fotónica e optoeletrónica. por sua vez, o segundo é um semicondutor proeminente pertencente à família dos dicalcogenetos de metais de transição bidimensionais que demonstrou excelentes propriedades optoeletrónicas, em particular a dependência da sua fotoluminescência em função do número de camadas, combinadas com a sua leveza e flexibilidade. vários parâmetros de deposição foram investigados em paralelo com uma extensa caracterização realizada através de microscopia ótica, espectroscopia raman, microscopia de força atômica, microscopia eletrónica de varredura, espectroscopia de fotoeletrões de raios-x, espectroscopia de energia dispersiva de raios-x e microscopia eletrónica de transmissão. no final, obtiveram-se dois procedimentos experimentais confiáveis que levam ao crescimento de filmes policristalinos de hbn compostos por poucas camadas (até 20cm2 ) e cristais monoatómicos de mose2 na ordem dos micrómetros. as amostras selecionadas foram testadas em dispositivos experimentais. as propriedades fluorescentes dos filmes de hbn foram medidas de modo quantificar seu desempenho como fonte de emissão de fotão único à temperatura ambiente. um dispositivo de deteção baseado em 2d mose2 foi montado de modo investigar a resposta ótica do material a vários graus de tensão."
    ],
    0.06666666666666667
  ],
  [
    [
      "nos dias de hoje torna-se essencial garantir a disponibilidade de toda a informação que circula em unidades na área da saúde, com a finalidade de auxiliar os profissionais de assistência médica. por conseguinte, não deve existir qualquer tipo de barreira inerente à comunicação entre os sistemas de informação, mesmo que estes sejam distintos quanto à sua implementação. é neste contexto que surge a interoperabilidade, isto é, a capacidade de diferentes sistemas estabelecerem a comunicação e troca de dados entre si, de forma eficaz, sem esforço adicional por parte do utilizador. este conceito é promovido pela utilização de padrões específicos para troca de informação no domínio da saúde, como por exemplo o health level 7 (hl7), e de sistemas de software, que proporcionam a normalização e partilha dos dados entre todos os sistemas, designados plataformas de integração. tendo em conta o elevado volume de mensagens a que este tipo de ferramentas poderá estar exposto, é indispensável que estejam operacionais 24 horas por dia, sem que exista qualquer tipo de sobrecarga de mensagens que comprometam o seu desempenho. surge assim o objetivo principal da presente dissertação, a aquisição de métricas relevantes que proporcionam a performance ideal dos motores de integração, promovendo um fluxo de mensagens contínuo sem qualquer tipo de quebra. o software utilizado como objeto de estudo designa-se mirth colmai (também denominado nextgen connect). embora este tipo de sistemas de integração seja amplamente utilizado nas unidades de saúde, existe pouco trabalho de investigação inerente à avaliação do desempenho de ferramentas de interoperabilidade neste domínio específico. tendo em consideração que a comunicação entre sistemas utilizada pela maioria deste tipo de plataformas é suportada por soluções message orientai middleware (mom), foram utilizados indicadores relacionados com abordagens de filas de mensagens e com boas práticas de processamento de fluxo de dados. deste modo, para além de se promover os casos de análise e avaliação do comportamento da plataforma conforme métricas consideradas, também permite aos utilizadores deste tipo de ferramentas de software deterem uma noção das circunstãncias onde retiram o melhor aproveitamento do seu funcionamento.",
      "nowadays it is essential to ensure the availability of all the information that circulates in health units, in order to help health professionals. therefore, there should be no inherent bather to communication between information systems, even if they are different in terms of their implementation. it's in this context that interoperability arises, the ability of different systems to establish communication and exchange of data with each other effectively, without additional effort on the part of the user. this concept is promoted by the use of specific standards for the exchange of information in the field of health, such as health level 7 (hl7), and software systems, which provide the standardization and sharing of data between all systems, called integration platforms. given the high volume of messages to which this type of tool may be exposed, it is essential that they are operational 24 hours a day, without any type of message overload that compromises their performance. thus emerges the main objective of this dissertation, the acquisition of relevant metrics that provide the optimal performance of the integration engines, promoting a continuous message flow without any kind of breakage. the software used as the object of study is called mirth connect (also called nextgen connect). although this type of integration systems is widely used in health units, there is low research work inherent in evaluating the performance of interoperability tools in this specific area. taking into account that the communication between systems used by most of this type of platforms is supported by message oriented middleroare (mom) solutions, indicators related to message queue approaches and good data flow processing practices were used. thus, in addition to promoting the cases of analysis and evaluation of the behavior of the platform according to the metrics considered, it also allows users of this type of software tools to have a notion of the circumstances where they take the best advantage of its operation."
    ],
    [
      "o processo de implementação de algoritmos criptográficos exige uma grande preocupação com a eficiên cia do código desenvolvida, mas sem descurar os aspetos de correção e segurança. uma implementação que apresente bom desempenho, mas não dê garantias totais da sua correção pode ser vulnerável a ataques, enquanto que uma outra implementação que garanta a segurança a custo da eficiência pode comprometer a usabilidade ou até ser descartada em contextos com recursos computacionais limitados. a framework jasmin pretende conciliar essa dicotomia, possibilitando desenvolver software criptográfico de elevado desempenho e altos níveis de confiabilidade, através da combinação de construções de alto e baixo nível. no projeto apresentado nesta dissertação recorre-se à linguagem jasmin para codificar dois algor timos criptográficos largamente utilizados: o algoritmo de cifragem autenticada com dados associados aes-gcm e o algoritmo de assinatura digital baseado na curva elíptica p-384. ao longo do mesmo tam bém são implementadas outras primitivas criptográficas, como o algoritmo de cifra de bloco aes-ctr e a função de hash sha-384, sempre tendo em consideração os conceitos de segurança inerentes ao desenvolvimento de software criptográfico, tais como a política constant-time.",
      "the process of implementing cryptographic algorithms requires great concern with the efficiency of the developed code, but without neglecting correctness and security aspects. an implementation that presents good performance results, but does not provide full guarantees of its correctness, may be vulnerable to attacks, while another implementation that guarantees security at the cost of efficiency may compromise usability or even be discarded in contexts with limited computational resources. the jasmin framework aims to reconcile this dichotomy, making it possible to develop high-performance cryptographic software with great levels of reliability, through the combination of high-level and low-level constructs. in the project presented in this dissertation, the jasmin language is used to develop two widely used cryptographic algorithms: the authenticated encryption with associated data algorithm, aes-gcm, and the p-384 elliptic curve digital signature algorithm. other cryptographic primitives are also implementes throughout, such as the block cipher counter mode of operation (aes-ctr) and the sha-384 hash function, always taking into account the security concepts inherent to the development of cryptographic software, such as the constant-time property."
    ],
    0.3
  ],
  [
    [
      "computer networks security is becoming an important and challenging topic. in particular, one currently witnesses increasingly complex attacks which are also bound to become more and more sophisticated with the advent of artificial intelligence technologies. intrusion detection systems are a crucial component in network security. however, the limited number of publicly available network datasets and their poor traffic variety and attack diversity are a major stumbling block in the proper development of these systems. in order to overcome such difficulties and therefore maximise the detection of anomalies in the network, it is proposed the use of adversarial deep learning techniques to increase the amount and variety of existing data and, simultaneously, to improve the learning ability of the classification models used for anomaly detection. this master’s dissertation main goal is the development of a system that proves capable of improving the detection of anomalies in the network through the use of adversarial deep learning techniques, in particular, generative adversarial networks. with this in mind, firstly, a state-of-the-art analysis and a review of existing solutions were addressed. subsequently, efforts were made to build a modular solution to learn from imbalanced datasets with applications not only in the field of anomaly detection in the network, but also in all areas affected by imbalanced data problems. finally, it was demonstrated the feasibility of the developed system with its application to a network flow dataset.",
      "a segurança das redes de computadores tem-se vindo a tornar num tópico importante e desafiador. em particular, atualmente testemunham-se ataques cada vez mais complexos que, com o advento das tecnologias de inteligência artificial, tendem a tornar-se cada vez mais sofisticados. sistemas de deteção de intrusão são uma peça chave na segurança de redes de computadores. no entanto, o número limitado de dados públicos de fluxo de rede e a sua pobre diversidade e variedade de ataques revelam-se num grande obstáculo para o correto desenvolvimento destes sistemas. de forma a ultrapassar tais adversidades e consequentemente melhorar a deteção de anomalias na rede, é proposto que sejam utilizadas técnicas de adversarial deep learning para aumentar o número e variedade de dados existentes e, simultaneamente, melhorar a capacidade de aprendizagem dos modelos de classificação utilizados na deteção de anomalias. o objetivo principal desta dissertação de mestrado é o desenvolvimento de um sistema que se prove capaz de melhorar a deteção de anomalias na rede através de técnicas de adversarial deep learning, em particular, através do uso de generative adversarial networks. neste sentido, primeiramente, procedeu-se à análise do estado de arte assim como à investigação de soluções existentes. posteriormente, atuou-se de forma a desenvolver uma solução modular com aplicação não só na área de deteção de anomalias na rede, mas também em todas as áreas afetadas pelo problema de dados desbalanceados. por fim, demonstrou-se a viabilidade do sistema desenvolvido com a sua aplicação a um conjunto de dados de fluxo de rede."
    ],
    [
      "hoje em dia, a sociedade ainda possui uma grande tendência para um consumismo desleixado e irracional, especialmente nas nossas casas, levando assim a consumos desnecessários de energia, e consequentemente, ao dano do nossos sistema ambiente. é pretendido, com este trabalho, desenvolver um sistema inteligente de gestão de consumos energéticos e baseado na aprendizagem das acções das pessoas que interagem com um determinado espaço. este sistema irá actuar sobre uma rede de sensores, de onde é extraída toda a informação necessária para a aprendizaegm e gestão dos recursos. com isto, é possível saber, de uma forma aproximada, que ações é que os utilizadores tomam, como por exemplo, a que horas do dia é que um dado utilizador entra num dado espaço. através dos sensores ambientais, como por exemplo, um sensor de luminosidade, poderemos também determinar quais as condições necessárias para saber também quando é que um dado utilizador vai desligar ou ligar um interruptor ou até ligar um ar condicionado. desta maneira, é possível conseguir adaptar o espaço que está a ser monitorizado, de modo a ser possível obter o melhor consumo energético e com isso tornar esse mesmo espaço um ambiente sustentável. esta solução deverá também informar o utilizador sobre que electrodomésticos (ou dispositivos que estão a ser monitorizados) são os mais consumidores (também deverá ter a percepção de qual é o que gasta menos) de forma a este poder regular o seu consumo numa fase inicial e posteriormente permitir ao sistema aprender.",
      "nowadays, society stills possesses a great tendency to a slouch and irrational consumerism, specially at homes, leading the people to unnecessary power consumes and consequently, harm our natural environment and even leading also to expensive power bills. it is intended, with this work, to develop an energetic resources management intelligent system, based on people behaviours that interact with a given physical space. this system must work alongside with a sensors network, from where important information can be collected, in order so that the system can learn. it is through this network that we can collect the information to be learnt (human behaviours and environmental values). this way, we can know, in an approximated way, which user’s actions have more influence in the electric consume and in the natural environment. through environmental sensors, we can, for instance, to know the hour of the day which a given user have more tendency to spend more electric power by using an air conditioner appliance. this way, it is possible to adapt and evolve the system to better fit the power consume and turn the physical space a sustainable environment. this solution should not only warn the user about which electric appliances (or monitored devices) are the most power over-consuming but also warn him which is the less-consuming one, allowing the user to better regulate their consumes in an initial phase and let the system learn."
    ],
    0.0
  ],
  [
    [
      "object detection in computer vision plays a pivotal role in applications such as autonomous vehicles, surveillance systems and medical imaging. however, a consistent challenge faced by the field is the scarcity of labeled data required for training robust object detection models. the manual annotation process for objects within images is labor-intensive and expensive, constraining access to large-scale annotated datasets, which in turn hinders progress in object detection research and applications. this work aims to respond to the pressing issue of data scarcity by exploring a series of automated learning mechanisms, harnessing pre-existing image classification datasets to craft custom-labeled datasets tailored for object detection. in its foundation, is the streamlining of the transformation of image classification datasets into accurately annotated object detection datasets by generating bounding box annotations and object labels. this data synthesis strategy serves the subsequent stages of model training, where object detection models are trained on the newly generated datasets without the need for labor-intensive manual annotations. the significance of this work resides in its potential to reduce the time and financial costs associated with manual labeling. this cost-effectiveness holds immense importance for organizations reliant on the practical application of object detection, particular so in more niche areas, potentially breaking down entry barriers for smaller enterprises. the validity of the methodology is put to the test within the domain of fruit detection, where annotated data notably sparse. performance is assessed using common metrics, including mean average precision, precision, and recall, with pre-existing annotations from the deepfruits dataset. in this work, we start by exploring a fully autonomous semi-supervised pipeline-based workflow which allowed to replicate the labeling of 45% of a subset of deepfruits. this strategy’s performance, which provided an object detection precision of approximately 69%, was then improved by introducing a manual filtering step for the removal of false positive detections, thus necessitating some amount of human interaction. however this, in turn, increased the new pipeline’s precision to 88%, allowing it to correctly recognize 60% of deepfruits’ objects. finally, we explored the possibility of converting this manual filtering stage to an automatic verification layer supported by an underlying convolutional neural network that partitions the detected objects into desirable or undesirable objects. here, we evaluated how the application of such image classifier would have improved the previous pipeline, which necessitated manual filtering. applying it would have allowed for the removal of 81% of all accumulated false positives.",
      "a deteção de objetos em visão por computador desempenha um papel fundamental em aplicações como veículos autónomos, sistemas de vigilância e imagem médica. no entanto, a escassez de dados anotados/etiquetados necessários para treinar modelos robustos de deteção de objetos é um desafio consistentemente enfrentado nesta área. o processo de anotação manual de objetos em imagens é demorado e dispendioso, restringindo o acesso a conjuntos de dados anotados em larga escala, o que, por sua vez, dificulta o progresso na pesquisa e aplicação na área deteção de objetos. este trabalho tem como objetivo responder à questão premente da escassez de dados, explorando uma série de mecanismos de automatizado automática, aproveitando conjuntos de dados de classificação de imagens pré-existentes para criar conjuntos de dados rotulados personalizados para deteção de objetos. na sua base, está a simplificação da transformação de conjuntos de dados de classificação de imagens em conjuntos de dados de deteção de objetos devidamente anotados, gerando anotações de bounding boxes e labels de objetos. essa estratégia de geração de dados serve as etapas subsequentes de treino de modelos, nos quais os modelos de deteção de objetos são treinados nos conjuntos de dados recém-gerados sem a necessidade de anotações manuais trabalhosas. o contributo deste trabalho reside no seu potencial de reduzir os custos de tempo e financeiros associados à anotação manual. esta eficiência de custos é de grande importância para organizações que dependem da aplicação prática de deteção de objetos, especialmente nos domínios mais específicos, potenciando a eliminação de barreiras de entrada para pequenas empresas. a validade desta metodologia é testada no domínio da deteção de frutas, onde os datasets são particularmente escassos. o desempenho é avaliado usando métricas comuns, incluindo a mean average precision, precision e recall através de anotações pré-existentes do dataset deepfruits. este trabalho começa por explorar um workflow totalmente autónomo semi-supervisionado que permitiu replicar as anotações de 45% de um subconjunto do deepfruits. o desempenho desta estratégia, que permitiu obter uma precisão de aproximadamente 69%, foi depois melhorado introduzindo uma etapa de filtragem manual para a remoção de falsos positivos, exigindo para isso alguma forma de interação humana. isto, por sua vez, aumentou a precisão do novo workflow para 88%, permitindo o reconhecimento correto de 60% dos objetos do deepfruits. finalmente, exploramos a possibilidade de converter esta etapa de filtragem manual numa camada de verificação automática apoiada por uma rede neural convolucional subjacente que categoriza os objetos detetados como sendo desejáveis ou indesejáveis. aqui, avaliamos o impacto que a aplicação de tal classificador de imagens teria ao ser aplicado ao workflow anterior. esta aplicação teria permitido a remoção de 81% de todos os falsos positivos acumulados."
    ],
    [
      "o cancro da mama é o tipo de cancro mais comum e a principal causa de morte por cancro nas mulheres. a deteção atempada é crucial para o sucesso do tratamento e para a redução da taxa de mortalidade. embora várias modalidades de imagem sejam utilizadas para detetar lesões mamárias, a ultrassonografia tornou-se uma das mais frequentes, uma vez que é segura, portátil, de baixo custo e permite uma examinação em tempo real. deste modo, a segmentação e classificação automáticas de tumores em imagens de ultrassom da mama podem auxiliar no seu diagnóstico, proporcionando uma segunda opinião aos especialistas. no entanto, realizar uma segmentação e classificação de lesões com precisão, através desta modalidade de imagem, é desafiante devido à má qualidade de imagem e elevada variabilidade das lesões. recentemente, algoritmos de deep learning têm demonstrado grande potencial na área do processamento de imagem, nomeadamente para a segmentação e classificação em imagens de ultrassom da mama. contudo, ainda há necessidade de investigação e melhoria para ser possível aplicar com confiança estas abordagens na prática clínica. adicionalmente, o ultrassom pode ser utilizado para orientar a agulha durante a biópsia mamária, um procedimento que exige elevado rigor. o projecto onconavigator, no âmbito do qual esta dissertação foi realizada, visa combinar imagem médica em tempo real com um robô médico colaborativo para melhorar a precisão do rastreio do cancro da mama e na biópsia mamária guiada por ultrassom. considerando a necessidade de métodos automáticos para a segmentação e classificação para melhorar a prática clínica e de modo a aplicar inteligência artificial no robô médico colaborativo, nesta dissertação, foi desenvolvida uma nova multi-task learning network para a segmentação e classificação simultânea de tumores em imagens de ultrassom de mama. o método proposto foi avaliado em 810 imagens de ultrassom de dois conjuntos de dados, o busi e o udiat, tendo obtido um dice de 80.72%, na seg mentação, e area under the curve de 94.34%, na classificação. em suma, o método demonstrou ser bem sucedido no delineamento e categorização de lesões e revelou potencial para ser incorporado num robô médico colaborativo para intervenções de cancro da mama e para auxiliar no seu diagnóstico.",
      "breast cancer is the most common type of cancer and the leading cause of cancer death in women. early detection is crucial for successful treatment and for reducing the mortality rate. although several imaging modalities are used to detect breast lesions, ultrasound has become one of the most frequent as it is safe, portable, cost-effective and allows real-time examination. thus, automatic tumor segmentation and classification in breast ultrasound images may aid their diagnosis, providing a second opinion to the expertise. however, accurate segmentation and classification of lesions using this imaging modality is challenging due to the poor image quality and high lesion variability. recently deep learning algorithms have shown great potential in the image processing field, namely for breast ultrasound segmentation and classification. nevertheless, there is still a need for research and improvement to confidently apply these approaches in clinical practice. additionally, ultrasound can be used for needle guidance during breast biopsy, a procedure that requires high accuracy. the onconavigator project, under which this dissertation was conducted, aims to combine real-time medical imaging with a collaborative medical robot to improve the precision of breast cancer screening and ultrasound-guided breast biopsy. considering the need for automatic methods for segmentation and classification to improve clinical practice and to apply artificial intelligence to the collaborative medical robot, in this dissertation, a novel multi-task learning network for simultaneous tumor segmentation and classification in breast ultrasound images was developed. the proposed method was evaluated on 810 ultrasound images from two datasets, busi and udiat, and obtained a dice of 80.72% in segmentation and area under the curve of 94.34% in classification. in summary, the method proved successful in delineating and categorizing lesions and revealed potential to be incorporated into a collaborative medical robot for breast cancer interventions and to assist in its diagnosis."
    ],
    0.06666666666666667
  ],
  [
    [
      "in traffic environments, road signs have a key role to control, warn, and command or prohibit the driver of certain actions. traffic sign maintenance is essential to prevent negative events. in order for these traffic signs to play the role they were designed for, periodic onsite inspections are essential and followed out to determine if signs are in good condition and visible, both during the day and night. however, periodic inspections are time and cost consuming. another issue is related to the drivers’ awareness to the traffic signs on the road. many factors, both internal and external to the driver, may potentially contribute to him missing a sign. given the purpose of this dissertation, we will focus primarily on the external factors such as the sign being damaged or occluded, or distractions caused by the many gadgets inside the vehicle. due to all these extraneous influences, a traffic sign recognition system may help the driver to respect these signs and increase significantly their safety, as well as the others around them. some high-end vehicles already have such a warning system, at least for danger signs. however, drivers with these vehicles represent a small fraction of the total driving force. this dissertation aims at bringing such a system to a much broader audience. smartphones are one of the most used devices by society today, mostly due to the many functionalities they provide in day to day life and their relative accessible monetary value. the increased computational power and cameras’ quality improvement of these devices over the years make them good candidates to support the access to this kind of technology to all. in other words, smartphones of this day and age have the necessary resources to be used as instruments for sign recognition. hence, we propose a dual purpose community based approach. on the one hand, each driver can use his mobile device to detect, recognize and geolocate traffic signs, contributing to the traffic sign central repository. detection is performed using cascade classifiers, while a convolutional neural network supports the recognition phase. the repository, based on the information received from the clients, can be used to provide sign status reports and to enable more direct and timely inspection instead of relying on prescheduled global inspections. on the other hand, drivers would have access to the database of traffic signs, therefore being able to receive real-time notifications regarding traffic signs such as speed limit signs, school proximity, or road construction signs. hence, allowing the system to perform its function even if the recognition phase is not active when used in a low computational power device.",
      "em ambientes rodoviários, os sinais de trânsito têm um papel fulcral para controlar, avisar e ordenar ou proibir o condutor de realizar certas ações. é essencial a manutenção dos sinais de trânsito para prevenir acontecimentos negativos. para que os sinais de trânsit desempenhem a sua função, as inspeções rodoviárias periódicas são essenciais para determinar se os sinais estão em bom estado e visíveis, quer de dia, quer de noite. no entanto, as inspeções são bastante dispendiosas. outro problema está associado à consciência dos condutores em relação aos sinais de trânsito nas estradas. muitos fatores, quer sejam eles internos ou externos, podem contribuir para um condutor não reparar num sinal, tal como a obstrução ou danificação do mesmo ou mesmo distração causado pelos gadgets dentro do veículo. devido a todas estes influências, um sistema de reconhecimento de sinais de trânsito pode ajudar o condutor a respeitar estes sinais e aumentar significativamente a sua segurança e dos restantes em seu redor. alguns veículos de alta gama já possuem este tipo de sistemas de alerta, pelo menos para sinais de trânsito. porém, esses veículos representam uma pequena fração da força motriz total. esta dissertação visa levar esse sistema a um público muito mais amplo. os smartphones são um dos dispositivos mais usados pela sociedade nos dias de hoje, muito devido às funcionalidades que disponibilizam no dia-a-dia e seu valor monetário relativamente acessível. o aumento do poder computacional e melhoramento da qualidade da câmara destes dispositivos ao longo dos anos, fazem destes bons candidatos para suportar o acesso deste tipo de tecnologia a todos. por outras palavras, os smartphones deste de hoje possuem os recursos necessários para serem usados como instrumentos para o reconhecimento de sinais. portanto, propomos uma abordagem baseada na comunidade com um duplo propósito. por um lado, cada condutor pode usar o seu dispositivo móvel para detetar, reconhecer e georeferenciar sinais de trânsito, contribuindo para um repositório centralizado. a deteção é realizada através de cascade classifiers, enquanto que uma rede neuronal convolucional trata da fase de reconhecimento. o repositório, baseado na informação recebida por parte dos clientes pode ser usada para fornecer relatórios acerca do estado dos sinais, de modo a possibilitar inspeções mais diretas e atempadas em vez de inspeções globais pré-agendadas. por outro lado, os condutores teriam acesso a uma base de dados de sinais de trânsito e portanto, permitindo criar notificações em tempo real sobre sinais de trânsito tais como sinais de limite de velocidade, proximidade de escolas ou sinais de construção de estradas."
    ],
    [
      "in today's world, data is a ubiquitous concept, and the healthcare ecosystem is no exception, as healthcare organisations are inundated with data that is key to the quality of care. in recent years, the digitalisation of healthcare has changed the way this complex system operates, and it is possible to see the potential for new innovative solutions that apply new insights from big data, data science and artificial intelligence to revolutionise healthcare as a whole. the implementation of digital solutions in healthcare can provide new insights from data and be an extremely helpful tool to improve care and population health, leading to increased clinical efficiency and effectiveness, improved cost and resource containment for the healthcare system, and consequently improved professional satisfaction and patient experience. thus, data science can be a powerful and impactful tool in the healthcare ecosystem, as efforts need to be made in data exploration and visualisation solutions that are appropriate for healthcare professionals to facilitate their decision-making processes. the main objective of this thesis is to use data modelling methods to create a user-friendly data visualisation dashboard that is suitable for specific end users, in this case doctors, to assist them in task and time management and thus healthcare decision making. it will also highlight the potential positive impact of data science in healthcare and the importance of developing data visualisation solutions that are appropriate for healthcare professionals and able to combine the data and information they need on a single viewing platform.",
      "no mundo de hoje, os dados são um conceito omnipresente e o ecossistema de cuidados de saúde não é exceção, uma vez que as organizações de cuidados de saúde são inundadas com dados que são fundamentais para a qualidade dos cuidados. nos últimos anos, a digitalização dos cuidados de saúde alterou a forma corno este sistema complexo funciona e é possível ver o potencial de novas soluções inovadoras que aplicam novos conhecimentos de big data, ciência dos dados e inteligência artificial para revolucionar os cuidados de saúde como um todo. a implementação de soluções digitais nos cuidados de saúde pode fornecer novos conhecimentos a partir dos dados e ser uma ferramenta extremamente útil para melhorar os cuidados e a saúde da população, conduzindo a uma maior eficiência e eficácia clínica, a uma melhor contenção de custos e recursos para o sistema de saúde e, consequentemente, a uma maior satisfação profissional e experiência do doente. assim, a ciência dos dados pode ser urna ferramenta poderosa e impactante no ecossistema de saúde, sendo necessário desenvolver esforços em soluções de exploração e visualização de dados que sejam adequadas aos profissionais de saúde para facilitar os seus processos de tomada de decisão. o principal objetivo desta dissertação é, usando métodos de modelação de dados, criar um dashboard de visualização de dados de fácil utilização, adequado a utilizadores finais específicos, neste caso médicos, para os ajudar na gestão de tarefas e do tempo e, por conseguinte, na tornada de decisões no domínio dos cuidados de saúde. além disso, será demonstrado o potencial impacto positivo da ciência dos dados nos cuidados de saúde e a importância de desenvolver soluções de visualização de dados que sejam adequadas aos profissionais de saúde e capazes de combinar os dados que necessitam numa única plataforma de visualização."
    ],
    0.06666666666666667
  ],
  [
    [
      "nowadays, there is a massive growth in energy consumption in the it sector, which is leaving a huge footprint in terms of energy consumption despite its benefits. with this, the topic of energy consumption and how to improve it has become one of the most talked-about topics today. several developments have been made to find the most efficient solutions to the various problems that users and developers encounter. but this is far from being an easy task for both, as there is still very little information available, or sometimes the solutions don’t meet the needs of each one. with this in mind, this dissertation aims to verify which browser is more efficient in the android environment since there is not much information in this area. for this, we selected seven browsers and ran four test scenarios in order to force the browsers. to test, we recorded a script for each browser in each scenario, trying to mimic the use of a regular user. the reran tool was used to record and repeat each script five times, and the trepn tool was used to monitor it. the results obtained allowed us to conclude which browser was more efficient among the seven selected.",
      "atualmente, existe um grande crescimento do consumo energetico do sector de it, que apesar dos seus benefícios, está a deixar uma enorme pegada no que diz respeito ao consumo energetico. com isto, o tópico do consumo energético e como melhorar começou ser um dos mais falados atualmente. diversos desenvolvimentos foram feitos neste âmbito de maneira a encontrar as soluções mais eficientes para os diversos problemas que os utilizadores e os programadores encontram. mas isto está longe de ser uma tarefa fácil tanto para um como para o outro, sendo que ainda existe muita pouca informação disponível ou por vezes as soluções não vão de encontro às necessidades de cada um. com isto em mente, esta dissertação tem como objetivo verificar qual o browser é mais eficiente no ambiente android, visto que não existe muita informação nesta área. para isto, nós selecionamos sete browsers e fizemos quatro cenários de teste, de maneira a forçar os browsers. de modo a conseguir testar, gravamos um script para cada browser em cada cenário, tentando imitar a utilização de um utilizador normal. foi usada a ferramenta reran para gravar e repetir cinco vezes cada script e para a sua monitorização é usado a ferramenta trepn. os resultados obtidos permitiram concluir um ranking de qual o browser foi mais eficiente entre os sete selecionados."
    ],
    [
      "cerebellar ataxia arises from damage or dysfunction that affects the cerebellum and its pathways. as a result, the motor abilities of individuals with this condition become weakened. robotics-assisted therapy is still an emerging area, but it has several advantages that could boost the rehabilitation of these individuals. considering this problematic, walkit smart walker is being developed. its main purpose is to improve the treatment of ataxic patients through intelligent and multidisciplinary rehabilitation sessions. thus, it is equipped with several sensors that provide monitoring capabilities through a continuous evaluation of the end-user gait and posture. a vast amount of data is acquired during each session by the walker sensors. for health professionals to analyse this data and have feedback on the patient’s status throughout therapy, tools are needed to control, manage, and monitor sessions in a clear, practical and intuitive way. therefore, the main goal of this dissertation is centred on implementing an effective way to store the acquired data, along with the development of software that satisfies these requirements. to address these goals, a polyglot persistence database system, composed of a relational and a non-relational database, was implemented to store the required data while maintaining efficiency. furthermore, a web application was developed to provide, not only to health professionals, but also to patients themselves, the management of the rehabilitation sessions with the walker. the application provides an individual and temporal analysis of the sessions through interactive graphics adapted to each patient. additionally, it allows the management of the several patients who are/were in treatment and the addition of clinical ratting scales, which are useful to assess their motor condition and adapt therapies as needed. in this way, professionals can have a better perception of the patient’s condition, and can show patients their evolution, possibly contributing to increase their motivation in therapy. moreover, in the context of this dissertation, the embedded software of walkit smartw, which allows the therapy configuration, was optimized. this software had no security mechanisms, thus the main goal was on the implementation of techniques capable of making the software secure. additionally, other functionalities such as feedback alerts, were added to the existing application. throughout the development of this project, it was possible to have continuous feedback from health professionals of the hospital of braga. usability tests and questionnaires were also applied, and the results were very promising, enhancing the need for a system with these characteristics. professionals claimed the system may help in analysing the patient clinical status in an intuitive form while keeping them motivated during treatments.",
      "a ataxia cerebelar surge a partir de danos ou disfunções que afetam o cerebelo e as suas vias. como resultado, as capacidades motoras dos indivíduos que possuem esta condição ficam fragilizadas. a terapia assistida por robôs é ainda uma área em desenvolvimento, no entanto apresenta diversas vantagens que poderão agilizar os tratamentos destes indivíduos. atendendo a esta problemática, o walkit smartw encontra-se a ser desenvolvido. o seu principal propósito é auxiliar os tratamentos de pacientes ataxicos através de sessões de reabilitação inteligentes e multidisciplinares. para tal, é composto por um conjunto de sensores que fornecem uma monitorização e avaliação contínua da marcha e da postura do utilizador. uma grande quantidade de dados é adquirida ao longo de cada sessão através dos sensores. de forma a que os profissionais de saúde analisem estes dados e tenham feedback do estado do paciente ao longo da terapia, são necessárias ferramentas que permitam controlar, gerir e monitorizar as sessões, de forma clara, prática e intuitiva. o principal objetivo desta dissertação centra-se na implementação de uma estratégia eficiente para armazenar os dados, juntamente com o desenvolvimento de um software que satisfaça estes requisitos. para cumprir estes objetivos, um sistema de base de dados com persistência poliglota, composto por uma base de dados relacional e uma não relacional, foi implementado para armazenar os dados mantendo a eficiência. além disso, uma aplicação web foi desenvolvida para proporcionar, não só aos profissionais de saúde, como também aos próprios pacientes, a gestão das sessões de reabilitação com o andarilho. a aplicação disponibiliza uma análise individual e temporal das sessões através de gráficos interativos adaptados a cada paciente. adicionalmente, possibilita também a gestão dos diversos pacientes que estão/estiveram em tratamento, e a adição de escalas de classificação clínica, que são úteis para avaliar a condição motora e adaptar as terapias conforme necessário. desta forma, os profissionais conseguem ter uma melhor perceção acerca do estado do paciente, e os pacientes podem ver a sua evolução, contribuindo para aumentar a motivação na terapia. ainda no contexto desta dissertação, otimizou-se a aplicação embebida no software do andarilho walkit, que permite as configurações da terapia. o software era isento de qualquer mecanismo de segurança, pelo que o maior foco centrou-se na aplicação de técnicas capazes de o tornar seguro. adicionalmente, outras funcionalidades, como alertas e configurações de algoritmos, foram adicionadas à aplicação existente. ao longo do desenvolvimento deste projeto, foi possível obter o feedback contínuo de profissionais de saúde do hospital de braga. testes e questionários de usabilidade foram também aplicados, e os resusltados foram bastante promissores, reforçando a necessidade de um sistema com estas características. os profissionais afirmaram que o sistema irá ajudar a analisar o estado do paciente de forma intuitiva, mantendo-o motivado durante os tratamentos."
    ],
    0.06666666666666667
  ],
  [
    [
      "com o passar dos anos, acompanhado pela evolução da tecnologia, existe um aumento acentuado no número de dados acumulados no setor de contratação pública em portugal. este aumento abre caminho para a possibilidade de tirar proveito desses dados, com recurso à utilização de técnicas emergentes de inteligência artificial, de modo a melhorar o funcionamento do processo de feitura de contratos públicos em portugal. no panorama da contratação pública, um dos grandes problemas que afetam a qualidade dos contratos celebrados é a existência de irregularidades não detetadas aquando da celebração dos contratos, essas irregularidades geram assim contratos que não cumprem as regras definidas para os contratos e que podem comprometer a qualidade dos bens e serviços providenciados pelo estado. portanto, esta dissertação visa recolher um dataset de contratos públicos a partir do portal disponi bilizado pelo governo português, processar e analisar os dados recolhidos, codificar as regras do código dos contratos públicos num sistema de regras, investigar e utilizar técnicas de inteligência artificial de modo a desenvolver um pipeline com a finalidade de encontrar padrões de suspeição de conluio e detetar irregularidades, e, por fim, conceber modelos de machine learning para prever os valores futuros das despesas de cada entidade ou região. para sustentar o trabalho desenvolvido foram analisadas e relatadas na dissertação algumas implemen tações existentes de técnicas de inteligência artificial em contratação pública, juntamente com algumas abordagens de deteção de fraudes, assim como foram analisados diversos paradigmas e algoritmos de ml. por fim é demonstrado de que forma os modelos de ml foram concebidos e otimizados, e é feita a análise de resultados dos modelos criados. a investigação e experimentação realizada abre perspetivas para o futuro da aplicação de soluções de inteligência artificial na área da contratação pública.",
      "over the years, with the evolution of technology, there has been a sharp increase in the number of data accumulated in the public procurement sector in portugal.this increase paves the way for the possibility of taking advantage of this data by using emerging techniques of artificial intelligence, in order to improve the functioning of the public procurement process in portugal. within the public procurement panorama, one of the major problems affecting the quality of the contracts signed is the existence of irregularities that are not detected when the contracts are made. these irregularities thus generate contracts that do not comply with the rules defined for contracts and that can compromise the quality of the goods and services provided by the state. therefore, this dissertation aims to collect a dataset of public contracts from the portal provided by the portuguese government, process and analyze the collected data, codify the rules in a rule-based system, investigate and use artificial intelligence techniques in order to develop a pipeline with the purpose of finding patterns of suspicion of collusion and detecting irregularities. finally, there is also the objective of designing and developing machine learning models to predict the future values of expenditure of each city or region. in this work were also discussed implementations of artificial intelligence techniques in the public procurement field, along with the analysis of fraud detection approaches, as well as ml paradigms and algorithms. in addition, it is demonstrated how ml models have been designed and optimized, followed by the analysis of the results of each model. the research and experimentation conducted opens up perspectives for the future of the application of artificial intelligence solutions in the area of public procurement."
    ],
    [
      "in the healthcare industry, the patient’s nutrition is a key factor in their treatment process, as every user has their own specific nutritional needs and requirements. for example, after a major surgery, a patient should eat products with high fiber while avoiding processed foods and dairy. an appropriate nutrition policy can therefore complement the patient’s recovery process, alleviating possible symptoms. food recommender systems are platforms that offer personalised suggestions of recipes to users. these systems are often implemented in food recipe websites, offering similar sug gestions. they are also used for improving the user’s health and recommending healthier recipes while keeping their preferences in consideration. however, there is an absence of usage of recipe recommendation systems in the healthcare sector. multiple challenges in representing the domain of food, coupled with the patient’s needs, make it complicated to implement these systems in healthcare services and continuous care. in the context of this master’s dissertation, the aim was to design, develop, and explore a new generation platform for the provision, planning, and reservation of food plans, com prised of web and mobile tools. a key feature of this platform is the suggestion of meal plans to each department, taking into account the patient’s nutritional requirements. data regarding the user’s nutritional requirements were collected and analysed, as well as feedback from health professionals and users from the social cafeteria. the collected information supported the development of a food recommendation system. these tools will help nutrition professionals at the santa casa da misericórdia of vila verde in their work, namely with the making of meal plans for multiple departments, each with their specific nutritional requirements.",
      "na área da saúde, a nutrição do paciente é um fator principal no seu processo de recuperação, onde cada utente tem as suas necessidades e requisitos nutricionais específicos. assim, após unia cirurgia, um paciente deve comer alimentos com alto teor de fibra, evitando alimentos processados e lacticínios. ora, uma política de nutrição apropriada pode ajudar e comple-mentar a terapia do paciente, aliviando potenciais efeitos secundários. sistemas de recomendação de alimentos (food recommender systems) são plataformas que oferecem sugestões personalizadas de receitas a utilizadores. estes sistemas são frequente-mente implementados em páginas web de receitas, oferecendo sugestões similares. são, também, utilizados para melhorar a alimentação dos utilizadores, ao recomendar receitas mais saudáveis, tendo em consideração as suas preferências. contudo, existe urna lacuna na implementação destes sistemas em cuidados de saúde. há múltiplos desafios em represen-tar o domínio dos alimentos, bem como as necessidades médicas dos pacientes dificultam a implementação em cuidados de saúde e cuidados continuados. no âmbito deste projeto de dissertação, foi desenhada, desenvolvida e explorada unia plataforma de nova geração para o aprovisionamento, planeamento e reserva de planos alimentares, composta por ferramentas web e móveis. uma funcionalidade chave desta plataforma é a sugestão de ementas a cada valência, tendo em consideração os requisitos nutricionais dos pacientes. dados relativos às necessidades nutricionais dos utilizadores foram recolhidos e analisa-dos, tal como as opiniões feedback de profissionais de saúde e dos utilizadores da cantina social. as informações recolhidas serviram de suporte para o desenvolvimento de um sis-tema de recomendação ao apoio à decisão. estas ferramentas vão auxiliar o trabalho dos profissionais de nutrição da santa casa de misericórdia de vila verde, nomeadamente no planeamento de refeições a múltiplas valências, cada uma com os seus requisitos nutri-cionais específicos."
    ],
    0.0
  ],
  [
    [
      "emotion is an essential part of what means to be human, but it is still disregarded by most technical fields as something not to be taken into account in scientific or engineering projects. however, the understanding of emotion as an aspect of decision making processes and of modelling of human behaviour is essential in order to create a better connection between humans and their tools and machines. this dissertation focuses on the measurement of emotion of users through the use of non-intrusive methods, like measuring inputs and reactions to stimuli, along with the creation of a tool that measures the emotional changes caused by visual output created by the tool itself. usage of the tool in a test environment and the subsequent analysis of the data obtained will allow for conclusions about the effectiveness of the method, and if it is possible to apply it to future studies on human emotions by investigators in the fields of psychology and computation.",
      "emoção é uma parte essencial do que nos faz humanos, mas mesmo assim há muitos nos campos mais técnicos e científicos que a desconsideram como algo que não deve ser tido em conta em projetos científicos ou de engenharia. a compreensão da emoção é no entanto um aspeto chave para o estudo dos processos de decisão humanos, assim como para a modelação de comportamentos, algo essencial para a criação de uma melhor ligação entre o homem e as suas máquinas e ferramentas. esta dissertação foca-se na medição da emoção de utilizadores através de métodos não-intrusivos, como medições de inputs em computadores e reações a estímulos, assim como na criação de uma ferramenta que meça as mudanças emocionais causadas nos utilizadores por certos estímulos visuais provenientes da própria ferramenta. utilização da ferramenta num ambiente de teste e a subsequente análise dos dados obtidos permitirão a tomada de conclusões sobre a eficiência deste método, assim como sobre se é viável a aplicação do mesmo a estudos futuros da emoção humana realizados por investigadores nas áreas de psicologia ou computação."
    ],
    [
      "one of the challenging problems in bioinformatics is to computationally characterize sequences, structures and functions of proteins. sequence-derived structural and physico-chemical properties of proteins have been used in the development of machine learning models in protein related problems. however, tools and platforms to calculate features and perform machine learning (ml) with proteins are scarce and have their limitations in terms of effectiveness, user-friendliness and capacity. here, a generic modular automated platform for the classification of proteins based on their physicochemical properties using different ml algorithms is proposed. the tool developed, as a python package, facilitates the major tasks of ml and includes modules to read and alter sequences, calculate protein features, preprocess datasets, execute feature reduction and selection, perform clustering, train and optimize ml models and make predictions. as it is modular, the user retains the power to alter the code to fit specific needs. this platform was tested to predict membrane active anticancer and antimicrobial peptides and further used to explore viral fusion peptides. membrane-interacting peptides play a crucial role in several biological processes. fusion peptides are a subclass found in enveloped viruses, that are particularly relevant for membrane fusion. determining what are the properties that characterize fusion peptides and distinguishing them from other proteins is a very relevant scientific question with important technological implications. using three different datasets composed by well annotated sequences, different feature extraction techniques and feature selection methods (resulting in a total of over 20 datasets), seven ml models were trained and tested, using cross validation for error estimation and grid search for model selection. the different models, feature sets and feature selection techniques were compared. the best models obtained for distinct metric were then used to predict the location of a known fusion peptide in a protein sequence from the dengue virus. feature importances were also analysed. the models obtained will be useful in future research, also providing a biological insight of the distinctive physicochemical characteristics of fusion peptides. this work presents a freely available tool to perform ml-based protein classification and the first global analysis and prediction of viral fusion peptides using ml, reinforcing the usability and importance of ml in protein classification problems.",
      "um dos problemas mais desafiantes em bioinformática é a caracterização de sequências, estruturas e funções de proteínas. propriedades físico-químicas e estruturais derivadas da sequêcia proteica têm sido utilizadas no desenvolvimento de modelos de aprendizagem máquina (am). no entanto, ferramentas para calcular estes atributos são escassas e têm limitações em termos de eficiência, facilidade de uso e capacidade de adaptação a diferentes problemas. aqui, é descrita uma plataforma modular genérica e automatizada para a classificação de proteínas com base nas suas propriedades físico-químicas, que faz uso de diferentes algoritmos de am. a ferramenta desenvolvida facilita as principais tarefas de am e inclui módulos para ler e alterar sequências, calcular atributos de proteínas, realizar pré-processamento de dados, fazer redução e seleção de features, executar clustering, criar modelos de am e fazer previsões. como é construído de forma modular, o utilizador mantém o poder de alterar o código para atender às suas necessidades específicas. esta plataforma foi testada com péptidos anticancerígenos e antimicrobianos e foi ainda utilizada para explorar péptidos de fusão virais. os péptidos de fusão são uma classe de péptidos que interagem com a membrana, encontrados em vírus encapsulados e que são particularmente relevantes para a fusão da membrana do vírus com a membrana do hospedeiro. determinar quais são as propriedades que os caracterizam é uma questão científica muito relevante, com importantes implicações tecnológicas. usando três conjuntos de dados diferentes compostos por sequências bem anotadas, quatro técnicas diferentes de extração de features e cinco métodos diferentes de seleção de features (num total de 24 conjuntos de dados testados), sete modelos de am, com validação cruzada de io vezes e uma abordagem de pesquisa em grelha, foram treinados e testados. os melhores modelos obtidos, com avaliações mcc entre 0,7 e o,8 e precisão entre 0,85 e 0,9, foram utilizados para prever a localização de um péptido de fusão conhecido numa sequência da proteína de fusão do vírus do dengue. os modelos obtidos para prever a localização do péptido de fusão são úteis em pesquisas futuras, fornecendo também uma visão biológica das características físico-químicas distintivas dos mesmos. este trabalho apresenta uma ferramenta disponível gratuitamente para realizar a classificação de proteínas com am e a primeira análise global de péptidos de fusão virais usando métodos baseados em am, reforçando a usabilidade e a importância da am em problemas de classificação de proteínas."
    ],
    0.02727272727272727
  ],
  [
    [
      "this document serves as a master’s dissertation on a degree in software engineering, in the area of language engineering. the main goal of this work is to infer the profile of a programmer, through the analysis of his source code. after such analysis the programmer shall be placed on a scale that characterizes him on his language abilities. there are several potential applications for such profiling, namely, the evaluation of a programmer’s skills and proficiency on a given language, or the continuous evaluation of a student’s progress on a programming course. throughout the course of this project, and as a proof of concept, a tool that allows the automatic profiling of a java programmer should be developed.",
      "este documento refere-se a uma dissertação do mestrado em engenharia informática, na área da engenharia de linguagens. o principal objetivo desta dissertação é inferir o perfil de um programador através da análise do seu código fonte. após a análise, o programador será automaticamente colocado numa escala que o caracteriza quanto às suas capacidades na linguagem. existem várias potenciais aplicações para a perfilagem de programadores, por exemplo, avaliar as capacidades e proficiência de um programador numa dada linguagem ou, a avaliação continua de alunos numa disciplina de programação. como prova de conceito, é esperada a implementação de uma ferramenta que permita perfilar automaticamente programadores."
    ],
    [
      "this dissertation begins with a brief introduction, where the main objective, segmentation of an organ/structure into sub-segments and performing a radiation analysis after a radiotherapy treatment. then, there is a short introduction to what cancer is, as well as some of the most relevant to this dissertation. after, the implementation of the various interfaces created, for the identification of the contours of the structures and for the final objective. finally, the results obtained from these interfaces, where its possible to observe the fulfillment of the proposed objectives. finally the conclusions obtained and the validation by a doctor of the field, and proposals for future work, namely the segmentation of more complex structures, such as rectum and sigma-colon.",
      "esta dissertação inicia-se com uma breve introdução, onde se pode ler o principal objectivo, realizar uma segmentação de um órgão/estrutura em sub-segmentos e realizar uma análise da radiação após um tratamento de radioterapia. de seguida, há uma pequena introdução ao que é o cancro, bem como alguns dos de maior relevância para esta dissertação. seguidamente, a implementação das várias interfaces criadas, quer para identificação dos contornos das estruturas, quer para o objectivo final. por fim os resultados obtidos dessas interfaces, onde se pode observar o cumprimento dos objectivos propostos. por fim as conclusões obtidas, juntamente com a validação pelo doutor da área em questão e propostas para trabalho futuro, nomeadamente a segmentação de estruturas mais complexas, como o recto e o sigma-cólon."
    ],
    0.3
  ],
  [
    [
      "this dissertation was carried out at utilmédica - produtos medicos hospitalares, lda which was born on june 9, 2004, as a result of the perception of gaps in the market for the supply of medical - hospital products and equipment to health professionals. one of their main goals is to provide healthcare professionals with the best solutions for the noble mission of ensuring the welfare of us all. due to this vision and consequent growth, the company’s working methods also need to grow. therefore, the company wanted to find a solution to the growing quotation requests by the various messaging platforms in which they are present. starting from this problem, a system was developed that identifies the products contained in a given message and sends an automatic quotation reply. this system was named rissa and to develop it, it was necessary to analyse the content of previous email messages, in order to develop a nlp model that could identify the entities present in future email messages. in addition to this, rissa also contains a search system that filters only the products available from the company. rissa had to integrate into an existing infrastructure without impacting the company’s op eration. this integration had to deal not only with external services, but also with internal services and privacy policies. in the end, this system was implemented in the company in a real work situation to obtain production results.",
      "esta dissertação realizou-se na empresa utilmédica - produtos medicos hospitalares, lda que nasceu a 9 de junho de 2004, como resultado da perceção de lacunas no mercado para o fornecimento de produtos e equipamentos médicos - hospitalares aos profissionais de saúde. um dos seus principais objetivos é fornecer aos profissionais de saúde as melhores soluções para a nobre missão de garantir o bem-estar de todos nós. devido a esta visão e consequentemente ao crescimento, os métodos de trabalho da empresa também precisam de crescer. por isso a empresa gostava de encontrar uma solução para o crescente pedido de orçamentos pelas várias plataformas de mensagens. partindo deste problema, foi desenvolvido um sistema que identifica os produtos contidos numa determinada mensagem e envia uma resposta automática de orçamento. este sistema foi apelidado de rissa e para o desenvolver foi necessário analisar o conteúdo das mensagens de email anteriores de modo a desenvolver um modelo de nlp que fosse capaz de identificar as entidades nas mensagens de email futuras. para além disto, rissa contém um sistema de pesquisa de modo a filtrar apenas os produtos disponibilizados pela empresa. rissa teve de se integrar numa infraestrutura já existente sem afetar o funcionamento da empresa. esta integração teve de lidar não só com os serviços externos, mas também com serviços e políticas de privacidade internas. no final, este sistema foi implementado na empresa numa situação de trabalho real para se obter resultados de produção."
    ],
    [
      "hoje em dia, a tomada de decisões de forma rápida e eficaz é essencial nas organizações de saúde. neste sentido, surgem os sistemas de apoio à decisão, as plataformas de business intelligence e os sistemas de tratamento de dados. de forma a apoiar a decisão no âmbito farmacêutico surgem plataformas de previsão, as quais pretendem auxiliar ao máximo a tomada de decisão por parte dos prestadores de saúde. no âmbito desta dissertação, foi realizado um projeto com o objetivo de extrair conhecimento de forma automatizada a partir de informações passadas e traduzi-las de forma a desenvolver um sistema de previsão de vendas para a área farmacêutica. tradicionalmente, na área da previsão, é comum a utilização de modelos estatísticos, no entanto é interessante perceber se o deep learning consegue acompanhar os resultados obtidos através destes modelos. para o efeito, foi elaborado um estudo comparativo entre modelos de previsibilidade, conseguidos através de modelos estatísticos e conexionistas. para os primeiros fez-se uso de funções de modelação disponíveis em librarias da linguagem de programação r e no segundo foram aplicadas redes neuronais recorrentes, nomeadamente as long short term memory, através de bibliotecas disponíveis em python para construção de um modelo deep learning. as metodologias desenvolvidas através dos diferentes modelos de previsibilidade foram aplicadas a três casos de estudo, cada um associado a um conjunto de dados diferente. assim, tornou-se possível analisar o comportamento dos modelos desenvolvidos quando aplicados a conjuntos de dados distintos. por último, foram apresentados os resultados obtidos para os três casos de estudo, referentes à aplicação de ambas as práticas, e feita uma comparação das mesmas. foi verificado o sucesso da utilização de algoritmos de deep learning na área da previsão, obtendo melhores resultados que aqueles conseguidos através dos tradicionais modelos de previsão estatísticos. este trabalho permitiu perceber o potencial que o deep learning apresenta, sendo no entanto necessário mais trabalho futuro para dar enfâse a esta afirmação.",
      "nowadays, making decisions quickly and effectively is essential in health organizations. in this sense, decision support systems, business intelligence platforms and data processing systems begin to emerge. to support the decision in the pharmaceutical field, there are platforms for forecasting, which are intended to help health decision makers to the maximum extent possible. this dissertation has the objective of extracting knowledge in an automated way from past information, and translating it in order to develop a sales forecasting system for the pharmaceutical area. traditionally, in the area of forecasting, it is common to use statistical models, however it is interesting to see if deep learning can follow the results obtained through these models. for this purpose, a comparative study was developed for predictability models, achieved through statistical and connectionist models. for the statistical models, it was used functions present in libraries of the programming language r and for the second models, it was applied recurrent neural networks, namely the long short term memory, through libraries available in python to construct a deep learning model. the methodologies developed through the different predictability models were applied to three case studies, each associated to a different data set. thus, it became possible to analyze the behavior of models developed when applied to different datasets. finally, the results obtained for the three case studies using both practices were presented and it was verified the success of the deep learning algorithms in the area of forecasting. they achieved better results than those obtained through the traditional statistical prediction models. this work allowed to realize the potential that deep learning presents, being nevertheless necessary more future work to give emphasis to this affirmation."
    ],
    0.12857142857142856
  ],
  [
    "precise and efficient monitoring is vital to ensure that a network works according to the intended behavior, as well as quickly acting to the problems found. the task of monitoring a network becomes complex with increasing network size and heterogeneity. the available network monitoring and management solutions are not only costly but also difficult to use, configure and maintain. this work aims to continue the development of a p2p system to detect network anomalies to help network administrators. the system should be easily used by isp network administrators. initially, state-of-the-art research will be presented, where various technologies related to the objective of the dissertation will be highlighted. multiple requirements of monitorization systems will be exposed and there will be a description of the applicability of a p2p system in a monitorization system. afterward, all the processes of creation of the system and the existing entities will be described as well as their communication capabilities. all the ways an administrator can interact with the system will also be presented. then, the process of definition of a metalanguage will be exposed, to allow the administrators to configure and pre-program the network in an effective and varied manner. the process of optimization of the monitoring system will be described, to reduce the traffic of the p2p network. it will be also described how the system became fault-tolerant, recovering its state if any entity has a problem. subsequently, the implementation of each one of the developed mechanisms and the architecture of the system will be exposed, where it’s explained which technologies were used and justified some paths chosen to achieve the dissertation objectives. the system will be tested in a network emulator and the resulting data from the created mechanisms will be analyzed to validate the correct behavior of what was developed.",
    [
      "this document presents and discusses the outcomes of a project in the area of pedagogical tools to support teaching. this project is part of the work in the second year of the master degree in software engineering and was accomplished at universidade do minho in braga, portugal. the main outcome of this master project is an application that allows a teacher to plan his classes by describing in a text file the topics and subtopics of the school program that he has to teach in a lesson, linking to each concept the multimedia support materials and questions that he intends to do (orally, on paper, or on the computer). this application is based on a domain specific language (dsl) specially tailored to described easily lesson plans no matter the course or the teaching topic. the application contains all user-entered lesson plans, which are automatically processed to create a classroom presentation for each subject, supporting and guiding a given class. whenever intended by the user, the appli cation also processes all external resources, introducing hyperlinks to them in the various fields of the lesson plan. the application is publicly available at the language-processing research group server.",
      "este documento apresenta e discute os resultados de um projeto na área de ferramentas pedagógicas para apoiar o ensino. este projeto faz parte do trabalho do segundo ano do mestrado em engenharia de software e foi realizado na universidade do minho em braga, portugal. o principal resultado deste projeto de mestrado é uma aplicação o que permite ao professor planear as suas aulas descrevendo num ficheiro de texto os tópicos e subtópicos do programa escolar que ele deve ensinar numa aula, interligando a cada conceito os materiais de suporte multimédia e perguntas que ele pretende fazer (oralmente, no papel ou no computador). esta aplicação é baseada numa linguagem específica de domíınio (dsl), especialmente adaptada para descrever facilmente planos de aula, independentemente do curso ou do tópico de ensino. a aplicação contém em todos os planos de aula inseridos pelo utilizador, que são processados automaticamente para criar uma apresentação na sala de aula para cada disciplina, dando suporte e servindo de guião para uma determinada aula. sempre que pretendido pelo utilizador, a aplicação também processa todos os recursos externos, introduzindo links para eles nos vários campos do plano de aula. a aplicação está disponível publicamente no servidor do grupo de pesquisa de processamento de linguagens."
    ],
    0.3
  ],
  [
    [
      "o desenvolvimento metodológico de aplicações web multi-camada, satisfazendo o modelo mvc e contendo uma camada de persistência de dados suportada por bases de dados relacionais, tem sido suportada fundamentalmente por duas grandes plataformas de desenvolvimento: java web e .net. o aparecimento do open source framework ruby on rails, num momento em que a criação de aplicações web é uma área fundamental da engenharia de software, obriga a uma análise detalhada das características desta terceira plataforma. este trabalho tem por objetivo fundamental estudar de forma comparativa as arquiteturas de software e tecnologias propostas por ruby on rails versus as tecnologias java web (i.e apache struts2) e, ainda, comparar as metodologias e tecnologias de suporte ao desenvolvimento subjacentes a ambos os ambientes.",
      "the methodological development of multi-tier web applications, satisfying the mvc model and implementing a layer containing data persistence supported by relational databases, has been supported mainly by two major development platforms: java web and .net. the emergence of the open source framework ruby on rails, when the creation of web applications is a key area of engineering software, requires a detailed analysis of the characteristics of this third platform. this work is fundamental to comparatively study the software technologies and architectures proposed by ruby on rails versus technologies java web (i.e apache struts2), and the methodologies and supporting technologies underlying both development environments."
    ],
    [
      "nos dias de hoje, inúmeras instituições recebem, todos os dias, uma vasta quantidade de informação e, muitas das vezes em papel. desta forma, existe uma preocupação cada vez maior quer no consumo excessivo de papel quer na gestão de uma grande quantidade de informação. com o intuito de simplificar a gestão documental, o governo tem concebido algumas estratégias, particularmente na administração pública (ap), com base em normas e orientações provenientes da comissão europeia. assim, surge uma dessas estratégias, o projeto ”m51-clav- arquivo digital: plataforma modular de classificação e avaliação da informação pública”, da direção-geral do livro, dos arquivos e das bibliotecas (dglab). o objetivo principal deste projeto é a automatização de alguns processos relativos a classificação e avaliação de documentação na administração pública portuguesa, utilizando um referencial comum que permite o desenvolvimento de instrumentos de natureza transversal a aplicar em contexto organizacional. desta forma, nesta dissertação, pretende-se ter fora da plataforma ”clav - classificação e avaliação da informação pública”, um ambiente que permita às pessoas manipular e editar a informação relativa aos processos de negócio da administração pública (ap). desse modo, numa fase inicial foi feito um xml schema, com base na macroestrutura funcional (mef), com os dados da clav. posteriormente, pretende-se ver de que forma é que alguns dos invariantes sobre os processos de negócio podem ser colocados a nível do schema, e depois implementados no xonomy - um editor xml em javascript.",
      "nowadays, numerous institutions receive, every day, a vast quantity of information, most of which in paper. thus, we see an increasing worrying with the excessive consump tion of paper and information processing. in order to simplify the documental management, the government has come up with some strategies, particularly the public administration (pa), based in orientations and norms from the european commission. therefore, one of these strategies is the project ”m51-clav-digital archive: modular platform for classification and evaluation of public information”. the main goal of this project is the automatization of some processes related with the classification and evalua tion of documents in the portuguese public administration, using a common referential that allows the development of transversal instruments to be applied in an organizational context. thus, in this dissertation, it is intended to have outside the clav platform, an environ ment that allows people to manipulate and edit information related to the public adminis tration (ap) business processes. therefore, in an initial phase an xml schema, based on functional macrostructure (mef), with the data of the clav was done. subsequently, we intend to see how some of the invariants about the business processes can be placed at the schema level, and then implemented in xonomy - an xml editor in javascript."
    ],
    0.06666666666666667
  ],
  [
    [
      "the c programming language is perhaps the most widespread in the design of safety-critical systems. however, its adoption suffers from disadvantages because it lacks in safety, entailing extensive and expensive verification processes. there are other programming languages, such as ada and spark, that offer safety features that automatically comply with the current safety standards. nevertheless, the software industry continues to use c after all and its associated verification process to build safety-critical software. rust is a modern programming language that promises to soften such insecurities by design, thus improving on the development of safety-critical software. due to its ownership model, rust can ensure memory safety at compile time. because it is a systems programming language, it is also a promising language for embedded systems. the main aim of the project reported in this dissertation is to understand how rust can alleviate the certification process of safety-critical software, while evaluating its maturity for embedded systems. we analyse in which platforms rust is available and compare embedded rust to other languages used in the safety-critical domain. this analysis consists in comparing rust safety features with coding guidelines commonly used in the software industry. some case studies are carried out, such as preemptive and cooperative scheduling (both in c and rust to better understand the programming differences in these languages), a driver for an accelerometer and finally a program that uses the scheduler, the accelerometer and the leds of a micro-controller, where one thread reads acceleration values and another thread turns leds on/off according to such readings. the dissertation ends with an overview of the results obtained. not only a comparison is given with coding guidelines used in industry, but also concerning the case studies developed. it also anticipates some important work that could be added, as well as some details where rust could be improved to become prominent in the industry of safety critical software.",
      "a linguagem de programação c é talvez a que tem mais representação no design de sistemas críticos. contudo, a sua utilização tem algumas desvantagens pois falha em segurança, o que resulta em processos de verificação extensos e dispendiosos. existem outras linguagens de programação, como ada e spark, que possuem funcionalidades de segurança que automaticamente correspondem aos standards de segurança. apesar disso, o que se verifica é que a indústria continua a usar c e o consequente processo de verificação para desenvolver software crítico. rust é uma linguagem moderna que promete atenuar tais inseguranças pelo design, melhorando assim o desenvolvimento de software crítico. graças ao seu modelo de ownership, a linguagem consegue assegurar uma utilização da memória de forma segura no momento da compilação do código. como rust é uma linguagem de programação de sistemas, esta é promissora para ser usada nos sistemas embebidos. o objectivo principal desta dissertação é investigar como é que rust pode suavizar o processo de certificação de software crítico, e avaliar a maturidade de rust para sistemas embebidos. assim, analisamos em que plataformas rust está disponível e comparamos rust embebido com outras linguagens utilizadas neste domínio. esta análise consiste em comparar as características de segurança de rust com as normas de codificação utilizadas na indústria. alguns casos de estudo foram desenvolvidos, tais como schedulers preemptivos e cooperativos (tanto em c como em rust para perceber melhor quais as diferenças em programar nestas linguagens), um driver para usar um acelerómetro e, por fim, um programa que faz uso do scheduler, acelerómetro e leds presentes no microcontrolador, tendo uma thread a ler valores de aceleração e outra thread a ligar ou desligar os leds de acordo com essas leituras. a dissertação acaba com uma visão global dos resultados obtidos. não é feito apenas uma comparação com as normas de codificação usadas na indústria, como também uma comparação dos casos de estudo desenvolvidos. também exploramos algum trabalho importante que pode ser desenvolvido no futuro, bem como alguns detalhes onde a linguagem pode ser melhorada para poder fazer parte da indústria crítica."
    ],
    [
      "the current progress of sequencing systems facilitates the sequencing of the genomes and transcriptomes of countless organisms on our planet. however, it is not simple to measure the quality of the processed data, mainly in the study of non-model organisms, for which there is little if any, information available. the korf lab developed a method for the evaluation of genomes integrity, through the identification of 248 core eukaryotic genes (cegs) that are present in nearly all of the eukaryotes. the main goal of this work is to evaluate the use of the cegs in rna-seq of non-model organisms. for that two software’s were developed: seqqirefmetrics to calculate a set of referencebased quality metrics, including identification, chimerism, accuracy and contiguity, based on the literature, and three new metrics, comprising fragmentation(1,2,3,4,5+), coverage and non-match, increasing the number of metrics available for transcriptome quality assessment; and seqqiidentifycegs to identify and report the number of cegs present in each transcriptome assembly. to carry out the main objective, rna-seq data from nine model organisms (arabidopsis thaliana, aspergillus nidulans, caenorhabditis elegans, drosophila melanogaster, homo sapiens, mus musculus, oryza sativa, saccharomyces cerevisiae and xenopus tropicalis), processed with trinity, were used to evaluate how ceg detection correlates with the quality of the transcriptomes. in order to identify cegs, protein sequences from assembled transcripts were predicted with transdecoder. metrics calculated by seqqirefmetrics were associated with the number of cegs identified by seqqiidentifycegs in each assembled transcriptome, through linear regressions. among these metrics only contiguity and coverage were used to create predictive models, achieving an r2 of 0.787 and 0.640; and a rmse of 5.86 and 6.90, respectively. these findings indicate that the cegs can be used as a quality tool. in fact, the linear regressions enable to infer prospectively the quality of the assembled transcripts, without the necessity of additional information, such as a reference genome sequence or structural annotations. this approach is extremely important for rna-seq of non-model organisms, where there is no such information to evaluate the quality of the assembled transcripts in a reliable manner.",
      "os progressos nas plataformas de sequenciação atuais permitem a obtenção dos genomas e transcritomas dos inúmeros organismos que habitam o nosso planeta. contudo, não é simples avaliar a qualidade dos dados já processados, principalmente em estudos de organismos não modelo, para os quais existe pouca, se alguma, informação disponível. o grupo de investigação “the korf lab” desenvolveu um método para avaliar a integridade de sequências genómicas, através da identificação de 248 “core eukaryotic genes” (cegs) que são conservados nos eucariontes. o principal objetivo deste trabalho é avaliar a utilização dos cegs em rna-seq de organismos não modelo. de modo a atingir este objectivo dois softwares foram desenvolvidos: seqqirefmetrics, para calcular um conjunto de métricas baseadas em referência, incluindo “identification”, “chimerism”, “accuracy” e “contiguity”, com base na literatura, e três novas métricas, “fragmentation(1,2,3,4,5+)”, “coverage” e “non-match”, aumentando assim o numero de métricas disponíveis para a avaliação da qualidade de transcritomas; e seqqiidentifycegs para identificar e reportar o número de cegs presentes em cada transcritoma. os dados de rna-seq de nove organismos modelo (arabidopsis thaliana, aspergillus nidulans, caenorhabditis elegans, drosophila melanogaster, homo sapiens, mus musculus, oryza sativa, saccharomyces cerevisiae e xenopus tropicalis), processados com o trinity, foram usados para avaliar como a detecção dos cegs se correlaciona com a qualidade dos transcritomas. de modo a identificar os cegs, as sequências proteicas dos transcritos assemblados foram determinadas com o transdecoder. as métricas calculadas com seqqirefmetrics foram associadas com o número de cegs identificados com seqqiidentifycegs, em cada transcritoma assemblado, através de regressões lineares. entre estas métricas apenas “contiguity” e “coverage” foram usadas para criar modelos preditivos, atingindo um r2 de 0,787 e 0,640; e um rmse de 5,86 e 6,90, respetivamente. estes resultados sugerem que os cegs poderão ser usados como uma ferramenta de qualidade. na verdade, as regressões lineares permitem inferir a qualidade dos transcritos assemblados, sem a necessidade de informação adicional, como um genoma de referência ou anotações estruturais. este método é assim extremamente importante para estudos de rna-seq de organismos não modelo, onde não existe tal informação que permita avaliar a qualidade dos transcritos de um modo viável."
    ],
    0.3
  ],
  [
    [
      "a era digital trouxe a incorporação dos computadores em vários setores do nosso quotidiano. tem-se verificado, ao longo dos anos, uma crescente dependência na tecnologia, o que se traduz na necessidade de sistemas cada vez mais resilientes, rápidos, seguros e disponíveis. consequentemente, a complexidade dos sistemas tem vindo a crescer, o que leva à implementação de mecanismos de persistência e atualização de dados mais difíceis de testar. para elevar o nível de dificuldade, os diversos sistemas de ficheiros possuem particularidades que afetam de forma significativa estes mecanismos e a consistência das aplicações após a ocorrência de falhas de energia e dos sistemas operativos. evitar a perda total ou parcial dos dados deve ser um ponto fulcral para sistemas de armazenamento com fortes garantias de durabilidade e coerência. na literatura, os sistemas capazes de voltar a um estado coerente após uma falha de energia são chamados de crash-consistent. muito do trabalho relacionado foca-se na crash consistency dos sistemas de ficheiros e, quando se foca nas aplicações, verifica-se a falta de ferramentas para reprodução de bugs. quando um utilizador reporta problemas encontrados numa dada aplicação, é útil para as equipas de desenvolvimento terem uma ferramenta que rapidamente reproduz o bug e ajude na sua correção. esta dissertação propõe o lazyfs+, um sistema de ficheiros que simula a perda total e parcial de dados através da injeção de faltas baseada em software. a nossa solução possui uma cache interna, o que permite uma gestão determinística dos dados das aplicações e a injeção de faltas reprodutível. um dos seus pontos fortes é o facto de imitar comportamentos que alguns sistemas de ficheiros apresentam, como a reordenação de escritas, sem se prender a uma implementação específica. para além disso, facilita a análise das operações executadas pela aplicação e permite obter informação sobre os dados não sincronizados. o lazyfs+ provou-se útil através da reprodução de bugs já conhecidos e da identificação desses mesmos bugs em versões antigas das aplicações, nunca antes reportados. para além disso, foi possível encontrar novos bugs, quer em versões antigas, quer em versões mais recentes das aplicações. estes últimos foram reportados com todos os passos de reprodução realizados com o lazyfs+. como resultado, atualmente este está a ser integrado nos testes do sistema de armazenamento chave-valor etcd.",
      "the digital era has brought the incorporation of computers into various sectors of our daily lives. over the years, there has been an increasing dependence on technology, which translates into the need for increasingly resilient, fast, secure and available systems. consequently, the complexity of systems has been growing, which leads to the implementation of data persistence and update mechanisms that are more difficult to test. to increase the level of difficulty, the different file systems have particularities that significantly affect these mechanisms and the consistency of applications after power and operating system failures. avoiding total or partial loss of data must be a key point for storage systems with strong guarantees of durability and consistency. in the literature, systems capable of returning to a consistent state after a power failure are called crash consistent. much of the related work focuses on the crash consistency of file systems, and when it focuses on applications, there is a lack of tools for reproducing bugs. when a user reports issues encountered in a particular application, it is useful for development teams to have a tool that quickly reproduces the bug and helps with its fix. this dissertation proposes lazyfs+, a file system that simulates total and partial data loss through software-based fault injection. our solution has an internal cache, which allows deterministic management of application data and reproducible fault injection. one of its strengths is the fact that it imitates beha viors that some file systems present, such as the reordering of writings, without being tied to a specific implementation. furthermore, it facilitates the analysis of the operations carried out by the application and allows to obtain information about unsynchronized data. lazyfs+ proved useful by reproducing already known bugs and identifying these same bugs in older versions of applications, never before reported. furthermore, it was possible to find new bugs, both in old and newer versions of the applications. the latter were reported with all reproduction steps performed with lazyfs+. as a result, this is currently being integrated into testing of the etcd key-value store."
    ],
    [
      "the increasing pervasiveness and lower cost of electronic devices equipped with sensors is leading to a greater and cheaper availability of localized information. the advent of the internet has brought phenomena such as crowd-sourced maps and related data. the combination of the availability of mobile information, community built maps, with the added convenience of retrieving information over the internet creates the opportunity to contextualize data in new ways. this work takes that opportunity and attempts to generalize the detection of driving events which are deemed problematic as a function of contextual factors, such as neighbouring buildings, areas, amenities, the weather, and the time of day, week or month. in order to research the problem at hand, the issue is first contextualized properly, providing an overview of important factors, namely smart cities, data fusion, and machine learning. that is followed by a chapter concerning the state of the art, that showcases related projects and how the various facets of road traffic expression are being approached. the focus is then turned to creating a solution. at first this consists in aggregating data so as to create a richer context than would be present otherwise, this includes the retrieval from different services, as well as the composition of a unique view of the same driving situation with new dimensions added to it. and then models were created using different machine learning methods, and a comparison of results according to selected and justified evaluation metrics was made. the compared methods are decision tree, naive bayes, and support vector machine. the different types of information were evaluated on their own as potential classifiers and then were evaluated together, leading to the conclusion that the various types combined allow for the creation of better models capable of finding problems with more confidence in such results. according to the tests performed the chosen approach can improve the performance over a baseline approach and point out problematic situations with a precision of over 90%. as expected by not using factors concerning the driver state or acceleration the scope of problems which are detected is limited in domain.",
      "a expansão e menor custo de dispositivos eletrónicos equipados com sensores está a levar a uma maior e mais barata disponibilidade de informação localizada. o advento da internet criou fenómenos como a criação de mapas e dados relacionados gerados por comunidades. a combinação da disponibilidade de informação móvel e mapas construídos pela comunidade, em conjunto com uma obtenção de informação através da internet mais conveniente, criou a oportunidade de contextualizar os dados de novas maneiras. este trabalho faz uso dessa oportunidade e tenta generalizar eventos de condução que são considerados problemáticos em função de factores contextuais, tais como a presença de edifícios, áreas, e comodidades na vizinhança, o clima, e a hora do dia, a semana, ou o mês. de modo a investigar esta questão, o problema é contextualizado como emergente no tópico de cidades inteligentes, e explorado com recurso a fusão de dados e a aprendizagem máquina. o estado da arte é exposto, através de projectos relacionados à expressão do tráfego rodoviário, dando relevo às várias facetas até então investigadas por outros autores de modo a enquadrar o trabalho presente. dado o enquadramento e concretização do problema, é proposta uma solução. esta solução passa por inicialmente agregar dados de modo a enriquecer o contexto, incluindo a recolha destes de vários serviços, e uma composição dos dados recolhidos numa perspectiva única referente a uma situação de condução. após este enriquecimento dos dados, são criados modelos com base em diferentes técnicas de aprendizagem máquina. os métodos utilizados são decision tree, naive bayes, e support vector machine. os resultados conseguidos com estes modelos são depois comparados de acordo com as métricas de avaliação seleccionadas. uma comparação foi feita também com diferentes tipos de informação separadamente e também em conjunto, levando à conclusão de que os vários tipos combinados permitem a criação de melhores modelos capazes de encontrar problemas com mais confiança nos resultados produzidos. de acordo com os testes executados a abordagem escolhida consegue melhorar resultados de um modelo base e descobrir situações problemáticas de condução com uma precisão acima dos 90%. no entanto, como seria de esperar, o âmbito dos problemas detectados tem um domínio limitado aos aspectos seleccionados."
    ],
    0.06666666666666667
  ],
  [
    [
      "a educação e o sucesso académico são de grande relevância na sociedade atual, nomeadamente para o futuro profissional, económico e pessoal dos jovens. tendo isto em conta, é de grande interesse e vital importância para as instituições de ensino poder prever a variação das notas dos alunos, principalmente alunos em risco de reprovação, uma vez que se podem alterar métodos de ensino e aplicar medidas corretivas e estratégias de intervenção para apoiar alunos de baixo desempenho, tendo em conta as suas necessidades. a introdução de inteligência artificial e de técnicas de fusão de dados nesta área pode ser muito interessante, podendo melhorar a eficiência na deteção de alunos em risco de insucesso escolar. deste modo, esta dissertação visa o desenvolvimento de dois casos de estudo onde se pretende o desenvolvimento de algoritmos e procedimentos relativos à fusão e integração dos dados e a criação de modelos preditivos. no primeiro caso de estudo, é proposto desenvolver modelos de previsão para prever variações nas notas de alunos do ensino básico nas disciplinas de português e matemática do primeiro para o segundo período, através da implementação da técnica de early fusion. como segundo caso de estudo desta dissertação, propôs-se o desenvolvimento de modelos preditivos para a previsão da variação das notas de alunos do ensino secundário nas disciplinas de português e matemática do segundo para o terceiro período do 12º ano, havendo, neste processo, a implementação de duas técnicas de fusão de dados - early fusion e late fusion. diante dos melhores modelos candidatos obtidos, comprovou-se que a fusão de dados obtém um bom desempenho na criação de modelos preditivos para a previsão da variação de notas, e que ambas as técnicas de fusão testadas são competentes, aparentando melhorar os resultados da previsão relativamente a modelos criados a partir dos conjuntos de dados de forma separada. palavras-chave fusão de dados, machine learning, insucesso escolar, early fusion, late fusion, edu cação, inteligência artificial",
      "education and academic success are of great importance in today’s society, particularly for the professional, economic and personal future of young people. with this in mind, it is of great interest and vital importance for educational institutions to be able to predict the variation in students’ grades, especially students at risk of failing, since teaching methods can be changed and corrective measures and intervention strategies can be applied to support underperforming students, taking their needs into account. the introduction of artificial intelligence and data fusion techniques in this area could be very interesting and could improve efficiency in detecting students at risk of school failure. this dissertation therefore aims to develop two case studies in which algorithms and procedures are developed for merging and integrating data and creating predictive models. in the first case study, it is proposed to develop predictive models to predict variations in the grades of middle school students in portuguese and mathematics from the first to the second term, by implementing an early fusion technique. as the second case study of this dissertation, we proposed the development of predictive models for predicting variations in high school students’ grades in portuguese and mathematics from the second to the third term of the 12th grade. in this process, two data fusion techniques were implemented - early fusion and late fusion. in view of the best candidate models obtained, it was proven that data fusion performs well in creating predictive models for predicting grade variations, and that both fusion techniques tested are competent, seemingly improving prediction results compared to models created from separate datasets."
    ],
    [
      "microglia are a type of glial cell residing in the central nervous system and represent about 10 to 15% of the brain cell population. these cells don’t produce electrical impulses and are responsible for fundamental physiological and pathological processes, as they represent the first line of immune defence within the central nervous system. thus, the quantification of these cells is essential in a clinical context, as it allows better monitoring and planning of treatments for different pathologies. conventional cell counting involves a specific set of tools and devices developed for this purpose. this process is time-consuming and imprecise due to being heavily dependent on the operator. currently, most processes are performed manually. however, other approaches have been studied and developed to improve the counting process, making it less time-consuming, more efficient and reduce the error associated with factors external to the counting. that said, the objective of this dissertation is to study the best approach to automate the quantification of microglial cells, ranging from classical to deep learning methodologies. combined with the appropriate image processing and analysis techniques, the classical approach proves to be an adequate solution. however, in recent years, approaches based on deep learning have shown promising performance in various image analysis tasks, such as classification, detection and segmentation. the approaches developed to automate the quantification process were tested on a set of images built in partnership with researchers from the school of medicine of the university of minho. as for the classical methodology approach, a protocol was developed within imagej, which was combined with image processing techniques that allowed the automation of the counting process. based on convolutional neural networks, the classification problem referring to a deep learning methodology obtained an accuracy of 0.9021 and managed to classify the 661 images in 5 minutes and 44 seconds. the two approaches, considered optimal within each methodology, are competitive with the state-of-the-art methods, as they allowed for the automation of the quantification process, and showed a significant improvement in reproducibility, efficiency and reduced error associated with human factors.",
      "a microglia é um tipo de célula glial residente no sistema nervoso central e representa cerca de 10 a 15% da população de células cerebrais. estas células não produzem impulsos elétricos, são responsáveis por processos fisiológicos e patológicos fundamentais, e representam a primeira linha de defesa dentro do sistema nervoso central. assim, a quantificação destas células é fundamental num contexto clínico, pois permite uma melhor monitorização e planeamento de tratamentos para diversas patologias. a contagem convencional de células envolve um conjunto específico de ferramentas e dispositivos desenvolvidos para esse fim. este processo é demorado e impreciso devido a estar bastante dependente do operador. atualmente, a maioria dos processos são feitos manualmente. no entanto, outras abordagens têm sido estudadas, com o intuito de melhorar o processo de contagem, para tornar o mesmo menos demorado, mais eficiente e reduzir o erro associado a fatores externos à contagem. posto isto, o objetivo desta dissertação é o de estudar a melhor abordagem para automatizar a quantificação de células microgliais indo desde os métodos clássicos aos de deep learning. combinado com as devidas técnicas de processamento e análise de imagem, a abordagem clássica mostra-se uma solução adequada. contudo, nos últimos anos, abordagens baseadas em deep learning evidenciaram um desempenho promissor em várias tarefas de análise de imagens, como classificação, deteção e segmentação. as abordagens desenvolvidas para automatizar o processo de quantificação foram testadas num conjunto de imagens construído em parceria com elementos da escola de medicina da universidade do minho. quanto à abordagem da metodologia clássica, foi desenvolvido um protocolo dentro do imagej, que aliado com técnicas de processamento de imagem permitiu automatizar o processo de contagem. com base em redes neuronais convolucionais, o problema de classificação referente a uma metodologia de deep learning obteve uma accuracy de 0.9021 e conseguiu classificar as 661 imagens em 5 minutos e 44 segundos. as duas abordagens, consideradas ótimas dentro de cada metodologia, são competitivas com os métodos do estado da arte, pois permitiram automatizar o processo, mostraram uma significativa melhoria na reprodutibilidade e eficiência."
    ],
    0.3
  ],
  [
    [
      "quantum computing is an emergent field that brings together quantum mechanics, computer science and information theory, which promises improvements to classical algorithms such as simulation of quantum systems, cryptography, data base searching and many others. among these algorithms, quantum walks may provide a quadratic speed up when compared to their classical counterparts, allowing improvements to applications such as element distinctness, searching problems, matrix product verification and hitting times in graphs. the present work offers a general theoretical overview, simulation and circuit implementation of the coined, staggered and continuous-time quantum walk models. the first two chapters of this thesis are dedicated to the definition of the theoretical framework, simulation in python and comparison of the aforementioned quantum walk models for the simple case of the dynamics in a line graph and for the search algorithm in a complete graph. this is then used as a benchmark for the final chapter, devoted to building and testing the circuits corresponding to models mentioned above in ibm’s qiskit. a main contribution of this dissertation concerns the circulant graph approach to diagonal operators for continuous-time quantum walks.",
      "a computação quântica é uma área emergente, que junta os campos de mecânica quântica, ciências da computação e teoria da informação, com a promessa de melhoramentos a algoritmos clássicos tais como a simulação de sistemas quânticos, criptografia, busca em base de dados, e outros. entre estes algoritmos, as caminhadas quânticas surgem com um ganho quadrático de complexidade em comparação às caminhadas clássicas, possibilitando melhor desempenho em aplicações como distinção de elementos, problemas de busca, verificação de produtos de matrizes e tempos de alcance em grafos. o trabalho atual oferece uma visão geral de um ponto de vista teórico, de simulação e de implementação de circuitos, relativos aos modelos de caminhadas quânticas com moeda, escalonadas e contínuas no tempo. os primeiros dois capítulos desta tese são dedicados à definição da estrutura teórica, simulação em python e comparação dos modelos supracitados, para o caso simples da dinâmica na linha, e para o problema de busca num grafo completo. isto será então utilizado como referência para o capítulo final, dedicado à construção e teste dos circuitos correspondentes aos modelos supracitados. uma contribuição principal desta dissertação diz respeito à abordagem de grafos circulantes para realização de caminhadas quânticas continuas no tempo."
    ],
    [
      "as redes de ecrãs públicos de área alargada estão-se a tornar um paradigma emergente e representam uma transformação radical em relação à maneira como encaramos a disseminação da informação em locais públicos. estas redes com o sua natureza ubíqua levantam alguns desafios para quem tem que as desenhar, instalar e usar. é bastante importante perceber quais são as principais compromissos a assumir quanto ao desenho das redes de ecrãs, principalmente em relação aos seus componentes e respectivos protocolos, para desta forma podermos oferecer uma rede aberta, global e sobretudo escalável. a partir destas ideias o trabalho de caracterizar os componentes de rede é um dos pontos essenciais para alcançar um desenvolvimento fundamentado deste sistema. também é fundamental ter uma avaliação dos desenvolvimentos respeitantes à desempenho do sistema e à forma como o aumento do numero de intervenientes no mesmo afecta o seu comportamento. assim este trabalho, complementando essa caracterização e classificação inicial, pretende desenvolver uma ferramenta que permita às demais equipas multidisciplinares criar cenários e modelos de simulação para confirmar se as suas decisões quanto aos padrões a implementar são os que melhor se adequam aos requisitos destas redes.",
      "large-scale pervasive public displays networks are becoming an emerging paradigm and represent a radical transformation in the way we think about information dissemination in public spaces. these networks with its pervasive nature rise a number of challenges for those who have to design, test, deploy and use this kind of networks. it is imperative to understand what are the key tradeoffs in the design of pervasive displays networks, mainly on their components and respective protocols, in order to provide a fully open, global and most importantly scalable displays network. starting from these ideas the work of characterize the network components is a key step to accomplish a well grounded development of the system. also the assessment of those developments regarding the performance of the system and how the increasingly number of elements changes its behavior is imperative. thus this work, in addition of that initial characterization and classification, tries to develop a tool to enable multidisciplinary teams create scenarios and simulation models to confirm if their design patterns are the ones that better suite the requirements of a pervasive displays network."
    ],
    0.0
  ],
  [
    [
      "with the evolution of smartphones and the growing number of its users, mobile applicati ons are regularly used by more and more people. because of this growing use of mobile applications, it is critical to ensure their quality. the graphical user interface (gui) is a very relevant component in these applications, since it enables the user to interact with the avai lable resources. a malfunctioning of it may make it impossible for the application to work properly and, consequently, for the software system to be invalidated. one way to ensure a smooth operation of the system is by performing software tests. model-based testing (mbt) is a black-box technique that checks whether software has the expected behavior. the mbt focuses on generating and running tests from a system under test (sut) model. this dissertation continues the development of an mbt tool called tom. after validating the web components of said tool, we have now developed a component of generation and execution of test cases for android applications. in the course of the dissertation we show case the various decisions and changes made in the tom tool during the implementation of this new component, presenting at the end two case studies to prove the operation of the android component.",
      "com a evolução dos smartphones e o crescente número dos seus utilizadores, as aplicações móveis são utilizadas regularmente por cada vez mais pessoas. devido a este crescente uso de aplicações móveis, é fundamental garantir a sua qualidade. a interface gráfica do utilizador (gui) é uma componente muito relevante nestas aplicações, pois possibilita a interação do utilizador com os recursos disponíveis. um mau funcionamento da mesma pode impossibilitar o bom funcionamento da aplicação e consequentemente, do sistema de software. uma forma de garantir um bom funcionamento do sistema é através da realização de testes de software. os testes baseados em modelos (mbt) são uma técnica black-box que verifica se um software tem o comportamento esperado. o mbt foca-se na geração e execução de testes a partir de um modelo do sistema sob teste (sut). esta dissertação continua o desenvolvimento de uma ferramenta de mbt denominada tom. após o sucesso apresentado na parte web por esta ferramenta, desenvolveu-se agora uma componente de geração e execução de casos de teste para aplicações android. no decorrer da dissertação, encontram-se as varias decisões tomadas e alterações efetuadas na ferramenta tom durante a implementação desta nova componente, sendo apresentado no final dois casos de estudo para comprovar o funcionamento da componente android."
    ],
    [
      "every day, huge amounts of data are generated in the healthcare environments from several sources, such as medical sensors, emrs, pharmacy and medical imaging. all of this data provides a great opportunity for big data applications to discover and understand patterns or associations between data, in order to support medical decision-making processes. big data technologies carry several benefits for the healthcare sector, including preventive care, better diagnosis, personalized treatment to each patient and even reduce medical costs. however, the storage and management of big data presents a challenge that traditional data base management systems can not fulfill. on the contrary, nosql databases are distributed and horizontally scalable data stores, representing a suitable solution for handling big data. most of medical data is generated from sensor embedded devices. the concept of iot, in the healthcare environment, enables the connection and communication of those devices and other available resources over the internet, to perform or help in healthcare activities such as diagnosing, monitoring or even surgeries. iot technologies applied to the healthcare sector aim to improve the access and quality of care for every patient, as well as to reduce medical costs. this master thesis presents the integration of both big data and iot concepts, by developing an iot platform designed for data collection and analysis for medical sensors. for that purpose, an open source platform, kaa, was deployed with both hbase and cassandra as nosql database solutions. furthermore, a big data processing engine, spark, was also implemented on the system. from the results obtained by executing several performance experiments, it is possible to conclude that the developed platform is suitable for implementation on an healthcare environment, where huge amounts of data are rapidly generated. the results also made it possible to perform a comparison between the performance of the platform with cassandra and hbase, showing that the last one presents slightly better results in terms of the average response time.",
      "atualmente, uma grande quantidade de dados é gerada todos os dias em ambientes hospitalares provenientes de diversas fontes, como por exemplo sensores médicos, registos eletrónicos, farmácias e imagens médicas. todos estes dados proporcionam uma grande oportunidade para aplicações de big data, permitindo revelar e interpretar padrões ou associações entre os dados de forma a auxiliar no processo de tomada de decisão médica. as tecnologias de big data comportam diversos benefícios para o sector de saúde, incluindo a prestação de cuidados preventivos, diagnósticos mais eficientes, tratamento personalizado para cada paciente e até mesmo reduzir os custos médicos. no entanto, o armazenamento e a gestão da big data apresenta um desafio que os sistemas de gestão de base de dados tradicionais não são capazes de ultrapassar. não obstante, as bases de dados nosql representam uma solução de armazenamento de dados distribuída e escalável horizontalmente, sendo, portanto, apropriadas para lidar com big data. uma grande parte dos dados médicos é gerada através de dispositivos embebidos com sensores. o conceito de iot, no ambiente das unidades de saúde, permite a conexão e comunicação desses dispositivos e outros recursos disponíveis através da internet, de forma a realizar ou auxiliar nas atividades de saúde, como por exemplo o diagnóstico, a monitorização ou atá mesmo em cirurgias. as tecnologias iot visam melhorar o acesso e qualidade dos cuidados de saúde para todos os pacientes, bem como reduzir os custos na prestação dos mesmos. esta tese de mestrado apresenta, assim, a integração de ambos os conceitos de big data e iot, propondo o desenvolvimento de uma plataforma projetada para a recolha e análise de dados de sensores médicos. para essa finalidade, foi utilizada uma plataforma iot de código aberto, kaa, juntamente com duas bases de dados nosql, hbase e cassandra. adicionalmente, foi também implementado um mecanismo de processamento de dados, também de código aberto, o spark. com base nos resultados obtidos através da realização de diversas experiências de avaliação de desempenho, foi possível concluir que a plataforma desenvolvida é adequada para a implementação em ambientes de prestação de cuidados de saúde, onde grandes quantidades de dados são rapidamente geradas. os resultados permitiram também realizar uma comparação entre o desempenho da plataforma com cassandra e com hbase, realçando que esta última apresenta resultados ligeiramente melhores em termos do tempo médio de resposta."
    ],
    0.12857142857142856
  ],
  [
    [
      "a curva roc (receiver operating characteristic) é uma ferramenta analítica eficaz para testes clínicos. a análise permite visualizar a variação de sensibilidade e especificidade para uma dada região de corte através de um simples, mas robusto gráfico bidimensional. num contexto biológico, testes podem ser influenciados por múltiplas variáveis externas e como tal a análise roc pode não ser a ideal ou gerar resultados incompletos. é então necessário saber que variáveis afetam determinado teste clínico de forma a determinar os melhores parâmetros para determinado teste ou até descartar determinada metodologia mediante a situação. o ajuste da curva roc a covariáveis permite a normalização do efeito das mesmas ou diretamente ajustar a curva para os seus efeitos. software direcionado ao ajuste da curva roc é, infelizmente, escasso e muitas vezes difícil de manusear por utilizadores não especializados. recentemente o pacote aroc foi lançando para r que disponibiliza vários recursos para estes ajustamentos, no entanto a dificuldade de utilização mantém-se. a combinação deste pacote com a estrutura shiny, um pacote que permite o desenvolvimento de aplicações interativas, tem por objetivo a criação de um programa grátis e acessível que permita uma análise mais aprofundada disponível para todos os investigadores. realroc foi capaz de replicar resultados de um caso de estudo que analisou a influência do sexo no sistema de pontuação crib e respetiva previsão de mortalidade, demonstrando a usabilidade e acessibilidade do programa que será disponibilizado online e potencialmente contribuir para novos desenvolvimentos na área.",
      "receiver operating characteristic (roc) curves are a powerful analytical tool for clinical tests. the analysis allows the visualization of varying sensitivity and specificity for a given threshold through a simple, yet robust, two-dimensional plot. in a biological framework, tests can be influenced by multiple external variables, as such, standard roc analysis may not be suitable or may provide incomplete data. it is then necessary to know which variables influence clinical test results to determine optimal conditions for trials or even to disregard a given method of evaluation in certain contexts. adjusting for covariates allows roc analysis to normalize the effects of the variable in question or to directly adjust the curve for its effects. unfortunately roc software that is able to conduct such an adjustment is sparse and proven difficult to use for non technical users. recently, the aroc package for r was released and provides a robust resource for such adjustments however with he same usability problems previously stated. by combining this package with the shiny framework, an r package that allows the creation of interactive applications, we hope to provide an accessible and free software that allows this extra depth of analysis to be available for all researchers. realroc was able to mimic the results of a case study analysing the affects of sex to the crib score and resulting mortality rates that proving its practicality and will be made available online and hopefully contribute to the advancement of software in this field."
    ],
    [
      "o desenvolvimento de aplicações e serviços baseados em web está a crescer todos os dias cada vez mais. as facilidades que nos oferecem, entre elas a alta-disponibilidade e acessibilidade, levou a que as grandes empresas de tecnologia investissem neste tipo de tecnologias, surgindo assim aplicações como o evernote, o google photos, o dropbox, o slack, entre outras. associadas à utilização constante destas aplicações e serviços pelos seus clientes estão as enormes quantidade de dados criados, bem como os dados gerados a partir destes. com a necessidade de armazenar e processar esses de forma rápida e eficiente, estes serviços tem vindo a optar pela utilização de serviços de computação em nuvem de terceiros. existem vantagens claras associadas à migração de dados para estas plataformas, desde a redução de custos associados armazenamento, manutenção e compra de infraestruturas, até às conveniências oferecidas pela disponibilização ferramentas de monitorização e configuração avançadas, entre muitas outras. associado também à utilização desta plataformas de cloud computing estão também os problemas com a privacidade dos dados por elas armazenadas. apesar dos esforços, por parte dos fornecedores destes serviços, em negar o acesso a entidades não autorizadas, existem ameaças fora do seu controlo e temos visto muitas vezes que o acesso a dados sensíveis por terceiros tem um risco elevado associado. com vista a combater este aspeto existem hoje em dia soluções capazes de garantir a confidencialidade dos dados em bases de dados relacionais e não relacionais, através de técnicas criptográficas. estas soluções estão usualmente associadas a arquiteturas específicas de forma a precaverem sempre esta questão de segurança dos dados em todos os momentos. estas arquiteturas implicam um maior esforço computacional do lado do cliente, pois é desse lado que se encontra toda a lóogica da aplicacional e mecanismos de segurança. esta dissertação oferece uma nova arquitetura web onde maior parte do trabalho aplicacional é delegado para as infraestruturas de nuvem maximizando assim o desempenho da aplicação, tirando para isso partido da arquitetura browser servidor característica destes sistemas.",
      "the development of web-based applications and services is growing every day like never before. the facilities that we offer, such as high availability and accessibility, have led large technology companies to invest in this type of technology, resulting in applications such as evernote, google photos, dropbox, slack, among others. associated with the constant use of these applications and services by their customers are the huge amount of data created, as well as the data generated from them. with the need to store and process these data quickly and efficiently, these services have been choosing to use third-party cloud computing services. there are clear advantages associated with data migration for these platforms, from the cost savings associated with storage, maintenance and purchase of infrastructures, to the conveniences offered by the provision of advanced monitoring and configuration tools, among many others. also associated with the use of this cloud computing platforms are the problems with the privacy of the data they store. in order to combat this aspect, there are solutions nowadays capable of guaranteeing the confidentiality of the data in relational and non-relational databases, through cryptographic techniques. these solutions are usually associated with specific architectures in order to always guard this data security issue at all times. these architectures are in themselves associated with a greater computational effort on the client side, since it is on this side that all the logic of the application and security mechanisms are found. this dissertation offers a new web architecture where the most part of the application workload is delegated to the cloud infrastructures maximizing application performance, taking for that advantage of the browser architecture."
    ],
    0.3
  ],
  [
    [
      "a grande quantidade de dados gerada todos os dias na indústria, e nomeadamente, na área da saúde, impulsiona a utilização de tecnologias de informação (tis) para o seu registo, tratamento e exploração, com o intuito de adquirir conhecimento com valor assim como instrumentos para o apoio na tomada decisões. na unidade de cuidados materno-infantis do centro hospitalar do porto (chp), o centro materno infantil do norte (cmin), os pro ssionais de saúde lidam com utentes em condições delicadas e situações que requerem a tomada de medidas rápida e e ciente. um vasto conhecimento dos processos de ginecologia e obstetrícia (go) pode ser crucial para promover as boas práticas médicas e evitar eventos adversos na mãe e no recém-nascido. a aplicação bem sucedida de sistema de apoio à decisão clínica (sadc) em ambiente clínico e a recetibilidade dos pro ssionais de saúde e de ti do cmin à introdu ção de novos conceitos e tecnologias, motivam o desenvolvimento de novos artefactos. neste sentido, este projeto aplica os conceitos de business intelligence (bi) e descoberta de conhecimento sobre informação disponível nos sistemas de informação (sis) da instituição, através de uma grande variedade de metodologias, métodos e tecnologias para construir soluções e apoiar os serviços prestados na unidade de cuidados materno-infantis. um dos artefactos criados é o desenvolvimento de indicadores clínicos e de desempenho a partir da tecnologia de bi pentaho community edition (ce). este processo inicia-se com a extração, transformação e carregamento da informação numa estrutura multidimensional, o data warehouse (dw), permitindo a posterior representação da informação através da aplicação de bi. esta plataforma integra indicadores dos módulos de go, de triagem e da admissão hospitalar do cmin. na componente da descoberta de conhecimento, vários estudos são efetuados, tendo em conta as preocupações dos pro ssionais de saúde e as necessidades da instituição, provando a viabilidade de utilizar técnicas de data mining (dm) na construção de modelos de previsão no sector da saúde. os nascimentos pré-termo, a seleção do tipo de parto mais adequado, a categoriza ção das utentes e o seu percurso pela unidade de go e os tempos de espera de pré e pós-triagem são alguns dos problemas analisados. os estudos alcançam resultados promissores e clinicamente relevantes, permitindo a identi cação de fatores de risco clínico. a título de exemplo, adquiriam-se modelos de previsão para os nascimentos pré-termo com valores de sensibilidade e especi cidade de 89% e 93%. de forma a presentear os pro ssionais de saúde com as soluções desenvolvidas, foi criada uma plataforma de alto-nível que integra os produtos de bi e os modelos de previsão de dm, culminando o objetivo do projeto num artefacto nal, que visa o apoio às práticas clínicas, a qualidade dos cuidados prestados e a consequente satisfação dos utentes.",
      "in industry, particularly in the health sector, there has been an explosive growth in the number of data produced every day. the information technologies arise to extract, process and explore these large quantities of information, aiming to get real knowledge and assist the decision making process. in centro materno infantil do norte (cmin), the maternal and perinatal care unit of centro hospitalar do porto (chp), the physicians handle patients in di cult conditions and situations that require fast and e cient measures. a vast knowledge on the gynecology and obstetrics (go) processes can be crucial to promote decent medical practices and avoid adverse events im mother and child. the successful application of decision support systems (dsss) in a clinical environment and the physician's acceptance on the introduction of new technologies encourage the development of new artifacts. accordingly, the current project applies business intelligence (bi) and knowledge discovery concepts in data available from cmin, building solutions to support health services and provide the maternity unit with quality care. one of the artifacts consists of clinical and performance indicators developed with pentaho community edition (ce). by means of the extract, transform and load (etl) process, a multidimensional strucuture called data warehouse (dw) is built, allowing the later data representation by applying bi. the plataform compose go indicators on the child delivery, the triage process and the hospital admission, in cmin. in the knowledge discovery component, many studies are conducted, regarding the physicans' concerns and the healthcare needs, showing the feasability of using data mining (dm) methods to create prediction models, in the health sector. some of the considered business problems are the pre-term birth, the most appropriate type of delivery, the categorization of the maternity patients and their route through the go services and the pre-triage and triage waiting time in the emergency unit of cmin. the studies attain promissing and clinically relevant results, allowing the identi cation of risk factors in clinical environment. for instance, in the prediction of preterm births, dm models presenting sensitivity and speci city of 89% and 93% were aquired. in order to provide physitions with the developed solutions, a high-level plataform is nally built, deploying the bi products and the dm predicting models, culminating the project goal in a nal artifact, which supports clinical practices, improves the quality of services and, ultimatly, increases the patients' satisfaction."
    ],
    [
      "as redes sociais tornaram-se na nova forma de comunicar e, consequentemente, uma importante fonte de informação. mais concretamente, o twitter, desde a sua criação, tornou-se numa das redes sociais mais utilizadas. esta popularidade permitiu um aumento do número de investigações na área de text mining usando o twitter para diferentes aplicações, como saúde e política. nesta área, a classificação de documentos tem sido aplicada a vários dados, nomeadamente tweets, para analisar tendências, entender o comportamento humano e prever determinados eventos. no entanto, nem sempre é possível ter os datasets desejados para efectuar essa classificação e análise. para resolver o problema encontrado, esta dissertação, proposta pela omniumai, pretende explorar as abordagens já existentes para a extração e classificação de dados do twitter, focando-se principalmente na língua portuguesa. para isso, foi desenvolvida uma api capaz de extrair tweets de acordo com um determinado tópico de interesse, e criar datasets classificados automaticamente com labels de relevância. foi ainda desenvolvida uma pipeline de classificação de tweets com base nas abordagens de deep learning encontradas no estado de arte para a classificação de documentos. o produto final consiste numa framework, twitter observatory, que permite aos utilizadores criar datasets de acordo com um determinado tópico de interesse e analisar esses mesmos datasets. para testar a framework desenvolvida, foram selecionados dois casos de estudo: covid-19 e a invasão russa da ucrânia em 2022. relativamente a estes dois tópicos, dois datasets foram extraídos e classificados de acordo com a relevância dos tweets, contendo, respetivamente, 2,268,575 e 219,887 tweets em português. foi feita uma análise exploratória destes dados e os resultados de classificação usando modelos de deep learning foram apresentados. para validar esses resultados, foi utilizado o dataset existente crisislex, traduzido para português.",
      "social media have become the new form of communication and, therefore, an important source of information. more specifically, twitter, since its foundation, became one of the most used social media platforms. its popularity enabled the creation of an enormous amount of content, and a lot of research has been done using twitter in different areas, such as health and politics. in the text mining field, document classification has been applied to twitter to analyse trends, human behaviour or predict some events. however, it is not always possible to have the desired datasets to perform the classification and analysis. to solve the problem described, this dissertation, proposed by omniumai, aims to explore existing approaches to extract and classify twitter data, in particular regarding the portuguese language. for that, it was developed an api capable of extracting tweets according to a given topic of interest, and creating datasets automatically classified with relevance labels. a classification pipeline of tweets was also devel oped based on the deep learning approaches found in the state of the art for document classification. the final product consists of a framework, twitter observatory, that allows users to create datasets according to a particular topic of interest and analyse those datasets. to test the developed framework, two case studies were selected: covid-19 and the russian invasion of ukraine in 2022. regarding these two topics, two datasets were extracted and automatically labelled according to the relevance of the tweets, containing, respectively, 2,268,575 and 219,887 tweets in portuguese. an exploratory analysis of this data was performed and the classification results using deep learning models were presented. to validate those results, it was used an existing dataset, the crisislex dataset, translated into portuguese."
    ],
    0.0
  ],
  [
    [
      "the fast fourier transform is a family of algorithms indispensable for the computation of the discrete fourier transform. as a result, these transforms are the core of many applications in several areas and are required to be computed efficiently in many scenarios. the continuous evolution of gpus has increased the popularity of parallelizable algorithm implementations on this type of hardware. traditionally gpus were associated to graphics background, however, with the popularization of the compute functionality of this hardware, most modern gpus now have this capability, hence, algorithms now are more likely to be implemented in the general-purpose compute pipeline of gpus. as a result, many applications take advantage of compute programming in gpgpu-capable frameworks such as glsl, a high-level shading language frequently used in the context of computer graphics. in this dissertation we provide, refine and compare gpu-driven implementations of the family of fft algorithms in glsl, with the goal to provide programmers with efficient and simplified compute kernels for this transform, from the classic cooley-tukey algorithm to more suitable algorithms for the gpu such as the stockham algorithm with higher radix. accordingly, we also use the cufft nvidia framework for reference in the comparisons of the glsl algorithms implementations with the goal to analyse their significance on the tradeoff of using specialized implementations of the fft algorithms or integrating dedicated software tools for any case of application. finally, we demonstrate how all improvements discussed in this dissertation culminate in performance improvement in a real-time rendering technique that heavily depends on multiple of these transforms in the nau3d engine as a case of study.",
      "a transformada rápida de fourier é um algoritmo ou uma família de algoritmos indispensáveis para o cálculo da transformada discreta de fourier. assim, essas transformadas são o núcleo de muitas aplicações em diversas áreas e precisam ser calculadas de forma eficiente em muitos cenários. a evolução contínua dos gpus aumentou a popularidade das implementações de algoritmos paralelizáveis neste tipo de hardware. tradicionalmente, os gpus eram associadas ao fundo gráfico, no entanto, com a popu larização da funcionalidade de compute desse hardware, os gpus mais modernos agora têm essa capacidade, portanto, os algoritmos agora são mais propensos a serem implementados na compute pipeline de propósito geral dos gpus. como resultado, muitas aplicações aproveitam a programação em compute em frameworks compatíveis com gpgpu como glsl, uma linguagem de shading de alto nível usada recorrentemente no contexto de computação gráfica. nesta dissertação fornecemos, refinamos e analisamos implementações em gpu da família de algoritmos fft em glsl, com o objetivo de fornecer aos programadores compute kernels eficientes e simplificados para esta transformada, desde o clássico algoritmo de cooley-tukey até algoritmos mais adequados para o gpu. da mesma forma, também usamos a framework cufft nvidia como referência nas comparações das implementações dos algoritmos em glsl com o objetivo de analisar a sua importância no tradeoff entre usar implementações especializadas dos algoritmos fft ou integrar ferramentas de software dedicadas para qualquer caso de aplicação. por fim, demonstramos como todas as melhorias discutidas nesta dissertação culminam na melhoria de desempenho numa técnica de renderização em tempo real que depende de ffts na engine nau3d como caso de estudo."
    ],
    [
      "a gestão de equipamentos de redes de acesso é um problema complexo e pode tomar dimensões que tornam o tratamento individual de cada equipamento um conceito pouco interessante quer do ponto de vista económico quer do ponto de vista logístico. na altice labs foi desenvolvida uma solução para gerir os equipamentos das redes de acesso: o agora, um network management system. este sistema é modular e escalável, razões pelas quais é constituído por vários componentes que configuram o modelo fcaps model (fcaps) da gestão de infraestruturas de rede. este modelo define as categorias de falha, configuração, responsabilidade, desempenho e segurança. [19] neste contexto foi desenvolvido no agora um módulo de automatização. o módulo zero touch provision (zero touch provision (ztp)) foi desenvolvido com o intuito de automatizar o aprovisionamento de optical line terminal (olt)’s. este módulo faz uso de templates, baseados em playbooks ansible, e que são adaptados ao cenário de cada cliente. este módulo enquadra-se na categoria de configuração do modelo fcaps. o objetivo foi estender o conceito de automatização até aos optical network termination (ont)’s, que são essencialmente os aparelhos que residem no cliente e terminam a ligação de fibra ótica, sendo que existem ainda modelos mais recentes de ont’s que integram já funcionalidades de routing. deste modo, foi estudado neste trabalho o desenvolvimento de um módulo baseado no zero touch provision (ztp) que suporte a configuração automática das interfaces finais de rede, as (ont’s). a automatização deste processo simplifica instalação de serviços no cliente, uma vez que os equipamentos passam a não necessitar de configurações manuais: tudo o que operador tem de fazer é correr um conjunto de testes que asseguram o correto funcionamento do equipamento uma vez configurado. as instalações passam assim a ser mais rápidas e menos suscetíveis a erros. para além disto, este processo permite um aprovisionamento dos equipamentos mais rápido, assim como uma maior coerência nas configurações dos diversos equipamentos que constituem a rede. este trabalho foi focado num sub-grupo de problemas e tópicos de investigação que tiveram de ser cobertos antes de se avançar para a implementação final do ztp optical network unit (onu)’s: o desenho de uma arquitetura escalável, o estudo da possibilidade de exploração de concorrência no processamento do aprovisionamento de equipamentos, o desenvolvimento de um método de relacionar um equipamento com as regras de aprovisionamento disponíveis, o estudo das práticas de segurança a implementar de acordo com o estado da arte assim como o estudo das soluções possíveis para a concretização da monitorização do módulo de software.",
      "the managment of network equipments is a complex problem of huge dimensions that makes the indivual handling of each equipment a very unappealing task both from an economic and logistic stand point. altice labs developed a solution to manage the access network equipments: agora, which is a network management system. this system is modular and scalable, as it is made up of various compo nents that make up for the network infrastrutcture managment model fcaps. this model implies the following categories: failure, configuration, accountability, performance and security. [19] in this context an automation module was developed in agora. the module zero touch provision (ztp) was developed for the olt’s provisioning. this module uses templates, based on ansible playbooks, that are adapted for each client scenario. this module embodies the configuration category of the fcaps managment model. the goal was to extend the automation concept to the ont’s, which essentialy are the client devices like, for example, fiber-gateways and routers. in short, the development of a new module based on the zero touch provisioning (ztp) that supports the automatic configuration of final network interfaces (ont’s) was studied. the automation of this process simplifies the installation and/or configuration process of services in the client, since the equipments will no longer need manual configurations: everything the operator has to do is to run a set of tests to ensure that the equipment is correctly configured. the deployments will then be quicker and less error prone. furthermore, this process allows for faster provisioning of the network equipments as well as greater coherence among all equipment configurations. this work was focused in a subgroup of problems and the investigation of topics that had to be covered before advancing for the final implementation of ztp onu’s: the design of a scalable architecture, the study of the possibility of exploring concurrency mechanisms in the processing of equipment provisions, the development of a method of relating an equipment with the available provision rules, the study of security mechanisms according to the state of the art as well as the study of possibilities for accomplishing monitoring in the software module."
    ],
    0.3
  ],
  [
    [
      "strategic programming is a powerful technique used in language processing to define functions that traverse abstract syntax trees. with strategies, the programmer only indicates the nodes of the tree where the work has to be done, and the strategy used to traverse the whole tree and apply the function that works only on the defined nodes. in haskell, there are two libraries that implement strategies: strafunski and an equivalent library developed by di: ztrategic. beyond that, we also have the kiama library which is implemented in the scala programming language. the ztrategic library uses memorization in order to save work. using memorization, the elimination of all occurrences of \"bad smells\" in an abstract tree of a program is done only once! in this thesis, we present a detailed study of the performance of the kiama, ztrategic, and memoized ztrategic libraries, using different strategic problems and input languages.",
      "programação estratégica é uma técnica poderosa usada em processamento de linguagens para definir funções que atravessam árvores de sintaxe abstracta. com estratégias o programador apenas indica os nodos da árvore onde o trabalho tem de ser feito, e depois que estratégia é utilizada para atravessar toda a árvore e aplicar a função que faz trabalho apenas nos nodos definidos. em haskell existem duas bibliotecas de combinadores que implementam estratégias: strafunski e uma biblioteca equivalente desenvolvida no di: ztrategic. existe também outra biblioteca desenvolvida em scala, kiama. a biblioteca ztrategic usa memorização de modo a poupar trabalho. usando memorização, a eliminação de todas a ocorrências do \"mau cheiro\" numa árvore abstracta de um programa é feita apenas uma vez! nesta tese faz-se um estudo detalhado da performance das bibliotecas kiama, ztrategic, e memoized ztrategic, utilizando diferentes problemas de programação estratégica e diferentes linguagens de input."
    ],
    [
      "falls are one of the most common causes of injuries in the elderly population. as a result, treatment costs have also increased. recent efforts to restore lower limb function in these populations have seen an increase in the use of wearable robotic systems, however, fall prevention measures in these systems require early detection of loss of balance to be effective. in short, the development of technologies, such as a brain-computer interface, that is capable of recognizing situations at risk of falling based on the loss of balance caused by several factors, is essential. previous studies have investigated whether kinematic variables contain information about an impending fall, but few have examined the potential of using electroencephalography (eeg) as a predictor of falling and how the brain responds to prevent a fall. perceived disturbances of balance are always accompanied by a specific cortical activation, called disturbance-evoked potential (pep). in this study, the recognition of daily activities (walking, lifting, crouching, going up and down stairs) was also part of the initial objective, however, due to the challenges encountered, the object of study of the present work was focused on the recognition and binary classification of the presence of loss of balance (peps) in brain signals. thus, this dissertation intends to take the first steps toward the decoding of brain activity in response to imbalanced events. initially, to acquire the data, an experimental protocol was designed, so that the participants, using eeg, were submitted to gliding-like perturbations while walking on the treadmill. two healthy subjects were exposed to a glide-like perturbation, and these perturbations occurred interspersed over a period lasting 30 to 60 seconds. each subject performed 2 experiments, that is, perturbations provoked while the individual walked on the treadmill: i) at a speed of 1.6 km/h and ii) at a speed of 2.5 km/h. based on the approached methods, the perturbation evoked potential (pep) components were found between 70-155 ms after the onset of the external perturbation. to decode pre-processed egg data, four (4) artificial neural networks were tested and different network architecture parameters and electrode layouts were compared. overall, the convolutional neural network trained to predict eeg balance disturbances had a far superior classification performance than the other architectures, whose mean accuracy was 91.51 ˘ 2.91%, using a short window length of 200 ms. the electrode layout composed of 5 channels (fz, c3, cz, c4, and pz) presented the shortest execution time to train the model, whose average value was 196 ˘ 44.24ms. in addition, it was possible to verify that the use of a single electrode (cz) obtained satisfactory precision results (86.47 +/- 0.03%). these discoveries may contribute to the development of a system capable of detecting equilibrium disturbances in real-time.",
      "as quedas são uma das causas mais comuns de lesões na população idosa. como resultado, custos com o tratamento têm também aumentado. esforços recentes para restaurar a função dos membros inferiores nessas populações viram um aumento no uso de sistemas robóticos vestíveis, no entanto, as medidas de prevenção de quedas nesses sistemas exigem a detecção precoce da perda de equilíbrio para serem eficazes. em suma, é fundamental o desenvolvimento de tecnologias, tal como interface cérebro-computador, que seja capaz de reconhecer situações risco de queda com base na perda de equilíbrio ocasionada por diversos fatores. estudos anteriores investigaram se as variáveis cinemáticas continham informações sobre uma queda iminente, mas poucos examinaram o potencial do uso da eletroencefalografia (eeg) como um sinal de previsão de queda e como o cérebro responde para evitar uma queda. as perturbações do equilíbrio percebidas são sempre acompanhadas por uma ativação cortical específica, chamada de potencial evocado por perturbação (pep). neste estudo, também fazia parte do objetivo inicial o reconhecimento das atividades diárias (andar, levantar, agachar, subir e descer escada), porém, devido aos desafios encontrados, o objeto de estudo do presente trabalho foi focado no reconhecimento e classificação binária da presença de perda de equilíbrio (peps) em sinais cerebrais. assim, esta dissertação pretende dar os primeiros passos em direção a decodificação da atividade cerebral em resposta a eventos de desequilíbrio. inicialmente, para adquirir os dados, um protocolo experimental foi delineado, de forma que os participantes, usando eeg, fossem submetidos a perturbações do tipo deslizamento enquanto andavam sobre a esteira. dois sujeitos saudáveis foram expostos a perturbação do tipo deslizamento, e estas perturbações ocorreram intercaladamente em um período de duração de 30 a 60 segundos. cada sujeitou realizou 2 experimentos, ou seja, perturbações provocadas enquanto o indivíduo andava na passadeira: i) a uma velocidade de 1.6 km/h e ii) a uma velocidade de 2.5 km/h. com base nos métodos abordados os componentes de potencial evocado de perturbação (pep) foram encontrados entre 70-155 ms após o início da perturbação externa. para decodificar dados pré-processados do egg, quatro (4) redes neurais artificiais foram testados e diferentes parâmetros da arquitetura de rede e layouts de eletrodos foram comparados. no geral, a rede neural convolucional treinada para prever as perturbações do equilíbrio do eeg teve um alcance desempenho de classificação superior as demais arquiteturas, cuja precisão média foi de 91,51 ˘ 2,91%, usando um comprimento de janela curto de 200 ms. o layout de eletrodo composto por 5 canais (fz, c3, cz, c4 e pz) apresentou o menor tempo de execução para treinar o modelo, cujo valor médio foi 196 ˘ 44.24ms. além disso, foi possível verificar que a utilização de um único eletrodo (cz) obteve resultados de precisão satisfatórios (86.47 +/- 0.03%). essas descobertas podem contribuir para o desenvolvimento de um sistema capaz de detectar perturbações do equilíbrio em tempo real."
    ],
    0.0
  ],
  [
    "the most important element in a network of public displays is a piece of software, the player, it is responsible for interpreting the presentation instructions, which are sent in a specific format, and make the content visible to the users according to those instructions. one of the big issues regarding this type of software is their restrict system requirements, a player is usually conceived having a specific target platform, this creates some issues when deploying new displays. with the increasing development of web technologies emerges a solution to this issue: a web based player with low system requirements and the capability to be deployed in a wider range of platforms. the goal of this investigation is to design and implement a web based player using web technologies such as html5 and javascript, the new chrome packaged apps technology is also being looked at as a way to easily distribute and deploy the software. ultimately this web based player aims to increase the reach and availability of the public displays networks by creating a platform to which the non-proprietary developer can create content to.",
    [
      "a huge amount of medical data is being generated each day, leaving the doctors unable to analyze such volume and make a good diagnosis for the patient. the emergence of big data frameworks for data analysis leverages the automatic analysis of healthcare data in a faster and accurate manner, by scanning which information is relevant, and, consequently, detecting diseases in earlier stages. nowadays, it is estimated that about 9% to 38% of the world’s population has sleep ap nea. unawareness of the disease’s presence can lead to the development of cardiovascular diseases, and consequently, death. the detection of sleep apnea syndrome through the tra ditional method, polysomnography (psg), becomes not only expensive but also inconvenient for the patient. therefore, systems based on electrocardiogram (ecg) can improve the qua lity of a patient’s health by overcoming these inconveniences. this master thesis relies on deep learning (dl) networks, such as convolutional and recurrent neural networks for sleep apnea detection. the computational complexity of these models depends on its size, types of layers and data. this complexity also increases the computation time of the training task leading to several hours spent training on a single machine. for this work, we propose a sleep apnea detection system based on ecgs, alongside with a distributed version of it, which parallelizes the training computation, reducing the overall learning time, while not compromising the model performance. the results obtained for sleep apnea detection encourage the use of electrocardiograms for the detection of this disease. our model achieved a value of 93% of sensitivity on the physionet database, being the highest value compared to other studies described in the literature. besides this, on the distributed environment it was accomplished similar output quality, reducing the training time by approximately 50%, from the centralized to distributed learning. the model was trained with the sleep heart health study (shhs) data, achieving the highest results compared to the work described in the literature that used the same dataset. in comparison with the previous dataset, the model trained and tested with the shhs was not able to attain a similar quality output. however, this corroborates the large diversity of the shhs data. moreover, when it was tested if this model could classify the physionet data, it achieved promising results of 73,7%, 73,8%, 68,3% and 63,5% of accuracy, sensitivity, f1- score, and precision, respectively, which lead us to conclude that the shhs trained model could be able to generalize to new data. in addition to this, on the distributed environment it was achieved equal output perfor mance for shhs, reducing the training time by approximately 90%.",
      "diariamente são geradas grandes quantidades de dados, impossibilitando a sua análise em tempo real. em particular, num ambiente hospitalar, os médicos foram perdendo a capacidade de analisar todos os dados relativos a um paciente, o que compromete o seu diagnóstico. as ferramentas de big data vieram colmatar esta falha, promovendo a análise de grandes quantidades de dados. esta análise permite selecionar apenas a informação relevante, fazendo com que os dados realmente necessários sejam vistos pelos médicos de uma forma rápida, permitindo assim a deteção de doenças precocemente. atualmente, estima-se que entre 9% a 38% da população mundial tenha apneia do sono. o não diagnóstico desta doença pode levar ao desenvolvimento de doenças cardiovasculares e, consequentemente, à morte. a deteção desta síndrome pelo método tradicional, polissonografia (psg), torna-se não só dispendiosa, mas também inconveniente para o paciente. com isto, o desenvolvimento de sistemas menos invasivos baseados em electrocardiogramas (ecg) podem melhorar a qualidade de vida do paciente, ultrapassando estes inconvenientes. tendo por base estas afirmações, o objetivo principal desta dissertação assenta na criação e no estudo de modelos de deep learning (dl) para a deteção do síndrome da apneia do sono. para isto, foram construídos três modelos distintos que assentam em camadas convolucionais e recorrentes. a complexidade computacional destes modelos depende não só do seu tamanho, mas também dos tipos de camada e dos dados. esta complexidade faz com que haja um aumento no tempo de computação durante o treino dos modelos, sendo que em apenas uma máquina, este treino pode chegar a várias horas ou dias. posto isto, foi desenvolvido um sistema de deteção de apneia do sono baseado em ecgs, juntamente com uma versão distribuída do mesmo, que paraleliza e reduz o tempo de treino. os resultados obtidos mostraram-se promissores e salientam a importância do uso dos electrocardiogramas para a deteção da apneia do sono. neste sentido, com os dados retirados do physionet, o nosso modelo apresentou um valor de sensibilidade de 93%, sendo o valor mais alto comparado com outras alternativas propostas pelo estado da arte. alem disso, no ambiente distribuído foram alcançados valores semelhantes de desempenho, reduzindo o tempo de treino em aproximadamente 50% comparativamente ao ambiente centralizado (apenas uma máquina). para além do physionet database, o modelo foi também treinado com os dados do sleep heart health study (shhs), obtendo melhores resultados que os descritos na literatura. em comparação com os dados anteriores, este último obteve resultados menos satisfatórios o que e corroborado pela a grande diversidade de patologias presentes nos dados. além disso, após testar este modelo com os dados do physionet, foram alcançados resultados auspiciosos de 73,7 %, 73,8 %, 68,3 % e 63,5 % de exatidão, sensibilidade, f1-score e precisão, respetivamente, o que nos leva a concluir que o modelo treinado do shhs pode generalizar para dados que nunca viu. além disso, no ambiente distribuído, foram alcançados valores similares de desempenho face ao ambiente centralizado, reduzindo, aproximadamente, 90% do tempo de treino."
    ],
    0.12857142857142856
  ],
  [
    [
      "ergoaware aims at developing technology that reduce the worker’s exposure to ergonomic risk (e.g. poor postures) as there is a high incidence of work-related musculoskeletal disorders (wrmsds), which represents an economic burden of 240 billion euros and is driven by the industry 4.0 paradigm. another goal of ergoaware is also to optimize human-robot collaborative processes to shape the robot’s assistance according to the individual physiological requirements of each user. this dissertation main goal is the development of a desktop graphical interface application to allow visualization of the workers kinematic model, allow real-time monitoring of postural and fatigue metrics, and provide an abstraction layer for the ros backend in order to allow the configuration of the integrated sensing technologies and develop control strategies. the objective will be for users to be able to manipu late ergoaware’s adjustable parameters in an intuitive manner, with a user-friendly interface for easy tool usability.",
      "a ergoaware visa desenvolver tecnologia que reduza a exposição do trabalhador ao risco ergonómico (por exemplo, posturas deficientes) já que existe uma elevada incidência de distúrbios músculo-esqueléticos relacionados com o trabalho (wrmsd) que representa um encargo económico de 240 mil milhões de eu ros e é impulsionada pelo paradigma da indústria 4.0. outro objetivo da ergoaware passa também por otimizar os processos de colaboração entre o robot humano e o robot para moldar a assistência do robot de acordo com os requisitos fisiológicos individuais de cada utilizador. o principal objetivo desta dissertação assenta no desenvolvimento de uma interface gráfica para uma aplicação desktop para permitir a visualização do modelo cinemático dos trabalhadores, permitir um acompanhamento em tempo real da métrica postural e de fadiga e fornecer uma camada de abstracção para o backend ros, a fim de permitir a configuração das tecnologias de deteção integradas e desenvolver estratégias de controlo. esta será desenvolvida para que os utilizadores consigam manipular os parâmetros ajustáveis do ergoaware de maneira intuitiva, com uma interface amigável para uma fácil usabilidade do desfrutador da ferramenta."
    ],
    [
      "quantum simulation represents a formidable challenge for classical computers due to the intricate behavior of quantum systems. digital quantum computers aim for precise approximations of a wide range of quantum systems. within the realm of quantum simulation, the study of spin systems plays a pivotal role, providing insights into complex properties challenging to model through classical means. this work focuses on investigating chiral spin systems through the development of a dedicated quantum circuit for chirality measurement. in this masters dissertation, we present an overview of the current state-of-the-art in quantum simulation of chiral spin systems and introduce our approach to addressing this challenge. scalar spin chirality, a three-body physical observable, holds a critical position both in classical magnetism, where it characterizes non-coplanar spin textures, and in quantum magnetism, serving as an order parameter for chiral spin liquids. in the context of quantum information, scalar spin chirality serves as a witness to genuine tripartite entanglement. in this study, we delve into various methodologies to tackle the problem at hand, subjecting them to comparison. the objective is to identify the most suitable approach that demands fewer quantum resources. our best proposed method introduces an indirect measurement scheme based on the hadamard test, designed to estimate the scalar spin chirality for general quantum states. we apply this innovative approach to measure chirality in two specific types of quantum states: the generic one-magnon states of a ferromagnet and the ground state of a model characterized by competing symmetric and antisymmetric exchange interactions. our research findings highlight the practicality of achieving a single-shot determination of scalar chirality for chirality eigenstates, leveraging the power of quantum phase estimation with a single auxiliary qutrit. this novel methodology extends beyond providing a solution to the chirality measurement problem; it also unifies the theory of chirality in both classical and quantum magnetism. the implications of our work offer valuable insights and pave the way for future quantum research endeavors in this domain.",
      "a simulação quântica desafia os computadores clássicos devido ao comportamento complexo dos sistemas quânticos. os computadores quânticos digitais buscam aproximar uma ampla variedade de sistemas quânticos. na simulação quântica, a análise de sistemas de spin é essencial para compreender propriedades complexas difíceis de modelar com abordagens clássicas. este estudo concentra-se em investigar sistemas de spin com quiralidade por meio do desenvolvimento de um circuito quântico dedicado para medição. nesta dissertação de mestrado, apresentamos um panorama atual da simulação quântica de sis temas de spin com quiralidade e introduzimos nossa abordagem para abordar esse desafio. o operador de quiralidade, um observável físico de três corpos, desempenha um papel crítico no magnetismo clássico, caracterizando texturas de spin não coplanares, e no magnetismo quântico, atuando como um parâmetro de ordem para líquidos de spin com quiralidade. no contexto da informação quântica, a quiralidade serve como testemunha de entrelaçamento tripartite genuíno. neste estudo, exploramos várias metodologias para abordar o problema em questão, comparando-as com o objetivo de identificar a mais eficiente em termos de recursos quânticos. o nosso melhor método proposto é um esquema de medição indireta baseado no teste de hadamard, criado para estimar a quiralidade do spin em estados quânticos gerais. aplicamos esta abordagem para medir a quiralidade em dois tipos específicos de estados quânticos: os estados genéricos de um magnon de um ferromagneto e o estado fundamental de um modelo com interações de troca simétricas e antissimétricas em competição. as descobertas de nossa pesquisa destacam a praticidade de alcançar uma determinação única da quiralidade do spin para estados com quiralidade, aproveitando o poder da estimativa de fase quântica com um único qutrit auxiliar. essa metodologia inovadora vai além de fornecer uma solução para o problema de medição de quiralidade; ela também unifica a teoria da quiralidade tanto no magnetismo clássico quanto no magnetismo quântico. as implicações de nosso trabalho oferecem conhecimentos valiosos e abrem caminho para futuras esforços de pesquisa quântica nesse domínio."
    ],
    0.06666666666666667
  ],
  [
    [
      "there is a shortage of manufacturing management software solutions for businesses with various manual processes, and that offer a wide range of products. existing solutions can become very expensive for small and medium-sized enterprises, and can discourage them to take the next step towards the 4th industrial revolution. this dissertation consists of joint work with tipoprado, artes gráficas, to develop a package tracking and production performance analysis platform. the company has a notable number of different clients and offers different types of services. this way, different packages may go through different paths on the production pipeline. given this, to offer a more close and customized service, tipoprado, wants to develop a package tracking platform. this tracking is not geographical (delivery case), but about the package location over the production pipeline, giving clients the possibility to consult, in real-time, the actual state of their orders. apart from this, implementing this platform produces a significant level of data about packages and clients. one of the main goals is to treat, process and analyze this data, to improve production efficiency and be able to help the its managers make crucial decisions about the referred pipeline. production planning and predictions on delivery dates is the ultimate goal. this dissertation studies and implements the tracking method that best applies to tipoprado production pipeline, together with data analysis, and prediction options. the given platform will transport the company to a tech production vision, and kick start its journey through the fourth industrial revolution. it is also expected to increase customer engagement levels, which correlate with a higher number of sales.",
      "existe uma falta de soluções de software de gestão de manufatura para empresas com vários processos manuais, e que oferecem uma vasta gama de produtos. as soluções existentes podem-se tornar bastante dispendiosas para pequenas e médias empresas, e desencorajar as mesmas para dar o próximo passo em direção à 4a revolução industrial. esta dissertação consiste num trabalho conjunto com tipoprado, artes gráficas, para desen volver uma plataforma de tracking de encomendas e análise de desempenho de produção. a empresa possui um número notável de clientes diferentes e oferece diferentes tipos de serviços. dessa forma, encomendas diferentes podem seguir caminhos diferentes na linha de produção. dado isso, para oferecer um serviço mais próximo e personalizado, a tipoprado, deseja desenvolver uma plataforma de tracking de encomendas. esse tracking não é geográfico (caso de entrega), mas sobre a localização da encomenda no pipeline de produção, oferecendo aos clientes a possibilidade de consultar, em tempo real, o estado atual dos seus pedidos. além disso, a implementação desta plataforma produz um nível significativo de dados sobre encomendas e clientes. um dos principais objetivos é tratar, processar e analisar esses dados, melhorar a eficiência da produção e ajudar os responsáveis da empresa a tomar decisões cruciais sobre o referido pipeline. o planeamento da produção e as previsões nas datas de entrega é o objetivo final. esta dissertação estuda e implementa o método de tracking que melhor se aplica à linha de produção da tipoprado, junto com a análise de dados e opções de previsão. a plataforma fornecida transportará a empresa para uma visão de produção tecnológica e iniciará sua jornada na quarta revolução industrial. também é esperado que aumente os níveis de envolvimento do cliente, que se correlacionam com um número maior de vendas."
    ],
    [
      "the ability to efficiently determine the dynamics to which a quantum device conforms is vital for its reliable operation. thus, as quantum machines evolve, the means for their characterization must evolve alongside them, especially as they reach the limits of classical tractability. bayesian inference has been proposed as a solution, as it offers a flexible way of using experimental data to learn the dynamical parameters - or even models - governing the evolution of quantum systems. it gives rise to noise-resilient protocols, which are capable of quantifying their own uncertainty, of learning from scarce information, and of real-time estimation. importantly, and apart from the obvious applications in sensing devices, online processing enables adaptivity, a stepping stone for achieving fundamental limits of metrology via what is called quantum-enhanced estimation. like so, the exploitation of quantum control as a resource within the bayesian paradigm opens the door to uncertainties scaling at the heisenberg limit. most work on the subject has relied on simple methods for portraying the bayesian posterior, but they quickly become a bottleneck in realistic scenarios. the difficulty of the task grows with the complexity of the devices to be verified, which adds to the fact that they are ultimately open systems and as such undergo uncontrollable interactions with their surroundings. while capturing unwanted processes like decoherence is by itself a valuable endeavor, this poses extra challenges, and requires careful computational treatment. in general, when scaling up, the biggest difficulty of bayesian learning is numerical integration. in this context, pairing it with advanced monte carlo methods makes for remarkably robust algorithms, which can succeed under complex features and control a multitude of trade-offs. this dissertation aims to overview both of these methodologies, and to apply them to the characterization of noisy quantum computers. special emphasis is placed on state-of-the-art methods for statistical sim ulation - namely hamiltonian and sequential monte carlo, variants thereof, and subsampling approaches. using these strategies to post-process 150 single-shot measurements on a ramsey sequence, we learn an oscillation frequency and coherence time to 0.4% and 33% uncertainty respectively, compared to 15% and 333% using curve fits. with an echoed pulse set-up and 75 shots only, we then achieve an uncertainty of 0.3% for the qubit’s precession frequency, whereas standard fitting methods either do worse or fail completely. switching to online processing and further lowering the total shot count to 15, we use adaptive experimental design to infer the frequency to 5% uncertainty, compared to 44% using offline processing under otherwise identical conditions.",
      "a capacidade de determinar de forma eficiente a dinâmica que rege um dispositivo quântico é fulcral para a viabilidade da sua operação. logo, à medida que as máquinas quânticas evoluem, os meios para a sua caraterização devem acompanhá-las, sobretudo quando se atingem os limites da tratabilidade clás sica. a inferência bayesiana foi proposta como solução, já que oferece uma forma flexível de usar dados experimentais para estimar os parâmetros - ou até modelos - que governam a evolução de sistemas quân ticos. esta técnica produz protocolos resilientes a ruído, capazes de quantificar a sua própria incerteza, de aprender com base em informação escassa, e de estimar em tempo real. crucialmente, além das óbvias aplicações em sensores, o processamento em tempo real possibilita adaptatividade, um ponto de partida para alcançar limites fundamentais da metrologia no regime quântico. assim, o uso de controlo quântico dentro do paradigma bayesiano proporciona a progressão de incertezas no limite de heisenberg. a literatura tem recorrido maioritariamente a métodos simples para retratar distribuições bayesianas, mas estes são limitadores em cenários realistas. a dificuldade da tarefa aumenta com a complexidade dos dispositivos a verificar, o que é agravado pelo facto de estes serem sistemas abertos, que interagem de forma incontrolável com o ambiente. apesar do interesse inerente à captura de fenómenos indesejáveis como a decoerência, estes criam desafios adicionais, e exige um tratatamento computacional cuidado. em geral, quando a complexidade aumenta, a maior dificuldade da aprendizagem bayesiana é a integração numérica. neste contexto, combiná-la com métodos de monte carlo avançados produz algoritmos de uma robustez notável, capazes de lidar com atributos complexos e de gerir vários compromissos. esta dissertação pretende abordar estas duas metodologias, e aplicá-las à caraterização de compu tadores quânticos ruidosos. colocar-se-á especial ênfase no estado da arte da simulação estatística - nomeadamente monte carlo hamiltoniano e sequencial, variantes destes, e técnicas de subamostragem. usando estas estratégias para pós-processar 150 medições (isoladas) numa sequência de ramsey, infe rimos uma frequência de oscilação e tempo de coerência com incertezas de 0.4% e 33% respetivamente, comparando com 15% e 333% com ajuste de curvas. acrescentando um pulso de inversão, obtemos com apenas 75 medições uma incerteza de 0.3% para a frequência; métodos padrão deram resultados piores ou nulos. com processamento em tempo real e reduzindo as medições para 15, usamos me dições adaptativas para inferir a frequência com uma incerteza de 5%, em contraste com 44% usando pós-processamento e mantendo as restantes condições."
    ],
    0.0
  ],
  [
    [
      "este documento relata um projeto de mestrado, do segundo ano do mestrado em engenharia informática da universidade do minho, em braga, portugal. o projeto consiste no desenvolvimento de uma aplicação mobile de realidade aumentada a ser utilizada como recurso educacional para apoiar o ensino introdutório de programação de computadores, com o objetivo de aumentar a motivação e a perseverança dos estudantes, ajudando-os a superar certas dificuldades que muitos estudantes enfrentam na abstração e compreensão de diferentes tipos de estruturas de dados. para atingir os objetivos do projeto de mestrado, foi necessário, no início, definir uma linguagem de programação imperativa com várias estruturas de dados e operações para as manipular e usar, para depois utilizar a realidade aumentada em telefones móveis para criar um sistema que ajude a visualizar e entender as referidas estruturas de dados. nesta dissertação, a linguagem de programação referida é especificada e ilustrada com a ajuda de alguns exemplos. a arquitetura do sistema é proposto, e o seu desenvolvimento é descrito em detalhe, abordando os detalhes mais importantes. o sistema desenvolvido é apresentado, e os resultados obtidos a partir de um experimento envolvendo estudantes em uma sala de aula real são analisados.",
      "this document reports a master’s project, included in the second year of the master’s degree in informatics engineering at universidade do minho in braga, portugal. the project consists on the development of an augmented reality mobile application to be used as an educational resource, to support introductory teaching to computer programming, with the objective to increase student’s motivation as well as perseverance, helping them to overcome certain difficulties that many students experience in abstracting and understanding different types of data structures. to attain the master’s work objectives, it was necessary at the beginning to define an imperative programming language with several data structures and special operations to work and manipulate them, in order to use augmented reality on mobile phones to create a system that helps to visualize and handle the referred data structures. in this dissertation, the referred programming language is specified and illustrated with the help of some examples. the complex tool architecture is proposed, and its development is described in detail, going over the most important details. the overall system built is presented, and the results gathered from an experiment involving students in a real classroom are analyzed."
    ],
    [
      "reportar ocorrências relativas a espaços ou equipamentos públicos é uma tarefa essencial, pois a identificação e a resolução dos problemas devem ser breves, de modo a evitar novos como consequência da demora deste processo. atualmente, os cidadãos quando são confrontados com alguma ocorrência têm dificuldade em saber como e onde participar a mesma para que esta seja resolvida. pretende-se, com o desenvolvimento deste projeto, criar um meio de comunicação facilitador entre os cidadãos e as autarquias, que vise não só auxiliar os cidadãos a reportarem as mais variadas ocorrências relativas aos espaços ou equipamentos públicos, mas também permitir aos municípios gerir e monitorizar ocorrências reportadas nas diversas áreas de intervenção dos serviços municipais. o resultado do desenvolvimento deste projeto enquadra-se na criação de uma aplicação móvel e de uma aplicação web.",
      "reporting incidents related to municipal spaces or municipal equipment is vital as the identification and mitigation of the problem must be prompt, in order to avoid further issues. nowadays, when citizens are faced with an incident they do not know how and where to report it so that it gets fixed. this projects aims to develop a communications channel between the council and its citizens in order to, not only help citizens to easily report incidents related to municipal spaces or municipal equipment, but also to allow councils to manage and monitor events reported across the different areas in which the council has authority over. finally, the ultimate goal of this project is to create a mobile and web application that hosts the concept introduced above."
    ],
    0.06666666666666667
  ],
  [
    [
      "ageing brings a lot of problems in the human body. one really common issue is the lost of some mobility be it simply by losing control or by contracting some sort of illness. however, some times these problems can be tackled with some physical therapy and a recovery can be achieved, even if not full recovery. this master dissertation tries to enable a medical device with the ability of adapting to different types of disabilities, developing a system capable of tackling a range of mobility disorders through therapy. this work adds an autonomous mode where more severe problems can be treated leaving all the coordination to the walker letting the user focus on training their gate. it also adds a shared control mode where the task of guiding the robot can be offloaded to the user, targeting patients in a more advanced recovery state forcing them to focus on training their coordination and ambient perception besides from the treating their gate problems. the directed tests showed a system capable of adapting to any situation it was put through, with a robust set of emergency mechanism in case anything malfunctions. based on the analysed results and the answers to the survey, we can indeed say that the system would serve as a suitable integration in the asbgo*, walker enabling the walker with more versatility.",
      "o envelhecimento traz com ele muitos problemas para o corpo humano. um dos problemas mais comuns associados ao envelhecimento a perda total ou imparcial da mobilidade humana simplesmente por ir perdendo essa capacidade ou através da contração de uma doença. estes problemas podem totalmente ou parcialmente tratados através de fisioterapia. esta dissertação adiciona a um equipamento médico, o asbgo*, a habilidade de se adaptar a tipos diferentes de deficiências, através do desenvolvimento de um sistema capaz de tratar um leque de problemas de mobilidade. esta habilidade foi desenvolvida através da incluso de um modo autónomo, onde pacientes com problemas mais severos podem ser tratados deixando a navegao nos clculos do andarilho. isto adiciona a hipótese do utilizador se concentrar no treino da sua marcha. também foi adicionado o modo onde o controlo pode ser partilhado entre o utilizador e o andarilho. este modo foi desenvolvido para utilizadores num estado de recuperação mais avançado onde estes se conseguem concentrar na sua coordenao e na perceo do ambiente, para alm de poderem continuar a concentrarem-se na recuperação da marcha. os testes realizados mostraram um sistema capaz de se adaptar a qualquer situação que foi sujeito, com um conjunto de mecanismos de emergência para o caso de haver alguma avaria. com base nos resultados analisados e nas respostas aos inquéritos, podemos dizer que o sistema serviria como uma integração adequada no andador asbgo*, tornando o andarilho uma ferramenta mais versátil."
    ],
    [
      "the applications’ development paradigm has faced changes in recent years, with modern development being characterized by the need to continuously deliver new software iterations. with great affinity with those principles, microservices is a software architecture which features characteristics that potentially promote multiple quality attributes often required by modern, large-scale applications. its recent growth in popularity and acceptance in the industry made this architectural style often described as a form of modernizing applications that allegedly solves all the traditional monolithic applications’ inconveniences. however, there are multiple worth mentioning costs associated with its adoption, which seem to be very vaguely described in existing empirical research, being often summarized as \"the complexity of a distributed system\". the adoption of microservices provides the agility to achieve its promised benefits, but to actually reach them, several key implementation principles have to be honored. given that it is still a fairly recent approach to developing applications, the lack of established principles and knowledge from development teams results in the misjudgment of both costs and values of this architectural style. the outcome is often implementations that conflict with its promised benefits. in order to implement a microservices-based architecture that achieves its alleged benefits, there are multiple patterns and methodologies involved that add a considerable amount of complexity. to evaluate its impact in a concrete and empirical way, one same e-commerce platform was developed from scratch following a monolithic architectural style and two architectural patterns based on microservices, featuring distinct inter-service communication and data management mechanisms. the effort involved in dealing with eventual consistency, maintaining a communication infrastructure, and managing data in a distributed way portrayed significant overheads not existent in the development of traditional applications. nonetheless, migrating from a monolithic architecture to a microservicesbased is currently accepted as the modern way of developing software and this ideology is not often contested, nor the involved technical challenges are appropriately emphasized. sometimes considered over-engineering, other times necessary, this dissertation contributes with empirical data from insights that showcase the impact of the migration to microservices in several topics. from the trade-offs associated with the use of specific patterns, the development of the functionalities in a distributed way, and the processes to assure a variety of quality attributes, to performance benchmarks experiments and the use of observability techniques, the entire development process is described and constitutes the object of study of this dissertation.",
      "o paradigma de desenvolvimento de aplicações tem visto alterações nos últimos anos, sendo o desenvolvimento moderno caracterizado pela necessidade de entrega contínua de novas iterações de software. com grande afinidade com esses princípios, microsserviços são uma arquitetura de software que conta com características que potencialmente promovem múltiplos atributos de qualidade frequentemente requisitados por aplicações modernas de grandes dimensões. o seu recente crescimento em popularidade e aceitação na industria fez com que este estilo arquitetural se comumente descrito como uma forma de modernizar aplicações que alegadamente resolve todos os inconvenientes apresentados por aplicações monolíticas tradicionais. contudo, existem vários custos associados à sua adoção, aparentemente descritos de forma muito vaga, frequentemente sumarizados como a \"complexidade de um sistema distribuído\". a adoção de microsserviços fornece a agilidade para atingir os seus benefícios prometidos, mas para os alcançar, vários princípios de implementação devem ser honrados. dado que ainda se trata de uma forma recente de desenvolver aplicações, a falta de princípios estabelecidos e conhecimento por parte das equipas de desenvolvimento resulta em julgamentos errados dos custos e valores deste estilo arquitetural. o resultado geralmente são implementações que entram em conflito com os seus benefícios prometidos. de modo a implementar uma arquitetura baseada em microsserviços com os benefícios prometidos existem múltiplos padrões que adicionam considerável complexidade. de modo a avaliar o impacto dos microsserviços de forma concreta e empírica, foi desenvolvida uma mesma plataforma e-commerce de raiz segundo uma arquitetura monolítica e duas arquitetura baseadas em microsserviços, contando com diferentes mecanismos de comunicação entre os serviços. o esforço envolvido em lidar com consistência eventual, manter a infraestrutura de comunicação e gerir os dados de uma forma distribuída representaram desafios não existentes no desenvolvimento de aplicações tradicionais. apesar disso, a ideologia de migração de uma arquitetura monolítica para uma baseada em microsserviços é atualmente aceite como a forma moderna de desenvolver aplicações, não sendo frequentemente contestada nem os seus desafios técnicos são apropriadamente enfatizados. por vezes considerado overengineering, outras vezes necessário, a presente dissertação visa contribuir com dados práticos relativamente ao impacto da migração para arquiteturas baseadas em microsserviços em diversos tópicos. desde os trade-offs envolvidos no uso de padrões específicos, o desenvolvimento das funcionalidades de uma forma distribuída e nos processos para assegurar uma variedade de atributos de qualidade, até análise de benchmarks de performance e uso de técnicas de observabilidade, todo o desenvolvimento é descrito e constitui o objeto de estudo da dissertação."
    ],
    0.3
  ],
  [
    [
      "clouds play an important role in enhancing the realism of ambient and outdoor scenes in computer graphics (cg). however, rendering clouds and certain atmospheric elements remains a major challenge in this field. their representation involves processes such as modeling, photorealistic rendering, and animation. the main objective of this project was to explore techniques for modeling and rendering clouds. to this end, code was developed in glsl shader language to implement clouds, using the nau3d application for code compilation. different types of cloud modeling were studied, based on textures, geometric shapes, noise, real and/or satellite images. volumetric clouds were implemented, following the approach of schneider and vos [2015], hillaire [2016a], and högfeldt [2016]. a volume was used in which the voxels were filled with information stored in a weather texture. cloud rendering was performed using the ray marching algorithm. for cloud lighting, a study of differ ent light events was conducted independently of the participating media. lighting in this implementation was adapted from formulas presented in jarosz [2008] and scratchapixel [2022b], taking into account that the participating media is the cloud. various light phenomena were studied, including in-scattering. to address this effect, several phase functions were tested. tests were also conducted to evaluate computational costs, such as the time spent on gpu rendering of the implemented cloud shader and the required fps for rendering the atmosphere with clouds. these tests were performed to understand the balance between computational cost and visual results, allowing for customization of parameters based on this balance. the visual results obtained did not reach the level of state-of-the-art realistic clouds in cg, but there is room for improvement in the implemented clouds.",
      "as nuvens representam um papel importante no aumento do realismo de ambientes e cenas externas em computação gráfica (cg). a renderização de nuvens e alguns elementos da atmosfera ainda representam um grande desafio nesta área. a sua representação envolve processos como modelação, renderização fotorrealista e animação. o principal objetivo deste projeto foi explorar as técnicas de modelação e renderização de nuvens. neste sentido foi desenvolvido código em linguagem de shaders glsl para se implementar nuvens. a aplicação usada para compilar o código foi a nau3d. estudou-se os diferentes tipos de modelação de nuvem baseados em texturas, figuras geométricas, ruídos, imagens reais e/ou de satélite. optou-se por se implementar nuvens volumétricas, seguindo a abordagem de schneider and vos [2015], hillaire [2016a] e högfeldt [2016]. utilizou-se um volume no qual os voxels foram preenchidos com informação armazenada numa weather texture. a renderização das nuvens foi feita através do algoritmo de ray marching. para a iluminação da nuvem, foi realizado um estudo dos diferentes eventos de luz independentemente do participating media. para elaboração da iluminação nesta implementação, foram adaptadas as fórmulas apresentadas em jarosz [2008] e scratchapixel [2022b], tendo em conta que o participating media é a nuvem. vários fenómenos da luz foram estudados, incluindo o in-scattering. para tratar esse efeito foram testadas várias funções de fase. foram também realizados testes para se avaliar o custo computacional, como o tempo em gpu gasto apenas na renderização do shader das nuvens implementadas e os fps requeridos para a renderização da atmosfera com as nuvens. estes testes foram feitos de forma a se perceber o balanço entre o custo computacional e os resultados visuais, permitindo assim personalizar os parâmetros tendo em conta este equilíbrio. os resultados visuais obtidos não atingiram o nível do estado de arte de nuvens realistas em cg, mas existe margem para as nuvens implementadas serem melhoradas."
    ],
    [
      "the purpose of this dissertation is to ascertain the feasibility of quantum control. this is a rather informal concept, however in the context of this work it simply means the control flow of a quantum program being performed without resorting to measurements and without being mediated by an external classical computer. the approach consists in providing a definition of a conditional like statement which encapsulates the intended behaviour. the definition is given as a specific morphism in a biproduct dagger compact closed category. these were introduced by abramsky and coecke (2004) as a semantic framework capable of expressing the axioms of closed system finite dimensional quantum mechanics. later this framework was extended to capture open system quantum mechanics (selinger, 2007). as a consequence, and as it pertains to this dissertation, the construct presented in this work has an interpretation for both closed quantum systems (category of finite-dimensional hilbert spaces and linear maps) and open quantum systems (category of finite-dimensional hilbert spaces and completely positive maps). what was found is that in closed quantum systems the proposed construct transpires the idea of superposition of programs as conceptualized in previous works on the matter (badescu and panangaden, 2015) (ying et al., 2014), which gives validity to the notion of quantum control at least in this context. on the other hand, in open quantum systems the meaning of the conditional statement proposed takes the form of probabilistic branching dependent on the probability distribution of a bit. finally a comparison is made between this work and the one carried out by badescu and panangaden (2015) in the context of the qpl programming language (selinger, 2004), which concludes with a discussion about the incompatibility of quantum control in programming languages whose semantics are based on the open quantum system formalism.",
      "esta dissertação tem como propósito averiguar a viabilidade de controle quântico. sendo este de um certo modo um conceito informal, no contexto desta dissertação significa simplesmente o controle do fluxo da execução de um programa quântico sem recorrer a medições e sem ser mediado por um computador clássico externo. a abordagem consiste em especificar a definição de uma expressão condicional que encapsule o comportamento desejado. esta definição é dada como um determinado morfismo numa categoria biproduct dagger compact closed. estas foram introduzidas por abramsky and coecke (2004) como modelo semântico capaz de expressar os axiomas da mecânica quântica para sistemas fechados finitos. mais tarde este modelo foi expandido de forma a poder encorporar sistemas abertos (selinger, 2007). consequentemente, e no que diz respeito a esta dissertação, a construção aqui apresentada terá uma interpretação tanto para sistemas fechados (categoria de espaços de hilbert de dimensão finita e transformações lineares) como para sistemas abertos (espaços de hilbert de dimensão finita e mapeamentos completamente positivos). para sistemas quânticos fechados foi concluído que a construção proposta transparece a idéia de sobreposição de programas conceitualizado em investigações anteriores acerca do assunto (badescu and panangaden , 2015) (ying et al., 2014), o que valida a noção de controle quântico pelo menos neste contexto. por outro lado, para sistemas quânticos abertos a expressão condicional sugerida toma a forma de \"braching\" probabilistico dependente da distribuição probabilistica de um bit. por fim, é feita uma comparação entre este e o trabalho realizado por badescu and panangaden (2015) no contexto da linguagem de programação qpl (selinger, 2004), que conclui numa discussão sobre a imcompatibilidade de controle quântico em linguagens de programação cuja semântica é baseada em sistemas quânticos abertos."
    ],
    0.06666666666666667
  ],
  [
    [
      "pregnancy is a period of change and uncertainties. there is a constant search for information and reassurance that all the changes going through pregnant women’s body are normal. furthermore, during the last years the access to information has increased with the expansion of smartphones and their functionalities and of mobile apps. with this expansion, the concept of mhealth emerged as the use of mobile devices in medicine and public health. mhealth is centred on the patients’ care and aims to provide knowledge and empower patients to monitor their health. in cooperation with centro materno infantil do norte a supporting platform for preg nant women was developed called ”the 10 moons: pregnancy, childbirth and post partum period of cmin”. ”the 10 moons” goal is to provide pregnant women with access to their pregnancy booklet boletim de saude da gr ´ avida ´ , to personalised information and to a set of tools that will allow them to monitor their pregnancy as well as to record appointments and pregnancy-related documents. in order to make it available to the widest range of users, it was tried to use an efficient devel opment approach. hence, the software development methodology adopted for the implementation of the platform was the one of the progressive web app. overall, the development of the platform was divided into three main areas. this dissertation focus is the pregnancy monitoring and the provision of information in order to promote a healthy lifestyle and avoid complications during pregnancy.",
      "a gravidez e um período de mudança e incertezas. há uma constante procura de informação e de garantia de que todas as alterações pelas quais o corpo passa durante a gravidez são normais. além disso, nos últimos anos o acesso à informação tem aumentado com a expansão dos smartphones, das suas potencialidades e das aplicações móveis. com esta expansão surgiu o conceito de mhealth como o uso de dispositivos moveis na medicina e na saúde pública. mhealth centra-se no cuidado dos pacientes e procura dar aos seus utilizadores mais conhecimento e autonomia para monitorizar a sua saúde. em colaboração com o centro materno infantil do norte foi desenvolvida uma plataforma de apoio as grávidas chamada de ”as 10 luas: gravidez, parto e pós-parto do cmin”. ”as 10 luas” tem como objetivo fornecer as grávidas o acesso ao seu livro de gravidez boletim de saúde da grávida, a informação personalizada e a um conjunto de ferramentas que lhes permitam monitorizar a gravidez bem como registar consultas e documentos médicos relacionados com a gravidez. para a sua implementação foi procurado uma abordagem de desenvolvimento eficiente de forma a que a plataforma estivesse disponível para o maior numero de utilizadores. por conseguinte, progressive web app foi a metodologia de desenvolvimento de software adotada para a implementação da plataforma. o desenvolvimento da plataforma foi dividido em três grandes áreas. o grande foco desta dissertação e a monitorização da gravidez e o fornecimento de informação com o objetivo de promover um estilo de vida saudável e evitar complicações durante a gravidez."
    ],
    [
      "diariamente existem mudanças no ambiente organizacional e as organizações procuram adaptar-se e acompanhar as tendências do mercado. é necessário inovar nas tecnologias, haver flexibilidade e adaptabilidade por parte dos colaboradores e retirar o melhor proveito dos dados. nesse sentido, o conceito de business intelligence surge apresentando um conjunto de métodos e ferramentas para disponibilizar informação e suportar a tomada de decisão. o objetivo é otimizar e simplificar o processo de tomada de decisão, agilizar a extração de dados e sua divulgação como informação valiosa e possibilitar soluções de qualidade e confiáveis. neste projeto apostou-se no desenvolvimento e implementação de uma solução de business intelligence capaz de suportar a gestão de oficina no ambiente organizacional em questão. ao longo do desenvolvimento do projeto realizou-se a construção de um data warehouse, o levantamento de requisitos e métricas para implementação da solução, a construção de sistemas de processamento analítico e aplicações de front-end. o principal objetivo desta solução para a organização é a recolha e monitorização da informação para observar valores passados e atuais, de modo a compreender as tendências, observar pontos a melhorar e visualizar dados futuros. através da solução desenvolvida, a organização sentiu melhorias nas decisões diárias, uma maior capacidade de acesso ao detalhe e uma apresentação da informação de forma fidedigna e de qualidade.",
      "every day there are changes in the organizational environment and organizations seek to adapt to and keep up with market trends. it is necessary to innovate technologies, to have flexibility and adaptability from the collaborators and to take the best advantage of the data. because of this, the business intelligence concept emerges presenting a set of methods and tools to provide information and support decision making. the goal is to optimize and simplify the decision-making process, speed up data extraction and its disclosure as valuable information and enable quality and reliable solutions. in this project, i focused on the development and implementation of a business intelligence solution capable of supporting workshop management in the organizational environment in question. throughout the development of the project, a data warehouse was built, requirements and metrics were gathered for the implementation of the solution, analytical processing systems and front-end applications were built. the main objective of this solution for the organization is to collect and monitor information to observe past and current values in order to understand trends, observe points for improvement, and visualize future data. through the solution developed, the organization has experienced improvements in daily decisions, a greater ability to access detail, and a presentation of information in a reliable and quality manner."
    ],
    0.0857142857142857
  ],
  [
    [
      "this document is a thesis report for the final project of the master degree in informatics engineering, accomplished at universidade do minho in braga, portugal. the project consisted in the development of a tool capable of semi-automatically generating computer programming exercises named goliath. it applies natural language processing (nlp) methods and techniques, commonly used to construct short texts in predefined formats, to build typical computer programming exercises. in doing so, goliath intends to support teachers and students in their educational endeavours by providing dynamically generated exercises. it aims to ease the burden of creating study material and also provide constant and immediate access to new exercises. goliath was implemented to include two ai-based models: one to generate text (keys-to-text) and another to generate source code (codet5). a template-based exercise generation mechanism was added to its functionalities, taking advantage of a dsl designed specifically for this project. this dsl was fully designed (and its processor implemented) to allow the use of expressive templates that enable the generation of more than one version of an exercise. the entire application was tested and feedback was overall positive, pointing to the better aspects of the application and providing useful feedback for future iterations. with its objectives achieved, goliath may become a valuable contribution to the education support tools landscape.",
      "este documento consiste no relatório de dissertação para o projeto final do mestrado em engenharia informática, realizado na universidade do minho em braga, portugal. o projeto consistiu no desenvolvimento de uma ferramenta, chamada goliath, capaz de gerar semi automaticamente exercícios de programação. esta ferramenta aplica métodos e técnicas de processamento de linguagem natural (pln), frequentemente utilizados para a construção de textos curtos em formatos pré-definidos, para gerar exercícios típicos de programação. ao fazê-lo, goliath pretende apoiar professores e alunos em seus esforços educacionais, fornecendo exercícios gerados dinamicamente. visa tanto reduzir a carga de trabalho de criar material de estudo, como fornecer acesso constante e imediato a novos exercícios. goliath foi implementado para incluir dois modelos baseados em ia: um para gerar texto (keys-to-text) e outro para gerar código-fonte (codet5). um mecanismo de geração de exercícios baseado em templates foi adicionado às suas funcionalidades, com o auxílio de uma dsl desenvolvida especificamente para este projeto. esta dsl (e seu processador implementado) foi totalmente projetada para permitir o uso de templates expressivos que possibilitam a geração de mais de uma versão de um exercício. toda a aplicação foi testada e o feedback foi, em geral, positivo, apontando para os melhores aspectos da aplicação e proporcionando recomendações úteis para futuras iterações. com seus objetivos alcançados, goliath pode tornar-se numa contribuição valiosa para o cenário de ferramentas de apoio à educação."
    ],
    [
      "automation developments are enabling industrial restructuring through the incorporation of more efficient and accurate processes with less associated cost. consequently, robots are being increasingly used in the most various scenarios, including in safety critical domains. in such cases, the use of suitable methods to attest both the system’s quality and their safety is absolutely essential. following the current increase of complexity of cyber-physical systems, safety guards which used to be fully hardware dependent, are constantly migrating to software. here upon, middleware software to abstract systems hardware are constantly evolving and are being increasingly adopted. the common feature of these systems is usually associated with its modular architectures based on message-passing communication patterns. a notorious case is the ros middleware, where highly configurable robots are usually built by composing third-party modules. the verification of such systems is usually very hard, and its implemen tation in real industrial environments is, in most cases, impracticable. to promote adoption, this work advocates the use of lightweight formal methods associated with semi-automatic techniques that require minimal user input and provide valuable intuitive feedback. this work explores and proposes a technique to automatically verify system-wide safety properties of ros-based applications in continuous integration environments. it is based on the formalization of ros architectural models and nodes behaviours in electrum, a specification language of first-order temporal logic supported by a model-finder over which, system-wide properties are subsequently model-checked. in order to automate the analysis, the technique is deployed as an haros plug-in, a framework for quality assessment of ros software, specially aimed to its community. the technique proposal and its implementation under the haros framework are eval uated with positive results on a real agricultural robot, agrobv16, whose dimension and complexity are industrially representative.",
      "o constante desenvolvimento em processos de automação tem motivado reestruturações nos mais diversos processos industriais, aumentando a sua eficiência, e consequentemente, reduzindo os custos associados. as vantagens provocadas pela automação impulsionam a sua adoção nos mais amplos domínios, nomeadamente, em cenários considerados críticos. nestes casos, é vital a existência e adopção de técnicas que forneçam fortes garantias da qualidade e segurança dos sistemas. isto é de particular relevância aquando do desenvolvimento de sistemas ciber-físicos, onde se observa uma constante migração de safety guards, que eram usualmente implementadas ao nível do hardware, para lógica de software. de forma a acompanhar o aumento na complexidade destes sistemas, middlewares que permitem abstrair hardware tem sido adoptados de forma ubíqua. este são construídos predominantemente sobre arquiteturas modulares baseadas em message-passing. um caso notório são as aplicações ros, onde robôs altamente configuráveis são construídos através da composição de módulos externos. na maioria dos casos, a verificação destes sistemas é muito difícil, sendo que em ambientes industriais e geralmente impraticável. com vista a promover a adopção de técnicas que promovam a qualidade do software em ambientes de produção, este trabalho defende a utilização de lightweight formal methods associados a técnicas semi-automáticas que requerem intervenções mínimas por parte dos utilizadores, retornando feedback valioso de forma intuitiva. este trabalho explora e propõe uma técnica para verificação automática de system-wide safety properties em aplicações ros, cujos resultados podem ser estendidos para qualquer arquitetura modular baseada em message passing. a técnica fundamenta-se na formalização de modelos estruturais de arquiteturas ros, e especificações comportamentais dos seus nodos em electrum. após formalização do sistema, as propriedades são verificadas através de técnicas de model-checking. de forma a automatizar a análise, a técnica descrita neste documento é implementada através de um plug-in para haros, uma framework utilizada no control de qualidade de software ros. a técnica proposta, assim como a sua implementação sobre o ambiente haros, foram positivamente avaliadas aquando da sua aplicação em um caso real, agrobv16. um robô agrícola, cuja dimensão e complexidade são representativos daquilo que seria de esperar em verdadeiros ambientes industriais."
    ],
    0.3
  ],
  [
    [
      "this work focuses on the innovation of vehicular networks through the application of software-defined networks (sdn) to optimize connectivity and decision-making in urban mobility scenarios. using the ryu controller, the mininet-wifi emulation environment and the sumo urban mobility simulator, this research establishes a complete and realistic experimental environment for the study of vehicular networks. the ryu controller plays a key role in the dynamic orchestration of network decisions, enabling continuous adaptation to changes in the topology and communication demands of vehicular networks. mininet-wifi offers the ability to emulate mobility scenarios, making it possible to analyze connectivity and performance in dynamic urban environments. in addition, the sumo simulator accurately replicates urban roads, providing realistic modeling of the vehicle movements. the combination of these tools allows a comprehensive evaluation of the performance of vehicular networks in urban environments, as well as the study of resource management strategies and real-time decision-making. this research contributes to the advancement of vehicular communication technologies and offers valuable insights for the development of an efficient and safe urban mobility solutions. this study highlights the importance of integrating sdn, mobility emulation and road simulation to improve connectivity and quality of service in vehicular networks, providing a solid foundation for further researches in the area.",
      "este trabalho concentra-se na inovação das redes veiculares através da aplicação de redes definidas por software (sdn) para otimizar a conectividade e a tomada de decisões em cenários de mobilidade urbana. utilizando o controlador ryu, o ambiente de emulação mininet-wifi e o simulador de mobilidade urbana sumo, esta pesquisa estabelece um ambiente experimental completo e realista para o estudo de redes veiculares. o controlador ryu desempenha um papel fundamental na orquestração dinâmica das decisões de rede, permitindo a adaptação contínua às mudanças na topologia e nas demandas de comunicação das redes veiculares. o mininet-wifi oferece a capacidade de emular cenários de mobilidade, possibilitando a análise de conectividade e desempenho em ambientes urbanos dinâmicos. além disso, o simulador sumo replica com precisão as vias urbanas, proporcionando a modelagem realista de movimentos de veículos. a combinação dessas ferramentas permite a avaliação abrangente do desempenho das redes veiculares em ambientes urbanos, bem como o estudo de estratégias de gestão de recursos e tomada de decisões em tempo real. esta pesquisa contribui para o avanço das tecnologias de comunicação veicular e oferece contributos valiosos para o desenvolvimento de soluções de mobilidade urbana eficientes e seguras. este estudo destaca a importância da integração de sdn, emulação de mobilidade e simulação de vias para aprimorar a conectividade e a qualidade de serviço em redes veiculares, proporcionando uma base sólida para futuras pesquisas na área."
    ],
    [
      "atualmente, mais da metade da população mundial vive em centros urbanos e as estatísticas indicam que em 2050 essa percentagem rondará os 70%. a forte concentração da população em urbes, apresenta grandes desafios, principalmente devido à densidade populacional, habitação, circulação, ou a escalabi lidade de serviços. gerir esta realidade, garantindo as condições indispensáveis para uma alta qualidade de vida, é um desafio que as tecnologias inteligentes poderão ajudar a conseguir. a união europeia de fine cidades inteligentes, ou smart cities, como um conjunto de sistemas e de pessoas que interagem de forma inteligente utilizando energia, materiais, serviços e recursos de forma sustentável. assim, estima-se que o valor das tecnologias relacionadas com o controle e a monitorização do tráfego em smart cities é proporcional à redução dos acidentes de trânsito, congestionamentos urbanos, e outros impactos sociais. são exemplos, a necessidade de comunicação ou controle de tráfego a partir de ferramentas inteligentes que na atualidade é difícil de manipular já que possuem grande impacto económico e social. para esse efeito, é necessário a implementação de técnicas ou estratégias (amostragem, agregação e filtragem) que vão permitir monitorar fluxos de dados, a fim de garantir eficiência no tratamento de grandes volumes de dados nos múltiplos contextos das cidades. o objetivo desta dissertação é efetuar uma análise critica so bre estratégias de monitorização veicular, seu impacto e suas limitações frentes aos grandes volumes de tráfego gerados pelas smart cities. avaliam-se ainda técnicas contextuais que serviram para a construção de soluções frente aos desafios da mobilidade e transportabilidade no contexto urbano.",
      "currently, more than half of the world’s population lives in urban centers and statistics indicate that by 2050 this percentage will be around 70%. the high concentration of the population in cities presents major challenges, mainly due to population density, housing, traffic and the scalability of services. mana ging this reality, guaranteeing the essential conditions for a high quality of life, is a challenge that smart technologies can help to overcome. the european union defines smart cities as a set of systems and people that interact intelligently, using energy, materials, services and resources in a sustainable way. thus, it is estimated that the value of technologies related to traffic control and monitoring in smart cities is proportional to the reduction in traffic accidents, urban congestion, and other social impacts, such as the need for communication or traffic control from intelligent tools that are currently difficult to manipulate since they have a major economic and social impact. to this end, it is necessary to implement techniques or strategies (sampling, aggregation and filtering) that will make it possible to monitor data flows, in order to guarantee efficiency in the handling of large volumes of data in the multiple contexts of cities the aim of this dissertation is to carry out a critical analysis of vehicle monitoring strategies, their impact and their limitations in presence of the large volumes of traffic generated by smart cities. the work also evaluates monitoring technologies that have served to build solutions to the challenges of mobility and transportability in the urban context."
    ],
    0.3
  ],
  [
    [
      "efficient or green computing is becoming a key issue in current programming techniques, going beyond high performance computing, by simultaneously considering issues such as energy or power consumption. in heterogeneous environments, where different processors and accelerators co-processors may coexist, there is a real opportunity to reduce the overall energy consumption of the system by using scheduling decisions in run-time that can have a good and quick response to changes in the different components. current tools to aid the development of efficient applications lack yet these run-time facilities. this motivated the development of a new framework with a power-aware scheduler for heterogeneous environments, pash-frame, whose prototype is the key object of this dissertation. this work extended previous performance-based scheduling work to include run-time power-aware features, adding tools to measure power consumption at each device and using different scheduling decisions to get the best outcome according to pre-defined targets by the end user. to evaluate the overall behaviour of pash-frame, several tests were performed: 1000 saxpy tasks with vector sizes varying from 16 thousand elements to 256 thousand; 200 sgemm tasks with matrices varying from 64 thousand elements to 16 million and finally a test that combines the two previous ones. results show that the scheduling algorithm implemented in the framework can achieve good results in some cases, in spite of not being able to make some critical decisions when it comes to energy consumption reduction like forcing a component to idle to save energy.",
      "computação eficiente (ou green computing) está a tornar-se um dos maiores desafios nas técnicas de programação actuais, considerando simultâneamente os problemas de computação de alta performance bem como a energia e o consumo total. em ambientes heterogéneos, onde diferentes processadores e aceleradores como co-processadores podem coexistir, existe uma grande oportunidade para reduzir o consumo energético global do sistema ao utilizar decisões de escalonamento em tempo real que conseguem ter uma boa resposta rápida a mudanças nos diferentes componentes. as ferramentas actuais para ajudar na programação de aplicações eficientes ainda não têm estas ferramentas de leitura de energia em tempo real. isto serviu de motivação para criar uma nova framework com um escalonador para sistemas heterogéneos consiente do gasto de energia, a pash-frame, em que o seu protótipo vai ser explicado nesta dissertação. este trabalho é uma continuação de trabalho prévio em escalonamento baseado em alta performance ao incluir ferramentas de medição de energia em tempo real e ao fornecer decisões de escalonamento baseadas nesses valores para ter o melhor desempenho de acordo com as escolhas do seu utilizador. para avaliar o comportamento da pash-frame, vários testes foram feitos: o primeiro teste foi de 1000 tarefas do algoritmo saxpy com o tamanho dos vetores a variar entre 16 mil elementos e 256 mil; o segundo teste foi de 200 tarefas do algoritmo sgemm com os tamanhos das matrizes a variar entre 64 mil elementos e 16 milhões de elementos e por fim o terceiro teste é uma combinação dos dois primeiros. os resultados obtidos mostram que o algoritmo de escalonamento implementado na framework consegue obter bons resultados em alguns casos, apesar de não conseguir fazer algumas decisões críticas para o escalonamento com vista a reduzir o consumo global do sistema, como forçar um componente a ficar inativo para poupar energia."
    ],
    [
      "os orgãos de gestão dos hospitais optam cada vez mais pela implementação de sistemas de monitorização baseados na tecnologia rfid, tendo em vista sobretudo a redução de custos associados a perda e roubo de equipamentos, o aumento da segurança de pacientes e pro ssionais e a deteção do acesso de pessoal a zonas não autorizadas. testes e estudos de otimização, nomeadamente em relação a configuração dos sensores na arquitetura da rede rfid, devem ser realizados para aumentar o desempenho destes sistemas. a simulação surge neste contexto como uma importante ferramenta de apoio, uma vez que permite que os estudos sejam endereçados a partir de um ambiente computacional, evitando as desvantagens inerentes à realização de testes no próprio hospital. recentemente, o estudo da questão de conservação de energia, em redes de sensores de tracking de objetos, tem atraído muita atenção. a previsão de trajetórias pode ser utilizada para determinar o conjunto de sensores que num determinado momento devem ser desativados para reduzir consumos energéticos e, consequentemente, aumentar o tempo de vida do sistema. o objetivo deste trabalho é integrar as duas temáticas referidas - simulação e previsão - no desenvolvimento de um sistema inteligente capaz de simular e prever a trajetória de uma entidade numa área preenchida com sensores. a área escolhida para a simulação consiste no piso pediátrico de um hospital no norte de portugal, onde um sistema rfid de monitorização se encontra atualmente implementado para a monitorização de pacientes. a análise do sistema desenvolvido é realizada através do estudo de cenários, onde estatísticas obtidas em diferentes condi cões de simulação são analisadas sob diversos prismas. os resultados provam que o sistema proposto é capaz de simular com sucesso o movimento de uma ou mais entidades num ambiente hospitalar real, e de estimar localizações com uma precisão média de 62% para um modelo de simulação que tem em consideração a existência de variabilidade e aleatoriedade no movimento. a previsão é realizada através da aplicação de um algoritmo de data mining, denominado sk-means, ao histórico de percursos de cada entidade, e permite a obtenção de padrões de movimento específicos e distintos para cada uma delas. as localizações previstas são apresentadas ao utilizador em tempo real, de forma dinâmica. pretende-se que no futuro o sistema possa ser implementado no hospital para permitir a visualização em tempo real dos pacientes.",
      "many hospitals choose to rely on rfid tracking systems to avoid the theft or loss of equipment, increase patient and staff safety, finding missing patients or staff, and issuing warnings about personnel access to unauthorized areas. optimization studies, concerning the configuration of the sensors in the rfid network architecture, must be made in order to increase the performance of these systems. the simulation arises in this context as an important supporting tool, since it allows studies to be addressed from a computing environment, avoiding the disadvantages of testing in the hospital. recently, the issue of energy saving in object tracking sensor networks has attracted much attention. trajectory prediction is a software approach which provides, in real time, the set of sensors that can be deactivated to reduce power consumption and thereby increase the system's lifetime. hence, the system proposed here aims to integrate the aforementioned strategies - simulation and prediction { in the development of an intelligent tracking simulation system able to simulate and predict an entity's trajectory in an area tted with sensors. the chosen locale for the simulation of object movements was the paediatric oor of a hospital in the north of portugal, where an rfid monitoring system is currently employed for patient monitoring. the analysis of the system is developed through the study of scenarios where statistics obtained in different simulation conditions are analyzed from several perspectives. the results show that the proposed system is able to successfully simulate the movement of one or more entities, in an actual hospital environment, and to estimate locations with an average accuracy of 62 % for a simulation model which takes into account the existence of variability and randomness in the movement. the prediction is accomplished by applying a data mining algorithm, designated sk-means, to discover object movement patterns through historical trajectory data. the predicted locations are presented to the user in real time, dynamically. it is anticipated that this system will be incorporated in the hospital, to allow real time visualization of patients, in the future."
    ],
    0.0
  ],
  [
    [
      "atualmente os sistemas de apoio à decisão clínica (sadc) são um componente essencial para qualquer sistema informático clínico. a sua correta integração nos sistemas existentes permite uma maior eficiência na aquisição e disponibilização de dados importantes e relativos a cada paciente. deste modo, é imperativo que um sadc garanta a interoperabilidade semântica, isto é, a capacidade de comunicar com outras plataformas sem que hajam perdas de significado no seu conteúdo. é necessário também garantir que estes sistemas sejam de fácil integração, de modo a possibilitar a agregação da maior quantidade de dados clínicos possível dos diferentes sistemas existentes nas instituições. com isto, surge então a necessidade de optar por padrões que garantam a interoperabilidade semântica deste sistemas e que facultem meios para a sua rápida integração. neste sentido, emerge o openehr, um padrão e-health aberto que oferece os meios necessários para garantir as propriedades supracitadas anteriormente. adicionalmente, mas não menos importante, este apresenta um módulo de apoio à decisão que permite codificar a lógica de decisão presente nos planos e guidelines clínicos. são várias as definições existentes para o conceito de guideline, no entanto, pode-se assumir, para efeitos de simplificação, o guideline como sendo um conjunto de declarações que visam guiar o profissional de saúde na tomada de decisão no seu dia-a-dia. este tipo de sistema é relevante pois visa minimizar possíveis erros de teor humano que podem facilmente ser cometidos por parte dos profissionais que estão sujeitos diariamente a vários factores como turnos extensos, cansaço, entre outros e que em última instância podem compremeter a tomada de uma decisão acertada. assim, a presente dissertação tem como finalidade desenvolver um sistema de apoio à decisão clínica, baseado em regras e no modelo openehr, que permite a criação, gestão e execução de guidelines clínicos. para além disso, foi-se mais além e desenvolveu-se uma interface gráfica que faculta ao utilizador um meio simples, intuitivo e de agradável visualização.",
      "nowadays, clinical decision support system (cdss) are an essential component for any clinical com puter system. correctly integrated into existing systems allows for a higher efficiency in the availability of important data related to each patient. hence, it is imperative that a cdss ensures a semantic interop erability, in other terms, the capacity to communicate with other platforms without losing the content’s significance. it is also necessary to ensure that these systems are able to be easily integrated, in order to enable the aggregation of as much clinical data as possible from the different existing institutional systems. with this comes the need to rely on pattern based languages that guarantee a semantic interoperability of these systems and that provide crucial tools for fast integration. and so, openehr emerges, an open e-health pattern based language that offers the necessary means to cover the previously mentioned properties. additionally, openehr provides a decision support module that allows for the coding of the decision logic present in the clinical plans and guidelines. the existing definitions for the concept of a guideline tends to vary significantly, we can assume, for simplification purposes, that a guideline is a group of declarations with the objective of guiding the health professional in the decision making process of their daily professional lives. this type of system proves itself relevant by trying to minimize the human error component of the decision making process, seen as every day health professionals are subjected to factors that can diminish their focus, such as long shifts, fatigue and many others that can sometimes compromise their decisions and decision making speed. with all this in mind, the ultimate goal of this dissertation is to develop a clinical decision support system based on specific rules and on the openehr module, which will enable the creation, management and execution of clinical guidelines. furthermore, the choice was made to go beyond and develop a graphical interface that provides the user with simple, intuitive and pleasant means of visualization."
    ],
    [
      "hypatiamat is a portuguese project comprised of several applications that aim to develop the math skills of students from the 1st through 9th grades (basic education). the ingraining of mental calculation strategies, numbering systems, and logical operations lead to a better success rate in this subject in later years. one of the project’s components is the online platform (https://www.hypatiamat.com), which aims to foster autonomous learning through more interactive practices due to the current ease of technological access in this age group, by trying to appropriate teaching to everyday life. several tools are made available, such as videos, tutorials, explanations, questions, etc. on various math topics that students can easily access at any time. teachers that aim to enhance their students’ learning process using this digital approach can exercise it in multiple applications provided by the platform, where the interactions are carried out and controlled through these means. the monolithic architecture (written in php) has received contributions from multiple developers over the years in order to address the scalability issues introduced with this platform’s growing popularity, which thus far demanded manual efforts for maintenance and content insertion. as such, there has been an incremental process of modernization, turning the various constituent applications into distinct microservices. \"i want to solve questions about...\" is one of these applications where students are provided with a large selection of questions in the form of mini-games (multiple choice, true or false, ...), regarding the themes mentioned above. the first objective of the dissertation is to develop a back-office that allows the teachers in charge of the project to manage existing questions as well as add new ones for the students, since the current process requires updating the database manually. the second one is the modernization of the application’s interface at the technological level, by making use of adequate frameworks and programming languages and at the user level, by making an effort to maintain the intuitive workflow that led to its popularity but with a modernized design, in order to be consistent with other online tools.",
      "o hypatiamat é um projeto português constituído por várias aplicações que visa desenvolver as aptidões, na disciplina de matemática, de alunos do 1º ao 9º ano de escolaridade (educação básica). o enraizamento de estratégias de cálculo mental, sistemas de numeração e operações lógicas originam uma melhor taxa de sucesso nesta disciplina em anos posteriores. uma das componentes deste projeto é a plataforma online (https://www.hypatiamat.com), cujo propósito é fomentar a aprendizagem autónoma através de práticas mais interativas, devido à facilidade de acesso tecnológico atual desta faixa etária, tentando apropriar o ensino ao quotidiano. são disponibilizadas várias ferramentas, tais como vídeos, tutoriais, explicações, questões, etc sobre os vários temas da matemática (ensino básico) que os alunos podem facilmente aceder a qualquer momento. professores que pretendam enriquecer a aprendizagem dos seus alunos com esta metodologia digital podem exercê-lo nas várias aplicações que a plataforma disponibiliza, onde a interação é realizada e controlada através destes meios. a arquitetura monolítica (escrita em php) tem recebido contribuições de vários desenvolvedores ao longo dos anos de modo a colmatar os problemas de escalabilidade introduzidos com a popularidade crescente desta plataforma, que até agora exigia esforço manual para manutenção e inserção de conteúdo. assim, tem existido um processo incremental de modernização, tornando as várias aplicações constituintes em microsserviços distintos. a \"quero resolver questões de...\" é uma destas aplicações, onde são disponibilizadas aos alunos várias questões, sob a forma de mini-jogos (escolha múltipla, verdadeiro ou falso, ...), relativas aos temas mencionados anteriormente. o primeiro objetivo da dissertação é o desenvolvimento de um backoffice que permita aos professores responsáveis gerirem as questões existentes assim como adicionarem novas para os alunos, visto que o processo atual obriga a atualização manual na base de dados. o segundo é a modernização da interface da aplicação ao nível: tecnológico, utilizando frameworks e linguagens de programação adequadas ao problema; do utilizador, de modo a manter o fluxo intuitivo que gerou a sua popularidade mas tendo em conta um design mais atualizado para manter a consistência com outras ferramentas online."
    ],
    0.3
  ],
  [
    [
      "neste momento marcante do meu percurso académico quero agradecer a todos aqueles que, de alguma forma, contribuíram para a concretização dos meus objetivos. assim, dirijo as primeiras palavras ao meu orientador, o professor doutor orlando belo, pela sua disponibilidade e pelo seu apoio constante. a sua motivação, as suas sugestões e as suas críticas foram fundamentais. a sua experiência recheada de sabedoria foi um elemento-chave ao longo do desenvolvimento da presente dissertação. a minha mãe merece um agradecimento especial, pela compreensão e pelo apoio diário. a conﬁança que esta deposita em mim foi importante nas mais diversas tomadas de decisão ao longo da minha jornada académica. sem ela, as vitórias alcançadas ao longo de todos estes anos de aprendizagem não seriam possíveis. devo à minha mãe tudo o que sou e tenho. ela está sempre ao meu lado, é um exemplo de ser e de estar. dirijo, ainda, um agradecimento à bárbara, pela paciência, motivação, apoio e amizade. pelo seu empenho e pela sua valorização do meu trabalho. por me apoiar acima de tudo nos momentos mais difíceis. pelas suas sugestões de melhoria que nunca me deixaram desistir e que me mostraram sempre o melhor lado de todas as coisas. por ﬁm, um último agradecimento, aos meus colegas e amigos, que me apoiaram e que me ajudaram. estes estiveram presentes quer nos momentos de partilha de conhecimento, quer nos momentos de lazer.",
      "nowadays, a large part of human activity is stored in the form of data. these data are obtained from the interaction of man with the world around him through various techno- logical objects that surround him. like watches, smartphones, computers and sensors, all of them producing data that can be stored. it is not only the speed of a program execution or a query to a database that deﬁnes the quality of a program code or a database. now, in computer programs, the expenditure given to the energy level contributes signiﬁcantly to its performance. the growing concern over the green queries makes the progress in this area increasingly signiﬁcant. entities operating data centers turn, now, their eyes to this is- sue, as much of the value of their annual bills comes from the large consumption of energy associated with it. in this thesis, taking into account the research under this heading, it is evaluated energy consumption spent by queries on database systems supported by docu- ment stores. this evaluation provides us the necessary elements to make decisions about the process of creating queries in document stores and thus develop an evaluation model of energy consumption."
    ],
    [
      "currently, given the technological evolution, data and information are increasingly valuable in the most diverse areas for the most various purposes. although the information and knowledge discovered by the exploration and use of data can be very valuable in many applications, people have been increasingly concerned about the other side, that is, the privacy threats that these processes bring. this document follows an user-role approach within the data exploration process. these users are: data provider (provides the data), data collector (collects and stores the data provided), data publisher (transforms data and publishes it to be explored) and data explorer (retrieves information from data). all of them have privacy concerns and can address them with appropriate methods and techniques. in this master thesis we built a system named privas that aids the data publisher in its publishing process. currently he can assure the data privacy by adopting, manually choosing and then applying the privacy-preserving data publishing techniques (ppdp). privas accepts a repository with its description (written in a dsl) and creates a copy maintaining the information to be explored but assuring that involved individuals/organizations cannot be identified by applying ppdp techniques. privas automatically chooses the privacy models to apply according with the description, and applies the transformation. in the end of the process metrics about the privacy loss are reported. the domain specific language (dsl) – called privasl – was developed to easily allow the original repository description, the identification of the data entities that one wants to explore and the definition of the privacy level to be assured. to visually help end-users to describe their repositories, a web platform was developed – where after describing the repository, the correspondent privasl description is generated. in the end, an analysis on different kind of repositories, with different information using the privas tool, was made – conclusions were drawn about transformations and in privacy loss.",
      "atualmente, dada a evolução tecnológica, dados e informações são cada vez mais valiosos em diversas áreas e fins. embora as informações e conhecimento descobertos pela exploração/uso de dados possam ser muito valiosos em muitas aplicações, as pessoas têm se preocupado cada vez mais com o outro lado, ou seja, com as ameaças à privacidade que esses processos trazem. este documento segue uma abordagem baseada nos papéis dos participantes no processo de exploração de dados: data provider (fornece os dados), data collector (guarda os dados fornecidos), data publisher (transforma e publica os dados para serem explorados) e o data explorer (explora e extrai informações). todos têm preocupações com a privacidade e podem resolvê-las com métodos e técnicas apropriadas. nesta tese de mestrado construímos um sistema chamado privas que auxilia o data publisher na sua tarefa. atualmente, o data publisher pode garantir a privacidade dos dados adoptando e manualmente escolhendo e aplicando técnicas de preservação de privacidade na publicação de dados (ppdp). o privas aceita um repositório com a sua descrição (em uma dsl) e cria uma cópia, mantendo as informações a explorar, mas assegurando que indivíduos/organizações envolvidos não possam ser identificados, através das técnicas ppdp. o privas escolhe de forma automática (através da descrição) os modelos de privacidade a serem aplicados e aplica a transformação. no final, são produzidas métricas sobre a perda de privacidade. a linguagem específica de domínio (dsl) - denominada privasl - foi desenvolvida para permitir a descrição do repositório original com facilidade, a identificação das entidades que se pretende explorar e a definição do nível de privacidade a ser assegurado. para ajudar visualmente os utilizadores da ferramenta a descrever os seus repositórios, foi desenvolvida uma plataforma web – após se descrever o repositório, a descrição privasl correspondente ´e gerada. no final, analisaram-se diferentes tipos de repositórios, com diferentes informações utilizando o privas. retiraram-se conclusões sobre as transformações e sobre a perda de privacidade."
    ],
    0.06666666666666667
  ],
  [
    [
      "machine learning como um campo, parte integrante da área de inteligência artificial, tem crescido exponencialmente, principalmente nesta última década, onde passou de quase desconhecido pelo público em geral para a existência de carros autónomos e até robôs humanóides como o robô sophia da arábia saudita. a maioria de nós agora lida com inteligência artificial todos os dias, em anúncios direcionados por exemplo, o que é agora a norma. deep learning, um ramo específico de machine learning de onde originaram as redes neuronais, é vastamente utilizado no desenvolvimento de sistemas autónomos de alta complexidade. alguns destes sistemas em particular podem ser classificados como sistemas críticos, o que traz a necessidade de fornecer alguma forma de garantia de que estes sistemas vão sempre funcionar como é suposto, uma vez que qualquer falha em sistemas desta categoria pode ter consequências graves. isto naturalmente levanta preocupações relativas à segurança, levando a comunidade a procurar uma forma de obter tais garantias, eventualmente levando-os aos métodos de verificação formal para atingir os níveis de confiabilidade necessários para a adoção pública de tais sistemas. tem havido um interesse crescente quanto a este assunto, uma vez que as aplicações de redes neuronais estão em constante expansão, e muitas ferramentas de software já resultaram deste trabalho, sendo algumas dessas ferramentas analisadas nesta dissertação. este estudo vem contribuir para esse esforço, e tem como objetivo principal a avaliação do why3, a fim de compreender se esta ferramenta possui as características necessárias que lhe permitam juntar-se a estas ferramentas já existentes como um novo meio de verificação da correção de redes neuronais. para atingir este objetivo, primeiramente criamos um proof-of-concept a fim de analisar se o why3 fornece o suporte necessário para esta tarefa. em seguida, damos um passo em frente e formalizamos uma rede neuronal à escala de uma aplicação real no why3, de onde tiraremos as nossas conclusões. durante o trabalho sobre a formalização de redes neuronais, pretendemos também compilar um guia abrangente sobre why3, desde as funcionalidades que oferece, até exemplos de como pode ser aplicado explicados passo a passo, com o objetivo de oferecer uma base de conhecimento compreensiva para qualquer pessoa interessada em explorar o why3, contribuindo ao mesmo tempo para a escassa documentação existente sobre o why3.",
      "machine learning as a field, from the realms of artificial intelligence, has been growing exponentially, especially in this last decade, where it went from the general public barely even hearing about it to the existence of self-driving cars and even humanoid robots like the saudi arabian sophia. most of us now deal with ai everyday, in targeted ads for example, and it has become the norm. deep learning, a particular branch of machine learning from where neural networks stem, is widely used in the development of high-complexity autonomous systems. some of these systems in particular can be classified as critical systems, which brings the necessity of providing some form of guarantee that these will always work as intended, since any failure from this category of systems can have serious consequences. this naturally raises security concerns, hence leading the community to search for a way of attaining such guarantees, eventually leading them to formal verification methods to achieve the necessary reliability levels for the public adoption of said systems. there has been a growing interest regarding this matter, since the applications of neural networks are continuously expanding, and many software tools have already resulted from this work, some of which will be analysed in this dissertation. this study comes to contribute to this effort, and has as its main objective the evaluation of why3, in order to understand if this tool possesses the necessary characteristics that may allow it to join these already existing tools as a new mean of verifying the correctness of neural networks. to achieve this objective, firstly we create a proof-of-concept in order to analyse if why3 provides the necessary support for this task. then we go a step further and formalize a real life application scale neural network in why3, from where we will draw our conclusions. while working on the formalization of neural networks, we also aim to compile a comprehensive guide on why3, where we go from the functionalities that it provides, to examples of how it can be applied explained step by step, with the goal of offering an understandable knowledge base for anyone that may be interested in exploring why3, while also contributing to the scarce existing documentation of why3."
    ],
    [
      "nowadays, customer satisfaction is highly relevant in the business area, as it is only possible to design, produce and supply a product if a customer or partner is interested and invests in it. and so, it is extremely important to manage with as much rigor and care the product to be placed on the market as the buyer, costumer or group that will use it. atomius is an application used by bosch’s chemical team to manage products which need to be analyzed at a microscopic level and in detail in order to verify if a product has any defects, if it is damaged or if it has any other type of imperfection. this analysis aims to confirm any problem with equipment, whether notified by the factory, whether partner or customer, will have a cost for the company depending on its origin. this dissertation, written in a business context, describes the process of defining and monitoring the development of an interactive platform to support the analysis of products uploaded due to the need of verifying their composition. it allows chemical components to be associated with each product, as well as organize them into groups by state of their analysis and draw conclusions on these results. this allows drawing conclusions about the origin of the defect or imperfection, leading to the correction and verification of the remaining products at the production level or verifying if it is a unique case to be resolved.",
      "atualmente o suporte ao cliente é de elevada relevância em contexto de negócio, na medida em que tudo o que é pensado, produzido e fornecido é apenas possível fornecer rendimento se um cliente ou parceiro tiver interesse no produto e fizer investimento no mesmo. neste sentido, é de elevada importância tratar com o mesmo rigor e cuidado o produto a colocar em mercado como o próprio cliente, pessoa ou grupo que o irá usufruir. atomius é uma aplicação utilizada pela equipa química para gestão de produtos os quais precisem ser analisados a nível microscópico e em detalhe com o objetivo de verificar se um produto terá algum defeito, se se encontra danificado ou se tem algum outro tipo de imperfeição. esta análise tem como objetivo confirmar se algum problema com um equipamento, tanto notificado pela fábrica, parceiro ou cliente, terá algum custo para a empresa dependendo da origem do mesmo. a presente dissertação, desenvolvida em contexto empresarial, descreve o processo de definição e acompanhamento o design e desenvolvimento de uma plataforma interativa de apoio à análise de produtos inseridos após a necessidade de verificação da composição dos mesmos e permite que sejam associados a cada produto os meus constituintes químicos, bem como organizar os mesmo em grupos por estado da sua análise e tirar conclusões sobre estas análises. isto permite tirar conclusões sobre a origem do defeito ou imperfeição, levando à correção e verificação dos restantes produtos a nível da produção ou verificar se é um caso único a ser resolvido."
    ],
    0.0
  ],
  [
    [
      "o ciclo de vida dos produtos informáticos é um processo contínuo, durante o qual vão surgindo de uma forma regular novas actualizações, dia após dia, que frequentemente visam a melhoria dos processos de interação do utilizador com o produto. contudo, este processo é complicado, uma vez que responder de uma forma apropriada às necessidades de cada cliente não é uma tarefa fácil. de forma a combater esta tarefa, a wedo technologies decidiu implementar no seu principal produto de software, o raid, um sistema capaz de monitorizar as diversas interações dos clientes com o produto e com base na informação recolhida apresentar-lhes cenários de interação avançados que de alguma forma reflitam as suas necessidades, por exemplo, em termos de introdução de dados ou configurações de serviços. desta forma, é possível retirar algum trabalho aos utilizadores do software uma vez que o sistema adianta-se no seu fornecimento. com este sistema, a wedo pretende alargar um conjunto diverso de funcionalidades dos seus produtos de forma a facilitar os processos de interação com o utilizador, promovendo o desenvolvimento de um protótipo para a geração de recomendações adequadas a cada utilizador dos seus produtos de software, a partir de um conjunto base das suas preferências de utilização do sistema. basicamente, com este trabalho de dissertação estudou-se, projetou-se e desenvolveu-se esse sistema de recomendação, materializando, de certa maneira algo que m. o'brien referiu: “the web is leaving the era of search and entering one of discovery. what’s the difference? search is what you do when you’re looking for something. discovery is when something wonderful that you didn’t know existed, or didn’t kown how to ask for, finds you.” (m. o'brien, 2006).",
      "the life cicle of software products it’s a continuous process. in the middle of this process, new updates will appear in a regular basis, with the single goal of a continuous improvement between the interaction of the user and the software. however, this process can be complex, because it is not an easy task to meet all customer needs. with this goal in mind, wedo technologies took the first steps to build, in the main software product, raid, a system capable of monitoring every customer interaction with the software. with this data, we can present to the user a set of scenarios that reflect the client needs, like, fill in the data inputs or advance service configurations that could be done accordingly to what the customers want. with this system, wedo wants to expand a set of current features that already exists in the core product to ease the integrate the recommendation system in raid. to accomplish this, we intend to develop a prototype that should be able of generating a set of recommendations base on each user preferences. basically, with this dissertation we have studied, projected and developed this recommendation system, that in a certain way correspond to what m. o'brien said: “the web is leaving the era of search and entering one of discovery. what’s the difference? search is what you do when you’re looking for something.” (m. o'brien, 2006)."
    ],
    "com a evolução da internet of things, a presença de tecnologias de informação e comunicação está cada vez mais omnipresente e vários estudos realizados nesse âmbito têm proporcionado o desenvolvimento de novas arquiteturas baseadas no conceito de cloud computing. neste contexto, foram desenvolvidos novos modelos de serviços e novas ferramentas que se tornam importantes para a gestão dos dados adquiridos e a descoberta de novos conhecimentos relacionados com as realidades do nosso mundo e sobre os comportamentos, relacionados com mobilidade automobilística. devido à importância que os advanced driver assistance systems na indústria automóvel, pois garantem uma maior segurança e conforto na condução. com isto, surge uma oportunidade de ter os veículos conectados, a recolher dados durante a condução com a finalidade de gerar informação que seja útil para auxiliar e melhorar o processo de condução. uma arquitetura lógica para sistemas que visam realizar um systematic field data exploration, baseada no conceito cloud computing, é proposta neste trabalho. a arquitetura visa dar uma resposta sistemas de visam realizar uma recolha de dados sistemática, considerando a aquisição, armazenamento, processamento e partilha de dados.",
    0.06666666666666667
  ],
  [
    [
      "in recent years we have witnessed a great technological advance accompanied by an equally impressive increase in energy consumption, causing problems of both financial and environmental order. in order to counteract this tendency, green computing emerges with a number of measures for a more efficient use of computing resources without a great loss of performance. this essay is a study of several elements of information technology analyzed from the point of view of energy efficiency. with special emphasis on microprocessors, modern compiler design, development tools and optimization of code generation, a wide range of information is gathered on very relevant subjects through perspectives still not very considered by the community in general. also presented are two experimental studies that analyze the optimization of generated code for a set of benchmark programs in several programming languages with the aim of apraise the otimization impact on improving their energy consumption efficiency. a software measurement framework was also developed that, together with the methodologies presented in both studies, allows obtaining very precise and pertinent results for analysis. finally, a ranking was produced for 18 development tools, considering the execution time and energy consumption of the executables generated through their compilation profiles. this study also intends to contribute to an energy efficient technological advancement. all the work developed here may also serve as motivation so that these and other aspects of information technology may be seen through a greener perspective.",
      "nos últimos anos temos assistido a um grande avanço tecnológico acompanhado por um aumento igualmente impressionante do consumo energético, provocando problemas quer de ordem financeira quer de ordem ambiental. com o intuito de contrariar essa tendência, surge o green computing com várias medidas para uma utilização mais eficiente dos recursos computacionais sem grande perda de performance. esta dissertação apresenta um estudo relativo a diversos elementos das tecnologias de informação analisados do ponto de vista da eficiência energética. com especial destaque para microprocessadores, conceção moderna dos compiladores atuais, ferramentas de desenvolvimento e geração de código otimizado, é aqui reunida uma vasta gama de informação sobre assuntos bastante relevantes segundo perspetivas ainda pouco consideradas pela comunidade em geral. são também apresentados dois estudos experimentais que analisam a otimização do código gerado para um conjunto de programas benchmarks em várias linguagens de programação com o objetivo de compreender o impacto das otimizações no sentido de melhorar a eficiência energética dos programas compilados. foi também desenvolvida uma framework de medição por software que em conjunto com as metodologias apresentadas em ambos os estudos permite a obtenção de resultados bastante precisos e pertinentes de análise. por último é elaborado um ranking para 18 ferramentas de desenvolvimento considerando o tempo de execução e consumo energético dos executáveis gerados através dos seus perfis de compilação. este estudo pretende assim contribuir para um avanço tecnológico energeticamente mais eficiente. que todo o trabalho aqui desenvolvido possa também ele servir de motivação para que estes e outros aspetos das tecnologias de informação possam ser vistos através de uma perspetiva mais ecológica."
    ],
    [
      "as plantas são organismos fotossintéticos multicelulares essenciais para a vida humana e têm um enorme impacto na nossa economia. como o crescimento e a sobrevivência das plantas estão intrinse camente relacionados com o seu metabolismo, as suas respostas metabólicas às condições ambientais merecem ser estudadas. com esse objetivo nasceu a biologia de sistemas, que usa a computação de modelos teóricos e matemáticos de modo a analisar sistemas biológicos como um todo. estes modelos ajudam a perceber processos biológicos e a estudar mecanismos metabólicos em diferentes condições. vários métodos têm vindo a ser desenvolvidos de modo a criar modelos metabólicos mais precisos que integram diferentes ómicas. isto foi possível devido ao avanço das tecnologias de alto rendimento. tais modelos são capazes de gerar previsões de fluxo mais precisas. assim, neste estudo, métodos e algoritmos para integrar diferentes dados ómicos foram implementa dos com um modelo metabólico. três algoritmos foram estudados: gx-fba, riptide e examo. o modelo metabólico selecionado foi o da arabidopsis thaliana (aragem), com dados ómicos em condições de seca. os resultados dos algoritmos foram comparados com a literatura e com os modelos gerados pelo gimme. inicialmente, os resultados do gx-fba demonstravam-se promissores, com número de reações e metabolitos semelhante aos modelos do gimme. no entanto, após estudar as reações contendo fluxo, gx-fba aparentava não ser capaz de distinguir as condições de um modo significativo. em relação ao riptide, os resultados foram surpreendentes, sendo capaz de diferenciar as duas condições, apesar de inicialmente apresentarem significativamente menos reações e metabolitos, e de partilhar com o gimme reações importantes envolvidas na resposta do metabolismo à seca. por fim, o examo não conseguiu gerar modelos viáveis.",
      "plants are multi-cellular photosynthetic organisms vital for human life and have a tremendous impact on the economy. as plants’ growth and survival are directly related to their metabolism, their metabolic responses to environmental conditions must be studied. hence, systems biology which uses computational and mathematical models to analyse biological systems as a whole. these models help understand biological processes and also study metabolic mechanisms in different conditions. various methods have been developed to create more accurate metabolic models by integrating different context-specific omics data. this has become possible due to the advancement of high-throughput technologies. these models generate more accurate flux predictions. accordingly, in this study, methods and algorithms for integrating different omics data were implemented with a metabolic model. three algorithms were studied: gx-fba, riptide and examo. the metabolic model selected as the case study was of arabidopsis thaliana (aragem), with transcriptomic data from drought conditions. the algorithms results were compared to the literature and to the models generated by gimme. initially, gx-fba results appeared promising with similar number of reactions and metabolites to the model from gimme. however, after studying the reactions containing flux, gx-fba seemed to not be able to distinguish conditions in a significant manner. regarding riptide, the results were surprising, being able to differentiate the two conditions, despite of having significantly fewer reactions and metabolites, and sharing with gimme important reactions involved in the response of the metabolism to water deprivation. lastly, examo was not able to generate feasable models."
    ],
    0.3
  ],
  [
    [
      "progressivamente, os robôs tendem a ser mais cooperativos e inteligentes, para que possam interagir de forma natural com os seres humanos. uma tipologia de robôs frequentemente utilizada na interação com os seres humanos são os robôs humanoides, uma vez que se assemelham fisicamente à aparência do corpo humano e são capazes de prestar auxílio na realização de diversas tarefas do quotidiano. ao longo da última década, investigadores têm estudado a viabilidade do uso de robôs humanoides no estímulo da interação social em crianças com perturbação do espectro do autismo (pea). o autismo é um transtorno do neurodesenvolvimento que se manifesta precocemente, onde o comportamento dos indivíduos é caracterizado por padrões repetitivos, atividades ou interesses restritos e pela dificuldade na comunicação/ interação social. neste contexto, a presente dissertação tem como objetivo o desenvolvimento de um sistema de ensino interativo, capaz de promover o desenvolvimento sócio emocional em crianças com pea utilizando, como mediador, um robô humanoide. o sistema desenvolvido possui uma arquitetura capaz de incorporar diferentes atividades de ensino e utiliza o algoritmo you only look once (yolo) para deteção de objetos em imagens. com a finalidade de testar o sistema desenvolvido, foram desenvolvidas duas atividades de ensino, nomeadamente, o ensino de figuras geométricas e cores. finalmente, o sistema desenvolvido foi testado de duas formas distintas: a) sem o robô humanoide, sendo este substituído pela voz do computador; b) com robô humanoide, sendo este responsável pela interação robô-humano. os resultados revelam que o sistema de ensino desenvolvido é capaz de detetar, com elevada percentagem de precisão, as figuras geométricas e as cores pré-definidas na lista de atividades, podendo assim ser uma ferramenta promissora no ensino de crianças com dificuldades de aprendizagem.",
      "progressively, robots tend to be more cooperative and intelligent, so they can interact naturally with humans. a typology of robots frequently used in the interaction with humans are humanoid robots, once they physically resemble of the human body and they are able to help in various daily tasks. over the past decade, researchers have studied the feasibility of using humanoid robots to stimulate social interaction in children with autism spectrum disorder (asd). autism is a neurodevelopmental disorder that manifests itself early, where the behavior of individuals is characterized by repetitive patterns, restricted activities or interests, and difficulty in communication/social interaction. in this context, the present dissertation aims to develop an interactive teaching system, capable of promoting socio-emotional development in children with asd using, as a mediator, a humanoid robot. the developed system has an architecture capable of incorporating different teaching activities and uses the you only look once (yolo) algorithm to detect objects in images. in order to test the developed system, two teaching activities were developed, namely, the teaching of geometric figures and colors. finally, the developed system was tested in two different ways: a) without the humanoid robot, which was replaced by the computer voice; b) with humanoid robot, which is responsible for robot-human interaction. the results show that the developed education system is able to detect, with a high percentage of accuracy, the pre-defined geometric figures and colors in the list of activities, showing that the developed system is a promising tool able to teach children with learning difficulties."
    ],
    [
      "a avaliação é um momento determinante na elaboração de estratégias de sucesso da aprendizagem. em ambientes presenciais o educador pode observar o comportamento dos seus alunos e determinar caminhos que facilitem a avaliação e não induzam o stress e as consequências negativas no resultado da aprendizagem. nos ambientes de aprendizagem de e-learning torna-se impossível o contacto direto e, como tal, terão de existir formas facilitadoras de detetar e prevenir as situações de stress nos momentos de avaliação. urge portanto analisar o stress e determinar estratégias de resolução dos problemas que são causados por ele. neste trabalho pretende-se desenvolver um módulo de análise de stress em momentos de avaliação em contextos de aprendizagem em linha que possa indicar ao educador os momentos mais propícios para intervir assim como os conteúdos que causam maiores dificuldades. desta forma o educador poderá intervir de forma mais eficiente junto dos alunos que mais precisem.",
      "evaluation is a key moment in the development of strategies for successful learning. in traditional environments the teacher can observe the behavior of their students and determine ways that facilitate evaluation and do not induce stress and negative consequences on learning outcomes, this is possible because there’s a physical presence. in an e-learning environments it becomes impossible to promote de physical contact a there for there’s de need of tools that can help to detect a prevent stress situations in evaluation moments. it is therefore urgent to analyze the stress and determine strategies for solving the problems that are caused by it. this work aims to develop a stress analysis module for evaluation moments for e-learning environments that indicate the educator the proper occasion for his intervention as well as the contents that cause difficulty to the students. in this way the teacher can intervene more effectively with students who need more."
    ],
    0.0
  ],
  [
    [
      "the growth of the internet and embedded systems have allowed physical devices to collect and exchange data in the internet-of-things (iot). iot allows objects to be monitored and controlled remotely across an existing network infrastructure, while creating opportunities to assimilate computer systems with the real world. the expansion of iot’s connectivity has lead devices to exchange large amounts of data, due to constantly required monitoring. the output of these devices can be seen as streams with data made available incrementally over time. this has created a new demand to collect, process and analyze iot data in an efficient and scalable way. in the meantime, databases have been organizing collections of data for several decades. at a low level, database management systems (dbmss) to organize data efficiently. in particular, data stream management systems (dsmss) have emerged to handle uninterrupted flows of streaming data and integrate them with relational databases [aggarwal, 2007]. with this objective in mind, dsmss have distinguished from traditional dbmss with new architectures, data models, algorithms and specific query languages to deal with streams. as streams are uninterrupted, dsmss aim to process them incrementally. this lead to the continuous queries concept, where streaming data is processed with small batches each time. meanwhile, other database management systems have explored alternate ways to organize data. monetdb is a pioneer column-oriented relational database management system (rdbms), storing relations column-wise opposed to rows as the majority of rdbmss. columnar-wise storage allows several benefits such as per-column query parallelization, data compression and late materialization. monetdb is being developed at centrumwiskunde & informatica (cwi) in amsterdam since 1993, having achieved faster benchmark results than popular rdbmss such as postgresql [muhleisen, 2014]. this master thesis has the objective to create a streaming engine over monetdb while focusing on iot processing. amsterdam internet-of-things app (aiota) is a full-stack application aiming to be integrated easily with iot devices to collect streaming data, while taking advantage of monetdb’s columnar-wise storage to process it and deliver results immediately.",
      "o crescimento da internet e dos sistemas embebidos tem permitido expor dispositivos físicos na internet das coisas (iot) para a troca de dados. a iot permite a monitorização de objetos remotamente em infraestruturas de rede criando oportunidades para assimilar a computação com o mundo real. a expansão da conetividade da iot tem levado esses dispositivos a promover o intercâmbio de grandes quantidades de dados, em maior parte devido a monitorização constante. os dados resultantes desses dispositivos podem ser visto como streams, onde os dados são disponibilizados incrementalmente com o decorrer do tempo. como consequência, as streams criaram uma nova forma de colecionar, processar e analisar dados provenientes da iot de modo eficiente e escalável. ao mesmo tempo nas últimas décadas, as bases de dados tem organizado coleções de dados. a um nível mais baixo, os sistemas de gestão de bases de dados (dbmss) tem procurado metodologias para organizar os dados de forma eficiente. em particular, os sistemas de gestão de streams (dsmss) tem emergido com novos métodos para lidar com fluxos de dados ininterruptos e integrá-los com as bases de dados convencionais [aggarwal, 2007]. com este objetivo em mente, as dsmss tem-se distinguido dos dbmss com novas arquiteturas, modelos de dados, algoritmos e linguagens de interrogação para lidar com streams. como as streams são inenterruptas, as dsmss tem a finalidade de as processar incrementalmente. isto levou ao conceito de continuous queries, onde as streams so processadas com pequenas quantidades de cada vez e incrementalmente. entretanto outros sistemas de gestão de bases dados tem explorado metodologias alternativas para organizar os dados. o monetdb é um sistema de gestão de bases dados relacional colunar pioneiro, onde as relações são armazenadas por colunas em vez de linhas como a maioria dos rdbmss. o armazenamento colunar permite vários benefícios que não seriam possíveis com o armazenamento por linhas tais como paralelização de interrogações por colunas, compressão de dados e materilização mais tardia. o monetdb tem sido desenvolvido pelo centrum wiskunde & informatica (cwi) em amsterdão desde 1993, tendo alcançado benchmarks com melhores resultados em comparação com populares sistema de gestão de bases dados como o postgresql [muhleisen, 2014]. esta tese de mestrado tem o objetivo de criar uma extensão de streaming no monetdb com focus na iot. a amsterdam internet-of-things app (aiota) é uma aplicação full-stack com o objetivo de ser integrada facilmente com dispositivos iot para colecionar dados para streams, tendo ao mesmo tempo como vantagem o armazenamento colunar do monetdb para processar os dados e disponibilizar resultados imediatamente."
    ],
    [
      "os registos clínicos têm como função facilitar a continuação da prestação de cuidados, a documentação dos seus processos e a comunicação entre os pro ssionais de saúde. têm sido desenvolvidos protocolos para registos de dados clínicos, no entanto, existe pouco conhecimento sobre o que realmente é documentado. o registo clínico electrónico (rce) encontra-se em ampla expansão, recorrendo-se, cada vez mais, à sua implementação em unidades hospitalares, o que proporciona uma maior agilidade no tratamento dos processos e uma consequente melhoria na qualidade da abordagem ao historial do paciente. a crescente focalização na avaliação do desenvolvimento de produtos, baseada em testes de garantia e no design do produto, está na origem do termo usabilidade, que ganha uma importância vertiginosa, à medida que aumenta o número de pessoas que dependem de dispositivos técnicos para realizar tarefas. a usabilidade de um sistema informático pode ser de nida como uma qualidade inerente ao sistema, que possibilita que os utilizadores o utilizem com satisfação, e cácia e e ciência. de facto, a adoção do rce e a satisfa- ção do utilizador estão intimamente associados à usabilidade do sistema. ao contrário de muitas indústrias, onde a usabilidade é a norma em design de produto, a prática de usabilidade no rce tem sido esporádica, não sistemá- tica, casual e super cial, em parte devido à falta de estruturas especí cas de rce e de aplicação de métodos que avaliem a usabilidade. assim, da conjunção da problemática de avaliação do rce com o desa o de tirar o melhor partido das potencialidades dos sistemas rce já implementados, surge o presente projeto. pretende-se apresentar uma abordagem global dos vários aspetos relacionados com a avaliação da qualidade do rce nas unidades de saúde. pretende-se ainda realizar essa avaliação ao rce implementado no chaa, para se proceder à elaboração de um protótipo que contemple as alterações mais emergentes ao sistema. desta forma, foi avaliada a usabilidade dos sistemas sam, sonho, sape e aida, através de questionários, distribuídos a 38 participantes. constataramse que as maiores di culdades sentidas com os utilizadores dos sistemas relacionam-se com o elevado tempo de desempenho do sistema e com a falta de motivação para introduçao de registos informatizados. contudo, os dados recolhidos realçam as enormes vantagens desta desmaterialização hospitalar, como sejam a diminuição dos erros e do tempo de leitura e transmição dos registos clínicos.",
      "medical records have the task of facilitating the continuation of care, processes documentation and communication between health professionals. protocols have been developed to record clinical data. however, there is little knowledge about what is actually documented. electronic health record (ehr) is in full expansion, resorting increasingly to its implementation in hospitals, which provides greater exibility in handling cases and a consequent improvement in the quality of the approach to the patient's history. increased focus in the evaluation of products' development, based in assurance testing and product design, is at the origin of the term usability, which gains vertiginous importance given the increasing number of people who rely on technical devices to perform tasks. the usability of a computer system can be de ned as an inherent system quality, which enables users to use it with satisfaction, e ectiveness and e ciency. in fact, the adoption of ehr and user satisfaction are closely associated with the usability of the system. unlike many industries, where usability is the standard in product design, the practice of usability in ehr has been sporadic, unsystematic, casual and super cial, partly due to de ciency of speci c ehr structures and lack of usability assessment methods. thus, this project arises from the conjunction of the ehr problematic assessment with the challenge of making the most of the already implemented ehr systems potential. it seeks to present a comprehensive approach to various aspects related to evaluating the quality of ehr in healthcare facilities. we also intend to conduct this evaluation in the implemented centro hospitalar do alto ave (chaa) ehr systems, to proceed with the development of a prototype that incorporates the needed changes. we evaluated the usability of sam, sonho, sape and aida systems, through questionnaires distributed to 38 participants . we found that users' biggest di culties are related to exceeding time spent with system perfomance and lack of motivation in healthcare professionals to the introduction of computerized records. however, the data collected highlight the enormous bene ts of this implementation, such as the reduction of errors and the time spent reading and transmit of clinical records."
    ],
    0.0
  ],
  [
    [
      "nos últimos anos, tem surgido um grande interesse na aplicabilidade das tecnologias de business intelligence (bi) na área da saúde. a grande satisfação resultante da sua implementação em outras áreas fez com que os profissionais de saúde juntamente com os profissionais de tecnologias de informação (ti) cooperassem para a conceção e o desenvolvimento de plataformas de bi em ambiente clínico. a grande motivação para a sua implementação adveio da possibilidade de conceber um sistema de apoio à decisão (sad) médica, que suportasse o processo de tomada de decisão e que permitisse que este fosse realizado de forma mais rápida e eficaz. além disso, com a implementação do processo clínico eletrónico, surgiu a necessidade de dar usabilidade aos dados dos utentes armazenados nas bases de dados, tornando-se isso possível com a implementação da plataforma de bi. neste projeto, incidiu-se numa área médica específica dentro da especialidade de ginecologia e obstetrícia, relacionada com a interrupção voluntária da gravidez (ivg), por ser uma unidade relativamente recente, onde não existe nenhuma tecnologia semelhante implementada e, ainda, por ser um foco de interesse para os profissionais de saúde. o local de estudo foi o centro materno infantil do norte (cmin), pertencente ao centro hospitalar do porto (chp), onde se teve acesso às fontes de dados necessárias para o desenvolvimento da plataforma de bi. além disso, foram também desenvolvidos modelos de data mining (dm), igualmente integrados na plataforma de bi, que permitem prever quais as utentes que recorrem à ivg que constituem um grupo de risco e quais as utentes que necessitam de acompanhamento da equipa de enfermagem durante o processo da ivg. os resultados obtidos foram bastante satisfatórios, uma vez que foram registados valores de 93% para a métrica da sensibilidade na questão relacionada com a probabilidade das utentes pertencerem ao grupo de risco, e valores de 91% e de 87% para a sensibilidade e acuidade, respetivamente, no problema relacionado com a previsão do local de realização de uma das etapas do processo de ivg. na seleção da tecnologia a utilizar para o desenvolvimento da plataforma de bi, optou-se pelo pentaho bi suite, depois de realizada uma pesquisa aprofundada sobre ferramentas open source. após a implementação da plataforma de bi, pode-se afirmar que o resultado foi satisfatório, uma vez que todos os indicadores de desempenho requisitados foram apresentados. nestes indicadores, estão incluídos a distribuição das utentes por idades, por profissão, por localidade, pela presença na última consulta de avaliação, entre outros. além disso, a informação representada é credível, pois esta foi submetida a um processo de validação por parte dos profissionais de saúde.",
      "in recent years, there has been great interest in the applicability of business intelligence (bi) technologies in healthcare. the great satisfaction resulting from its implementation in other areas meant that health professionals along with information technologies (it) professionals cooperate for the design and development of bi platforms in a clinical environment. the major motivation for its implementation came from the possibility of designing a medical support decision system, that supports the decision making process and allows this to be done more quickly and efficiently. furthermore, with the implementation of electronic medical process, the need arose to give the usability of users data stored in databases, making it possible with the implementation of the bi platform. this project is focused on a specific medical area, within the gynaecology and obstetrics specialty, related to voluntary interruption of pregnancy (vip), being a relatively new unit, where there is no similar technology implemented and also for being a focus of interest for health professionals. the study site was the centro materno infantil do norte (cmin) belonging to the centro hospitalar do porto (chp), where it was possible to access to the data sources necessary for the development of the bi platform. in addition, data mining (dm) models were developed, also integrated into the bi platform, that can predict which patients that resort to the vip belong to the risk group and which patients require monitoring of the nursing team during the vip process. the results were quite good, since it was reached a value of 93% of sensitivity in the question related to the probability of patients belong to the risk group, and values of 91% and 87% for sensitivity and accuracy, respectively, in the problem related to the probability of patients require monitoring of the nursing team during the vip process . in selecting the software to be used for the development of bi platform, the choice was the pentaho bi suite, after conducting a thorough research on open source tools. after the implementation of the bi platform, it can be stated that the result was satisfactory, since all the required performance indicators were presented. in these indicators are included the distribution of patients by age, by profession, by location, by the presence in the latter medical appointment for evaluation, among others. furthermore, the information represented is credible because it has undergone a process of validation by the health professionals."
    ],
    [
      "o desempenho escolar na área da matemática é uma preocupação crescente junto da comunidade educativa, tendo em conta o elevado insucesso escolar e correspondente abandono escolar precoce. os professores tentam perceber qual a melhor forma para captar a atenção dos alunos ou incutir um maior interesse nestes relativamente a esta disciplina. para além das técnicas utilizadas no ensino da matéria, os professores começam a aderir cada vez mais às novas tecnologias que auxiliam o ensino, uma vez que, estas permitem captar uma maior atenção por parte das crianças e jovens que, desde cedo, estão familiarizadas com as tecnologias atuais. o hypatiamat é um projeto, orientado mais em específico para alunos do 1º ao 9º ano, cujo principal objetivo passa por despertar, junto dos alunos, o gosto pela matemática e consequentemente aumentar o aproveitamento escolar nesta disciplina. para este efeito, esta plataforma fornece diversos conteúdos hipermédia como explicações, resumos, aplicações, jogos, etc. para além disso, esta plataforma realiza periodicamente diversos campeonatos com alguns dos seus jogos para milhares de alunos. no entanto, estes pecam por não possuírem a opção de multijogador online, o que dificulta a sua realização e impossibilita que os utilizadores possam jogar entre si os diversos jogos da plataforma de forma remota. deste modo, esta dissertação centra-se na implementação deste modo multijogador online num dos jogos do hypatiamat, para futuramente servir como modelo de um guião a ser implementado nos restantes jogos que a plataforma hypatiamat possui.",
      "school performance in mathematics is a growing concern within the educational com munity: everyone is worried about the school failure and the corresponding early school leaving. teachers are concerned with questions about what is the best way to capture the students’ attention in this subject. moreover, in addition to the techniques they already use, teachers are increasingly embracing new technologies that aid in teaching, as they allow them to capture more attention from children and young people who are familiar with technology from an early age. hypatiamat is a project that aims to awaken in students a taste for mathematics and consequently increase school success in this subject, being oriented more specifically to students from 1st to 9th grade. for this purpose, this platform provides several hypermedia contents such as explanations, summaries, applications, games, etc. in addition, this platform periodically holds several championships with some of its games for thousands of students. however, these do not currently support any online multiplayer functionalities, which makes it difficult to hold these championships and makes it impossible for users to play the platform’s various games remotely. having said this, this dissertation focuses on the implementation of this online multiplayer mode in one of the hypatiamat games, with the purpose of serving as a model of a guide to be followed in the future in the implementation of these same functionalities in the other games that the hypatiamat platform has."
    ],
    0.09999999999999999
  ],
  [
    [
      "these days, positioning systems are global navigation satellite system (gnss) based – such as global positioning system (gps) or the european galileo – have been deployed worldwide, due to their efficiency, reliability, and need. today using gps for navigation or localization is quite common, this technology shaped our world and it is now part of our life. however, these satellite-based positioning systems fail to provide good results inside infrastructures. if someone is inside a building, walls and other objects inside will attenuate the signals, making them unreliable for obtaining a position. for example, an indoor positioning system (ips) offering localization services inside a hospital could bring a lot of benefits, namely patient orientation, locating doctors and nurses for emergency responses, or immediately locating critical instrumentation, among others. in the case of a warehouse, it can be used for better logistics, optimization of resources and autonomous vehicle driving. other related contexts can be found in airports, museums and shopping malls, where ipss can be used to support indoor navigation. there is a large number of solutions created for this challenge, using technologies such as bluetooth low energy (ble), wi-fi, ultra-wideband (uwb), light detection and ranging (lidar), and infrared, among many others. these are usually associated with techniques such as proximity, trilateration, triangulation, and fingerprinting. this work will focus on using wi-fi technology using the fingerprinting technique, i.e., wi-fi fingerprinting for large indoor enviroments.",
      "atualmente, os sistemas de posicionamento baseados em gnss – como o gps ou o galileo europeu – foram implementados em todo o mundo devido à sua eficiência, confiabilidade e necessidade. hoje usar o gps para navegação ou localização é bastante comum, esta tecnologia moldou nosso mundo e agora faz parte de nossa vida. no entanto, esses sistemas de posicionamento baseados em satélite não fornecem bons resultados dentro das infraestruturas. se alguém estiver dentro de um prédio, paredes e outros objetos no interior atenuarão os sinais, tornando-os pouco confiáveis para estimar uma posição. por exemplo, um ips que ofereça serviços de localização dentro de um hospital pode trazer muitos benefícios, como orientação ao paciente, localização de médicos e enfermeiros para respostas de emergência, localização imediata de instrumentação crítica, entre outros. no caso de um armazém, pode ser utilizado para uma melhor logística, otimização de recursos e condução autónoma de veículos. outros contextos relacionados podem ser encontrados em aeroportos, museus e shopping centers, onde o ipss pode ser usado para dar suporte à navegação interna. existe um grande número de soluções criadas para este desafio, utilizando tecnologias como ble, wi-fi, uwb, lidar, infrared, entre muitas outras. estas geralmente estão associados a técnicas como proximidade, trilateração, triangulação e fingerprinting. este trabalho foca-se no uso da tecnologia wi-fi e o método fingerprinting, ou seja, fingerprinting wi-fi para grandes ambientes indoor."
    ],
    [
      "no contexto atual, o contínuo desenvolvimento tecnológico permite um acesso fácil e rápido a variadíssimos serviços e plataformas, por meio de dispositivos móveis. por este motivo, o volume e diversidade de tráfego tem crescido de forma exponencial também. o conhecimento do tráfego que circula nas redes atuais torna-se indispensável, seja para ajudar a melhorar a gestão e configuração dos elementos e serviços de rede, seja para os utilizadores terem a oportunidade de gerir melhor os recursos na utilização dos seus dispositivos móveis e aplicações. assim, este trabalho pretende aprofundar o estudo das características do tráfego gerado por dispositivos móveis, no acesso a determinadas plataformas/serviços. também se espera incluir uma componente de machine learning para previsão da experiência do utilizador. além disso, pretende-se obter base de comparação entre as versões web e aplicacional de um mesmo serviço.",
      "in the current context, continuous technological development allows easy and fast access to a wide range of services and platforms, through mobile devices. for this reason, the volume and diversity of traffic have grown exponentially as well. knowledge of the traffic circulating in current networks is essential, either to help improving the management and configuration of the network elements and services, or for users to have the opportunity to better manage resources when using their mobile devices and applications. thus, this work intends to deepen the study of the characteristics of the traffic generated by mobile devices when accessing certain platforms/services. it is also expected to include a machine learning component to predict the user experience. in addition, it is intended to obtain a basis for comparison between the web and application versions of the same service."
    ],
    0.0
  ],
  [
    [
      "the gaming industry has undergone some changes with the investments and professionalization of the sector, changing the way of playing video games from traditional leisure to be like a sportsman job. there are currently organized multiplayer video games competitions with professional players, know as electronic sports (esports). these professional video games players can be compared to athletes once they’re part of a team and with training, their performance can be improved as well as, given certain factors, conditioned. the emergence of team coaches was naturally introduced, and he’s responsible for optimizing team performance. due to this fact, arises the need to develop tools with the aim of improving the performance of these professional players as well as increasing the duration of their careers by taking care of their physical and mental health. it was proposed for this study the development of a performance optimization and reporting platform for esports to help the coaches and players, continuously and automatically collecting their behavioral states and reporting the obtained results in order to guide the training to improve individual and team performance. this platform was tested in a real environment, with professional teams as a case study, where it was possible to analyze the impact of mental fatigue and behavioral biometric performance on devices interaction in players’ game results.",
      "a indústria de jogos de video eletrónicos sofreu algumas alterações com investimentos e profissionalização do setor, mudando o modo de tradicional lazer a jogar para um trabalho similar a um atleta. atualmente, há competições organizadas, de jogos eletrónicos multijogador, com jogadores profissionais, conhecidas como esports. esses jogadores profissionais podem ser comparados com atletas, uma vez que fazem parte de uma equipa e o seu desempenho pode ser melhorado com treino e condicionado através de determinados fatores. o surgimento de treinadores de equipa foi naturalmente introduzido sendo ele responsável por otimizar o desempenho da equipa. devido a estes factos, surge a necessidade de desenvolver ferramentas com o objetivo de melhorar o desempenho desses profissionais, bem como aumentar a duração de suas carreiras, cuidando de sua saúde física e mental. foi proposto para este estudo o desenvolvimento de uma plataforma de otimização de desempenhos e relatórios para esports para auxiliar os treinadores e jogadores, recolhendo de forma contínua e automática os seus estados comportamentais e reportando os resultados obtidos, com a finalidade de orientar o treino para melhorar o desempenho individual e de equipa. a plataforma foi testada em ambiente real, com equipas profissionais como caso de estudo, onde foi possível analisar o impacto da fadiga mental e do desempenho biométrico comportamental na interação em dispositivos nos resultados dos jogos dos jogadores."
    ],
    [
      "a interface de um programa é um elemento importante na experiência que o utilizador tem com o software, pois constitui o principal método de interação com a lógica do programa. a existência de métodos fiáveis de verificação de sistemas de software permite o a conceção destes de acordo com a especificação e, em casos mais críticos, evitar erros com consequências graves. estes métodos rigorosos, no entanto, contrastam com a prática mais comum no desenho de interfaces. um dos métodos mais utilizados para o desenho e avaliação de interfaces é a prototipagem. os protótipos permitem transmitir aspetos do design da interface e até avaliar a sua usabilidade, mas não oferecem as garantias sobre o seu funcionamento que os métodos de verificação oferecem. o ivy workbench é uma ferramenta que suporta a modelação do comportamento de sistemas interativos e a verificação formal dos mesmos. a ferramenta contém um conjunto de plugins que suportam o processo de modelação e análise, incluindo um editor de modelos, um verificador de propriedades e um animador. este último permite visualizar e interagir com os modelos, mas não suporta associá-los a mockups representativos das interfaces. a interação com os modelos facilita a sua validação por parte de quem os está a desenvolver. não facilita, no entanto, a comunicação com os potenciais clientes do sistema modelado, para quem um protótipo será um meio mais eficaz de comunicação. neste documento propõe-se uma solução para o problema acima, assente no desenvolvimento de um novo plugin capaz de suportar a construção e animação de protótipos de sistemas interativos modelados no ivy. é descrito todo o processo de desenvolvimento, desde o levantamento de requisitos, até exemplos de aplicação que permitem demonstrar as novas funcionalidades existentes.",
      "a program’s interface is the most important element in the user’s experience with the soft ware, because it is the primary method through which the user interacts with the program’s logic. the availability of reliable software verification methods allows its conception accor ding to specification and, in critical cases, to avoid errors with grave consequences. these rigorous methods, however, are in contrast with the traditional approaches to user interface design. one of the most used methods for designing and evaluating interfaces is prototy ping. prototypes allow communication of design and usability aspects of a software system, but don’t offer any guarantees about their behaviour. ivy workbench is a tool that supports modeling interactive system’s behavior and their respective formal verification. the tool contains a set of plugins that enable the modeling and analysis process, including a model editor, a trace analyzer and an animator. this last feature allows visualization and interaction with models but doesn’t support linking them with mockups that represent them. model interaction makes it easier for developers to validate its behavior. however, it doesn’t facilitate the communication of the system’s behavior to potential clients, to which a prototype would be the most efficient communication path. in this document a solution to the above problem is presented, based on the develop ment of a new plugin that will support the construction and animation of prototypes of interactive systems modeled in ivy. the document describes the undertaken development process, from requirements elicitation to practical examples that help demonstrate the new available features."
    ],
    0.3
  ],
  [
    [
      "model checking is a technique used to automatically verify a model which represents the specification of some system. to ensure the correctness of the system the verification of both static and dynamic properties is often needed. the specification of a system is made through modeling languages, while the respective verification is made by its model-checker. most modeling frameworks are not ready to verify models rich in both kind of properties thereby limiting the verification of dynamic systems with rich configurations. electrum is a modeling language which mixes the best of the alloy and tla specification languages, with the capability of handling the problem mentioned above. this language is supported by two model-checking techniques – one bounded and one unbounded. nonetheless, the electrum’s bounded model-checker has limitations, thus, this dissertation aims to overcome them with the purpose of improving the analysis procedure of electrum models, in particular, the definition of a new electrum’s semantics through a translation into kodkod as well as the creation of a novel procedure of verifying electrum models in parallel. hence, in order to achieve these goals, a temporal extension to the kodkod constraint solver was implemented.",
      "model-checking é uma técnica utilizada para verificar automaticamente modelos que representam uma determinada especificação de um sistema. para assegurar a correção de um sistema, e necessário verificar dois tipos de propriedades – dinâmicas e estáticas. a especificação de um sistema é realizada através das linguagens de modelação, enquanto que a verificação do mesmo é feita pelo seu respectivo model-checker. a maior parte das frameworks de modelação não se encontram preparadas para verificar os dois tipos de propriedades, limitando assim a verificação de sistemas dinâmicos com configurações ricas. o electrum é uma linguagem de modelação que conjuga o melhor de alloy e tla, com a capacidade de contornar o problema acima enunciado. esta linguagem tem associadas duas técnicas de model-checking, uma bounded e outra unbounded. no entanto, o bounded model-checker do electrum apresenta limitações, portanto, esta dissertação tem como objectivo resolver tais limitações a fim de melhorar o procedimento de análise dos modelos electrum. concretamente, a definição de uma nova semântica para o electrum através de uma tradução para kodkod, bem como a criação de um novo processo de verificação de modelos electrum em paralelo. consequentemente, estes dois últimos objectivos são alcançados através da construção de uma extensão temporal para o kodkod."
    ],
    [
      "sociedade atual tem vindo a adotar a conveniência dos serviços bancários online. estes, no entanto, tem enfrentado um problema crescente de fraudes. uma das soluções que emergiu no mercado para tentar contrariar esta tendência é o protocolo chip authentication program (cap), que fornece ao sistema mecanismos de autenticação forte, baseados em dois fatores de autenticação. contudo, este implica a utilização de um dispositivo dedicado, que, pelo investimento financeiro avultado envolvido e pela falta de aceitação dos clientes em transportar um dispositivo adicional sempre que pretendem usufruir dos serviços bancários online, tem funcionado como barreira à massificação deste protocolo. com a presença cada vez mais relevante dos smartphones na sociedade e com o surgimento de smart cards para estes dispositivos, os objetivos desta dissertação focaram-se, fundamentalmente, em chamar a atenção para a potencialidade destas tecnologias para aplicações com requisitos de segurança críticos e em incentivar a utilização do protocolo cap, ultrapassando os principais entraves à sua adoção e contribuindo com uma solução que permita reduzir as fraudes em serviços bancários online. o resultado desta dissertação é um sistema baseado no protocolo cap, que reúne as características que melhor caracterizam um smartphone com as propriedades de segurança que os smart cards acrescentam a um sistema. a utilização do smartphone, um dispositivo bem mais ubíquo do que o dispositivo dedicado tipicamente usado no protocolo cap, permite reduzir o investimento necessário, uma vez que não é necessário fornecer dispositivos aos clientes, e reflecte-se também numa maior aceitação por parte do utilizador final que não tem de transportar um dispositivo adicional para usufruir dos serviços bancários online.",
      "today’s society has adopted the convenience of online banking. these services, however, have faced a growing fraud problem. one solution that emerged to try to counteract this tendency is the chip authentication program (cap) protocol, which provides strong authentication mechanisms based on two-factor authentication. the main obstacle to the massification of this protocol is the use of a dedicated device, which involves a substantial financial investment and damages consumer. with the increasing presence of smartphones in society and the emergence of smart cards for these devices, the objectives of this dissertation have focused mainly on drawing attention to the potential of these technologies for applications with critical security requirements, and promoting the use of cap protocol, overcoming the main obstacle to its adoption and contributing with a solution for reducing frauds in online banking. the result of this master thesis is a system based on the cap protocol, which brings together the best features of a smartphone with the security properties that smart cards can add to a system. the use of smartphones, a device far more ubiquitous than the dedicated device typically used in the cap protocol, reduces the investment required, since it is not necessary to provide a device to clients, and also reflects in a greater acceptance by the enduser, who does not have to carry an additional device to take advantage of online banking."
    ],
    0.0
  ],
  [
    [
      "a crescente adoção de assistentes digitais faz com que os casos de uso para os quais estes são utilizados sejam cada vez mais abrangentes. isto aliado ao facto dos dispositivos iot estarem cada vez mais acessíveis, leva a que comece a ser comum os utilizadores controlarem diversos dos seus dispositivos através dos assistentes digitais. o presente documento retrata a dissertação com o tema gestão e controlo de dispositivos iot através a interação com assistentes digitais. o principal foco desta passa pela investigação da gestão de múltiplos dispositivos de iot, inseridos no mesmo ambiente e que possam ser geridos/controlados através de uma plataforma de interação por voz, neste caso um assistente digital. embora já existam soluções disponíveis para desenvolvimento de aplicações neste sentido, estas são muito recentes carecendo tanto ao nível do número de funcionalidades como da flexibilidade oferecida para sua utilização. esta investigação resulta numa proposta de arquitetura para aplicações deste domínio e da implementação de um sistema que permita fazer a gestão e controlo de um conjunto de dispositivos inteligentes inseridos no contexto de uma smart home. a arquitetura deste sistema e das aplicações que o constituem dever ao possibilitar a inclusão de novas funcionalidades inexistentes nos assistentes digitais atuais e a implementação de algumas destas funcionalidades devera ser apresentada como casos de estudo ao longo do presente documento.",
      "the increasing adoption of digital assistants makes their use cases more widespread. this, coupled with the fact that iot devices are becoming more and more accessible, increases the number of users that control many of their devices through digital assistants. this document portrays the dissertation on managing and controlling iot devices through digital assistants interaction. the main focus is the investigation on multiple iot devices management, taking in mind that these should be placed in the same environment and that can be managed/controlled through a voice interaction platform, in this case, a digital assistant. although there are already solutions available for building applications for digital as sistants, they are very recent, lacking in both the number of features available and the flexibility offered for their use. thus, this research should result in an architecture proposal for applications within this domain and the implementation of a system that allows the management and control of a set of smart devices within the context of a smart home. the proposed architecture of this system and its applications should allow to include new features that do not exist in current digital assistants and the implementation of some of these features should be presented as case studies throughout this document."
    ],
    [
      "q-learning is one of the most popular reinforcement learning algorithms. it can solve different complex problems with interesting tasks where decisions have to be made, all the while using the same algorithm with no interfer ence from the developer about specific strategies. this is achieved by processing a reward received after each decision is made. in order to evaluate the performance of q-learning on different problems, video games prove to be a great asset for testing purposes, as each game has its own unique mechanics and some kind of objective that needs to be learned. furthermore, the results from testing different algorithms on the same conditions can be easily compared. this thesis presents a study on q-learning, from its origins and how it operates, showcasing various state of the art techniques used to improve the algorithm and detailing the procedures that have become standard when training q-learning agents to play video games for the atari 2600. our implementation of the algorithm following the same techniques and procedures is ran on different video games. the training performance is compared to the one obtained in articles that trained on the same games and attained state of the art performance. additionally, we explored crafting new reward schemes modifying game default rewards. various custom rewards were created and combined to evaluate how they affect performance. during these tests, we found that the use of rewards that inform about both good and bad behaviour led to better performance, as opposed to rewards that only inform about good behaviour, which is done by default in some games. it was also found that the use of more game specific rewards could attain better results, but these also required a more careful analysis of each game, not being easily transferable into other games. as a more general approach, we tested reward changes that could incentivize exploration for games that were harder to navigate, and thus harder to learn from. we found that not only did these changes improve exploration, but they also improved the performance obtained after some parameter tuning. these algorithms are designed to teach the agent to accumulate rewards. but how does this relate to game score? to assess this question, we present some preliminary experiments showing the relationship between the evolution of reward accumulation and game score.",
      "q-learning é um dos algoritmos mais populares de aprendizagem por reforço. este consegue resolver vários problemas complexos que tenham tarefas interessantes e decisões que devem ser tomadas. para todos os problemas, o mesmo algoritmo é utilizado sem haver interferência por parte do desenvolvedor sobre estratégias específicas que existam. isto tudo é alcançado processando uma recompensa que é recebida após tomar cada decisão. para avaliar o desempenho de q-learning em problemas diferentes, os jogos eletrónicos trazem grandes vantagens para fins de teste, pois cada jogo tem as suas próprias regras e algum tipo de objetivo que precisa de ser compreendido. além disso, os resultados dos testes usando diferentes algoritmos nas mesmas condições podem ser facilmente comparados. esta tese apresenta um estudo sobre q-learning, explicando as suas origens e como funciona, apresentando várias técnicas de estado da arte usadas para melhorar o algoritmo e detalhando os procedimentos padrão usados para treinar agentes de q-learning a jogar jogos eletrónicos da atari 2600. a nossa implementação do algoritmo seguindo as mesmas técnicas e procedimentos é executada em diferentes jogos eletrónicos. o desempenho durante o treino é comparado ao desempenho obtido em artigos que treinaram nos mesmos jogos e atingiram resultados de estado da arte. além disso, exploramos a criação de novos esquemas de recompensas, modificando as recompensas usadas nos jogos por defeito. várias recompensas novas foram criadas e combinadas para avaliar como afetam o desempenho do agente. durante estes testes, observamos que o uso de recompensas que informam tanto sobre o bom como o mau comportamento levaram a um melhor desempenho, ao contrário de recompensas que apenas informam sobre o bom comportamento, que acontece em alguns jogos usando as recompensas por defeito. também se observou que o uso de recompensas mais específicas para um jogo pode levar a melhores resultados, mas essas recompensas também exigem uma análise mais cuidadosa de cada jogo e não são facilmente transferíveis para outros jogos. numa abordagem mais geral, testamos mudanças de recompensas que poderiam incentivar a exploração em jogos mais difíceis de navegar e, portanto, mais difíceis de aprender. observamos que estas mudanças não só melhoraram a exploração, como também o desempenho obtido após alguns ajustes de parâmetros. estes algoritmos têm como objetivo ensinar o agente a acumular recompensas. como é que isto está relacionado com a pontuação obtida no jogo? para abordar esta questão, apresentamos alguns testes preliminares que mostram a relação entre a evolução da acumulação de recompensas e da pontuação no jogo."
    ],
    0.06666666666666667
  ],
  [
    [
      "atualmente, escalabilidade, manutenibilidade e disponibilidade são algumas das medidas mais utilizadas na avaliação qualitativa de software. com uma presença cada vez maior de produtos de software no nosso dia a dia, há consequentemente a necessidade de torná-los melhores aos olhos do utilizador, surgindo novos desafios a serem explorados e superados na hora de projetar e desenvolver produtos de software. mais focado neste tema de dissertação de mestrado, a resiliência é de facto um ponto chave para o sucesso de um qualquer produto de software. cada vez mais as pessoas se encontram diretamente ligadas a produtos de software no seu dia a dia, o que torna o bom funcionamento destes essencial. assim sendo, o estudo de metodologias que permitam aumentar a resiliência e consequentemente a disponibilidade destes serviços ganha relevância. o principal objetivo desta dissertação é desenvolver uma metodologia para aumentar a resiliência de soluções orientadas aos microsserviços. assim, é fundamental primeiro entender quais soluções já desenvolvidas para esse fim. após reunir um conjunto de técnicas para aumentar a resiliência, analisamos um caso de estudo procurando possíveis problemas de resiliência. para além desta procura de vulnerabilidades, foram apresentadas propostas para a sua resolução, tendo em conta o conjunto de soluções já levantado. por fim, e avançando para a construção da metodologia alvo da dissertação, procedeu-se à análise de todas as propostas apresentadas, bem como a caracterização das interações problemáticas. desta forma, foi possível extrair a informação necessária do estudo para a construção da metodologia. como resultado deste estudo, também foi possível identificar uma nova proposta para aumentar a resiliência diante das necessidades do estudo de caso e da recorrência em que esta se tornou útil.",
      "currently, scalability, maintainability, and availability are some of the most used measures in the qualitative evaluation of software among developers. with an increasing presence of software products in our daily lives, there is, the need to make these products better in the eyes of the user, therefore raising new challenges to be explored and overcome when designing and developing software products. this work focuses on this master’s thesis theme, resilience is in fact a key point for the success of any software product. more and more people are directly connected to software products in their daily lives, which makes their smooth functioning essential. therefore, the study of methodologies that allow the increasing availability of these services undoubtedly gains relevance. the major objective of this dissertation is to develop a methodology for increasing the resilience of microservices-based solutions. thus, it was essential to first understand what solutions had already been developed for this purpose. after assembling a set of techniques for increasing resilience, we analyzed a case study and searched for possible resilience problems. besides this search for vulnerabilities, proposals were made for their resolution, taking into account the set of solutions already raised. finally, and moving towards the construction of the dissertation’s target methodology, an analysis was performed of all the proposals made as well as the characterization of problematic interactions, making it possible to generalize the study and reach the objective of the dissertation. as a result of this study, it was also possible to identify a new proposal to increase resilience given the needs of the case study and the recurrence in which it has become useful."
    ],
    [
      "os estudos relacionados com a neuroimagem têm assumido nos últimos anos grande importância por parte da comunidade médica e científica. o aumento da esperança média de vida faz com que se registem cada vez mais doenças do cérebro, das quais as demências e as doenças degenerativas têm assumido especial importância. na tentativa de perceber quais as alterações anatómicas registadas no cérebro com a idade e aquando do surgimento destas patologias começaram a ser realizados estudos estruturais a este onde a ressonância magnética tem-se demonstrado como principal ferramenta para este estudo. atualmente existem diversas técnicas que possibilitam o estudo estrutural e anatómico do cérebro todavia ainda não existe nenhuma técnica que possibilite o estudo integral de todas as características estruturais do cérebro; no entanto a medição da espessura cortical, volumetria e morfometria baseada em vóxel têm assumido especial preponderância no estudo destas características. o objetivo principal do presente trabalho consistiu em efetuar uma análise por regiões e por vóxeis de forma a perceber quais as regiões cerebrais que eram mais afetadas com a idade no estudo da volumetria, espessura cortical e da área, para isto foram utilizados um método convencional, e o glmfit para a análise por regiões e o qdec e o spm8 para o estudo por vóxeis. para se poder efetuar os estudos referidos anteriormente foi necessário pré-processar todos os dados em estudo através da utilização da aplicação freesurfer que possibilitou a correção de pequenos erros originados durante a aquisição das imagens. com esta dissertação conclui-se que os métodos utilizados para a deteção do comportamento das variáveis em estudo nas análises por regiões se demonstram coerentes entre si e entre os dados bibliográficos consultados, todavia na análise por vóxeis as conclusões não foram tão lineares sendo mesmo impossível efetuar uma comparação entre esses métodos, pois os resultados obtidos foram totalmente distintos.",
      "studies of neuroimaging have assumed great importance in recent years by the medical and scientific community. the increase in life expectancy increases also the number of brain desease, including dementia and degenerative diseases with particular importance. in order to understand the anatomical alterations caused by aging and these diseases, structural magnetic resonance imaging has proven to be a valuable tool for . currently there are several techniques that enable structural and anatomical study of the brain however there is still no technique that enables the comprehensive study of all the structural features of the brain, however cortical thickness, volumetric and voxel-based morphometry measurements have assumed particular preponderance in study of these topics. the main objective of this study was to perform an analysis by region and voxel in order to understand which brain regions were affected with aging in term of volumes, cortical thickness and area, so for that were used for a conventional method, and glmfit for analysis by regions and qdec and spm8 for study by voxels. in order to make the studies described above, it was necessary to pre-process all the data in the study by using the freesurfer application that enabled the correction of minor errors originated during image acquisition. with this thesis it is possible to conclude that the methods used for the detection of the variables behavior under study are coherent among them and according to the literature, however in the analysis by voxels the findings were not as linear as expected and it was even impossible to performe a comparison between these methods once the results were completely different."
    ],
    0.0
  ],
  [
    [
      "breast cancer is currently one of the most commonly diagnosed cancers and the fifth leading cause of cancer-related deaths. its treatment has a higher survivorship rate when diagnosed in the disease’s early stages. the screening procedure uses medical imaging techniques, such as mammography or ultrasound, to discover possible lesions. when a physician finds a lesion that is likely to be malignant, a biopsy is performed to obtain a sample and determine its characteristics. currently, real-time ultrasound is the preferred medical imaging modality to perform this procedure. the breast biopsy procedure is highly reliant on the operator’s skill and experience, due to the difficulty in interpreting ultrasound images and correctly aiming the needle. robotic solutions, and the usage of automatic lesion segmentation in ultrasound imaging along with advanced visualization techniques, such as augmented reality, can potentially make this process simpler, safer, and faster. the onconavigator project, in which this dissertation integrates, aims to improve the precision of the current breast cancer interventions. to accomplish this objective various medical training and robotic biopsy aid were developed. an augmented reality ultrasound training solution was created and the device’s tracking capabilities were validated by comparing it with an electromagnetic tracking device. another solution for ultrasound-guided breast biopsy assisted with augmented reality was developed. this solution displays real-time ultrasound video, automatic lesion segmentation, and biopsy needle trajectory display in the user’s field of view. the validation of this solution was made by comparing its usability with the traditional procedure. a modular software framework was also developed that focuses on the integration of a collaborative medical robot with real-time ultrasound imaging and automatic lesion segmentation. overall, the developed solutions offered good results. the augmented reality glasses tracking capabilities proved to be as capable as the electromagnetic system, and the augmented reality assisted breast biopsy proved to make the procedure more accurate and precise than the traditional system.",
      "o cancro da mama é, atualmente, um dos tipos de cancro mais comuns a serem diagnosticados e a quinta principal causa de mortes relacionadas ao cancro. o seu tratamento tem maior taxa de sobrevivência quando é diagnosticado nas fases iniciais da doença. o procedimento de triagem utiliza técnicas de imagem médica, como mamografia ou ultrassom, para descobrir possíveis lesões. quando um médico encontra uma lesão com probabilidade de ser maligna, é realizada uma biópsia para obter uma amostra e determinar as suas características. o ultrassom em tempo real é a modalidade de imagem médica preferida para realizar esse procedimento. a biópsia mamária depende da habilidade e experiência do operador, devido à dificuldade de interpretação das imagens ultrassonográficas e ao direcionamento correto da agulha. soluções robóticas, com o uso de segmentação automática de lesões em imagens de ultrassom, juntamente com técnicas avançadas de visualização, nomeadamente realidade aumentada, podem tornar esse processo mais simples, seguro e rápido. o projeto onconavigator, que esta dissertação integra, visa melhorar a precisão das atuais intervenções ao cancro da mama. para atingir este objetivo, vários ajudas para treino médico e auxílio à biópsia por meio robótico foram desenvolvidas. uma solução de treino de ultrassom com realidade aumentada foi criada e os recursos de rastreio do dispositivo foram validados comparando-os com um dispositivo eletromagnético. outra solução para biópsia de mama guiada por ultrassom assistida com realidade aumentada foi desenvolvida. esta solução exibe vídeo de ultrassom em tempo real, segmentação automática de lesões e exibição da trajetória da agulha de biópsia no campo de visão do utilizador. a validação desta solução foi feita comparando a sua usabilidade com o procedimento tradicional. também foi desenvolvida uma estrutura de software modular que se concentra na integração de um robô médico colaborativo com imagens de ultrassom em tempo real e segmentação automática de lesões. os recursos de rastreio dos óculos de realidade aumentada mostraram-se tão capazes quanto o sistema eletromagnético, e a biópsia de mama assistida por realidade aumentada provou tornar o procedimento mais exato e preciso do que o sistema tradicional."
    ],
    [
      "nos dias de hoje, devido à acentuada evolução que a tecnologia tem vindo a sofrer, as aplicações informáticas tornaram-se indispensáveis no nosso dia-a-dia. inúmeras áreas beneficiam, das mais variadas formas, das aplicações que tem ao seu dispor e a área da saúde não foge à regra. no que toca aos lares de idosos, tendo em conta que estes estão cada vez mais lotados, os auxiliares de saúde não têm mãos a medir em relação à saúde dos utentes. no entanto, ainda são usados métodos arcaicos, que passam por manter toda a informação relativa aos tratamentos farmacológicos em formato físico, o que conduz a uma probabilidade de erro humano bastante elevada. para que seja possível tornar mais eficiente e reduzir a probabilidade de erro nos tratamentos farmacológicos dos utentes nos lares, é essencial que o processo de administração e planeamento destes tratamentos seja agilizado. desta forma, pretende-se desenvolver uma aplicação que vise minimizar os problemas anteriormente mencionados, assegurando assim informação atualizada a todo o instante, históricos fidedignos, controlo sobre stocks de medicação, entre outros.",
      "nowadays, due to the accentuated evolution that technology has been undergoing, informatics applications have become a must in our daily lives. countless areas benefit, in the most varied ways, from the applications they have at their service and the health area is no exception. when it comes to nursing homes, taking into account that they are at their maximum capacity, the healthcare assistants have to much work on their hands. however, archaic methods are still used, which involve keeping all information related to pharmacological treatments in physical format, which leads to a very high probability of human error. in order to make it more efficient and reduce the error of pharmacological treatments of patients in nursing homes, it is essential to streamline the procedure of planning and administration of these treatments. because of this, we intend to develop a mobile application to minimize the problems previously addressed, assuring that the information is constantly updated, that the historic of the patient is reliable, that exists control over medication stock, among others."
    ],
    0.06666666666666667
  ],
  [
    [
      "over the last years, the implementation and evolution of computer resources has been improving both the financial and temporal efficiency of clinical processes, as well as the security in the transmission and maintenance of their data, also ensuring the reduction of clinical risk. currently, the importance of all the information flowing in health institutions is unquestionable. in this way, it is essential that institutions, more specifi cally hospital institutions, have a good hospital information system (his) in order to collect and analyze information, also helping to support decision making. the most common application of these type of systems is the electronic health record (ehr), which, despite bringing many benefits, is still associated with a low level of usability. however, the different systems present in hospitals are distributed and heterogeneous. since the interaction between these systems is crucial these days, there is the agency for integration, diffusion and archive (aida) implemented in some portuguese hospitals. aida is a platform developed to enable the dissemination and inte gration of information generated in a health environment by different systems, inclu ding for example information on complementary diagnostic and therapeutic means (mcdt). previous research has shown that health professionals often do not analyze and act accurately and appropriately on test results. preventing errors during the access to mcdt is essential as this is a crucial step in the diagnostic process, thus avoiding negative consequences for the patient. in this sense, a new mcdt visualization platform (aida-mcdt) was implemented in this project, specifically in the hospital center of porto (chp), with several new functionalities in order to make this process faster, intuitive and efficient, always guaranteeing the confidentiality and protection of patients’ personal data and significantly improving the usability of the system, leading to a better delivery of health care.",
      "nos últimos anos, a implementação e a progressão dos recursos computacionais na área da saúde têm vindo a melhorar tanto a eficiência monetária como temporal dos processos clínicos, bem como a segurança na transmissão e na manutenção dos dados envolvidos, garantindo também a redução do risco clínico. atualmente, a importância da informação que flui nas instituições de saúde é inquestionável. deste modo, é essencial que as instituições, mais especificamente instituições hospitalares, tenham bons sistemas de informação hospitalar (sih) de modo a recolher e analisar a informação, auxiliando também na descoberta de conhecimento e no apoio às tomadas de decisão. a aplicação mais comum desses tipos de sistemas é o processo clínico eletrónico (pce) que apesar de trazer imensos benefícios ainda está associado a um baixo nível de usabilidade. contudo, os diferentes sistemas presentes nos hospitais são distribuídos e heterogéneos. sendo que a interação entre esses sistemas é crucial nos dias de hoje, surge a agência para a integração, difusão e arquivo de informação médica e clínica (aida) implementada em alguns hospitais portugueses. a aida é uma plataforma desenvolvida para permitir a disseminação e integração de informações geradas num ambiente de saúde pelos diferentes sistemas, incluindo por exemplo informações sobre os meios complementares de diagnóstico e terapia (mcdt). pesquisas anteriores mostraram que os profissionais de saúde muitas vezes não conseguem analisar e agir de forma precisa e apropriada relativamente aos resultados dos exames. a prevenção da ocorrência de erros durante o processo de acesso aos mcdt é essencial uma vez que este é um passo crucial no processo de diagnóstico, evitando assim consequências negativas para o paciente. neste sentido, surge este projeto de desenvolvimento de uma nova plataforma de visualização de mcdt (aida-mcdt), implementada no centro hospitalar do porto (chp), com diversas novas funcionalidades de modo a tornar este processo mais rápido, intuitivo e eficiente, garantindo sempre a confidencialidade e proteção dos dados pessoais dos pacientes e melhorando significativamente a usabilidade do sistema, levando consequentemente a uma melhor prestação de cuidados de saúde."
    ],
    [
      "com os atuais avanços tecnológicos, cada vez mais informação está a ser digitalizada, resultando assim num aumento exponencial nos dados guardados em formato digital. este crescimento sem precedentes tem levantado preocupações acerca do espaço e custo dos sistemas de armazenamento, criando uma necessidade de explorar mecanismos que visem mitigar este problema. uma estratégia que se foca neste problema é a técnica conhecida por deduplicação, que se baseia no facto que dados idênticos estão a ser gerados e armazenados repetidamente, resultando assim num consumo desnecessário de espaço de armazenamento em disco. deste modo, a deduplicação propõe uma análise dos dados armazenados e, subsequentemente, a eliminação de cópias redundantes, economizando espaço e custos de armazenamento. serviços como dropbox e google drive aplicam essa estratégia, contudo, o processamento de dados que pertencem a vários utilizadores fomenta preocupações de privacidade e segurança, especialmente quando este é realizado em fornecedores de serviços de armazenamentos terceiros. a abordagem tradicional para resolver estes problemas é os utilizadores enviarem os seus dados já cifrados. contudo, usar uma cifra probabilística implica que dados idênticos podem resultar em textos cifrados diferentes, o que torna impossível encontrar copias redundantes e, consequentemente, aplicar a deduplicação. deste modo, propomos o s2dedup, um sistema de deduplicação seguro que explora tecnologias emergentes de segurança assistida por hardware. mais especificamente, a solução proposta recorre ao intel sgx (software guard extensions), de forma a permitir a deduplicação de dados entre utilizadores em infraestruturas de armazenamento de terceiros, sem descuidar da segurança e privacidade dos seus dados. além disto, o s2dedup foi projetado para oferecer suporte a vários esquemas de segurança, cada um oferecendo diferentes níveis de espaço poupado, desempenho e privacidade. esta característica é fundamental para garantir a aplicabilidade do s2dedup a uma gama ampla de sistemas com requisitos diferentes. um protótipo do s2dedup é implementado e avaliado com cargas de trabalho sintéticas e realistas, assim como comparado com as soluções alternativas de deduplicação seguras do estado da arte. os resultados mostram que é possível implementar técnicas de segurança mais robustas e ao mesmo tempo manter bons resultados de desempenho e até mesmo alcançar, em alguns casos, uma melhoria na eficácia da deduplicação em comparação com as soluções do estado da arte.",
      "with the current advancements in computer technologies, more and more information is being digitized, resulting in an exponential increase of digital data. this unprecedented growth raises concerns about the space and cost of data storage, creating the need to explore mechanisms that strive to mitigate this ever-increasing data problem. a strategy that addresses this issue is the technique known as deduplication, which lever ages from the fact that identical data is being generated and stored repeatedly, consuming unnecessary storage space. therefore, deduplication proposes an analysis of the stored data and subsequently the elimination of redundant copies, thus saving storage space and costs. services like dropbox and google drive support deduplication, however, eliminating redundant information across data belonging to multiple users raises privacy and security concerns, specially when this is done at third-party untrusted infrastructures. the con ventional approach to ensure data privacy is for the users to outsource their data in an encrypted format. however, using standard probabilistic encryption implies that identical data will result in different ciphertexts, which makes it impossible to find redundant copies, and consequently apply deduplication. therefore we propose s2dedup, a secure deduplication system that explores emergent hardware-assisted security technologies. in more detail, the proposed solution leverages intel software guard extensions to enable cross-user privacy-preserving deduplication at third-party storage infrastructures. furthermore, s2dedup is designed to support multiple security schemes, each providing different trade-offs in terms of deduplication space savings, storage performance, and privacy. such feature is key to improve s2dedup’s applicability to a wider range of applications with different requirements. a prototype of s2dedup is implemented and evaluated with both synthetic and realistic workloads whilst being compared to the state of the art secure deduplication solutions. the results show that it is possible to implement more robust security techniques, while maintaining overall interesting performance results and even achieve, in some cases, an improvement of deduplication effectiveness when compared to the state of the art solutions."
    ],
    0.12857142857142856
  ],
  [
    [
      "a violência tem sido parte integrante da humanidade. existem diferentes tipos de violência, sendo a violência de cariz físico mais recorrente no nosso quotidiano, afetando cada vez mais a vida de muitas pessoas. o reconhecimento da ação humana tem sido crescentemente estudada nos últimos anos. o áudio (microfones) e vídeo (câmaras) são as formas mais utilizadas na captação de violência. o reconhecimento da ação humana através do vídeo representa uma importante área na visão por computador. no entanto, a captação de vídeo requer uma grande capacidade de processamento e de desempenho, tanto de hardware como software. o áudio surge assim como um fator capaz de colmatar estes problemas. no entanto, a deteção de áudio é altamente suscetível a grandes flutuações de precisão, dependendo do ambiente acústico em que está inserido. na presente dissertação, pretendeu-se comparar os diferentes algoritmos de machine learning com o intuito de averiguar qual o melhor algoritmo a utilizar para detetar violência em áudio. a revisão da literatura revelou que o áudio pode ser classificado usando algoritmos de machine learning, sendo a sua conversão em imagens (mel spectrogram) a metodologia habitualmente utilizada, tendo sido a abordagem tomada. além disto, estudaram-se os algoritmos frequentemente utilizados na classificação de áudio, tendo estes sido utilizados para posterior avaliação. os resultados obtidos demonstram um bom desempenho das redes neuronais efficientnet, sendo que as redes que obtiveram melhor precisão foram a efficientnetb1 e efficientnetb0, com 95.06% e 94.19%, respetivamente. adicionalmente, verificou-se que a rede mobilenetv2 é a mais incapaz de classificar entradas de violência, com uma classificação de 92.44%. a rede neuronal efficientnetb1 apresentou uma melhor capacidade na classificação de violência em áudio.",
      "violence has been an integral part of humanity. there are different types of violence, violence of a physical nature being the most recurrent in our daily lives, increasingly affecting the lives of many people. the recognition of human action has been increasingly studied in recent years. audio (microphones) and video (cameras) are the most commonly used means of capturing violence. recognition of human action through video represents a major area in computer vision. however, video capture requires a large processing capacity and performance of both hardware and software. audio thus emerges as a suitable means of overcoming these problems. however, audio detection is highly susceptible to large fluctuations in accuracy, according to the acoustic environment in which it is set. thus, a good audio representation is critical to perform audio-based classification. in this dissertation, it was intended to compare different machine learning algorithms to find the best algorithm to be used when detecting violence in audio. the literature review revealed that audio can be classified using machine learning algorithms, being its conversion into images (mel spectrogram) the most commonly used methodology, and therefore this approach was adopted. in addition to this, algorithms frequently used in audio classification were studied, and were used for further analysis. the results showed a good performance of the efficientnet neural networks, where the networks that achieved the best accuracy were efficientnetb1 and efficientnetb0, with 95.06% and 94.19%, respectively. additionally, it was observed that the mobilenetv2 network is the most unable to classify violence entries, with a classification of 92.44%. the efficientnetb1 neural network performed better in classifying violence in audio."
    ],
    [
      "impacto negativo que o ser humano tem provocado no meio ambiente precisa de ser contrariado, assim, são necessárias soluções para atuar nesse sentido, soluções estas que precisam de assegurar a sustentabilidade entre questões ambientais, económicas, e sociais. esta dissertação aborda este tópico no contexto de sistemas de informação, mais concretamente, na interacção social de uma pessoa com o meio em que está inserida. a ideia surgiu para colmatar uma lacuna existente nas soluções computacionais para gestão de sustentabilidade, pois estas centravam o problema nas questões externas ao utilizador, descartando o seu grau de conforto. para isso, foi implementado um sistema de simulação de emoções humanas, baseado nas características psicológicas dos utilizadores, com o propósito de integrar este sistema numa plataforma informática de apoio à sustentabilidade. esta simulação pediu emprestado os conceitos de computação afectiva, área da informática que tem ganho importância nos últimos anos, e é responsável por fundir os trabalhos em psicologia e computação. assim, o trabalho presente nesta tese apresenta dados sobre a relação psicologia, sustentabilidade, computação afectiva e como estes campos se podem interligar. foi desenvolvida uma plataforma utilizando tecnologia de agentes virtuais que recolhessem informações sobre o ambiente e procedessem ao cálculo de um estado emocional. os resultados obtidos com o trabalho desenvolvido foram animadores, e revelam que este tipo de simulação poderá ser uma vantagem, para que um sistema de suporte à sustentabilidade, baseado em inteligência ambiente, possa tomar decisões e atuar sobre um meio, tendo informação prévia ou uma hipótese do estado emocional dos seus utilizadores.",
      "the negative impact that humans have caused in the environment needs to be counteracted, thereby solutions are required to act. in this sense, these solutions need to ensure sustainability from environmental, economic, and social issues. this dissertation addresses this topic in the context of information systems, namely, social interaction of a person with the environment in which he operates. the idea came to fill a gap in computational solutions for sustainability management, as these focused the problem on the external issues to the user, discarding their degree of comfort. for this it was implemented a simulation system of human emotions based on psychological characteristics of users, in order to integrate this system into a computing platform to support sustainability. this simulation borrowed the concepts of affective computing, area of computing that has gained importance in recent years, and is responsible for merging the work in psychology and computing. therefore, the present work in this thesis presents information on the relationship of psychology, sustainability, affective computing and how these fields can be interconnected. it was developed a platform using virtual agent technology to collect information about the environment and should proceed to the calculation of an emotional state. the results obtained from the work were encouraging, and show that this type of simulation can be an advantage for a system to support the sustainability based on ambient intelligence, it can make decisions and act upon an ambient, having prior information or hypothesis of the emotional state of its users."
    ],
    0.0
  ],
  [
    [
      "data consistency often needs to be sacrificed in order to ensure high-availability in large scale distributed systems. conflict-free replicated data types relax consistency by always allowing query and update operations at the local replica without remote synchronization. consistency is then re-established by a background mechanism that synchronizes the replicas in the system. in state-based crdts replicas synchronize by periodically sending their local state to other replicas and by merging the received remote states with the local state. this synchronization can become very costly and unacceptable as the local state grows. delta-state-based crdts solve this problem by producing smaller messages to be propagated. however, it requires each replica to store additional metadata with the messages not seen by its direct neighbors in the system. this metadata may not be available after a network partition, since a replica can be forced to garbage-collect it (due to storage/memory limitations), or when the set of direct neighbors of a replica changes (due to dynamic memberships). in this dissertation we further improve the synchronization of state-based crdts, by introducing the concept of join decomposition of a state-based crdt and explaining how it can be used to reduce the synchronization cost of this variant of crdts. we validate our proposal experimentally on google cloud platform by comparing the state-based synchronization algorithm against the classic and improved versions of the delta-state-based algorithm. the results of this comparison show that our proposed techniques can greatly reduce state transmission, even under normal operation when the network is stable.",
      "frequentemente a consistência dos dados é sacrificada para garantir alta-disponibilidade em sistemas distribuídos de grande escala. conflict-free replicated data types relaxam a consistência permitindo operações de query e update na réplica local sem sincronização remota. nos state-based crdts as réplicas sincronizam periodicamente enviando o seu estado local para as outras réplicas e combinando os estados remotos recebidos com o estado local. esta sincronização pode tornar-se muito custosa e inaceitável à medida que o estado local cresce. delta-state-based crdts resolvem este problema produzindo mensagens mais pequenas para serem propagadas. no entanto, requer guardar metadados adicionais com as mensagens que ainda não foram vistas pelos vizinhos diretos no sistema. estes metadados podem não estar disponíveis depois de uma partição na rede, visto que a réplica pode ser forçada a apagá-los (devido a limitações de armazenamento/memória), ou quando o conjunto dos vizinhos diretos da réplica muda (devido a vistas dinâmicas). nesta dissertação melhoramos ainda mais a sincronização de state-based crdts, introduzindo o conceito de join decomposition de um state-based crdt e explicando como é que pode ser usado para reduzir o custo de sincronização desta variante de crdts. validamos a nossa proposta experimentalmente na google cloud platform comparando o algoritmo de sincronização de state-based crdts com a clássica e melhoradas versões do algoritmo dos delta-state-based. os resultados desta comparação mostram que as técnicas propostas podem reduzir muito a transmissão de dados, mesmos em operação normal quando a rede está estável."
    ],
    [
      "deduplication is a technique that allows finding and removing duplicate data at storage systems. with the current exponential growth of digital information, this mechanism is becoming more and more desirable for reducing the infrastructural costs of persisting such data. therefore, deduplication is now being widely applied to several storage appliances serving applications with different requirements (e.g., archival, backup, primary storage). however, deduplication requires additional processing logic for each storage request in order to detect and eliminate duplicate content. traditionally, this processing is done in the i/o critical path (inline), thus introducing a performance penalty on the throughput and latency of requests being served by the storage appliance. an alternative solution is to do this process as a background task, thus outside of the i/o critical path (offline), at the cost of requiring additional storage space as duplicate content is not found and eliminated immediately. however, the choice of what type of strategy to use is typically done manually and does not take into consideration changes in the applications' workloads. this dissertation proposes hiods, a hybrid deduplication solution capable of automati cally changing between inline and offline deduplication according to the requirements (e.g., desired storage i/o throughput goal) of applications and their dynamic workloads. the goal is to choose the best strategy that fulfills the targeted i/o performance objectives while optimizing deduplication space savings. finally, a prototype of hiods is implemented and evaluated extensively with different storage workloads. results show that hiods is able to change its deduplication mode dy namically, according to the storage workload being served, while balancing i/o performance and space savings requirements efficiently.",
      "a deduplicação é uma técnica que permite encontrar e remover dados duplicados guardados nos sistemas de armazenamento. com o crescimento exponencial da informação digital que vivemos atualmente, este mecanismo está a tornar-se cada vez mais popular para reduzir os custos das infraestruturas onde esses dados se encontram alojados. de facto, a deduplicação é, hoje em dia, usada numa grande variedade de serviços de armazenamento que servem diferentes aplicações com requisitos particulares (ex.: arquivo, backup, armazenamento primário). no entanto, a deduplicação adiciona uma camada de processamento extra a cada pedido de armazenamento, de modo a conseguir detetar e eliminar o conteúdo redundante. tradicionalmente, este processo é realizado durante o caminho crítico do i/o (inline), causando perdas de desempenho e aumentos na latência dos pedidos processados. uma alternativa é alterar o processamento para segundo plano, aliviando assim os custos no caminho crítico do i/o (offline). esta solução requer espaço de armazenamento adicional, visto que os duplicados não são encontrados nem eliminados imediatamente. no entanto, a estratégia a seguir é escolhida de forma manual, não tendo em consideração qualquer possível mudança na carga de trabalho das aplicações. esta dissertação propõe assim o hiods, um sistema de deduplicação híbrido capaz de alterar entre o modo inline e offline de forma automática considerando os requisitos (ex.: débito do sistema de armazenamento desejado) das aplicações e das suas cargas de trabalho dinâmicas. por fim, um protótipo do hiods é implementado e avaliado exaustivamente. os resultados mostram que o hiods é capaz de alterar o modo de deduplicação de forma dinâmica e de acordo com a carga de trabalho, considerando os requisitos de desempenho e a eliminação eficiente dos dados duplicados."
    ],
    0.0
  ],
  [
    [
      "over the past 20 years, data has increased in a large scale in various fields. this explosive increase of global data led to the coin of the term big data. big data is mainly used to describe enormous datasets that typically includes masses of unstructured data that may need real-time analysis. this paradigm brings important challenges on tasks like data acquisition, storage and analysis. the ability to perform these tasks efficiently got the attention of researchers as it brings a lot of oportunities for creating new value. another topic with growing importance is the usage of biometrics, that have been used in a wide set of application areas as, for example, healthcare and security. in this work it is intended to handle the data pipeline of data generated by a large scale biometrics application providing basis for real-time analytics and behavioural classification. the challenges regarding analytical queries (with real-time requirements, due to the need of monitoring the metrics/behavior) and classifiers’ training are particularly addressed.",
      "nos os últimos 20 anos, a quantidade de dados armazenados e passíveis de serem processados, tem vindo a aumentar em áreas bastante diversas. este aumento explosivo, aliado às potencialidades que surgem como consequência do mesmo, levou ao aparecimento do termo big data. big data abrange essencialmente grandes volumes de dados, possivelmente com pouca estrutura e com necessidade de processamento em tempo real. as especificidades apresentadas levaram ao aparecimento de desafios nas diversas tarefas do pipeline típico de processamento de dados como, por exemplo, a aquisição, armazenamento e a análise. a capacidade de realizar estas tarefas de uma forma eficiente tem sido alvo de estudo tanto pela indústria como pela comunidade académica, abrindo portas para a criação de valor. uma outra área onde a evolução tem sido notória é a utilização de biométricas comportamentais que tem vindo a ser cada vez mais acentuada em diferentes cenários como, por exemplo, na área dos cuidados de saúde ou na segurança. neste trabalho um dos objetivos passa pela gestão do pipeline de processamento de dados de uma aplicação de larga escala, na área das biométricas comportamentais, de forma a possibilitar a obtenção de métricas em tempo real sobre os dados (viabilizando a sua monitorização) e a classificação automática de registos sobre fadiga na interação homem-máquina (em larga escala)."
    ],
    [
      "na era atual, marcada pela rápida evolução tecnológica, testemunhamos transformações profundas que permeiam todos os setores da sociedade. a incessante inovação tecnológica tem redefinido a forma como vivemos, trabalhamos e interagimos, proporcionando uma revolução que molda a essência da vida moderna. no âmbito da saúde, essa revolução tecnológica não é apenas evidente, como também tem o potencial de redefinir padrões e expetativas. os avanços tecnológicos na área da saúde oferecem benefícios significativos, desde diagnósticos mais precisos a tratamentos mais personalizados. a integração de tecnologias como inteligência artificial e big data, não apenas amplia o alcance dos serviços de saúde, como também melhora a eficiência e a qualidade do atendimento. os softwares desempenham um papel central nesta transformação, proporcionando ferramentas poderosas para profissionais de saúde. a gestão eficiente de dados, análises avançadas e aprimoramento da comunicação entre equipas médicas são apenas alguns dos benefícios oferecidos por estas soluções. a relevância crescente destes softwares destaca-se na contribuição das operações clínicas e na melhoria dos processos de tomada de decisão, resultando numa prestação de cuidados mais eficaz e centrada no paciente. a elaboração da presente dissertação considerou o estudo minucioso da seguinte questão de investigação: ”qual o impacto da utilização de uma aplicação de análise de dados no domínio da medicina e na melhoria da qualidade de vida dos pacientes? ”para responder a esta questão, foi desenvolvida a analyticcare, uma aplicação de monitorização de análises clínicas, com a finalidade de ampliar a capacidade de comparação entre pacientes, proporcionando aos profissionais de saúde uma ferramenta valiosa. este artefacto representa uma resposta às demandas crescentes por soluções que otimizem a análise comparativa de casos clínicos. por fim, foi realizada uma prova de conceito - análise swot, para avaliar a eficiência da aplicação, destacando não apenas a sua viabilidade técnica, mas também a sua capacidade de promover avanços na eficácia dos cuidados de saúde, contribuindo para um ambiente clínico mais sofisticado e integrado.",
      "in the current era marked by rapid technological evolution, we witness profound transformations that benefit all sectors of society. relentless technological innovation has redefined how we live, work, and interact, bringing about a revolution that shapes the essence of modern life. in the realm of healthcare, this technological revolution is not only evident but also has the potential to redefine standards and expectations. technological advances in healthcare offer significant benefits, from more precise diagnostics to personalized treatments. the integration of technologies such as artificial intelligence and big data not only expands the reach of healthcare services but also improves efficiency and the quality of care. software plays a central role in this transformation, providing powerful tools for healthcare professionals. efficient data management, advanced analytics, and improved communication among medical teams are just a few of the benefits offered by these solutions. the growing relevance of such software stands out in contributing to clinical operations and improving decision-making processes, resulting in more effective and patient-centered care. the preparation of this dissertation considered the detailed study of the following research question: ”what is the impact of using a data analysis application in the field of medicine and improving patients’ quality of life?”to answer this question, analyticcare, a clinical analysis monitoring application, was developed with the aim of expanding the ability to compare patients, providing healthcare professionals with a valuable tool. this artefact represents a response to the growing demands for solutions that optimise the comparative analysis of clinical cases. finally, a proof of concept - swot analysis was carried out to assess the efficiency of the application, highlighting not only its technical feasibility, but also its ability to promote advances in healthcare efficiency, contributing to a more sophisticated and integrated clinical environment."
    ],
    0.0857142857142857
  ],
  [
    [
      "combining different programs or code fragments is a natural way to build larger programs. this allows programmers to better separate a complex problem into simple parts. furthermore, by writing programs in a modular way, we increase code reusability. however, these simple parts need to be connected somehow. these connections are done via intermediate structures that communicate results between the different components, harming performance because of the overhead introduced by the allocation and deallocation of multiple structures. fusion, a very commonly used technique in functional programming, aims to remove the creation of these unnecessary structures, as they don’t take part in the final result. with the introduction of streams and lambda expressions, java made its way into a more functional programming style. yet, these mechanisms lack optimization and the adaptation of fusion techniques used by some compilers for functional languages could benefit the performance of java streams. in this thesis, we study how functional fusion can be adapted to java streams.",
      "combinar diferentes programas ou fragmentos de código é uma forma natural de construir programas maiores. isto permite aos programadores melhor separar um problema complexo em partes simples. além disso, ao escrever programas de forma modular, estamos a aumentar a reutilização do código. contudo, estas partes têm de ser ligadas de alguma maneira. estas conexões são feitas via estruturas intermédias que comunicam os resultados entre os diferentes componentes, prejudicando a performance com o overhead introduzido pela alocação e desalocação de várias estruturas. a fusão, uma técnica muito usada em programação funcional, pretende remover a criação destas estruturas desnecessárias, uma vez que não tomam parte no resultado final. com a introdução de streams e expressões lambda, o java abriu caminho para um estilo de programação mais funcional. mesmo assim, estes mecanismos não possuem otimização e a adaptação de técnicas de fusão utilizadas por alguns compiladores de linguagens funcionais poderiam beneficiar a performance das streams do java. nesta dissertação, é estudado como a fusão em programação funcional pode ser adaptada às streams do java."
    ],
    [
      "os sistemas de povoamento de data warehouses, vulgarmente designados por sistema de etl – extract-transform-load –, constituem a base de qualquer sistema de data warehousing. no entanto, poucas são as vezes em que a sua implementação ocorre de uma forma linear, metódica, seguindo um dado modelo de trabalho devidamente comprovado. usualmente, estes sistemas estabelecem uma “ponte” entre os sistemas operacionais, muitas vezes de natureza diversa, e os sistemas de data warehousing, de forma a que seja possível assegurar o povoamento dos seus data warehouses, de uma forma regular e atual. como tal, é muito normal terem que lidar com um volume de dados considerável e envolvendo processos de tratamento bastante complexos. esses processos, que representam trabalho extra para o etl, só são necessários devido à da elevada permeabilidade dos sistemas operacionais que facilitam a ocorrência de fenómenos de inconsistência e de omissão de valores. para que tal não aconteça, as atuais técnicas e modelos de implementação baseados em processos típicos de “tentativa-erro” deverão ser abandonados desde início, dando lugar a uma arquitetura pensada com vista num melhor desempenho evitando, assim, situações em que um aumento no volume de dados do processo, tende a revelar um efeito “bola de neve” em termos do nível de performance do sistema. neste trabalho de dissertação desenvolvemos uma técnica baseada em process mining que, recorrendo aos registos de execução detalhados de um processo etl - logs -, permite descobrir todo o processo etl a montante. na posse dos dados relativos a cada passo de execução do processo etl (tempo médio de execução, frequência absoluta, etc), podemos definir um modelo matemático que ilustra o “bem-estar”, ou seja, o desempenho do nosso sistema através da correlação de todas estas variáveis. desta forma, ao torná-lo acessível aos administradores dos sistemas, introduzimos um novo paradigma no desenvolvimento e manutenção de processos etl, mais preocupado com questões como a performance ou um conhecimento mais aprofundado do impacto das decisões arquiteturais que são tomadas, nomeadamente a nível da escolha de componentes para executar cada passo do nosso etl.",
      "etl – extract – transform – load – systems is the common name for the systems behind the data warehouses’ populating process. in fact, they’re the core piece of any data warehousing system. however, most of the times its implementation does not occur in a regular way. usually, these systems establish the “bridge” between the operational environment, most of the times a heterogeneous one, so that its populating process proceeds in a regular and up to date way. therefore, it’s normal for these processes to cope with large volumes of data involving complex validation processes. these validation processes, that represent an extra effort for the etl, are only necessary thanks to a high permeability of the operational systems, that facilitates the occurrence of value omissions or inconsistencies. in order to reverse the situation, the current adhoc technique must be abandoned from the very beginning, leaving place to a new one, much more pragmatic and performance oriented. this approach is going to avoid the “snowflake” effect regarding the decrease in performance that is usually notable as the volume of data increases. in this work, we introduced a new process mining based technique that, using the detailed execution records of an etl process – the so-called logs, allows us to discover the nature of the etl process behind these logs. in possession of detailed data concerning each step of our process (mean time, absolute frequency, etc), we can define a new mathematical model that illustrates the “well-being”, that is, the degree of performance of our system, by establishing the correlation between the collected variables. thus, by making it accessible to the system admins, we’re introducing a new paradigm regarding the development and maintenance of etl processes, much more concerned with issues like the performance or the knowledge behind the impact of our architectural decisions, mainly when we’re deciding about the components we’re going to use to execute each step of our etl."
    ],
    0.3
  ],
  [
    [
      "a plataforma web clav (https://clav.dglab.gov.pt/) visa na disponibilização de informação e ferramentas que facilitem e normalizem as práticas de classificação e avaliação de toda a documentação circulante na administração pública portuguesa. um dos seus principais objetivos é a redução da utilização dos documentos físicos que armazenam essa informação, substituindo o uso dos mesmos pelo formato digital. assim sendo, dá-se uma redução do uso do papel, significando uma menor perda de informação e uma maior facilidade no tratamento dos processos, não havendo necessidade de transitar esses documentos físicos. à data de início desta dissertação, o clav já continha alguns workflows especificados em linguagem bpmn, sendo que estes ainda não eram automatizados. nesta dissertação, pretendeu-se dar continuidade ao trabalho anteriormente realizado no clav, ou seja, partindo das especificações bpmn já existentes, seria necessário revê-las e alterá-las, caso se justificasse, e acrescentar os workflows ainda em falta (araújo, 2021). com o principal objetivo de tornar o clav mais inteligente e automático, pretende-se investir na automatização dos workflows especificados anteriormente, de modo que as interfaces fossem capazes de reagir a esses workflows e às mudanças que estes pudessem sofrer. por último, existia a necessidade de acrescentar à plataforma clav a possibilidade da edição gráfica das especificações bpmn. assim sendo, seria possível que o utilizador administrador conseguisse melhorar e alterar os fluxos de trabalho, através da edição gráfica dos modelos bpmn na plataforma, com a possibilidade de, no futuro, executar essa edição em runtime.",
      "the clav web platform (https://clav.dglab.gov.pt/) aims at providing information and tools that facilitate and standardize the practices of classification and evaluation of all documentation circulating in the portuguese public administration. one of its main goals is to reduce the use of physical documents that store this information, replacing their use by digital format. thus, there is a reduction in the use of paper, meaning less loss of information and greater ease in processing processes, with no need to transit these physical documents. at the beginning of this dissertation, the clav already contained some workflows specified in the bpmn language, but these were not yet automated. in this dissertation, we intended to continue the work previously done on the clav, that is, starting from the existing bpmn specifications, it would be necessary to review them and change them, if justified, and add the missing workflows (araújo, 2021). with the main objective of making the clav more intelligent and automatic, it is intended to invest in the automation of the workflows specified above, so that the interfaces would be able to react to these workflows and the changes that they might undergo. finally, there was a need to add to the clav platform the possibility of graphical editing of the bpmn specifications. therefore, it would be possible for the administrator user to improve and change workflows by graphically editing the bpmn models in the platform, with the possibility of performing this editing in runtime in the future."
    ],
    [
      "arm trustzone é um “ambiente de execução confiável” disponibilizado em processadores da arm, que equipam grande parte dos sistemas embebidos. este mecanismo permite assegurar que componentes críticos de uma aplicação executem num ambiente que garante a confidencialidade dos dados e integridade do código, mesmo que componentes maliciosos estejam instalados no mesmo dispositivo. neste projecto pretende-se tirar partido do trustzone no contexto de uma framework segura de monitorização em tempo real de sistemas embebidos. especificamente, pretende-se recorrer a components como o arm trusted firmware, responsável pelo processo de secure boot em sistemas arm, para desenvolver um mecanismo de atestação que providencie garantias de computação segura a entidades remotas.",
      "arm trustzone is a security extension present on arm processors that enables the development of hardware based trusted execution environments (tees). this mechanism allows the critical components of an application to execute in an environment that guarantees data confidentiality and code integrity, even when a malicious agent is installed on the device. this projects aims to harness trustzone in the context of a secure runtime verification framework for embedded devices. specifically, it aims to harness existing components, namely arm trusted firmware, responsible for the secure boot process of arm devices, to implement an attestation mechanism that provides proof of secure computation to remote parties."
    ],
    0.3
  ],
  [
    [
      "the human urge and pursuit for information have led to the development of increasingly complex technologies, and new means to study and understand the most advanced and intricate biological system: the human brain. large-scale neuronal communication within the brain, and how it relates to human behaviour can be inferred by delving into the brain network, and searching for patterns in connectivity. functional connectivity is a steady characteristic of the brain, and it has been proved to be very useful for examining how mental disorders affect connections within the brain. the detection of abnormal behaviour in brain networks is performed by experts, such as physicians, who limit the process with human subjectivity, and unwittingly introduce errors in the interpretation. the continuous search for alternatives to obtain faster and robuster results have put machine learning and deep learning in the leading position of computer vision, as they enable the extraction of meaningful patterns, some beyond human perception. the aim of this dissertation is to design and develop an experiment setup to analyse functional connectivity at a voxel level, in order to find functional patterns. for the purpose, a pipeline was outlined to include steps from data download to data analysis, resulting in four methods: data download, data preprocessing, dimensionality reduction, and analysis. the proposed experiment setup was modeled using as materials resting state fmri data from two sources: life and health sciences research institute (portugal), and human connectome project (usa). to evaluate its performance, a case study was performed using the in-house data for concerning a smaller number of subjects to study. the pipeline was successful at delivering results, although limitations concerning the memory of the machine used restricted some aspects of this experiment setup’s testing. with appropriate resources, this experiment setup may support the process of analysing and extracting patterns from any resting state functional connectivity data, and aid in the detection of mental disorders.",
      "o desejo e a busca intensos do ser humano por informação levaram ao desenvolvimento de tecnologias cada vez mais complexas e novos meios para estudar e entender o sistema biológico mais avançado e intrincado: o cérebro humano. a comunicação neuronal em larga escala no cérebro, e como ela se relaciona com o comportamento humano, pode ser inferida investigando a rede neuronal cerebral e procurando por padrões de conectividade. a conectividade funcional é uma característica constante do cérebro e provou ser muito útil para examinar como os distúrbios mentais afetam as conexões cerebrais. a deteção de anormalidades em imagens de ressonância magnética é realizada por especialistas, como médicos, que limitam o processo com a subjetividade humana e, inadvertidamente, introduzem erros na interpretação. a busca contínua de alternativas para obter resultados mais rápidos e robustos colocou as técnicas de machine learning e deep learning na posição de liderança de visão computacional, pois permitem a extração de padrões significativos e alguns deles para além da percepção humana. o objetivo desta dissertação é projetar e desenvolver uma configuração experimental para analisar a conectividade funcional ao nível do voxel, a fim de encontrar padrões funcionais. nesse sentido, foi delineado um pipeline para incluir etapas a começar no download de dados até à análise desses mesmos dados, resultando assim em quatro métodos: download de dados, pré-processamento de dados, redução de dimensionalidade e análise. a configuração experimental proposta foi modelada usando dados de ressonância magnética funcional de resting-state de duas fontes: instituto de ciências da vida e saúde (portugal) e human connectome project (eua). para avaliar o seu desempenho, foi realizado um estudo de caso usando os dados internos por considerar um número menor de participantes a serem estudados. o pipeline foi bem-sucedido em fornecer resultados, embora limitações relacionadas com a memória da máquina usada tenham restringido alguns aspetos do teste desta configuração experimental. com recursos apropriados, esta configuração experimental poderá servir de suporte para o processo de análise e extração de padrões de qualquer conjunto de dados de conectividade funcional em resting-state e auxiliar na deteção de transtornos mentais."
    ],
    [
      "cancer is one of the major causes of death in developed countries. it is not a single disease, but a group of different types of diseases with specific symptoms, treatments and prognosis. early diagnosis and prognostic assessment are essential to select the best treatment for each case. deep learning is a branch of machine learning that became popular in recent years. deep learning methods have been employed in a broad range of areas including self-driving cars, natural language processing, computer vision, health, among others. the main goal of the thesis is to develop deep learning methods to predict cancer and its outcome from transcriptomics data. reviewing literature, exploring datasets, developing pipelines and validating the methods using a case study are some of the tasks needed to achieve the goals of the thesis. the developed methods are implemented as a pipeline for creating models from gene expression data. the framework is capable of reading and pre-processing these data, and training, optimizing and evaluating traditional machine learning and deep learning models. the framework was showcased by using the metabric dataset as a case study, which contains samples from breast cancer patients. the gene expression microarray data from the dataset was used to generate traditional, deep learning and multi-task models. the models were used to predict the expression of estrogen receptor (er), the subtype of breast cancer regarding er, human epidermal growth factor (her-2) and progesterone receptor (pr) and the prognosis of breast cancer patients with nottingham prognostic index (npi), respectively. another dataset allowed the use of single-cell rnaseq data and confirmed the main trends of the results. overall, the results were promising with classification tasks obtaining good results while regression models had a poorer performance. while the best results were obtained with traditional machine learning models, deep learning models were near and could provide better results if the dataset contained a larger number of samples.",
      "o cancro é uma das principais causas de morte em países desenvolvidos. não é uma única doença, mas um grupo de diferentes tipos de doenças com sintomas, tratamentos e prognósticos específicos. o diagnóstico precoce e a determinação do prognóstico são essenciais para selecionar o melhor tratamento para cada caso. \"deep learning\" é um ramo da área da aprendizagem máquina que se tornou popular nos últimos anos. métodos de \"deep learning\" têm sido empregados num conjunto de áreas alargado nas quais se incluem veículos autónomos, processamento de linguagem natural, visão por computador, saúde, entre outras. o objetivo principal desta dissertação é o de desenvolver métodos de \"deep leaming\" para prever cancro e o seu prognóstico a partir de dados de transcriptómica. a revisão da literatura, a exploração de conjuntos de dados, o desenvolvimento de \"pipelines\" e a validação dos métodos usando casos de estudo são alguns das tarefas necessárias para cumprir os objectivos do trabalho. os métodos desenvolvidos constituem uma \"pipeline\" para criação de modelos a par-tir de dados de expressão genética. a plataforma é capaz de ler dados de expressão genética, fazer pré-processamento, treino, otimização e avaliação de modelos de apren-dizagem máquina tradicionais e de \"deep learning\". a plataforma foi demonstrada usando o dataset do molecular taxonomy of breast can-cer international consortium (metabric) que contém amostras de pacientes com cancro da mama, como caso de estudo. os dados de expressão genética de microarrays foram usados para gerar modelos de aprendizagem máquina tradicionais, modelos de \"deep leaming\" e modelos multi-tarefa. os modelos foram usados para prever a expressão do receptor de estrogénio (er), do fator de crescimento epidérmico humano 2 (her-2) e do recetor da proges-terona (pr), bem como para prever o prognóstico de pacientes usando o índice de prognóstico de nottingham (npi). um segundo conjunto de dados permitiu uma validação adicional, considerando dados de rnaseq. de forma geral, os resultados foram promissores com as tarefas de classificação a obterem bons resultados enquanto os modelos de regressão tiverem um menor desempenho. en-quanto os melhores resultados foram obtidos com modelos de aprendizagem máquina tradicionais, os modelos de \"deep learning\" estiveram perto e poderiam obter melhores reultados se os dados tivessem um maior número de amostras."
    ],
    0.02727272727272727
  ],
  [
    [
      "augmented reality (ar) relies on overlaying 3d objects over a real environment on a screen display. recently this technology has begun to gain a lot of attention by the common consumer because it is starting to be used in devices available to them, one of those devices being mobile smartphones. hence, it is relevant to seek the limits and explore the potential that these devices are capable of, in order to build a balanced ar experience. the purpose of this work was to study the limits of developing a localization technique for an android smartphone which was accurate and fast. this technique was to be used in an ar application which allowed the user to view large 3d environments related to the real scene (e.g a room could be turned into an aquarium). the research was initially targeted at finding the right technique to localize the device, by assessing the accuracy of algorithms like inertial navigation systems, monocular visual odometry and slam on a hardware limited device - a smartphone. the results obtained with these algorithms did not meet the precision requirements to avoid misalignments between virtual and real entities. the solution found was to use 2d markers around the real scene with the help of a computer tool which comprised a novel feature which aided the user in positioning the markers with visual cues painted on the 3d model’s surface. markers detected by the smartphone’s camera allowed the computation of the position of the device. these markers are detected using vuforia’s sdk image target recognition. furthermore, some improvements were implemented to provide more robust tracking for large 3d models. a framework was devised which consists of the android ar application and the computer tool which, aside from helping the user position the markers, can build ar setups and deploy them to the mobile app. finally, some tests were made which assess the accuracy of the implementation. example ar scenes were built to examine how the application fares in very different environments, one being a room and the other a public square and streets in a village.",
      "realidade aumentada consiste na sobreposição de objetos 3d sobre um ambiente real. a transição para smartphones desta tecnologia tem aumentado a sua popularidade sobre consumidores comuns. desta forma, torna-se relevante explorar e definir o potencial destes dispositivos, de forma a construir uma experiência equilibrada de realidade aumentada. o objetivo desta dissertação é analisar os limites de desenvolver uma técnica de localização para um smartphone android que seria rápida e precisa. esta técnica posteriormente foi usada numa aplicação cujo propósito seria permitir ao utilizador a visualização de grandes modelos 3d, que estariam relacionados com o ambiente real (e.g uma sala poderia ser transformada num aquário). o estudo científico focou-se inicialmente em encontrar a técnica mais correta para localizar o dispositivo, ao avaliar a precisão de algoritmos como inertial navigation systems, monocular visual odometry e slam num dispositivo limitado em termos de hardware - um smartphone. os resultados obtidos com estes algoritmos não cumpriram requisitos de precisão de forma a evitar desalinhamentos entre entidades virtuais e reais. a solução formulada consistiu em usar marcadores 2d à volta da cena real com a ajuda de uma ferramenta para computador que suporta uma funcionalidade que ajuda o utilizador a posicionar os marcadores usando cores pintadas nas superfícies do modelo 3d. marcadores detetados pela câmara do smartphone permitem a computação da posição do dispositivo. estes marcadores são detetados pelo image target recognition do sdk da vuforia. foram ainda efectuadas algumas modificações ao algoritmo base de tracking de forma a fornecer resultados mais robustos para grandes modelos 3d. foi desenvolvida uma arquitetura que consiste na aplicação android e na ferramenta para computador, que, além de ajudar o utilizador no posicionamento dos marcadores, também é capaz de construir cenas de realidade aumentada e de carregar essas cenas para a aplicação móvel. alguns testes foram feitos de forma a analisar a robustez da implementação. algumas cenas exemplo de realidade aumentada foram construídas de forma a investigar qualidade da aplicação em diferentes cenários, nomeadamente uma sala e uma praça e ruas adjacentes de uma vila."
    ],
    [
      "classical quantum simulators are essential tools for studying quantum systems and simulating quantum algorithms. the hardware limitations of nisq (noisy intermediate scale quantum) devices make being able to build, test and run a quantum circuit/algorithm many times, and even test it under various noise scenarios, in a classical computer, extremely useful. currently, the most prominent technique for classically simulating quantum circuits is known as schrodinger type simulation. the memory usage of simulations using this technique increases exponentially with the number of qubits in a circuit, reaching prohibitive memory values relatively fast. this serves as motivation to investigate complementary classical quantum simulation techniques. the present work offers an investigation on how to improve the runtime of the feynman path-sum approach for classical simulation of quantum circuits, taking into account the computational basis input and output states given and the branching structure generated by branching gates in a quantum circuit. the main contributions of this dissertation are two feynman path-sum based simulation algorithms. these algorithms were able to successfully simulate quantum circuits with a large number of qubits (> 30) using polynomial space and it was demonstrated that the time complexity of these algorithms is more strongly influenced by the circuit structure rather than the circuit size.",
      "os simuladores quânticos clássicos são ferramentas essenciais para o estudo de sistemas quânticos e simulação de algoritmos quânticos. as limitações de hardware dos dispositivos nisq (noisy intermediate-scale quantum) tornam extremamente útil a possibilidade de construir, testar e executar um circuito/algoritmo quântico muitas vezes, e mesmo testá-lo sob vários cenários de ruído, num computador clássico. actualmente, a técnica mais proeminente de simulação clássica de circuitos quânticos e conhecida como simulação do tipo schrodinger. a utilização de memoria das simulações que utilizam esta técnica aumenta exponencialmente com o número de qubits num circuito, atingindo valores de memória proibitivos com relativa rapidez. este facto serve de motivação para investigar técnicas complementares de simulação quântica clássica. o presente trabalho oferece uma investigação sobre a forma de melhorar o tempo de execução do método de soma de caminhos de feynman para a simulação clássica de circuitos quânticos, tendo em conta os estados, na base computacional, de entrada e saída e a estrutura de ramificação gerada pelas portas de ramificação num circuito quântico. as principais contribuições desta dissertação˜ são dois algoritmos de simulação baseados na soma de caminhos de feynman. estes algoritmos foram capazes de simular com sucesso circuitos quânticos com um grande número de qubits (> 30) usando espaço polinomial e foi demonstrado que a complexidade temporal destes algoritmos e mais fortemente influenciada pela estrutura do circuito do que pelo tamanho do circuito."
    ],
    0.3
  ],
  [
    [
      "process algebras are mathematical structures used in computer science to study, model, and verify concurrent systems. essentially, a process algebra consists of a language (used to specify a system that one wishes to study), a semantic domain to interpret the language (which allows the interpretation and the study of the system) and a set of axioms related to the language operators (which facilitates the derivation of properties of the system be ing studied). these basic ingredients make process algebras powerful tools, with many applications in the development of concurrent systems and many successful stories in in dustry, bunte et al. (2019); groote and mousavi (2014). the three classical examples of a process algebra are: ccs introduced by robin milner, acp introduced by jan bergstra and jan willem klop, and csp introduced by tony hoare. of the three, the first stands out because of its main goal to isolate and study the elementary principles of communication and concurrency. the development of quantum mechanics supports the design of computational systems ruled by quantum laws, which, in the context of certain problems, perform significantly better than any classical computational system. this is exemplified with shor and grover algorithms, respectively used in the factorization of integers and in unstructured searching. moreover, quantum computing has applications in the communications area, having as main examples the quantum teleportation protocol and the bb84 communication protocol. however, due to their high sensitivity to noise, quantum computers have a very limited memory space, and therefore they usually integrate a qram architecture: essentially, a net work of classical computers that process and manage a general task list, invoking quantum computers only when high-cost computational tasks arise. this highlights the importance of extending the theory of process algebras to the quantum domain. in fact, some quantum process algebras were already proposed in last years: examples include qccs, developed by mingsheng ying et al based on ccs, and the cqp algebra, introduced by simon gay and rajagopal nagarajan. related to qccs, a typing system was developed, where the typable processes are exactly the valid qccs processes. current quantum process algebras assume the existence of an ideal quantum system, i.e. a quantum system immune to noise. in contrast, the aim of this dissertation is to study and develop a quantum process algebra in which this assumption is discarded. more specifically, we do not assume that a quantum state can be stored indefinitely, it may become corrupted over time, or in other words, have a limited time of coherence. for that goal, 1) we developed a new quantum process algebra that merges the strengths of qccs and cqp, in particular, recursion, memory allocation, and a typing system, so that we can study complex quantum systems that integrate the qram architecture; 2) we extended the new process algebra with a notion of time so that we could study its effects on quantum states; and 3) we developed a number of case studies, via the mentioned extension, in which the coherence time of quantum systems has a central role. this includes, for example, a simplified version of the ibm cloud, which provides access to a quantum computer via web.",
      "álgebras de processos são estruturas matemáticas usadas em ciências da computação para o estudo, modelação, e verificação de sistemas concorrentes. essencialmente, uma álgebra de processos é composta por uma linguagem (usada para especificar o sistema que se deseja estudar), um domínio semântico para interpretação dessa linguagem (o que permite interpretar e estudar o sistema) e um conjunto de axiomas relativos aos operadores da linguagem (o que facilita a derivação de propriedades do sistema em estudo). estes ingredientes básicos tornam as álgebras de processos ferramentas poderosas, com várias aplicações no desenvolvimento de sistemas concorrentes e várias histórias de sucesso na indústria, bunte et al. (2019); groote and mousavi (2014). os três exemplos clássicos de álgebras de processos são: ccs introduzida por robin milner, acp introduzida por jan bergstra e jan willem klop, e csp introduzida por tony hoare. das três, destaca-se a primeira como sendo aquela cujo objectivo principal é isolar e estudar os princípios elementares da comunicação e da concurrência. o desenvolvimento da mecânica quântica, permitiu conceber sistemas de computação regidos pelas leis quânticas, que, no contexto de certos problemas, têm um desempenho significativamente melhor que qualquer sistema computacional clássico. isto é exemplificado com os algoritmos de shor e de grover, usados respectivamente na factorização de inteiros e em procuras não estruturadas. para além disso, a computação quântica tem aplicações na área da comunicação, tendo como exemplos principais o protocolo de teletransporte quântico e o protocolo de comunicação bb84. no entanto devido à sua elevada sensibilidade ao ruído, os computadores quânticos têm um espaço de memória bastante limitado, e portanto costumam integrar uma arquitectura qram, em que uma rede de computadores clássicos processam e gerem uma lista geral de tarefas, invocando o computador quântico apenas quando surgem certas tarefas de elevado custo computacional. isto sublinha a importância de estender a teoria das álgebras de processos ao domínio quântico. de facto, algumas álgebras de processos quânticos já foram propostas nos últimos anos: exemplos incluem qccs, desenvolvida por mingsheng ying et al e que tem por base ccs, e a álgebra cqp, introduzida por simon gay e rajagopal nagarajan. relacionado com qccs, um sistema de tipos foi desenvolvido, onde os processos tipados são exatamente os processos válidos em qccs. as actuais álgebras de processos quânticos assumem a existência de um sistema quântico ideal, i.e. um sistema quântico insensível ao ruído. em contraste, o objectivo deste trabalho é o estudo e desenvolvimento de uma álgebra de processos quânticos em que esta assunção seja descartada. mais especificamente, não assumimos que um estado quântico possa ser guardado indefinidamente, mas que possa tornar-se corrompido ao longo do tempo, ou por outras palavras, que tenha um tempo limitado de coerência. para tal, 1) desenvolvemos uma nova álgebra de processos quânticos que reúne diversos pontos fortes de qccs e cqp, em particular recursividade, alocação de memória, e sistemas de tipagem, para que possamos estudar sistemas quânticos complexos e que integrem uma arquitectura qram; 2) estendemos a nova álgebra de processos com uma noção de tempo para podermos estudar o efeito deste sobre os estados quânticos; e 3) desenvolvemos uma série de casos de estudo, através da extensão referida, em que o tempo de coerência dos estados quânticos tem um papel central. isto inclui, por exemplo, uma versão simplificada da ibm cloud, que providencia acesso a um computador quântico via web."
    ],
    [
      "typically, testing an interactive system involves manually testing their possible interactions. since this is a manual process, it becomes very costly to check all possible interactions. in safety critical interactive systems this task is essential. one way to overcome this problem is to use tools for systematic analysis. ivy workbench is one of these tools. we plan to apply it to perform verification of safety critical interactive systems. the objectives for this dissertation are: development of a set of models of safety critical interactive systems; verification of relevant properties of the models; critical assessment of the modelling process and suggestion of improvements to the tool and language.",
      "normalmente, testar um sistema interativo envolve testar manualmente as possíveis interações com ele. uma vez que esta é uma abordagem manual, torna-se muito caro verificar todas as interacções possíveis. em sistemas interactivos de segurança crítica esta tarefa é essencial. uma forma de ultrapassar este problema é a utilização de ferramentas de análise sistemática. ivy workbench é uma dessas ferramentas. pretendemos aplicá-la para realizar a verificação de um sistema interactivo de segurança crítica. os objetivos para esta dissertação são: desenvolver um conjunto de modelos de sistemas interactivos de segurança crítica, verificar propriedades relevantes dos modelos, fazer uma avaliação crítica do processo de modelação e sugerir melhorias para a ferramenta e linguagem."
    ],
    0.0
  ],
  [
    [
      "durante várias décadas, o aumento do desempenho dos processadores era conseguido maioritariamente através do aumento da frequência de relógio. contudo, aumentar o desempenho através do aumento da frequência tornou-se cada vez mais difícil conduzindo a problemas de consumo energético e dissipação de calor. para resolver estes problemas, as arquiteturas mais recentes evoluíram num sentido de aumentar o número de processadores, e mais tarde, o número de núcleos. as arquiteturas de memória inicialmente adotadas, designadas arquiteturas de acesso uniforme à memória (uma), apresentaram algumas limitações. entre as arquiteturas uma existentes, existe a arquitetura de multiprocessamento simétrico (smp). esta arquitetura possui múltiplos processadores que acedem à memória principal utilizando um barramento partilhado pelos processadores que conduziu a problemas de contenção no acesso à memória principal. as arquiteturas de acesso não uniforme à memória (numa) surgiram durante os anos 90 com múltiplos bancos de memória. no entanto, os programadores que queiram tirar total partido das vantagens desta arquitetura, terão que lidar com novos desafios ao nível da programação, ao nível da afinidade dos fios de execução e das alocações de memória. esta dissertação mostra que a forma como os algoritmos memory-bound acedem a dados de memória principal numa arquitetura numa, pode ter impacto no seu desempenho. um conjunto de testes foi utilizado para demonstrar em que medida várias técnicas de afinidade podem contribuir para aumentar o desempenho de aplicações java e c em arquiteturas numa. os testes realizados com recurso a diferentes abordagens tais como ferramentas do sistema operativo, opções da jvm, técnicas de programação e até variáveis de afinidade para os compiladores, foram usados para aumentar a desempenho de dois casos de estudo em arquiteturas numa. a utilização destas abordagens permitiram aumentar o desempenho com um ganho adicional de até 1.8 vezes para as implementações em java, e de até 2.16 vezes para as versões em c das mesmas aplicações.",
      "during several decades, computers performance improvement was mostly achieved by increasing the clock frequency of the processor. however, increasing the clock frequency became technically harder due to problems of dissipation and energy consumption. to deal with this problems, computer architectures started to go in the direction of increasing the number of processors. the memory architectures initially adopted namely uniform memory access (uma), presented scalability limitations. the symmetric multiprocessing (smp) has multiple processors (potentially multicore) that access main memory uniformly using a system bus shared among all processors. this led to contention when accessing main memory. the non-uniform memory access (numa) came up during the 90’s with multiple memory banks. in numa systems, the number of memory banks can increase with the number of processors also scaling the overall memory bandwidth. these and other characteristics become especially attractive to speedup a class of algorithms whose scalability is limited by main memory performance designated by memory-bound algorithms. however, programmers who want to take full advantage of this memory architecture have to deal with new challenges, namely thread and memory affinity. this dissertation shows that how memory-bound algorithms access memory innuma architectures can have a significant impact on performance. a set of tests were used to identify how different techniques and affinity strategies can help improving the performance of java and c algorithms in numa architectures. the tests were performed using different approaches as operating system tools, jvm options, programming techniques or affinity variables for the compilers. these approaches allowed to increase the scalability of two case studies in a numa architecture giving an additional speedup for java applications up to 1.8 times, and up to 2.16 times for c implementations."
    ],
    [
      "nesta dissertação, é explorada a interseção vital entre tecnologia, integração de sistemas e necessidades em constante evolução no setor do retalho. o estudo começou com uma análise aprofundada dos requisitos tecnológicos e operacionais do retalho moderno, destacando-se a importância crítica da integração eficiente de dados para acompanhar as expectativas dos clientes. os objetivos centrais deste trabalho foram investigar, identificar e implementar uma framework low code capaz de integrar soluções oracle retail cloud com aplicações externas, com o intuito de simplificar, acelerar e reduzir os custos associados a esse processo. para alcançar esses objetivos, a pesquisa envolveu uma revisão exaustiva das tecnologias existentes, a seleção criteriosa das ferramentas de mercado mais adequadas e a análise cuidadosa das estratégias de segurança relacionadas com integração em cloud. a conclusão bem-sucedida deste trabalho culminou com a evolução do retail consult integration broker para atender aos requisitos de computação em nuvem e de baixo código, uma solução adaptável e robusta que atendeu às metas definidas. durante o processo de desenvolvimento, ajustes foram feitos para garantir uma solução ainda mais refinada. esta framework não apenas cumpriu os objetivos iniciais, mas também representou uma contribuição significativa para a modernização do setor do retalho. além de fornecer uma solução técnica, esta dissertação ressalta a importância estratégica de adotar abordagens low code e de computação em nuvem no contexto das operações retalhistas. a combinação dessas tecnologias oferece uma resposta eficaz às constantes mudanças do mercado, proporcionando às empresas a flexibilidade necessária para inovar, adaptar-se rapidamente e atender às expectativas dos clientes num ambiente tecnológico em constante evolução.",
      "this dissertation explores the vital intersection of technology, system integration, and ever-evolving needs in the retail sector. the study began with an in-depth analysis of the technological and operational requirements of modern retail, emphasizing the critical importance of efficient data integration to meet customer expectations. the main objectives of this work were to investigate, identify, and implement a low code framework capable of integrating oracle retail cloud solutions with external applications, aiming to simplify, accelerate, and reduce the costs associated with this process. to achieve these goals, the research involved a exhaustive review of existing technologies, criterious selection of the most suitable market tools, and a careful analysis of security strategies related to cloud integration. the successful conclusion of this work led to the evolution of the rcib to meet the demands of cloud computing and the low-code approach. this adaptable and robust solution not only met the defined goals but also represented a significant contribution to the modernization of the retail sector. beyond providing a technical solution, this dissertation highlights the strategic importance of adopting low code and cloud computing approaches in the context of retail operations. the combination of these technologies offers an effective response to the constant market changes, providing businesses with the necessary flexibility to innovate, adapt quickly, and meet customer expectations in an ever-evolving technological environment."
    ],
    0.06666666666666667
  ],
  [
    [
      "medical examinations in general and medical imaging in particular play an important role in disease detection and patient monitoring. most of the medical images produced are based on x-rays and are used by almost all medical specialties. they provide sensitive results and are relatively inexpensive compared to other techniques such as magnetic resonance imaging (mri). however, mri has proven to be a great diagnostic tool as it enables high-quality images in multiple planes or directions. progressively more researchers are exploring the use of deep learning (dl) algorithms to process and analyse medical images. the main goal of this work is to study and evaluate dl methods applied to two different types of medical images: 2d x-rays images and 3d mri scans. both of these types are often used for early detection, diagnosis, and treatment of diseases. existing dl architectures and image pre processing techniques have also been studied and developed for segmentation of various brain regions (using mris) and classification of chest diseases (using x-rays). convolution neural network (cnn) architectures have aroused interest in health domain because of their powerful learning ability. these networks use multiple convolutions, pooling and fully connected layers to learn the features. this work investigated several cnn architectures and their application in classification of chest x-rays. the use of transfer learning, a deep learning technique, was also studied. this technique enabled the initialization of the cnn with weights from a pre-trained model, whereby the knowledge was transferred from another model. fully convolutional networks (fcn), have been successfully used in the segmentation of biomedical images. in this work, an fcn based on the well-known u-net architecture was used to perform the segmentation of brain mri scans. u-net uses convolution, max pooling, concatenation and up sampling layers to extract the features. for the segmentation task the best result obtained with brain mri using dice similarity coefficient was 95.97%. for the classification task using chest x-rays, the best result using area under the roc curve was 96.40%.",
      "os exames médicos em geral e as imagens médicas em particular, desempenham um papel importante na deteção de doenças e no monitoramento dos pacientes. as imagens produzidas são na sua maioria raios-x, pois são utilizados em grande parte das especialidades médicas. os raios-x fornecem resultados sensíveis e são relativamente baratos quando comparados com outras técnicas, como a ressonância magnética (rm). as rm, no entanto, provaram ser uma ótima ferramenta de diagnóstico, uma vez que permitem obter imagens de alta qualidade em vários planos ou direções. progressivamente mais cientistas investigam o uso de algoritmos de deep learning (dl) para o processamento e análise de imagens médicas. o presente trabalho tem como principal objetivo o estudo e avaliação de técnicas de dl aplicadas a dois tipos de imagens médicas: imagens de raios-x (2d) e imagens de rm (3d). ambas são frequentemente usados para deteção precoce, diagnóstico e tratamento de doenças. estudaram-se também diferentes arquiteturas de dl e técnicas de pré-processamento de imagens para realizar a segmentação de várias regiões do cérebro (usando imagens de rm) e para a classificação de doenças torácicas (usando raios-x). a convolution neural network (cnn) é uma arquitetura que tem despertado um grande interesse no domínio da saúde devido à sua boa capacidade de aprendizagem. estas redes utilizam várias camadas de tipo convolution, pooling e fully connected para aprender as features. o presente trabalho investigou diferentes esquemas de cnn e avaliou a sua capacidade de classificar os raios-x tóracicos. também se estudou o uso de uma técnica de dl denominada transfer learning. esta técnica permitiu a inicialização da rede usando pesos provenientes de um modelo pré-treinado, transferindo o seu conhecimento. as fully convolutional networks (fcn) têm obtido um bom desempenho na segmentação de imagens biomédicas. neste trabalho, para segmentar as imagens de rm do cérebro, utilizou-se uma fcn baseada numa arquitetura amplamente conhecida, a arquitetura u-net. a u-net usa camadas de convolution, max pooling, concatenation e up sampling para extrair as features. na segmentação de imagens de rm cerebral, o melhor resultado foi um coeficiente de similaridade dos dados de 95,97%. na classificação de raios-x torácicos, o melhor resultado foi uma área sob a curva roc de 96,40%."
    ],
    [
      "atualmente, vivemos numa era extremamente tecnológica e exigente, traduzindo-se no incremento das necessidades dos utilizadores da internet, face aos serviços que esta lhes disponibiliza. neste sentido, existe uma crescente preocupação em desenvolver novos sistemas de rede virtuais, como é o caso das redes overlay, muitas delas ligadas à distribuição de conteúdos. porém, como consequência deste crescimento adoptivo das redes overlay, as infraestruturas dos fornecedores de rede (internet service provider (isp)) são sobrecarregadas com tráfego peer-to-peer (p2p), produzindo maiores dificuldades em relação à sua administração. assim, esta dissertação apresenta como objetivo primário conceber e promover o uso de uma rede overlay controlada pelo isp, que disponha de mecanismos de controlo de tráfego p2p, e que, em simultâneo, possibilite a oferta de um serviço de partilha de ficheiros aos seus clientes. como tal, foi especificada uma arquitetura de rede baseada no paradigma p2p, para suportar funcionalidades colaborativas com o isp, sendo composta por quatro entidades principais: peer de acesso, peer de encaminhamento, coordenador e isp. a partir desta, implementaram-se quatro aplicações correspondentes a cada uma das entidades, tendo sido criadas interfaces gráficas direcionadas para o isp e para os clientes (peers de acesso). foram implementados mecanismos básicos de partilha de conteúdos com suporte de dois modos de transferência (cliente-servidor e p2p), além de mecanismos de proteção e priorização de links/routers críticos (com a possibilidade de planeamento por datas), limitação de circulação de conteúdos na rede overlay e suporte a duas estratégias distintas de encaminhamento aplicacional, como principais medidas de apoio ao isp. posteriormente, a partir do emulador de rede common open research emulator (core), foram criados cenários distintos de teste, relativos aos mecanismos de colaboração com o isp implementados. nestes, além de ser analisado o tráfego transmitido por router, de modo a verificar os impactos dos mecanismos colaborativos com o isp, foi efetuada a análise das rotas percorridas na rede física/aplicacional. por fim, a partir dos cenários de teste efetuados, assume-se que os mecanismos de controlo de tráfego p2p, não afetam significativamente a qualidade do sistema da rede overlay, facilitando ao mesmo tempo as tarefas de administração do isp.",
      "we are currently living in an extremely technological and demanding era, resulting in an increase in needs of internet users about the services it offers them. in this sense, there is a growing concern to develop new virtual network systems, such as overlay networks, many of them linked to content distribution. however, as a consequence of this adoptive growth of overlay networks, network provider (isp) infrastructures are overloaded with p2p traffic, producing more considerable difficulties concerning its administration. thus, this dissertation presents as the primary objective to conceive and promote the use of an overlay network controlled by isp, which has p2p traffic control mechanisms and enabling, at the same time, a file sharing service to customers. as such, a network architecture based on the p2p paradigm was specified to support collaborative functionality with isp, consisting of four main entities: peer access, forwarding peer, coordinator and isp. from this, four applications corresponding to each of the entities were implemented, having been created graphical interfaces directed for the isp and the clients (access peers). underlying content sharing mechanisms with support for two transfer modes (client-server and p2p) were implemented, as well as protection and prioritization mechanisms for critical links/routers with the possibility of planning by dates, limitation of content circulation on the overlay network and support for two distinct application routing strategies as main support measures for isp. subsequently, from the common open research emulator (core) network emulator, distinct test scenarios were created concerning the collaboration mechanisms with the isp implemented. in these, besides analyzing the traffic transmitted by the router, to verify the impacts of the collaborative mechanisms with isp, the analysis of the routes traveled in the physical/applicational network was performed. finally, from the test scenarios performed, it is assumed that the p2p traffic control mechanisms do not significantly affect overlay network quality while facilitating isp administration tasks."
    ],
    0.12857142857142856
  ],
  [
    [
      "machine learning (ml) gives a computer system the ability to perform a certain task without being explicitly programmed to do it. although ml is not a new topic in the field of computer science, these techniques have been gaining increasing popularity due to advances in hardware (especially gpus). more powerful hardware supports more efficient training and a more responsive end-system, once deployed. these algorithms have proven to be particularly effective in image processing and feature detection, namely with deep neural networks. in the context of a vehicle, autonomous or not, perceiving its external and internal environment enables the ability to detect and identify left behind objects, its misuse or other potentially dangerous situations. this captured data is relevant to trigger vehicle intelligent responses. bosch is currently developing a system that has these capabilities and plans to leverage deep learning approaches to implement it. this work aimed to test and evaluate the suitability of a given embedded device for the project. it also determined the best strategy to implement deep learning solutions in the device. the supplied test bed was a nvidia software development kit (sdk) system for the embedded nvidia jetson tx2 device with the system-on-chip (soc) parker, an heterogeneous computing chip with 2 denver-cores (a nvidia implementation of arm-64 architecture), 4 cortexa57-cores (also arm-64), 256 pascal gpu-cores and support for up to 6 video cameras. the sdk includes several software library packages, including for image processing and ml. with the goal of fully exploiting the embedded device compute capabilities, this work studied several inference frameworks, going as far as implementing an inference engine from scratch (named deeploy) that produces inferences based on two libraries provided by nvidia: cudnn and tensorrt. deeploy was evaluated against well known and established frameworks, namely tensorflow, pytorch and darknet, in terms of efficiency, resource management and overall ease of use, maintainability and flexibility. this work also exploited key performance related features available on the device, such as power modes, half-precision floating point computation and the implemented shared memory architecture between the gpu-cores and the cpu-cores.",
      "machine learning dá a um sistema informático a capacidade de completar uma dada tarefa sem ser explicitamente programado para tal. apesar de machine learning não ser um tópico novo no campo da engenharia informática, estas técnicas têm-se tornado cada vez mais comuns devido a avanços no hardware (especialmente nos gpus). hardware mais computa-cionalmente capaz dá origem a treinos mais eficientes e a sistemas em campo mais rápidos. este tipo de técnicas, em especial redes neuronais, demonstraram-se eficazes no processa-mento de imagens e deteção de objetos. no contexto de um veículo, autónomo ou não, perceber o seu interior e o ambiente no qual este se insere é essencial para detetar objetos esquecidos, o uso indevido do mesmo ou outro tipo de situações perigosas. esta informação é essencial para desencadear respostas inteligentes por parte do veículo. a bosch está atualmente a desenvolver um sistema com estas capacidades e para o implementar pretende utilizar soluções baseadas em redes neu-ronais. com o projeto pretendeu-se testar e avaliar a aptidão de um dado dispositivo embe-bido para este projeto. serviu também para determinar a melhor estratégia para se fazer a implementação de redes neuronais neste dispositivo. os testes foram feitos num kit de desenvolvimento da nvidia que consiste num nvidia jetson tx2 que contém um chip de computação heterogéneo composto por 2 cores denver (implementação da nvidia da ar-quitetura arm-64), 4 cores cortexa57 (também arm-64), 256 cores gpu pascal e capacidade de se conectar até 6 camaras de vídeo. o kit de desenvolvimento inclui várias bibliotecas de software para processamento de imagem e até para ml. com o objectivo de tirar total partido das capacidades computacionais do sistema em-bebido, este trabalho explorou várias plataformas de inferência, implementando mesmo um motor de raiz capaz de fazer inferência recorrendo a duas bibliotecas desenvolvi-das pela nvidia: cudnn e tensorrt. foi também feita uma comparação entre as duas implementações desenvolvidas e frameworks tradicionais como tensorflow, pytorch e dark-net no que toca a eficiência, facilidade de manutenção e flexibilidade. este trabalho ex-plorou também as features chave que estão relacionadas com performance disponibilizadas pelo dispositivo embebido, como modos de consumo de energia, computação numérica de virgula flutuante de meia precisão e a arquitetura de memória partilhada implementada entre os múltiplos cores arm-64 e os cuda-cores do gpu."
    ],
    [
      "o presente trabalho foi efetuado para obtenção do grau de mestre em engenharia de redes e serviços telemáticos na universidade do minho e teve como primordial objetivo proceder ao desenvolvimento de um sistema de monitorização e otimização da eficiência num laboratório de análises clínicas. para isso, começou-se pelo estudo do estado da arte com intuito de tomar conhecimento das áreas em que se vai trabalhar bem como saber se já existem soluções semelhantes existentes. neste último ponto, não se encontrou nenhuma solução existente. por consequência, foi necessário o desenvolvimento de um dispositivo autónomo, que, combinando hardware e software recolha a informação enviada pelos equipamentos da siemens instalados no laboratório. paralelamente houve a necessidade de implementar um sistema que receba e armazene em base de dados estruturada a informação enviada pelo dispositivo desenvolvido. por fim, procedeu-se ao desenvolvimento de um dashboard para apresentação do resultado do tratamento dos dados armazenados (informação de business intelligence), relevante para a gestão e otimização do processo analítico.",
      "the present work was done in order to obtain the master degree in engineering of computer networks and telematic services at universidade do minho and the main goal was to develop a monitoring and optimization system in clínical analysis laboratories. to achieve this, we started by studying the state of the art in order to take knowledge of the fields we are going to work, as well as to know if a similar solution already exist. after several researches we pointed that at this time there isn’t any solution. consequently, it was necessary to develop a stand-alone device that, combining hardware and software will collect data sent by the siemens equipment’s installed in the laboratory. simultaneously, it was necessary to implement a system that receives and stores the data transmitted by the sent by the developed device in a structured. finally, a dashboard was developed in order to show the analysis of the relevant stored data (business intelligence information) for the management and optimization of the analytical process."
    ],
    0.0
  ],
  [
    [
      "during the last decades, artificial intelligence algorithms have been evolving to the point that they can achieve some amazing results like, identify and navigate roads, identify fraudulent transactions, personalize crops to individual conditions, discover new consumer trends, predict personalized health outcomes, optimize merchandising strategies, predict maintenance, optimize pricing and scheduling in real-time, diagnose diseases, among many others. however, although it can do all of that, it needs all the data to be correctly label, in other words, it can not, for example, diagnose a disease, such as a stroke, if it does not know what a stroke is, so if the algorithm has never been trained to identify strokes a new algorithm has to be created or the current one has to be retrained, similar issues happen in the other examples. this work focuses on this problem and tries to solve it by using a related in a high dimensional vector space, called semantic space, where the knowledge from known classes can be transferred to unknown classes.",
      "durante as últimas décadas, os algoritmos de inteligência artificial têm evoluído ao ponto de alcançarem resultados incríveis, como identificar e navegar estradas, identificar transações fraudulentas, personalizar colheitas para condições individuais, descobrir novas tendências de consumo, prever resultados de saúde personalizados, otimizar merchandising estratégias, prever manutenções, otimizar preços e agendamentos em tempo real, diagnosticar doenças, entre muitos outros. porém, embora possa fazer tudo isso, precisa que todos os dados sejam identificados corretamente, ou seja, não pode, por exemplo, diagnosticar uma doença, como um acidente vascular cerebral, se não souber o que é um avc, portanto, se o algoritmo nunca foi treinado para identificar avc’s um novo algoritmo precisa de ser criado ou o atual de ser retreinado, problemas semelhantes acontecem nos outros exemplos. esta tese foca-se neste problema e tenta resolvê-lo usando um espaço vetorial relacionado de alta dimensão, denominado espaço semântico, onde o conhecimento de classes conhecidas pode ser transferido para classes desconhecidas."
    ],
    [
      "a internet of things (iot) é uma das áreas tecnológicas que necessita de sistemas distribuídos que suportem o armazenamento e acesso à enorme quantidade de dados constantemente produzidos por centenas a milhares de dispositivos. até agora, a maioria dos sistemas desenvolvidos encontravam-se adaptados a instalações em centro de dados, impulsionados pela adoção de serviços de computação em nuvem, porém, o sistema distribuído large scale file system (lsfs), veio mudar o paradigma atual e mover o armazenamento distribuído para infraestruturas totalmente descentralizadas. este é um sistema de ficheiros peer-to-peer não estruturado, parcialmente compatível com a interface posix, que permite a realização de leituras por parte de múltiplos utilizadores, mas escritas por parte de um só utilizador. foi construído para atingir alta disponibilidade e resiliência e encontra-se preparado para escalar para infraes truturas do futuro. todavia, o lsfs não apresenta operações essenciais de um sistema de ficheiros, como a eliminação ou modificação de dados, a grande carga que é exercida sobre a rede tem consequências negativas no sistema como um todo e a forma como este foi avaliado levanta várias questões. com o propósito de resolver estes desafios, desenvolveu-se o improved large scale file system (ilsfs), um sistema de ficheiros que estende o sistema lsfs, dotando-o de uma melhor usabilidade, mas preservando todas as suas características fundamentais como a escalabilidade, a resiliência e a disponi bilidade. para isso, o sistema adota soluções, como tombstones, que viabilizam a eliminação de dados e a disponibilização de uma interface com maior compatibilidade com o standard posix, implementa métodos, como version vectors, que permitem o controlo de concorrência entre múltiplos utilizadores, e mecanismos, como caches, que ajudam a mitigar o problema de saturação da rede. os resultados obtidos, demonstram que o ilsfs, com todas as funcionalidades introduzidas, apre senta uma melhor usabilidade sem, no entanto, comprometer significativamente o desempenho. quando introduzido num caso de estudo real, demonstra-se que o sistema é capaz de escalar para ambientes de larga escala, com centenas de nodos, e mesmo quando submetido a cenários de instabilidade, onde a ocorrência de falhas aleatórias é a norma, o ilsfs mostra-se capaz de tolerar a falha de uma grande quantidade de nodos de armazenamento, sem que esta provoque uma disrupção do seu funcionamento.",
      "the internet of things (iot) is one of the main technological areas that widely needs distributed sys tems to accommodate the storage and access to large amounts of data, being continuously produced by hundreds to thousands of devices. until now, most developed systems were adapted to data centers installations, driven by the adoption of the cloud, nonetheless, one system, the large scale file system (lsfs), is attempting to change the current model and move the distributed storage to highly decentral ized infrastructures. this is an unstructured peer-to-peer file system, partially compatible with the posix interface, that allows multi-user reads, but only single-user writes. it was built to achieve high availability and resilience, and was designed to scale for the infrastructures of the future. however, lsfs does not offer key operations of a traditional file system, such as deleting and modifying files, the large load that is exerted over the network has negative consequences on the system, and its evaluation raises various concerns. in order to solve these challenges, the improved large scale file system (ilsfs) was developed, an extension of the lsfs system, providing it with better usability, but preserving all the fundamental features, its scalability, resilience and availability. to do so, the system adopts solutions, e.g., tombstones, which allow the development of the delete operation and the provision of a more complete posix interface, implements methods, such as version vectors, which enable for concurrency control among multiple users in the file system, and mechanisms, namely caches, that help mitigate the network saturation problem. the experimental results show that ilsfs, with all its features, presents better usability, without sig nificantly compromising performance. when placed in a real case study, it is further demonstrated that the system is capable of scaling to large scale environments, with hundreds of nodes, and that even when subjected to unreliable scenarios, in which the occurrence of random failures is the norm, ilsfs proves to be capable of tolerating the failure of a large number of storage nodes, without causing the disruption of its normal operation."
    ],
    0.3
  ],
  [
    [
      "com a constante evolução da economia e da sociedade, surge a crescente preocupação em estabelecer e manter modelos de negócios que não sejam apenas lucrativos, mas também sustentáveis a longo prazo. entre esses modelos, destaca-se o sistema de franchising, que, como qualquer empreendimento, enfrenta desafios complexos e singulares no quotidiano da sua gestão. ora, na presente dissertação, apresenta-se o desenvolvimento de uma plataforma de gestão de franchisings, criada em colaboração com a empresa wintouch. este sistema representará um valioso complemento à oferta de produtos já disponibilizados pela referida empresa sobre o mercado de software de gestão comercial. dessa forma, foi concebida uma aplicação web, e todos os componentes associados, que visam abordar os diversos domínios que envolvem a gestão de franchisings. essa solução foi projetada para atender às necessidades de dois tipos distintos de utilizadores intervenientes num negócio de franchising, nomeadamente, os franchisadores e os franchisados. ainda, a plataforma desenvolvida abrange uma ampla gama de funcionalidades, desde a manutenção da rede de franchising até a disponibilização de sistemas para controlar as encomendas e de monitorização do desempenho dos franchisados. por fim, considerando a ambição e complexidade inerentes a este projeto de desenvol vimento de software, foram adotados um planeamento e estratégia bem definidos, que compreenderam três fases distintas: uma investigação inicial sobre o tema em questão, a modelação cuidadosa e robusta da solução a ser desenvolvida e, por fim, o desenvolvimento efetivo da plataforma. todas essas fases estão minuciosamente documentadas ao longo deste relatório.",
      "with the constant evolution of the economy and society, there is a growing concern about es tablishing and maintaining business models that are not only profitable, but also sustainable in the long term. among these models is the franchising system, which, like any business, faces complex and unique challenges in its day-to-day management. this dissertation presents the development of a franchising management platform, created in collaboration with the company wintouch. this system will be a valuable addition to the range of products already available from this company in the commercial management software market. a web application and all the associated components were designed to address the various areas involved in managing franchisings. this solution was designed to meet the needs of two different types of users involved in a franchising business, namely franchisors and franchisees. in addition, the platform developed covers a wide range of functions, from maintaining the franchising network to providing systems for controlling orders and monitoring franchisee’s performance. finally, considering the ambition and inherent complexity in this software development project, a well-defined planning and strategy was adopted, comprising three distinct phases: initial research into the topic in question, careful and robust modeling of the solution to be developed and, ultimately, the actual development of the platform. all these phases are thoroughly documented throughout this report."
    ],
    [
      "“recommended for you”, a expressão que está a tornar-se progressivamente numa das mais vistas no mundo das aplicações. não é por acaso que, grande parte das aplicações web (mas não só) por onde navegamos diariamente queiram tentar adivinhar e mostrar-nos o que nos agrada e o que acham recomendado para nós. de facto, esta apresentação de conteúdos orientados ao utilizador, às preferências históricas demonstradas, tem o poder imenso de nos manter a navegar num site, de nos fazer comprar mais produtos, de nos fazer voltar à aplicação. esta “técnica” de recomendação de conteúdos que tem como base o estudo do historial do utilizador, das suas decisões tomadas e das suas demonstrações de preferências, assenta na extração de padrões que retratam o percurso histórico do mesmo. por sua vez, as decisões tomadas por este assentam num padrão preferencial exibido pelo utilizador que vai de encontro às características intrínsecas à sua personalidade. ora, com este estudo facilmente se entende que se podem retirar dados que irão caracterizar de forma inequívoca um utilizador, criando, desta forma, uma assinatura que o traduz, que o define e o torna único em relação aos demais. é nesta tradução de um conjunto de preferências temporais numa assinatura que define intemporalmente um utilizador, que assenta uma parte deste plano de trabalho. é neste contexto que surge todo este estudo, efetuado durante o presente projeto e sobre o qual incidirá esta dissertação. será difícil encontrar melhores exemplos de conteúdos que caracterizem um indivíduo do que a música que ele ouve. “start and play” será o conceito inerente a uma peça de software que se pretende futuramente construir e que procurará recomendar o que o utilizador deve ouvir, com base nas preferências demonstradas por este, assente no seu conjunto alargado de ações passadas. estas preferências definirão no seu conjunto a assinatura do utilizador, com a qual o sistema irá tentar definir e prever quais os conteúdos musicais que irá recomendar.",
      "\"recommended for you\", the expression that is gradually becoming one of the most popular in the world of applications. it is no coincidence that most of the web applications that we use daily want to guess and show us what we like and what they think that is recommended for us. in fact, this presentation of content oriented towards the user preferences, shown by the history, has the immense power to keep us in a site, make us buy more products or to make us turn back to the application. is this content recommendation \"technique\", which is based on the user history study, their decisions and preferences, it‟s based on the extraction of patterns that depict that same history. meanwhile, the decisions taken by this system are based on a preferred pattern displayed by the user, which meets the intrinsic characteristics of his personality. for that being, with this study we can easily understand that we can pull data that will characterize unequivocally a user, creating, thereby, a signature that reflects, defines and makes him unique when compared to others. it is in this transformation from a set of preferences to a temporal signature (that defines a user timelessly) that is based a part of this work plan. it is in this context that arises the entire study performed during this project and on which this document will focus. it will be difficult to find better examples of content that can characterize an individual than the music he hears. “start and play” is the concept inherent to a piece of software that aims to be built in the near future and that will try to recommend what an user should listen to, based on their past actions. these preferences will define the whole signature of the user, with which the system will attempt to define and predict what music content should be recommended."
    ],
    0.0
  ],
  [
    [
      "the objective of this dissertation is the development of an application capable of automatically generating synthetic datasets that are representative and, possibly, very large, directly from json and xml schemas, in order to facilitate the testing of software applications and scientific endeavors in areas such as data science or application development. for this purpose, it is intended to develop a new version of datagen, an online open-source application that allows the quick prototyping of datasets through its own domain specific language (dsl) of specification of data models. datagen is able to parse these models and generate synthetic datasets according to the structural and semantic restrictions stipulated, automating the whole process of data generation with spontaneous values created in runtime and/or from a library of support datasets. the objective of this new product, datagen from schemas, is to expand datagen’s use cases and raise the datasets specification’s abstraction level, making it possible to generate synthetic datasets directly from schemas. this new platform builds upon its prior version and acts as its complement, operating jointly and sharing the same data layer, in order to assure the compatibility of both platforms and the portability of the created dsl models between them. its purpose is to parse schema files and generate corresponding dsl models, effectively translating the json or xml specification to a datagen model, then using the original application as a middleware to generate the final datasets. the present dissertation details the entire creative process behind the development of this application: firstly, it frames the topic of study and its initial phase of investigation, debating relevant technologies and existing related work; then, the ideation phase of the product is addressed, projecting an adequate arquitecture and the reasons behind its design choices, as well as surveying technical requirements for datagen from schemas, while taking into account the conclusions reached through prior research; afterwards, the development phase is covered, carefully explaining the elaborated components, their properties and the data flow between them, for both the json and xml modules; finally, the reader is presented with conclusions taken from this project’s development and possible future work to implement, in order to improve the current solution.",
      "o objetivo desta dissertação é o desenvolvimento de uma aplicação que permita gerar auto maticamente datasets sintéticos representativos e possivelmente extensos, a partir de schemas de json e xml, de forma a facilitar a testagem de aplicações de software e empreendimentos científicos em áreas como a ciência de dados e o desenvolvimento de aplicações. para esta finalidade, pretende-se desenvolver uma plataforma assente sobre o datagen, uma aplicação que permite a prototipagem rápida de datasets através da sua própria domain specific language (dsl) de especificação de modelos de dados. o datagen é capaz de processar estes modelos e gerar posteriormente datasets sintéticos que obedeçam às restrições estruturais e semânticas estabelecidas, automatizando todo o processo de geração de dados com valores espontâneos gerados em tempo de execução e/ou provenientes de bancos de dados de suporte. o objetivo deste novo produto, datagen from schemas, é expandir os casos de aplicação do datagen e aumentar o nível de abstração da especificação de modelos de dados, tornando possível a geração de datasets sintéticos diretamente a partir de schemas. esta nova plataforma estará assente sobre a sua versão anterior e agirá como seu complemento, operando conjun tamente e partilhando a mesma camada de dados, de forma a assegurar a compatibilidade das plataformas e a portabilidade dos modelos criados entre ambas. o seu propósito é processar ficheiros schema e gerar modelos correspondentes na dsl, efetivamente traduzindo a especificação em json ou xml para um modelo do datagen, para depois usar a aplicação original como um middleware para gerar os datasets finais. a presente dissertação detalha todo o processo creativo por detrás do desenvolvimento desta aplicação: começa por enquadrar o tema de estudo e a sua fase inicial de investigação, debatendo tecnologias relevantes e trabalho relacionado existente; de seguida, é abordada a fase de ideação do produto, projetando uma arquitetura adequada para a solução e as razões por detrás das suas escolhas de design, e realizado um levantamento de requisitos técnicos para o datagen from schemas, tendo sempre em conta as conclusões alcançadas através de investigação prévia; depois, é relatada a fase de desenvolvimento do produto, explicando minuciosamente os componentes elaborados, as suas propriedades e o fluxo de dados entre eles, para ambos os módulos de json e xml; finalmente, são apresentadas ao leitor as conclusões retiradas do desenvolvimento deste projeto e possível trabalho futuro a implementar, de forma a melhorar a solução atual."
    ],
    [
      "internet of things (iot), está-se a tornar cada vez mais parte das nossas vidas e, quando aliada a computação na cloud, torna-se uma ferramenta muito poderosa devido a remover stress computacional de pequenas placas ineficientes. software como um serviço já representa uma grande parte do nosso dia-a-dia com empresas como a netflix a aplicar o conceito de forma muito bem-sucedida. no contexto de iot, este conceito ainda não está globalmente disseminado. a natureza heterogénea dos dispositivos representa um grande desafio para os conseguir integrar num sistema na cloud. os diferentes formatos e tipos de dados enviados para um middleware são difíceis de processar e, como consequência, leva a que exista uma grande pressão no programador para que todos os sensores sejam suportados. ao longo deste estudo são exploradas variadas arquiteturas de forma a ser possível desenhar um sistema eficiente e, como os diferentes protocolos de comunicação afetam a rede em termos de overhead e fiabilidade. o sistema concebido, baseado em toda o estudo realizado, consiste numa aplicação para salas inteligentes que infere quantas pessoas estão lá dentro através de probes de wifi, disponibiliza essa informação a utilizadores e é verificada a possbilidade de utilização de algoritmos de machine learning como forma de optimizar resultados. o sistema desenhado permite aos programadores adicionar outros dispositivos sem ter de se preocupar como as mensagens são recebidas, apenas necessitando de adicionar a lógica que extrai o conhecimento dos dados. no que toca à área de crowdsensing deste trabalho, a precisão do sistema foi melhorada quando comparando com outros algoritmos estudados.",
      "internet of things, or iot, is becoming more and more a part of our lives and when allied with cloud computing it becomes a very powerful tool by removing the computing stress from the small energy efficient boards. software as a service is already a major part of our day-to-day lives with companies like netflix successfully applying this concept. in iot this type of concept is not widely applied. the heterogeneous nature of devices poses a big challenge to integrate them in a cloud system, the different data and formats sent to a middleware are hard to process and puts pressure on the developer to ensure all sensors are supported. throughout this study we explore and design different types of architectures for efficient applications and how the different communication protocols affect the network when it comes to overhead and reliability. the conceived system, theoretically grounded on the research work, consists of a smart room application that senses how many people are inside a space through the process of wifi probing makes that information available to a user and makes use machine learning algorithms as a way to improve results. the design system allows developers to easily add new types of devices to the network without needing to worry how the messages are received, only needing to add domain logic to extract knowledge from the data. regarding the crowdsensing aspect of this work, the accuracy of the system was improved when compared to other algorithm."
    ],
    0.06666666666666667
  ],
  [
    [
      "as plataformas móveis estão cada mais enraizadas no nosso quotidiano. o conforto de escolher onde usar e a facilidade de as utilizar em qualquer lugar criam formas de informação e comunicação para as mais diversas áreas, facilitando assim num grande conjunto de tarefas. para algumas destas áreas, a disponibilização de uma plataforma móvel pode ser a chave para o sucesso dos objetivos pretendidos, oferecendo ao utilizador uma experiência completamente inovadora face aos métodos mais convencionais. a área do ensino encaixa perfeitamente nestes moldes, tendo em conta que o uso destas plataformas pode aumentar o envolvimento dos alunos nos seus deveres, através da projeção simples e intuitiva dos vários exercícios e ferramentas adequadas que fomentem os seus conhecimentos e aprendizagem, ajudando-os a alcançar melhores resultados. tendo isso em consideração, neste trabalho de dissertação, desenvolveu-se, numa primeira etapa, uma fundamentação teórica relativamente ao uso de plataformas móveis e a sua evolução ao longo dos anos, abordando-se com isso uma perspetiva totalmente voltada para a área do ensino. numa segunda fase, foram detalhados e analisados os diferentes tipos de plataformas móveis que atualmente imperam no mercado dos smartphones, bem como alguns exemplos de plataformas especialmente criadas para servirem como sistemas de aprendizagem. numa última etapa, com a escolha do tipo de plataforma a desenvolver para a dissertação, idealizou-se e implementou-se um sistema de interação adaptativo para suporte a processos de aferição do conhecimento de estudantes ao longo do tempo, em domínios específicos.",
      "mobile platforms are increasingly rooted in our daily lives. the comfort of choosing where to use and the ease of using them anywhere create forms of information and communication for the most diverse areas, facilitating a wide range of tasks. for some of these areas, a good mobile platform can be the key to the success of the intended objectives, offering the user a completely innovative experience compared to the most conventional methods. the education area fits perfectly in these contexts, considering that the use of these platforms can increase the involvement of students in their duties, through the simple and intuitive projection of the various exercises and appropriate tools that foster their knowledge and learning, helping them achieve better results. thinking about that, in this dissertation work, a theoretical foundation was developed in a first step regarding the use of mobile platforms and its evolution over the years, approaching with this a perspective totally focused on the area of education. in a second phase, the different types of mobile platforms that currently prevail in the smartphones market were detailed and analyzed, as well as some examples of platforms specially created to serve as learning systems. in a last step, with the choice of the type of platform to be developed for the dissertation, an adaptive interaction system was conceived and implemented to support the processes of measuring the knowledge of students over time, in specific domains."
    ],
    [
      "nas últimas décadas, a tecnologia tem sofrido uma evolução exponencial, assumindo um papel fundamen tal no nosso quotidiano e quando aplicada nas mais diversas áreas proporciona-nos melhorias significati vas na nossa qualidade de vida. por exemplo, a tecnologia permite-nos fazer compras sem sair de casa, que é um hábito que muitas pessoas têm e nos últimos tempos, devido ao panorama em que nos encon tramos, o seu crescimento tem sido ainda mais notável. frequentemente, enquanto compramos online, são nos apresentados itens que não têm qualquer interesse para nós. diante este problema, surgiram os sistemas de recomendação, que se tornaram uma parte fundamental e um dos fatores diferenciadores a nível aplicacional. o principal objetivo de um sistema de recomendação é, recorrendo a algoritmos de ma chine learning, produzir uma lista de itens ordenados de acordo com o grau de relevância esperado para um determinado utilizador, permitindo, por exemplo, evitar o problema de recomendações indesejadas. mais especificamente, nesta dissertação desenvolveu-se e incorporou-se um sistema de recomendação numa aplicação web de venda online de livros, onde as técnicas concebidas e implementadas permitiram melhorar a qualidade da recomendação e consequentemente a experiência aplicacional, com sugestões que vão de encontro às preferências do utilizador.",
      "in the last decades, technology has undergone an exponential evolution, assuming a fundamental role in our daily lives and when applied in the most diverse areas provides us with significant improvements in our quality of life. for example, technology allows us to shop without leaving home, which is a habit that many people have and in recent times, due to the panorama we find ourselves, its growth has been even more remarkable. often, when we buy online, most of the items presented are of no interest to us. faced with this problem, recommendation systems emerged, which became a fundamental part and a differentiating factor at the application level. the main objective of a recommendation system is, using machine learning algorithms, to produce a list of ordered items according to the degree of relevance expected for a given user, allowing, for example, to avoid the problem of unwanted recommendations. more specifically, in this dissertation a recommendation system was developed and incorporated into a web application for online book sales, where the techniques designed and implemented allowed to improve the quality of the recommendation and consequently the user’s application experience, with suggestions that go according to their preferences."
    ],
    0.06666666666666667
  ],
  [
    [
      "segmentation is a key method to extract useful information in electron tomography. manual segmentation is the most commonly used method, but it is subject to user bias and the process is slow. the lack of adequate automated processes, due to the high complexity and to the low signal-to-noise ratio of these tomograms, provided the main challenges for this dissertation: to develop a software tool to efficiently handle electron tomograms, including a novel 3d segmentation algorithm. tomograms can be seen as a stack of 2d images; operations on tomograms usually lead to computationally intense tasks. this is due to the large amount of involved data and to the strided and random memory access patterns. these characteristics represent serious problems on novel computing systems, which rely on complex memory hierarchy architectures to hide memory access latency time. a software tool with a user-friendly interface — tomseg — was designed, implemented and tested with experimental datasets, built with sequences of scanning electron microscopy images obtained using a slice and view technique. this tool lets users align, crop, segment and export electron tomograms, using computationally efficient processes. tomseg takes advantage of the most usual architectures of modern compute servers, namely based on multicore and many-core cpu devices, exploring vector and parallel programming techniques; it also explores the available gpu-devices to speedup critical code functions. validation and performance results on a compute server are presented together with the performance improvements obtained during the implementation and test phases. tomseg is an open-source tool for unix and windows that can be easily extended with new algorithms to efficiently handle generic tomograms.",
      "a segmentação é uma técnica fundamental na tomografia eletrónica para a extração de informação. a segmentação manual é o método mais utilizado, mas é um processo lento e sujeito à parcialidade humana. a falta de métodos automáticos adequados, muito devido à elevada complexidade e à baixa relação sinal-ruído destes tomogramas, conduziu aos principais desafios desta dissertação: desenvolver uma ferramenta de software para manusear tomogramas eletrónicos de forma eficiente, que inclui um novo algoritmo de segmentação 3d. os tomogramas podem ser vistos como uma pilha de imagens 2d; operações sobre tomogramas costumam originar tarefas computacionalmente exigentes. isto deve-se à grande quantidade de dados envolvidos e aos acessos espaçados e aleatórios à memória. estas características representam problemas sérios nos mais recentes sistemas de computação, que dependem de uma complexa arquitetura hierárquica para esconder o tempo de acesso à memória. desenhou-se, implementou-se e testou-se uma ferramenta de software com uma interface de utilização amigável — tomseg — utilizando conjuntos de dados experimentais, construídos a partir de sequências de imagens de microscopia eletrónica de varrimento obtidas através de uma técnica de slice and view. esta ferramenta permite aos utilizadores alinhar, cortar, segmentar e exportar tomogramas eletrónicos, utilizando processos computacionalmente eficientes. o tomseg tira vantagem das arquiteturas mais habituais dos servidores de computação atuais, nomeadamente daqueles baseados em dispositivos cpu multicore e many-core, explorando técnicas de programação vetorial e paralela; os dispositivos gpu podem ainda ser usados como aceleradores de algumas funções. vários resultados de validação obtidos num servidor de computação são apresentados, em conjunto com algumas melhorias obtidas durante as fases de implementação e teste. o tomseg é uma ferramenta de código aberto para unix e windows que pode ser estendida facilmente com novos algoritmos para manusear de forma eficiente qualquer tipo de tomogramas."
    ],
    [
      "methods for the study of the functional connectivity in the brain have seen several developments over the last years, however not yet in a fully realized manner. machine learning and complex network analysis are two promising techniques that together can help the process of better exploring functional connectivity for future clinical applications. machine learning and pattern recognition algorithms are helpful for mining vast amounts of neural data with increasing precision of measures and also for detecting signals from an overwhelming noise component (lemm, blankertz, dickhaus, & müller, 2011). complex network analysis, a subset of graph theory, is an approach that allows the quantitative assessment of network properties such as functional segregation, integration, resilience, and centrality (rubinov & sporns, 2010). these properties can be fed into classification algorithms as features. this is a new and complex approach that has no standard procedures defined, so the aim of this work is to explore the use of fmri-derived complex network measures combined with machine learning algorithms in a clinical dataset. in order to do so, a set of classifiers is implemented on a feature dataset built with brain regional volumes and topological network measures that, in turn, were constructed based on functional connectivity data extracted from a resting-state functional mri study. the set of classifiers includes the nearest neighbor, support vector machine, linear discriminant analysis and decision tree methods. a set of feature selection methods was also implemented before the classification tasks. every possible combination of feature selection methods and classifiers was implemented and the performance was evaluated by a cross-validation procedure. although the results achieved weren’t exceptionally good, the present work generated knowledge on how to implement this recent approach and allowed the conclusion that, for most cases, feature selection improves the performance of the classifier. the results also showed that the decision tree algorithm produces relatively good results without being associated with a feature selection method and that the svm classifier, together with rfe feature selection method, produced results on the same level as other work done with a similar approach.",
      "métodos para o estudo de conectividade funcional têm sofrido vários progressos ao longo dos últimos anos, no entanto, as suas potencialidades não estão a ser completamente exploradas. aprendizagem computacional e análise de redes complexas são duas técnicas promissoras que, em conjunto, são capazes de auxiliar no processo de melhor explorar conectividade funcional para futuras aplicações clínicas. aprendizagem computacional e reconhecimento de padrões permitem a extração de conhecimento a partir de imensas quantidades de informação neuronal, cada vez com melhor precisão de medidas e são capazes de encontrar sinal de interesse, mesmo na presença de uma grande componente de ruído (lemm et al., 2011). a análise de redes complexas é uma abordagem que permite a avaliação quantitativa das propriedades de rede (rubinov & sporns, 2010). estas propriedades podem ser usadas em classificação como atributos, o que é considerado uma a abordagem recente e complexa, pelo que não existem ainda procedimentospadrão definidos. deste modo, o objetivo deste trabalho é explorar o uso de medidas de redes complexas derivadas de conectividade funcional e combinadas com algoritmos de aprendizagem computacional em dados clínicos. para tal, um conjunto de classificadores foi implementado, tendo como atributos volumes de regiões cerebrais e medidas de rede que, por sua vez, foram construídas a partir de dados de conectividade funcional extraídos de um estudo de ressonância magnética funcional de repouso. um conjunto de métodos para a seleção de atributos também foi implementado antes de realizar as tarefas de classificação. todas as possíveis combinações destes métodos com os classificadores foram testadas e o desempenho foi avaliado através de cross-validation. apesar dos resultados obtidos não serem excecionalmente bons, o presente trabalho gerou conhecimento sobre a implementação desta nova abordagem e permitiu concluir que, na maioria dos casos, a seleção de características melhora o desempenho do classificador. os resultados também demonstram que o algoritmo de árvore de decisão produz relativamente bons resultados quando não está associado a um método de seleção de características e que o algoritmo de máquina de suporte vetorial, juntamente com o método de seleção de atributos rfe, deu origem a resultados ao mesmo nível de outro trabalho, realizado com uma abordagem similar."
    ],
    0.05454545454545454
  ],
  [
    [
      "a common practice in compiler design is to have an intermediate representation of the source code in static single-assignment (ssa) form in order to simplify the code optimization process and make it more efficient. generally, one says that an imperative program is in ssa form if each variable is assigned exactly once. in this thesis we study the central ideas of ssa-programs in the context of a simple imperative language including jump instructions. the focus of this work is the proof of correctness of a translation from programs of the source imperative language into the ssa format. in particular, we formally introduce the syntax and the semantics of the source imperative language (gl) and the ssa language; we define and implement a function that translates from source imperative programs into ssa-programs; we develop an alternative operational semantics, in order to be able to relate the execution of a source program and of its ssa translation; we prove soundness and completeness results for the translation, relatively to the alternative operational semantics, and from these results we prove correctness of the translation relatively to the initial small-step semantics.",
      "uma prática comum no design de compiladores é ter uma representação intermédia do código fonte em formato static single assigment (ssa), de modo a facilitar o processo subsequente de análise e optimização de código. em termos gerais, diz-se que um programa imperativo está no formato ssa se cada variável do programa é atribuída exatamente uma vez. nesta tese estudamos as ideias principais do ssa no contexto de uma linguagem imperativa simples com instruções de salto. o foco deste trabalho é a prova de correcção de uma tradução dos programas fonte para formato ssa. concretamente, definimos formalmente a sintaxe e a semântica da linguagem fonte (gl) e da linguagem em formato ssa; definimos e implementamos a função de tradução; desenvolvemos semânticas operacionais alternativas de modo a permitir relacionar a execução do programa fonte com a sua tradução ssa; provamos a idoneidade e a completude do processo de tradução relativamente às semânticas alternativas definidas; e a partir destes resultados demonstramos a correção da tradução em ordem às semânticas operacionais estruturais definidas inicialmente."
    ],
    [
      "atualmente, para se obter um modelo de negócio de sucesso, é necessário um investimento no comércio eletrónico e nas tecnologias inerentes. no desenvolvimento de uma loja online, é necessário que seja efetuada uma estruturação da mesma, de forma que o utilizador final consiga encontrar os produtos e/ou serviços desejados efetuando o menor número de cliques, tornando este processo o mais simples e cómodo possível. de forma semelhante, o processo de criação deste tipo de plataformas deve ser um processo relativamente simples e acessível a qualquer individuo. neste contexto, surge a plataforma para geração simples e intuitiva de lojas online consoante as preferências do utilizador. o funcionamento da plataforma consiste na manipulação e integração de um conjunto de componentes customizáveis ou templates, devidamente generalizados, de modo a ser possível gerar uma loja online com as funcionalidades e propriedades definidas pelo utilizador. o objetivo é o desenvolvimento de uma aplicação de generalização que permita automatizar o processo de criação de lojas online. o resultado deste projeto permitirá a um programador web criar uma loja online de forma rápida, através do download do código fonte da loja gerada, tendo apenas de efetuar uma parametrização de um conjunto de atributos específicos consoante o caso concreto a implementar, de modo a satisfazer as necessidades dos clientes.",
      "currently, to achieve a successful business model, investment in e-commerce and the technologies inherent to it is necessary. with technological evolution, the development of online commerce is a necessity to survive in such a competitive market. adding to this reality the fact that there are increasingly more users of these platforms around the world, the need to make online purchases securely assumes new importance. in the development of an online store, it is necessary to structure it in such a way that the end user can find the desired products and/or services with the least number of clicks, making this process as simple and comfortable as possible. similarly, the process of creating these types of platforms must be a relatively simple process accessible to any individual. the platform's operation involves the manipulation and integration of a set of templates, sufficiently generalized to enable the creation of an online store with the functionalities and properties defined by the user. the objective is the development of a generalized application that automates the process of creating online stores. in this context, the application of generalization arises for the development of online stores. the goal is the development of a generalization application that allows automating the process of creating online stores. the result of this project will allow a web programmer to quickly create an online store, having only to carry out a parameterization of a set of specific attributes according to the concrete case to be implemented, in order to meet the needs of the customers."
    ],
    0.06666666666666667
  ],
  [
    [
      "durante anos, o crescimento do feto e o seu desenvolvimento eram as medidas que os médicos usavam para estimar o desenvolvimento e crescimento do mesmo. no entanto em estudos recentes tem-se vindo a provar que o desenvolvimento e crescimento da placenta é de grande importância para estudar a saúde e crescimento do feto. para a correta análise do desenvolvimento da placenta, curvas de crescimento precisam de ser criadas para comparação com diferentes percentis. métodos de regressão linear foram usados para a criação de curvas de crescimento para a população portuguesa (nogueira et al., 2019). neste trabalho, usando os mesmos dados usados por nogueira et al. foram criadas curvas de crescimento usando um método estatístico conhecido por regressão de quantis que permite criar curvas usando um método mais robusto que o anteriormente usado. foi também objetivo deste trabalho a criação de uma aplicação que permita ao utilizador colocar os valores de crescimento da placenta e poder comparar com as curvas de crescimento. para isto, duas aplicações foram criadas. a primeira denominada app1 tenta simular as ligações a bases de dados existentes em hospitais, tentando ao máximo recriar as condições presentes num hospital. a segunda aplicação, app2, é mais simples e só precisa de documentos .csv com os valores da placenta, e permite a sua utilização sem necessidade de criação de algum tipo de ligação (https://samuelalves.shinyapps.io/app2/). ambos os objetivos foram conseguidos com sucesso, permitindo um avanço na área que cada vez mais se mostra de grande utilidade.",
      "for years, fetal growth and development were the measures that the doctors used to estimate fetal development and growth. however, recent studies have shown that the development and growth of the placenta is of great importance for studying the health and growth of the fetus. for correct analysis of placental development, growth curves need to be created for comparison with different percentiles. linear regression methods were used to create growth curves for the portuguese population (nogueira et al., 2019). in this work, using the same data used by nogueira et al., growth curves were created using a statistical method known as quantile regression, which allows the creation of curves using a more robust method than the one previously used (nogueira et al., 2019). it was also the objective of this work to create an application that allows the user to enter placental growth values and compare them with the growth curves. for this, two applications were created. the first one called app1 tries to simulate the connections to existing databases in hospitals, trying as much as possible to recreate the conditions present in a hospital. the second application, app2, is simpler and only needs .csv documents with the placenta values and allows its use without the need to create any kind of connection (https://samuelalves.shinyapps.io/app2/). both goals were successfully achieved, allowing for a breakthrough in the area that is increasingly proving to be of great use."
    ],
    [
      "it is well-known that consistency in shared memory concurrent programming comes with the price of degrading performance and scalability. some of the existing solutions to this problem end up with high-level complexity and are not programmer friendly. we present a simple and well-defined approach to obtain relevant results for shared memory environments through relaxing synchronization. for that, we will look into mergeable data types, data structures analogous to conflict-free replicated data types but designed to perform in shared memory. crdts were the first formal approach engaging a solid theoretical study about eventual consistency on distributed systems, answering the cap theorem problem and providing high-availability. with crdts, updates are unsynchronized, and replicas eventually converge to a correct common state. however, crdts are not designed to perform in shared memory. in large-scale distributed systems the merge cost is negligible when compared to network mediated synchronization. therefore, we have migrated the concept by developing the already existent mergeable data types through formally defining a programming model that we named global-local view. furthermore, we have created a portfolio of mdts and demonstrated that in the appropriated scenarios we can largely benefit from the model.",
      "é bem sabido que para garantir coerência em programas concorrentes num ambiente de memória partilhada sacrifica-se performance e escalabilidade. alguns dos métodos existentes para garantirem resultados significativos introduzem uma elevada complexidade e não são práticos. o nosso objetivo é o de garantir uma abordagem simples e bem definida de alcançar resultados notáveis em ambientes de memória partilhada, quando comparados com os métodos existentes, relaxando a coerência. para tal, vamos analisar o conceito de mergeable data type, estruturas análogas aos conflict-free replicated data types mas concebidas para memória partilhada. crdts foram a primeira abordagem a desenvolver um estudo formal sobre eventual consistency, respondendo ao problema descrito no cap theorem e garantindo elevada disponibilidade. com crdts os updates não são síncronos e as réplicas convergem eventualmente para um estado correto e comum. no entanto, não foram concebidos para atuar em memória partilhada. em sistemas distribuídos de larga escala o custo da operação de merge é negligenciável quando comparado com a sincronização global. portanto, migramos o conceito desenvolvendo os já existentes mergeable data type através da criação de uma formalização de um modelo de programação ao qual chamamos de global-local view. além do mais, criamos um portfolio de mdts e demonstramos que nos cenários apropriados podemos beneficiar largamente do modelo."
    ],
    0.3
  ],
  [
    [
      "transparent and flexible heating systems have become increasingly important for applications such as in defrosting windows, sensors, or heating displays. pedot:pss and silver nanowire (agnw) films offer the possibility of high transparency, flexibility, and conductivity, making them promising substitutes for ito, the most used material in these systems. in this work, these films were studied with the aim of utilizing screen-printing as a reproducible method to produce effective transparent and flexible heating systems with low input voltages. with that purpose, agnws were synthesized and the effect of different factors on the films’ resistances was studied: the films’ thickness; pedot:pss modifications with water, peg, glycerol, methanol, or dmso; post-treatments with methanol, dmso, ctab or sodium borohydride, and the utilization of sintered agnws, an ionic liquid, and pva. the results showed that depositing a layer of agnws on pedot:pss decreased its resistance slightly, while another layer of pedot:pss decreased it considerably. modifying pedot:pss and post-treating it with methanol had an insignificant effect on the resistances, while post-treatment with dmso generally lowered them. utilizing sintered agnws, an ionic liquid, pva, or post-treating the films in ctab or sodium borohydride was ineffective. as such, it was concluded that the resistance depended mainly on the thick ness of the films. when characterized thermally with an input voltage of 12 v, most films showed an insignificant increase in their temperature. nevertheless, films with two manually deposited layers of pe dot:pss post-treated with methanol with a layer of agnws, and films with screen-printed pedot:pss with a big mesh size and a layer of agnws were able to increase their temperature values by almost 5 °c. in conclusion, different methods were studied to improve the conductivity of films for transparent and flexible heating systems, which showed that the films’ thickness is a key factor in their resistance and, consequently, in their heating abilities.",
      "sistemas de aquecimento transparentes e flexíveis têm ganhado importância em aplicações como o desembaciamento de janelas, em sensores ou no aquecimento de displays. os filmes de pedot:pss e nanofios de prata (agnws) podem atingir alta transparência, flexibilidade e condutividade, tornando-os substitutos promissores do ito, o material mais usado nestes sistemas. neste trabalho, estes filmes foram estudados com o objetivo de utilizar a serigrafia como um método reprodutível para produzir sistemas de aquecimento transparentes e flexíveis eficazes com baixas voltagens de entrada. assim, agnws foram sintetizados e foi estudado o efeito de vários fatores nas resistências dos filmes: espessura dos filmes; modificações do pedot:pss com água, peg, glicerol, metanol ou dmso; pós-tratamentos com metanol, dmso, ctab ou borohidreto de sódio, e utilização de agnws sinterizados, de um líquido iónico, e de pva. os resultados mostraram que a deposição de uma camada de agnws sobre pedot:pss diminuiu ligeiramente a resistência, enquanto outra camada de pedot:pss a diminuiu consideravelmente. as modificações do pedot:pss e o pós-tratamento com metanol tiveram um efeito insignificante nas re sistências, enquanto o pós-tratamento com dmso geralmente teve um efeito redutor. a utilização de agnws sinterizados, líquido iónico, pva, ou o pós-tratamento dos filmes com ctab ou borohidreto de sódio foram ineficazes. como tal, concluiu-se que a resistência dependia principalmente da espessura dos filmes. quando caracterizados termicamente com uma voltagem de entrada de 12 v, a maioria dos filmes teve um aumento insignificante na sua temperatura. no entanto, filmes com duas camadas de pe dot:pss depositadas manualmente e pós-tratadas com metanol e com uma camada de agnws, e filmes com pedot:pss serigrafado com um grande tamanho de malha e uma camada de agnws, aumentaram a sua temperatura cerca de 5 °c. em conclusão, foram estudados diferentes métodos para melhorar a condutividade de filmes para sistemas de aquecimento transparentes e flexíveis, tendo-se observado que a espessura dos filmes é um fator chave na sua resistência e, consequentemente, no seu aquecimento."
    ],
    [
      "esta dissertação propõe uma solução a um problema existente num publico que abrange desde o estudante comum até ao profissional do quotidiano. qualquer indivíduo nesta audiência pode enfrentar obstáculos na estruturação de textos, como relatórios académicos ou profissionais, encontrando dificul dades em organizar as ideias para o formato textual desejado. a utilização de métodos de organização como os mapas mentais desempenha um papel crucial como primeiro passo na escrita, pois auxiliam no planeamento da escrita, um processo essencial na construção de qualquer tipo de texto, facilitando a organização de ideias. a dissertação aborda o desenvolvimento de uma aplicação web, tanto na sua vertente de frontend como na de backend. a aplicação integra a edição do mapa mental e do texto no mesmo ambiente, agilizando a escrita, algo que segundo a pesquisa realizada, não existe. o desenvolvimento da aplicação incluiu o planeamento da marca, nomeadamente a criação de logó tipos e slogans. seguindo-se a fase de design, onde foram realizados mockups das páginas. o nextjs foi selecionado como framework para o frontend e backend com o typescript. para a base de dados (bd), foi utilizado o mongodb com o docker, para um deployment local, escalável e fácil de migrar. a implementação do frontend foi estruturada em 3 camadas diferentes, como a camada de apresentação, de aplicação e a de negócios. no desenvolvimento do frontend foram utilizadas várias bibliotecas de react para auxiliar no desenvol vimento dos mapas mentais e do editor de texto, onde foram impostas regras de funcionamento, como por exemplo, a criação máxima de 5 níveis diferentes no mapa mental e a proibição de promover ou desprover nós que não cumpram com certos requisitos. o texto é estruturado ao longo da construção do mapa mental, podendo apenas gerir o conteúdo de cada nível criado. os dados são todos persistidos localmente no localstorage do browser para funcionamento offline. finalmente, para avaliação da aplicação, foi realizada uma experiência com um grupo de estudantes. estes responderam posteriormente a um questionário sobre o funcionamento da aplicação e sobre a experiência realizada. os resultados obtidos foram em média bastante positivos, tendo sido expostos alguns problemas em pequenas funcionalidades, que serão sujeitas a melhoramentos.",
      "this dissertation proposes a solution to a problem that exists in an audience that ranges from the average student to the everyday professional. anyone in this audience can face obstacles in structuring texts, such as academic or professional reports, finding it difficult to organize ideas into the desired textual format. the use of organizational methods such as mind maps plays a crucial role as a first step in writing, as they help in the planning of writing, an essential process in the construction of any type of text, facilitating the organization of ideas. the dissertation deals with the development of a web application, both in its frontend and backend aspects. the application integrates the editing of the mind map and the text in the same environment, speeding up writing, something that, according to the research carried out, does not exist. the development of the application included brand planning, namely the creation of logos and slogans. this was followed by the design phase, where page mockups were made. nextjs was selected as the framework for the frontend and the backend with typescript. for the database, mongodb was used with docker for local, scalable and easy-to-migrate deployment. a frontend implementation was structured in 3 different layers, such as the presentation layer, the application layer and the business layer in developing the frontend, various react libraries were used to help develop the mind maps and the text editor, where operating rules were imposed, such as the maximum creation of 5 different levels in the mind map and the prohibition of promoting or demoting nodes that do not meet certain requirements. the text is structured throughout the construction of the mind map, and you can only manage the content of each level created. all data is persisted locally in the browser’s localstorage for offline operation. finally, to evaluate the application, an experiment was carried out with a group of students. they then answered a questionnaire about how the application worked and the experience. the results obtained were, on average, very positive, with some problems being exposed in minor functionalities, which will be subject to improvement."
    ],
    0.06666666666666667
  ],
  [
    [
      "face ao crescimento do volume de tráfego de vídeo na internet, a monitorização eficiente de serviços de vídeo apresenta-se como um desafio de grande importância para a gestão das redes atuais e de próxima geração, onde múltiplos serviços, protocolos e tecnologias de acesso coexistem e competem por recursos. a monitorização eficiente de serviços envolvem a medição, ao precisa de parâmetros de interesse e o menor impacto possível na operação normal da rede. neste sentido as técnicas de amostragem de tráfego procuram obter informações sobre todo o tráfego considerando apenas um subconjunto dos pacotes em trânsito na rede, apresentando-se como uma solução escalável para os desafios impostos pela monitorização de serviços de vídeo. neste contexto, o presente trabalho de mestrado tem como principal objetivo analisar o desempenho da monitorização baseada em amostragem de tráfego na correta classificação e caracterização de serviços de vídeo.",
      "given the growth of video traffic volume in the internet, the effective monitoring of video services is presented as a major challenge for the management of current and nextgeneration networks, where multiple services, protocols and access technologies coexist and compete for resources. the efficient monitoring of network services involves, not only accurate measurement of parameters of interest, but also the lowest possible impact on the normal network operation. in this way, traffic sampling techniques aim to obtain information about all traffic, only considering a subset of packets traversing the network, presents itself as a scalable solution to face the challenges posed by monitoring video services. in this context, this master work has as main objective to analyze the performance of sampling-based network monitoring in the correct classification and characterization of video services."
    ],
    [
      "in malaysia, the incidence rate of dengue fever and dengue haemorrhagic fever has reached the level of epidemic, and its numbers keep growing. in the last few years, a big effort has been put into developing methods for predicting dengue outbreaks. however, the path for undertaking effectively those predictions, and therefore save human lives, is still a very long one. this dissertation work focused on the use of data mining techniques, for discovering hidden patterns on data obtained by crossing information related to patients infected with dengue in malaysia and meteorological data coming from the areas where those patients got infected.",
      "na malásia, a taxa de incidência de febre de dengue e febre hemorrágica de dengue atingiu o nível de epidemia, continuando os seus números a crescer. nos últimos anos, um grande esforço tem sido empreendido no desenvolvimento de métodos para prever surtos de dengue, mas o caminho para realizar de forma eficaz essas previsões, e, portanto, salvar vidas humanas, é ainda muito longo. este trabalho de dissertação incidiu no uso de técnicas de mineração de dados para descobrir padrões escondidos nos dados obtidos através do cruzamento de informação acerca de pacientes infectados com dengue na malásia e dados meteorológicos relativos às áreas geográficas onde os pacientes foram infectados."
    ],
    0.3
  ],
  [
    [
      "a plataforma para a computação orientada ao recurso (placor) foi desenhada como um ambiente de programação e execução de aplicações baseadas no modelo da computação orientada ao recurso (cor), especificado em cores, integralmente escrito em c++ moderno. a escolha do c++ trouxe enormes vantagens, no suporte à: i) programação orientada aos objetos, através da herança múltipla (na construção dos recursos); ii) programação genérica (permitindo abstrair na api as diferentes classes de recursos); iii) programação concorrente (para tirar partido de fios de execução e estruturas de sincronização nativas ao c++). a plataforma possui facilidades para: i) comunicação inter-domínios, ii) passagem de mensagens entre recursos comunicantes, iii) memória partilhada distribuída (dsm), iv) ativação remota de fios de execução (rpc), v) criação e gestão de recursos e vi) gestão da consistência entre todas as réplicas de um recurso. atualmente, o desenho de aplicações cor assenta nos recursos domínio, grupo, clausura, agente, proto-agente, dado, barreira, guarda e guarda para leituras/escritas. os domínios estabelecem o primeiro nível de concorrência/paralelismo, quer sejam criados no início da aplicação ou lançados dinamicamente. os agentes, pelo seu lado, estão associados ao grão fino de paralelismo e de comunicação por passagem de mensagens. o domínio, o grupo e a clausura são recursos estruturados que disponibilizam operações de adesão/saída de recursos; distingue-os o facto dos dois primeiros serem dinâmicos enquanto a clausura é estática, na medida em que as operações de adesão/saída são coletivas e o número total de membros é fixado inicialmente - características necessárias para o arranque paralelo de aplicações do tipo spmd e a passagem de mensagens intra-clausura. a guarda é usada para a criação de zonas de exclusão mútua distribuídas (leituras/escritas), a barreira para a sincronização entre agentes, enquanto o dado contempla os mecanismos de memória partilhada distribuída, usado para disponibilizar os dados do utilizador num ambiente de domínios distribuídos. a avaliação da plataforma tomou como exemplo de aplicação a leitura e processamento de eventos registados em ttree, recorrentemente usados na experiência atlas. as várias versões desenvolvidas justificaram a criação de um módulo específico, a unidade pool, que realiza o modelo fork-join. o experimento confirmou a viabilidade da orientação ao recurso como paradigma de programação híbrido que integra múltiplos fios de execução e sincronização distribuída, com facilidades de comunicação de grão fino para a passagem de mensagens e de comunicação em contextos seguros, o acesso remoto a memória e a ativação remota de agentes.",
      "the resource-oriented computing platform (placor) was designed as an application programming and execution environment based on the resource-oriented computing (cor) model, specified in cores, fully written in modern c++. the choice of c++ brought enormous advantages, in support to: i) object-oriented programming through multiple inheritance (in the construction of resources); ii) generic programming (allowing to abstract in the api the different classes of resources); iii) concurrent programming (to take advantage of run-time threads and synchronization structures native to c++). the platform has facilities for: i) inter-domain communication, ii) message passing between communicating resources, iii) distributed shared memory (dsm), iv) remote activation of execution threads (rpc), v) creation and management of resources and vi) consistency management among all replicas of a resource. currently, the design of cor applications relies on the resources domain, group, closure, agent, proto-agent, data, barrier, guard and read/write guard. the domains establish the first level of concurrency/parallelism, whether created at the beginning of the application or dynamically launched. agents, for their part, are associated with the fine grain of parallelism and communication by message passing. the domain, the group and the closure are structured resources that provide join/leave operations of resources; the first two are dynamic while the closure is static, the later meaning that the join/leave operations are collective and the total number of members is fixed initially - characteristics necessary for the parallel start of spmd-type applications and intraclosure message passing. the guard is used to create distributed mutual exclusion zones (read/write), the barrier for agent synchronization, while the data comprises distributed shared memory mechanisms, used to make the user’s data available in a distributed domain environment. the evaluation of the platform took as an application example the reading and processing of events registered in ttree, recurrently used in the atlas experience. the various versions developed justified the creation of a specific module, the pool unit, which performs the fork-join model. the experiment confirmed the feasibility of resource orientation as a hybrid programming paradigm that integrates multiple threads of execution and distributed synchronization, with fine grain communication facilities for message passing and communication in secure contexts, remote access to memory and remote activation of agents."
    ],
    [
      "nos últimos anos, tem existido um crescente interesse na avaliação dos parâmetros biométricos da pla centa e a sua relação com resultados obstétricos. evidências têm sido publicadas sugerindo que as medidas da placenta e a sua evolução são capazes de refletir alterações no desenvolvimento do feto e até mesmo doenças do recém-nascido e do adulto. tendo em conta, que os gráficos de crescimento desempenham um papel crucial na avaliação e vigilância da população pediátrica, surgiu o tema desta dissertação. o principal objetivo passa por estudar a aplicabilidade de modelos de regressão não linear em curvas de crescimento de parâmetros como o diâmetro 1 (d1) e 2 (d2), espessura (ep) e peso pla centar (pp) e peso fetal (pf). para isso, utilizou-se um conjunto de dados de parturientes portuguesas recolhidos no cgc (centro de genética clínica), porto. neste estudo foi utilizada uma abordagem de regressão semiparamétrica para a construção de cur vas de crescimento de referência. esta metodologia utiliza os modelos aditivos generalizados para lo calização, escala e forma (gamlss), lamda-mu-sigma (lms), lms com box-cox t (bct) e lms com box-cox-powe-exponential (bcpe), oferecendo vantagens distintas sobre os métodos tradicionais como a regressão quantílica. uma das principais vantagens do gamlss é a sua flexibilidade para acomodar qualquer distribuição estatística, permitindo a modelação de vários parâmetros biométricos. através da aplicação da metodologia proposta, foi demonstrado que com a utilização do método gamlss com bct e p-splines para os parâmetros d1 e d2 e do método lmst para os parâmetros ep, pp e pf, podemos alcançar curvas de crescimento representativas. além disso, foi desenvolvida uma apli cação web, disponível em https://placentalgrowth.shinyapps.io/uminho_pt/, utilizando o ambiente r. permite que profissionais de saúde e investigadores analisem e interpretem as curvas de crescimento desenvolvidas com facilidade. os resultados deste estudo fornecem informações importantes sobre o desenvolvimento da placenta e têm implicações significativas para a prática clínica em obstetrícia, permitindo o seu avanço e acompanhamento da saúde materno-fetal.",
      "in recent years, there has been a growing interest in assessing the biometric parameters of the placenta and their relationship with obstetric outcomes. evidence has been published suggesting that placental measures and their evolution can reflect changes in fetal development and even impact newborn and adult health. considering the crucial role that growth charts play in the evaluation and monitoring of the paediatric population, the topic of this dissertation emerged. the main objective was to study the applicability of nonlinear regression models in growth curves for parameters such as diameter 1 (d1) and 2 (d2), thickness (pt), placental weight (pw), and fetal weight (fw). for this purpose, a dataset of portuguese parturients collected at cgc (center for clinical genetics), porto, was used. this study employed a semi-parametric regression approach to construct reference growth curves. this methodology utilizes generalized additive models for location, scale, and shape (gamlss), lambda mu-sigma (lms), lms with box-cox t (bct), and lms with box-cox-power-exponential (bcpe), offering distinct advantages over traditional methods such as quantile regression. one of the key advantages of gamlss is its flexibility to accommodate any statistical distribution, allowing the modeling of various biometric parameters. through the application of the proposed methodology, it was demonstrated that using the gamlss method with bct and p-splines to d1 and d2 parameters and the lmst method to pt, pw, and fw parameters, representative growth curves can be achieved. furthermore, a web application, available at https://placentalgrowth.shinyapps.io/uminho_pt/, was developed using the r environ ment. it enables healthcare professionals and researchers to easily analyze and interpret the developed growth curves. the results of this study provide valuable insights into placental development and have significant implications for clinical practice in obstetrics, promoting its advancement and the monitoring of maternal-fetal health."
    ],
    0.3
  ],
  [
    [
      "we live in a digital era, in a world connected by technology. the incredible capabilities of our mobile phones and computers let us communicate and get data from all over the globe, in the instance of a millisecond. however, technological progress doesn’t stop. we persist in looking for faster connections, innovative applications and platforms, more efficient, scalable, and resilient. distributed systems are the fundamental basis driving this progress in several scientific and industry fields. epidemic protocols are crucial to ensure efficient data dissemination on these systems, providing fault tolerance, scalability, and availability. its relevance grows as networks become more dynamic and distributed, playing a main role in ensuring the reliability and efficient operation of these systems. progress is not possible without studies and experimental evaluation of proposed algorithms. although, as they are projected to systems compromising millions of nodes and processes, these studies are almost impossible at this scale, so most rely on simulation. discrete-event simulation is one of the major experi mental methodologies in several scientific and engineering domains. the used simulator is often seen as a technical detail, and many researchers develop their custom tool. simulation tools vary in complexity and application, catering to a wide range of industries and research domains. the choice of a specific tool depends on the nature of the simulation, the problem being addressed, and the preferences and expertise of the user. in this dissertation, we present, analyze, and compare a set of selected simulation tools, to choose the one that better fits epidemic protocol simulations in p2p systems. after choosing the most adequate simulation tool, we defined a generic simulation framework for epidemic protocols, and implementations of two different peer sampling services and one dissemination protocol. leveraging this framework, we perform a extensive evaluation of the different protocols.",
      "atualmente, vivemos na era digital, num mundo conectado pela tecnologia onde os nossos telemóveis e computadores pessoais possuem capacidades incríveis que nos permite, em milésimas de segundo, comunicar e obter informações vindas dos 4 cantos do mundo. no entanto, o avanço tecnológico não para, continuamos incessantemente à procura de conexões mais rápidas, aplicações e plataformas inovadoras, mais eficientes, mais escaláveis e mais resilientes. os sistemas distribuidos são a base fundamental que impulsiona todo este avanço em diversas áreas da ciência e da indústria. os protocolos epidémicos são essenciais para garantir a disseminação eficaz de informações nestes sistemas, fornecendo tolerância a falhas, escalabilidade e disponibilidade. a sua importância cresce à medida que as redes se tornam mais dinâmicas e distribuídas, desempenhando um papel crítico em garantir a confiabilidade e o funcionamento eficaz desses sistemas. o avanço não é possível sem o estudo e avaliação experimental de novos algoritmos e protocolos. porém, sendo estes projetados para sistemas distribuídos compostos por milhões de nós e processos, é quase impossível testá-los a esta escala, por isso a sua maioria depende da simulação. a simulação por eventos é uma das principais metodologias experimentais no domínio da ciência e da engenharia. temos à nossa disposição várias ferramentas de simulação que variam na sua complexidade e areas de aplicação. contudo nem sempre é fácil escolher a ferramenta mais adequada e muitos investigadores acabam por desenvolver o seu próprio simulador. nesta dissertação, apresentamos, analisamos e comparamos um conjunto de ferramentas de sim ulação selecionadas, de modo a escolher a ferramenta que melhor se adequa à simulação de proto colos epidémicos. após escolher a ferramenta mais adequada, definimos uma framework de simulação genérica, e implementação de 2 serviços de amostragem de nós e um protocolo epidémico. aproveitando esta framework, realizamos uma avaliação extensiva dos diferentes protocolos"
    ],
    [
      "com um crescimento exponencial tanto na área da inteligência artificial como dos vídeo jogos, a criação de plataformas que auxiliam os jogadores passou a ser fundamental. a criação de uma ferramenta analítica que estude detalhadamente o comportamento humano, abre portas a jogos mais dinâmicos, competitivos e justos. a análise do ecrã de um jogador permite identificar, detetar e rastrear movimentos de determinados objetos, em tempo real, podendo ter o intuito de o ajudar ou de o vigiar. seja qual for o caso, é necessário, primeiro, identificar e detetar os objetos visualizados, através de algoritmos de object detection. depois, já identificado o objeto, é possível prever a sua próxima localização, bem como rastrear o seu movimento, utilizando algoritmos de object tracking. intercalando o rastreamento com a deteção de objetos, quer quando este desaparece de vista, quer para obter confirmação que se está a seguir o objeto correto, é possível assim analisar o ecrã do jogador para o poder ajudar. esta dissertação tem como objetivo desenvolver um modelo capaz de identificar o movimento de um determinado objeto, em tempo real, no ambiente de um jogo, utilizando para isso técnicas de machine learning e computer vision, mais especificamente métodos de object detection e object tracking. o ambiente prático foi desenvolvido utilizando a biblioteca opencv para python, que tem ao dispor um diverso leque de algoritmos de computer vision e ainda permite a utilização paralela de cpu e gpu para a otimização destes mesmos algoritmos.",
      "with exponential growth both in the area of artificial intelligence and videogames, the creation of platforms that help players has become fundamental. the creation of an analytical tool that analyzes human behavior, opens the door to more dynamic, competitive and fair games. the analysis of a player’s screen allows identifying, detecting and tracking movements of certain objects, in real time, and can help or monitor them. whatever the case, it is first necessary to identify and detect the objects visualized, through object detection algorithms. then, once the object has been identified, it is possible to predict its next location, as well as track its movement, using object tracking algorithms. it is possible to analyze the player’s screen by interleaving object tracking with object detection, either when it disappears from view, or to obtain confirmation that the correct object is being followed, thus helping the player. this dissertation aims to develop a model capable of identifying the movement of a given object, in real time, in a game environment, using machine learning and computer vision techniques, more specifically methods of object detection and object tracking. the practical environment was developed using the opencv python library, which has a diverse range of computer vision algorithms available and also allows the parallel use of cpu and gpu for the optimization of these same algorithms."
    ],
    0.3
  ],
  [
    [
      "atualmente existe ainda uma relação assimétrica entre os utilizadores e os fornecedores de serviços disponibilizados pela internet. é prática comum, aquando da apresentação de um serviço, que o utilizador seja questionado sobre a aceitação, ou não, de um conjunto de políticas referentes ao uso de informação privada facultada ao fornecedor (por exemplo, a morada, o número de telefone, preferências, etc...). geralmente os utilizadores aceitam a política com base na confiança que têm no fornecedor e/ou no contrato formal que lhes é apresentado. os casos de violação de privacidade por parte de alguns fornecedores de serviços, vendendo ou facultando informação privada sobre os seus clientes a outros, são amplamente conhecidos e resultam em grande medida da falta de controlo que os utilizadores finais têm sobre a informação que entregam aos fornecedores. este problema também tem grande impacto no ambiente empresarial. quase toda a informação de uma organização é guardada em claro. mesmo que esta seja guardada num local seguro, aqueles que conhecerem bem o sistema poderão ter indevidamente acesso a informação privada da organização. além disto, se a organização for alvo de um ataque informático e o atacante conseguir aceder aos dados poderá consulta-los livremente. neste trabalho propomos a implementação de um mecanismo que possibilite o envio de informações sem que o utilizador tenha necessidade de confiar no local onde as mesmas serão armazenadas, através da utilização do conceito de “sticky policies”. através da utilização de técnicas criptográficas, é estabelecido um vínculo entre a informação cifrada e as políticas de acesso à informação. o sistema desenvolvido garante que, para um terceiro aceder às informações pessoais de um utilizador, terá que cumprir o conjunto de regras definidas pelo dono da informação. visto que um utilizador autorizado a aceder às informações pode ter um comportamento incorreto, partilhando indevidamente as informações, propomos também adicionar mecanismos de auditoria dos acessos à informação gerida pelo sistema.",
      "nowadays there is an asymmetrical relationship between users and service providers available over the internet. a common practice during service subscription is to ask users to accept a set of policies regarding use of private information (for example, address, telephone number, preferences, etc...). generally users agree to the policy based on the confidence they have in the supplier and/or the formal contract that is presented to them. cases of violation of privacy by some service providers, selling or providing private information about their customers to others, are widely known and result in large part from the lack of control that end users have over the information they deliver to suppliers. this issue also has great impact on business environment. almost all the information of an organization is stored in clear. even though it is stored in a safe place, those who know the system may have improper access to private information. in this work we propose the implementation of a mechanism for sending information without the user ever need to trust where they will be stored, using the concept of sticky policies. through the use of cryptographic techniques, a link is established between the encrypted information and their access control policies. the system ensures that when a third party tries to access the information, has to fulfill the set of rules defined by the owner of the information. since a user authorized to access the information may have an incorrect behavior, by improperly sharing information, we also propose to add auditing mechanisms to the information managed by the system."
    ],
    [
      "the growing need for process optimisation is one of the main drivers of technological modernisation in companies. process automation has been one of the main initiatives in this modernisation task. in many scenarios, the movement of any kind of vehicles or other moving objects requires human hands. in this context, this master’s thesis, included in the fifth year of the integrated master in informatics engineering in the university of minho, reports the work developed in a business context with the help of deloitte technologys s.a. that welcomed the student in a curricular internship program. the aim is to develop an implementation of of a simulator of automated guided vehicles in a controlled environment, which can reveal itself as an initiative for the modernisation of the logistic process of moving mobile objects or vehicles. in order to develop the master’s project, a work methodology was outlined. this methodology is supported on four crucial stages to reach the proposed objectives: documentary, proposal, pre-development and development. the documentary analysis was made to develop a better understanding of the context and to study other developed projects and tools that could be necessary for the proposal and development. in the solution proposal we started by dividing the problem in several components, in a kind of \"divide and conquer\" method. during this process, a conceptual map was developed, which will be analysed in the document and will guide us to the first conceptual architecture diagram. the next step is through the analysis of the conceptual architecture diagram to study which modules will be out of the scope of what is necessary for the full functioning of the master project, taking certain components as assumptions and substituting some modules by other solutions, so the development can be faster however in a way to not neglect the required functionality for the full operation of the solution. after these steps, the development of the solution will follow, putting into practice all the knowledge acquired in the previous stages. the master’s project culminated in five micro-services: vehicle, controller, map, vehicle/sessions micro-service and alert; and a web application that allows the user to check with a gui data about vehicles, sessions, alerts and the map of the controlled environment. these micro-services and the web application working simultaneously allow having a vehicle simulator automatically driven in a controlled environment.",
      "a crescente necessidade de otimização de processos é uma das principais motivações para a modernização tecnológica nas empresas. a automatização de processos tem sido uma das principais iniciativas nesta tarefa de modernização. em muitos cenários, a movimentação de qualquer tipo de veículos ou outros objetos móveis requer mão humana. neste contexto esta dissertação de mestrado, incluída no quinto ano do mestrado integrado em engenharia informática na universidade do minho, relata o trabalho desenvolvido em contexto empresarial com a ajuda da deloitte technologys s.a que acolheu o aluno num programa de estágio curricular. é procurado desenvolver uma implementação de de um simulador de veículos automaticamente conduzidos em ambiente controlados, que se pode revelar uma iniciativa para a modernização do processo logístico de movimentação de objetos móveis ou veículos. de forma a desenvolver o projeto de mestrado foi delineada uma metodologia de trabalho. esta metodologia está apoiada em quatro etapas crucias para atingir os objetivos propostos: documentário, proposta e pré-desenvolvimento e desenvolvimento. a análise documental foi feita para desenvolver uma melhor compreensão do contexto e estudar outros projetos desenvolvidos, e ferramentas que poderão ser necessárias para a proposta e desenvolvimento. na proposta de solução começou-se por repartir o problema em várias componentes, numa espécie de estratagema de \"divide and conquer\". no decorrer deste processo foi desenvolvido um mapa conceptual que será analisado no documento e guiar-nos-á ao primeiro diagrama conceptual de arquitetura. a etapa seguinte passa por através da análise do diagrama conceptual de arquitetura estudar quais módulos estarão fora do âmbito daquilo que é necessário para o pleno funcionamento do projeto de mestrado, tomando certos componentes como pressupostas e substituindo alguns módulos por outras soluções, assim o desenvolvimento poderá ser mais célere não descorando de funcionalidades necessárias para o pleno funcionamento da solução. após estas etapas, seguir-se-á o desenvolvimento da solução, pondo em prática todo o conhecimento adquirido nas etapas anteriores. o projeto de mestrado culminou em cinco micro-serviços: veículo, controlador, mapa, veículo/sessões micro-serviço e alerta; e uma aplicação web que permite o utilizador verificar com uma gui dados sobre veículos, sessões, alertas e o mapa do ambiente controlado. estes micro-serviços e a aplicação web em simultâneo permitem ter um simulador de veículos automaticamente conduzidos em ambiente controlado."
    ],
    0.0
  ],
  [
    [
      "os filmes finos multiferróicos magnetoelétricos, que apresentam um acoplamento entre a ordem magnética e elétrica, têm atraído recentemente bastante interesse científico e tecnológico. a possibilidade de controlar a magnetização (polarização) por meio de um campo elétrico (magnético) permite estudar as interações electro-magnéticas à nanoescala, criar nanoestruturas multifuncionais e alargar a gama de potenciais aplicações. nesse âmbito, combinando um material piezoelétrico com um material magnetostritivo, as interações mecânicas entre as fases induzem um acoplamento magnetoelétrico. esse acoplamento entre a fase piezoelétrica e a fase magnetostritiva dá origem a diversas aplicações em microeletrónica, por exemplo, memórias de múltiplos estágios ou sensores. neste contexto, foram produzidas compósitos magnetoelétricos constituídas por 0.85[0.6ba(zr0.2ti0.8)o3 − 0.4(ba0.7ca0.3)tio3] − 0.15srtio3 (0.85bczt-0.15sto), um bom piezo/ferroelétrico, livre de chumbo, e cofe2o4 (cfo), um material ferromagnético com alta magnetostrição. a técnica utilizada para a deposição das heteroestruturas em bicamada foi a ablação laser (pld). neste trabalho, foi a estudada a influência da espessura do cfo em dispositivos do tipo pt(111)/bczt-sto/cfo/au, e posteriormente, a influência do substrato no dispositivo nb:sto(001)/bczt-sto/cfo/au, comparando com os resultados obtidos para as estruturas anteriores. após a sua produção, estes dispositivos foram caraterizados a nível estrutural por difração de raios-x com incidência rasante (gixrd), a nível morfológica por microscopia eletrónica de varrimento (sem) e a nível da sua composição química por espetroscopia de dispersão de energia (eds). posteriormente foi realizada a caracterização ferroelétrica e dielétrica. os resultados obtidos por sem e gixrd mostram que as camadas de bczt-sto e cfo nos dispositivos de pt/bczt-sto/cfo/au são policristalinas e apresentam crescimento colunar. já para a estrutura nb:sto/bczt-sto/cfo/au, verificou-se o crescimento epitaxial dos filmes de bczt-sto e cfo que constituem a bicamada. as medidas de gixrd confirmam a formação da fase tetragonal do bczt-sto e a estrutura espinela cúbica do cfo. os ciclos p-e, obtidos para todos os dispositivos, permitem concluir que compósito apresenta comportamento ferroelétrico. as propriedades dielétricas dos filmes foram estudadas pela medição da capacidade (c) e das perdas dielétricas (tan 𝛿) em função da frequência e temperatura. verifica-se que bczt-sto apresenta um comportamento relaxor. através do modelo de havriliak-negami e incluindo a contribuição da condutividade, modelizou-se e ajustou-se a constante dielétrica imaginária, para obter os tempos de relaxação e energias de ativação, de maneira a saber os principais mecanismos de condução presentes nestas estruturas.",
      "multiferroic magnetoelectric films, presenting a coupling between their electric and magnetic degrees of freedom, have attracted significant scientific and technological interest. the possibility to control the magnetization (polarization) with an electric (magnetic) field allows the study of magnetic electric effects at the nanoscale, the creation of multifunctional devices and the widening of potencial applications. in this respect, by combining a piezoelectric with a magnetostrictive material, the mechanical interactions between the phases induce a magnetoelectric coupling. this coupling between the piezoelectric and the magnetostrictive phases then gives rise to various applications in microelectronics, such as multistage memories or sensors. in this context, bilayer magnetoelectric composites consisting of 0.85[0.6ba(zr0.2ti0.8 )o3 − 0.4(ba0.7ca0.3 )tio3 ] − 0.15srtio3 (0.85bczt-0.15sto), a lead-free, good piezoelectric, and cofe2o4 (cfo), a ferromagnetic material with high magnetostriction, were produced. the technique used for the bilayer deposition was pulsed laser ablation (pld). in this work, the influence of the cfo thickness in pt(111)/bczt-sto/cfo/au-type devices was studied, and subsequently, the influence of the substrate on the nb:sto(001)/bczt-sto/cfo/au device was investigated, comparing the results obtained with the former structures. for these devices, their structural characterization was performed by grazing incidence x-ray diffraction (gixrd), morphological characterization by scanning electron microscopy (sem), chemical characterization by energy-dispersive spectroscopy (eds) and ferroelectric, and dielectric properties characterization. the results obtained by sem and gixrd showed that the pt/bczt-sto/cfo/au devices are polycrystalline and exhibit columnar growth of the magnetoelectric bilayer. for the nb:sto/bczt sto/cfo/au structure, epitaxial growth of the bilayer was observed. gixrd measurements confirmed the formation of the tetragonal phase of bczt-sto and the cubic spinel structure of cfo. the p-e cycles obtained for all devices showed the ferroelectric behavior of the composite. the dielectric properties of the films were studied by measuring the capacitance (c) and the dielectric losses (tan δ) as a function of frequency and temperature, revealing the relaxor behavior of bczt-sto. by using the havriliak-negami model and including the contribution of conductivity, the imaginary dielectric constant was modeled and adjusted to obtain the relaxation times and activation energies, in order to understand the main conduction mechanisms, present in these structures."
    ],
    [
      "dna barcodes are short sequences of pre-defined gene regions that contain a sufficient amount of intra- and inter-species genetic information. high-throughput sequencing techniques are currently used to identify large sequences of dna barcodes in a species genome, in a relatively short time. domain experts require adequate self-contained tools to accurately and efficiently process dna barcode data in a reasonable time, taking advantage of current parallel and heterogeneous computing systems. they also expect to use these tools on different computing platforms, from laptops to high-performance servers, without requiring a broad knowledge in software engineering to develop efficient computational applications. the main goal of this project was to develop a framework and associated user-friendly tools for domain experts to efficiently support dna barcoding studies, providing an abstraction of the performance issues. 4specid is the key outcome of this work: an application software that integrates a semi-automated auditing and annotation tool for reference libraries, to ensure the quality standards of the compiled data, aiming to enable a grounded decision when identifying species from dna barcodes. its graphics interface aids the end user to specify the operations and it also simplifies data filtering and remote file handling. the c++ ported version (from matlab) was fully tested and is more robust than the original version. architecture features common to laptop and compute servers were exploited, namely parallel programming techniques and memory models. the presented validation and performance results show significant improvements on execution times, not only on the sequential version, but also by using the available parallel capabilities of the underlying computing platforms.",
      "os códigos de barras de adn são pequenas sequência de regiões genéticas predefinidas que contêm uma quantidade suficiente de informação genética intra e interespécies. técnicas de sequenciamento de alto desempenho são usadas na identificação de grandes sequências de códigos de barras de adn no genoma de uma espécie. no entanto, é necessário que sejam desenvolvidas ferramentas adequadas para que os especialistas de domínio processem dados de código de barras de adn de forma precisa e num intervalo de tempo viável, utilizando os sistemas de computação paralelos e heterogêneos que existem. destas ferramentas é esperado que possam ser utilizadas recorrendo a diferentes plataformas de computação, de laptops a servidores de alto desempenho, sem exigir um amplo conhecimento em engenharia de software para serem utilizadas ou usadas para a criação de outras ferramentas. o objetivo principal deste projeto é desenvolver uma estrutura que forneça uma abstração dos possíveis desafios de desempenho e permitir que especialistas no domínio tenham uma forma computacional eficiente para realizar um estudo de código de barras de dna. neste projecto desenvolveu-se uma ferramenta, 4specid, que visa permitir uma decisão fundamentada na identificação de espécies através de códigos de barras de dna: uma auditoria semi-automática e ferramenta de anotação para bibliotecas de referência, para garantir os padrões de qualidade dos dados compilados. este projeto também explorou as vantagens das arquiteturas de servidores de computação e laptops mais comuns, como técnicas de programação paralela e modelos de memória. os resultados de validação e desempenho apresentados mostram que é possível obter melhores tempos de execução utilizando as características disponíveis das plataformas subjacentes."
    ],
    0.0
  ],
  [
    [
      "file systems are widely used for storing digital information, as they offer abstractions that allow data to be intuitively separated and organized through files and directories, according to the requirements of applications and users. the continuous growth of data volume and complexity leads to the constant evolution of these systems. however, the complexity of integration of new features and lack of continuous support, leads to many file systems not being adopted in practice. in this sense, stackable file systems have emerged, which allow the development of complex file systems, providing existing systems with new functionalities through independent processing layers. despite this, the development of these systems presents some challenges, namely in terms of speed of implementation, portability, and resilience, since they are developed in kernel. in this way, later solutions emerged that allowed the development of file systems in user space, thus mitigating some of the problems identified in the development of this type of file systems. however, these solutions have not been properly explored in the development of remote file systems. therefore, this dissertation presents rsafefs, a platform that extends the safefs system to allow developing modular, flexible and extensible remote file systems in user space. the proposed solution enables extensible remote file system implementations that adjust to the requirements of different types of applications and storage workloads. it was then necessary to develop a layer that would allow an rsafefs instance to operate as a system server, and a communication layer, based on remote procedure calls (rpcs), to allow interoperability between client and server instances. to demonstrate the ease of integration of new features, taking advantage of the modularity and flexibility of rsafefs, the developed prototype was equipped with two layers of caching, namely data and metadata, which aim to improve system peformance. the results obtained with this prototype reveal that the file systems developed through rsafefs obtain performances comparable to remote storage solutions based on fuse. furthermore, with the processing layers developed it is possible to adjust the system to different types of workloads, allowing, for example, to improve system performance by 1.5× in certain workloads.",
      "os sistemas de ficheiros são atualmente uma das soluções mais utilizadas para o armazenamento de informação digital, pois oferecem abstrações que permitem separar e organizar de forma intuitiva os dados através ficheiros e diretorias, segundo os requisitos das aplicações e dos utilizadores. o contínuo crescimento do volume e complexidade de dados leva a constante evolução destas soluções. contudo, a complexidade de integração de novas funcionalidades e falta de suporte contínuo, leva a que muitos dos sistemas de ficheiros desenvolvidos não sejam adotados. neste sentido, surgiram os sistemas de ficheiros empilháveis, que permitem desenvolver sistemas de ficheiros complexos, dotando sistemas já existentes com novas funcionalidades através de camadas de processamento independentes. apesar disto, o desenvolvimento destes sistemas apresenta alguns desafios, nomeadamente a nível da rapidez de implementação, portabilidade, e resiliência, uma vez que são desenvolvidos ao nível do kernel. desta forma, mais tarde, surgiram soluções que permitiram desenvolver sistemas de ficheiros em espaço de utilizador, mitigando assim alguns dos problemas identificados no desenvolvimento deste tipo sistemas de ficheiros. contudo, estas soluções não têm sido devidamente exploradas no desenvolvimento de sistemas de ficheiros remotos. esta dissertação apresenta o rsafefs, uma plataforma que estende o sistema safefs para permitir desenvolver sistemas de ficheiros remotos modulares, flexíveis e extensíveis em espaço de utilizador. foi então necessário desenvolver uma camada que permitisse a uma instância rsafefs operar como um servidor do sistema, e meios de comunicação, baseados em protocolos remotos (rpcs), para permitir a interoperabilidade entre instâncias cliente e servidor. desta forma, a solução proposta permite desenvolver soluções de armazenamento remotas extensíveis e adaptáveis a requisitos de diferentes tipos de aplicações e cargas de trabalho. para demonstrar a facilidade de integração de novas funcionalidades, tirando partido da modularidade e flexibilidade do rsafefs, o protótipo desenvolvido foi dotado com duas camadas de caching, nomeadamente de dados e metadados, que procuram melhorar o desempenho do sistema. os resultados obtidos com este protótipo, revelam que os sistemas de ficheiros desenvolvidos através do rsafefs obtém desempenhos comparáveis com os de soluções de armazenamento remotos baseadas em fuse. ainda, com as camadas de processamento desenvolvidas é possível ajustar o sistema a diferentes tipos de cargas de trabalho, permitindo, por exemplo, melhorar o desempenho do sistema em 1.5× em determinadas cargas de trabalho."
    ],
    [
      "os índices são uma excelente ferramenta de comunicação, dada a sua simplicidade, a sua facilidade em compactar a informação e em simplificar medidas complexas. o facto dos índices compostos permitirem a divisão em várias dimensões e associar-lhes ponderações torna a sua compreensão mais simples. o aumento da sua utilização vem justificar a sua versatilidade, visto que, são utilizadas nas mais variadas áreas, desde a saúde até à educação. no enorme espetro da aplicação dos índices enquadram-se os de bem-estar, que são usualmente utilizados para prever o impacto de programas e medidas construídas, tendo em conta as suas necessidades de implementação. com o passar do tempo esta gama de índices foi sendo aplicada em vários outros domínios como, por exemplo, a progressão social ou a economia. nesta dissertação foi desenvolvido um sistema de índices baseado na utilização de um tutor inteligente, cujo principal objetivo é ajudar alunos e professores fornecendo um sistema personalizado de formação e de avaliação de conhecimento multidisciplinar. os alunos podem recorrer a este tutor para realizar várias atividades relacionadas com a aferição do conhecimento que possuem num dado domínio de estudo. alguma da informação mais relevante que deriva dessa utilização é armazenada num data warehouse, que a manterá de acordo com as dimensões de análise do índice. posteriormente, os dados destas dimensões são trabalhados de acordo com três métodos distintos: um sistema de equalização regular (equalizador), no qual o utilizador define os pesos de aferição como desejar, o analytical hierarchy process, em que o utilizador estabelece comparações entre todas as dimensões que originam os seus pesos e, por fim, através da classificação individual, na qual o utilizador classifica cada dimensão consoante uma escala e a sua opinião pessoal.",
      "indexes are an excellent communication tool, given their simplicity, their ease of compressing information and simplifying complex measurements. the fact that composite indexes allow the division into several dimensions and associate weights to them makes their understanding easier. the increase in their use justifies their versatility, as they are used in the most varied areas, from health to education. in the enormous spectrum of application of the indexes, wellbeing is included, which are usually used to predict the impact of programs and measures built taking into account the needs presented by those targeted by the index. over time, this range of indexes was applied to various other domains, such as social progression or economics. in this dissertation, an index system based on the use of an intelligent tutor was developed, whose main objective is to help students and teachers by providing a personalized system for training and evaluating multidisciplinary knowledge. students use the smart tutor to perform various activities, including answering quizzes. some of the most relevant information deriving from this use is stored in a data warehouse. all these data will result in the constituent dimensions of the index that allow revealing knowledge and attendance. the dimensions are then weighted through three different methods: the equalizer where the user manipulates the weights as desired, the analytical hierarchy process where the user makes comparisons between all the dimensions that give rise to their weights and, finally, through the individual classification in that the user ranks each dimension according to a scale and his personal opinion."
    ],
    0.3
  ],
  [
    [
      "the user interface (ui) provides the first impression of an interactive system and should, thus, be intuitive, in order to guide users effectively and efficiently in performing their tasks. user interface prototyping is a common activity in ui development, as it supports early exploration of the ui design by potential users. ui quality plays a crucial role in safety-critical contexts, where design errors can poten tially lead to catastrophic events. model-based analysis approaches aim to detect usability and performance issues early in the design process by leveraging formal analysis. they complement prototyping, which supports user involvement, but not an exhaustive analysis of the designs. the ivy workbench emerges as a model-based analysis tool intended for non-expert usage. the tool was originally focused on supporting modelling and verification, but more recently an effort began to combine the formal model capabilities with ui mock-ups, to produce more interactive prototypes than traditional mock-up editors support. this work addresses the enhancement of the prototyping features of the ivy workbench. the improvements of such features include the creation of a dynamic widget library that can vastly improve the quality of prototypes. such a library, however, should be compatible with several mock-up editors to attract a broader design community. the results of this work include an analysis of alternative prototyping tools, identifying potential features that can enhance the ivy workbench, the creation of a dynamic widget library that is compatible with several mock-up editors, and several improvements to ivy’s prototyping plugin, including the addition of code exporting functionalities. usability tests were conducted to validate the new features of the tool, with positive results. two mobile applications were also created, allowing users to test prototypes in their mobile devices.",
      "a ui proporciona o primeiro contacto entre um utilizador e um sistema interativo. assim, a ui deverá ser capaz de guiar o utilizador na execução das suas tarefas, de um modo eficiente e eficaz. a prototipagem de interfaces é uma atividade comum no processo de desenvolvimento de uis, já que permite a exploração antecipada do design de uma ui com potenciais utilizadores. a ui tem um papel bastante relevante no contexto de sistemas críticos, onde falhas no design podem gerar eventos catastróficos. as metodologias de análise baseadas em modelos procuram detetar potenciais falhas de usabilidade e desempenho, em fases iniciais do processo de desenvolvimento, através de análise formal. estas metodologias complementam o processo de prototipagem, que suporta o envolvimento dos utilizadores mas não oferece uma análise exaustiva do design. a ivy workbench surge como uma ferramenta de análise baseada em modelos que visa suportar utilizadores sem grandes conhecimentos de análise formal. embora originalmente focada na modelação e verificação, surgiu recentemente um esforço para combinar as capacidades da análise formal com mock-ups da ui. o objetivo é produzir protótipos com maior nível de interação do que os produzidos pelos tradicionais editores de mock-ups. o presente trabalho apresenta melhorias das capacidades de prototipagem da ferramenta ivy workbench. estas melhorias incluem a criação de uma biblioteca de widgets dinâmicos, que aperfeiçoa a qualidade dos protótipos desta ferramenta. esta biblioteca deverá ser compatível com múltiplos editores de mock-ups, de modo a atrair uma vasta comunidade de designers. os resultados deste trabalho incluem uma análise de alternativas de ferramentas de prototipagem, onde são identificadas funcionalidades que podem aprimorar a ferramenta ivy workbench; a criação de uma biblioteca de widgets dinâmicos, compatível com inúmeros editores de mock-ups; assim como várias melhorias efetuadas no plugin de prototipagem desta ferramenta, incluindo a adição de funcionalidades de exportação de código fonte. foram realizados testes de usabilidade para validar as novas funcionalidades da ferramenta com utilizadores, onde foram obtidos resultados positivos. finalmente, foram criadas duas aplicações móveis que permitem que os utilizadores testem os protótipos nos seus dispositivos móveis."
    ],
    [
      "as redes móveis ad hoc são compostas por nós móveis, que têm a capacidade de, autonomamente, criar uma rede de comunicações entre eles, sem assistência de um ponto de acesso, contrariamente às redes de infraestrutura. a comunicação é sem fios e ocorre diretamente entre os nós vizinhos, localizados no mesmo raio de alcance. no entanto, neste tipo de redes, a topologia da rede é muito dinâmica, devido à mobilidade dos nós, à entrada e saída de nós na rede e às quebras de ligações constantes, que provocam alterações inesperadas nas rotas. além disso, o meio de comunicação é sem fios e partilhado entre os nós vizinhos. o paradigma dos dados nomeados (ndn - named data networks) pode contribuir para minimizar alguns dos problemas das redes ad hoc. as ndns constituem uma alternativa às redes ip, apontada como uma das mais promissoras arquiteturas de rede da internet do futuro e baseiam-se no paradigma receiver driven. os utilizadores (consumidores de informação) apenas têm que saber qual o conteúdo (nome) pelo qual têm interesse, não sendo necessário conhecer a sua localização (endereço). para obter a informação que deseja, o consumidor propaga um pacote de interesse através da rede. quando este atinge um nó produtor ou um nó intermédio, que possua o conteúdo em cache, os dados são devolvidos pelo caminho inverso. nesta dissertação propôs-se uma nova estratégia de reenvio de interesses e dados, designada por mprstrategy (multipoint relay), numa rede de dados nomeados ad hoc. o objectivo é diminuir o envio redundante de pacotes e melhorar o desempenho da rede. os pacotes de interesses são enviados apenas por um subconjunto de nós vizinhos, escolhidos de modo a minimizar as retransmissões. além disso, o reenvio é atrasado de modo a poder reduzir as transmissões redundantes. os pacotes de dados seguem o percurso inverso, se possível. senão são reenviados da mesma forma. a estratégia foi implementada e avaliada no simulador ndnsim, mostrando melhorias de desempenho, quando comparada com outras alternativas.",
      "the mobile networks ad hoc are composed by mobile nodes, which have the ability to, autonomously, create a communications network between them, without assistance from an access point, in contrast to the infrastructure networks. the communication is wireless and occurs directly between the neighbor nodes, located in the same range. however, in this type of network, the topology of the network is very dynamic due to the mobility of the nodes, the input and output nodes in the network and to the breaks of connections constant, which cause unexpected changes of path. in addition, the means of wireless communications is shared among the nodes neighbor. the paradigm of the named data (ndn - named data networks) can help to minimize some of the problems of ad hoc networks. the ndns are an alternative to ip networks, and are viewed as one of the most promising network architectures for the future internet. this new paradigm is based on the receiver driven. users (consumers of information) just have to know what is the content of (name) by which you have an interest, not being necessary to know its location (address). to get the information that it want, the consumer propagates a interest packet through the network. when this reaches a producer or an intermediate node, which have the content cached, the data is returned by the reverse path. this dissertation proposed a new interests and data packet forwarding strategy, called mprstrategy (multipoint relay), for named-data ad hoc networks. the aim is to reduce packet transmission redundancy and improve network performance. the interests packets are sent only by a subset of neighboring nodes chosen to minimize retransmissions. furthermore, forwarding is delayed in order to reduce redundant transmissions. data packets follow the reverse route if possible. otherwise, they are forwarded in the same way. the strategy was implemented and evaluated in ndnsim simulator, showing improved performance compared to other alternatives."
    ],
    0.3
  ],
  [
    [
      "on a business context, it is responsibility of the software product support team analyze and solve, if necessary, problems that may arise on software products. sometimes, the reported problems are not a real defect, i.e., sometimes the client does not have a full understanding about all features of the software product. the team must evaluate and analyze all the problem reports that arrive every day. as products are spread across different customers, it is normal to have problem reports that are very similar to others that have already been solved for other clients and/or by another member of the support team. this dissertation proposes the development of a system that is able to analyze a problem report and then provide past problems that are similar to the one being analyzed. an artificial intelligence technique, named case-based reasoning, will be used to achieve such goals. existent case-based reasoning systems are neither complete nor adaptable to specific domains since the effort to adapt either the reasoning process or the knowledge representation mechanism, to a new domain, is too high. to address such drawbacks, a generic reasoning component will be designed and developed. this dissertation introduces a new approach to the typical case-based reasoning cycle where is possible to handle default, unknown and incomplete data.",
      "num contexto empresarial, é da responsabilidade da equipa de suporte ao produto de software analisar e corrigir, se necessário, problemas que possam surgir em produtos de software. muitas vezes esses mesmos problemas não o são, isto é, muitas vezes é o cliente que não tem uma percepção completa do funcionamento do produto. a equipa deve avaliar e analisar todos os problem reports que vão chegando dia após dia. como os produtos se encontram espalhados por diferentes clientes, é normal aparecerem problemas que são muito semelhantes com outros que já foram resolvidos para outros clientes e/ou por um outro membro da equipa de suporte. esta dissertação propõe o desenvolvimento de um sistema que seja capaz de analisar um problema e indicar problemas passados que sejam semelhantes com o que se está analisar. uma técnica da inteligência artificial, de seu nome raciocínio baseado em casos, será utilizada para atingir tais objectivos. os sistemas já existentes que usam esta técnica não são completos nem adaptáveis a domínios específicos, uma vez que o esforço para adaptar tanto o processo de raciocínio como a representação do conhecimento, para um novo domínio, é demasiado elevado. para solucionar tais problemas, um componente de raciocínio genérico será especificado e desenvolvido. esta dissertação introduz uma nova abordagem ao ciclo do raciocínio-baseado em casos onde é possível tratar informação desconhecida."
    ],
    [
      "the reconstruction of genomic-scale metabolic model (gismo)s is an increasingly growing methodology, which allows to develop models that can be used to perform in silico predictions on the phenotypical response of an organism to environmental changes and genetic modifications. these predictions allow focusing in vivo experiments on methodologies that will, theoretically, present better results, thus reducing the high costs on time and money spent in laboratorial experiments. gismos are a mathematical representation of the organism’s genome, in the form of metabolic networks. as complex as these can be, because of the large number of compounds involved in many different reactions and pathways, the treatment of all such data is not easily manually performed. several bioinformatics software were developed with the aims of improving this procedure, by automating many operations in the reconstruction process. metabolic models reconstruction using genome-scale information (merlin) is one of such tools, following a philosophy that thrives on providing an intuitive and powerful graphical environment, to annotate data on key metabolic components and building a complete genome-scale model. while already encompassing a wide range of tools, it is still a work in development. upon analyzing its functioning, several improvement opportunities were identified, mainly in existing operations. moreover, missing important features for the reconstruction of gismos were as well identified. this work details the results of this analysis and the improvements performed to enrich merlin’s toolbox.",
      "a reconstrução de modelos metabólicos à escala genómica (gismo) é uma metodologia em rápido crescimento, que permite o desenvolvimento de modelos para fazer previsões in silico sobre a resposta fenotípica de um organismo a alterações ambientais e a modificações genéticas. estas previsões permitem a focagem em experiências in vivo e em metodologias que, em teoria, apresentarão melhores resultados, e portanto reduzir os elevados custos em tempo e dinheiro gastos em experiências laboratoriais. gismos são uma representação matemática do genoma de um organismo, em forma de redes metabólicas. devido à complexidade que estas podem ter, devido ao elevado número de compostos envolvidos em muitas reações em diferentes redes, o tratamento de todos estes dados não é simples de ser feito manualmente. vários softwares bioinformáticos foram desenvolvidos com o objectivo de melhorar o procedimento, automatizando várias operações do processo de reconstrução. merlin é uma dessa ferramentas, e segue uma filosofia que prospera em providenciar um ambiente gráfico intuitivo e eficaz, para anotar informação de componentes metabólicos chave e construir a partir dela um modelo à escala genómica completo. apesar de já conter uma grande variedade de ferramentas, ainda está em fase de desenvolvimento. ao ser analisado o seu funcionamento, foram apontadas várias operações passíveis de ser melhoradas. também foram indentificadas em falta funcionalidades importantes par a resconstrução de gismo. este trabalho detalha os resultados desta análise e o que foi feito para enriquecer a caixa de ferramentas do merlin."
    ],
    0.0
  ],
  [
    [
      "a cegid primavera tem vindo a desenvolver a sua cloud de microsserviços para facilitar a integração dos seus vários produtos e dos seus produtos com sistemas externos, com um foco particular no reaproveitamento de funcionalidades e de componentes semelhantes em produtos diferentes. uma funcionalidade comum a muitos desses produtos é o reporting, que consiste na transformação de dados em informações úteis para o utilizador, sendo que neste caso o foco está centrado na impressão de documentos. esta dissertação tem como objetivo isolar toda a lógica de reporting num único conjunto de microsserviços, para que os todos os produtos primavera tenham acesso ao mesmo. o sucesso na construção deste microsserviço parte da escolha da melhor ferramenta de reporting para o contexto em questão e, principalmente, no desenho da melhor arquitetura de microsserviços.",
      "cegid primavera has been developing a microservices cloud to facilitate the integration between its internal products and external systems, with a particular emphasis on reusing similar features and components across different products. a common feature among many of these products is the reporting function, which involves transfor ming data into useful information for users, primarily through document generation. the main goal of this dissertation is to consolidate all the reporting logic within a single set of micro services, enabling all cegid primavera products to access it. the success of building this microservice depends on selecting the most suitable reporting tool for the given context and designing the optimal microservices architecture."
    ],
    [
      "quando falamos em utilizadores em risco num cenário rodoviário, vem-nos à cabeça os peões, ciclistas e possivelmente os motociclistas. este tipo de utilizadores, particularmente se se tratarem de crianças ou idosos, são especialmente vulneráveis em cenários de pe-rigo quer por erro humano ou por agentes externos. o trabalho realizado no contexto desta dissertação tem o intuito de diminuir o número de fatalidades ocorridas em situações de perigo nas nossas estradas, e tem como objetivo providenciar um método auxiliar de indicação destas mesmas situações de perigo. o tema das comunicações entre veículos tem vindo a ser abordado há alguns anos, mas ainda há muito trabalho que pode ser desenvolvido nesta vertente. aplicações capazes de comunicar diretamente com veículos circundantes podem aumentar a segurança rodoviária como um todo, não só dos peões como também dos condutores. a expansão da internet aos veículos vem possibilitar esta abordagem, sendo que é esperado que futuramente todos os veículos produzidos estejam apetrechados com algum tipo de mecanismo de comunicação. a ideia aqui patente é a de alargar os alertas existentes em veículos mais atuais para os dispositivos móveis dos peões, ou em sentido contrário, comunicar a presença, posição ou até direção de um peão ao veículo. a principal dificuldade nesta abordagem continua a ser tecnológica, isto dado o facto de não existir uma tendência para tecnologia comum de ampla utilização. atualmente os dis-positivos móveis utilizam tecnologias wifi/ 4g e os veículos sobretudo dsrc (brevemente lte-v). esta dificuldade inicial poderá, contudo, ser ultrapassada com recurso a dispositi-vos multi-tecnologia, instalados por exemplo na berma da via pública, sendo estes capazes de agregar e redistribuir informação. neste trabalho apresenta-se uma proposta para um sistema de alertas de segurança ro-doviária, de baixo custo, para condutores e utilizadores vulneráveis, baseado no uso de dispositivos móveis. a arquitetura do sistema é descrita, apresentado alternativas e jus-tificando as decisões tomadas. o sistema proposto foi implementado na totalidade num protótipo que foi posteriormente testado em cenários realistas com ajuda de um simula-dor construído para o efeito. os resultados obtidos permitem constatar a sua viabilidade prática e o funcionamento de acordo com o esperado.",
      "when we think about users at risk in a road scenario, the first thought that occurs reminds us of pedestrians, cyclists and possibly motorcyclists. these types of road users, particularly if they are children or the elderly, are especially vulnerable in dangerous scenarios whether caused by human error or by external agents. the work carried out in the context of this dissertation aims to reduce the number of fatalities occurring in dangerous situations on our roads, by providing an auxiliary method of indicating these same dangerous situations. the subject of communications between vehicles has been addressed in the last years, but there is still much work that can be developed in this area. applications capable of commu- nicating directly with surrounding vehicles can increase road safety as a whole, not only for pedestrians but also for drivers themselves. the expansion of the internet to vehicles makes this approach possible, and it is expected that in the future all vehicles produced will be equipped with some type of communication mechanism. the idea here is to extend the existing warnings on the most current vehicles to pedestrian’ mobile devices or, the opposite direction, to communicate the presence, position or even velocity and direction of a pedestrian to the vehicle. the main difficulty with this approach remains technological, given the lack of a trend towards common technology with a broad use by its users. currently the mobile devices use wifi / 4g technologies and the vehicles mainly dsrc (although lte-v might be arri- ving soon). however, this initial difficulty can be overcome by means of multi-technology devices installed, for example on the side of public roads, having the capability to aggregate and redistribute information. this dissertation presents a proposal for a low-cost road safety alert system for drivers and vulnerable users, based on the use of mobile devices. the system architecture is des- cribed, presented alternatives and justifying the decisions taken. the proposed system was fully implemented in a prototype that was later tested in realistic scenarios with the help of a simulator built for the purpose. the results obtained allow to verify its practical viability and the operation as expected."
    ],
    0.3
  ],
  [
    [
      "covid-19, a devastating virus that has been more and more controversial, fickle and problematic worldwide. this pandemic brought multiple changes and restrictions to the way we live, rather like a historical buoyed period before and after christ, translating, nowadays, into before and after covid-19. this pandemic forced the worldwide population to be subjected to some lockdown periods, and portugal was not an exception, in which the population could only leave their home for exceptional and essential situations. from those restrictions, a “new” way of work that has gained more and more popularity was born - the remote work or commonly known as work from home. thus, it becomes pressing to investigate which are the impacts of this profound change to remote work, in multiple domains (personal, professional,...). in short, the primordial objective of this dissertation is to study the impact of the referred change to remote work, due to the covid-19 pandemic, on software professionals in portugal. throughout this dissertation, all the objectives and study questions, along with the relevant state of the art on the theme are exposed. the construction and propagation of the chosen method (survey), the results arising from the collection and treatment of data originated from this survey, along with their analysis, conclusions, limitations and advantages are also highlighted. in total, 176 valid answers were collected from software professionals from portugal (mainly from braga). after the performed statistical analysis on the targeted population and focusing on the 10 elaborated research questions, one can conclude with certainty two major findings: (i) having worked in a remote regimen before the pandemic period has a strong relation with a higher frequency use of teleconference tools (microsoft teams, skype, zoom,...) after this period, and (ii) participants who do not feel safe about coming back to a fully on-site regimen are more likely to prefer a fully remote regimen than the ones who feel safe and the latter group is more likely to prefer a hybrid regimen. additionally, although not statistically significant and therefore without certainty, one can also imply that, p.e, (i) having dependants and someone’s support in their care could possibly negatively affect participants’ work; (ii) having dependants could possibly show a relation to a preference for a mainly on-site hybrid regimen, and (iii) company employee dimension could show a relation to participants’ feel of support to maintain productivity. in conclusion, this project has a strong pioneer investigation profile in portugal, with an applicability in the software engineering area.",
      "covid-19, um vírus devastador que tem sido cada vez mais polémico, inconstante e problematizado a nível mundial. esta pandemia trouxe inúmeras mudanças e restrições à forma como vivemos, assemelhando-se, de certa forma, a um período histórico balizado antes e depois de cristo, traduzindo-se, na atualidade, por antes e depois da covid-19. esta pandemia obrigou a que, por todo o mundo, a população fosse sujeita a períodos de confinamento, não sendo portugal uma exceção, em que a população apenas podia sair de casa para situações excecionais e essenciais. dessas restrições nasceu uma “nova” forma de trabalho que tem ganho cada vez mais popularidade - o trabalho remoto ou, comummente designado, teletrabalho. assim, torna-se premente investigar quais os impactos desta profunda mudança para um contexto de teletrabalho, em múltiplos domínios (pessoal, profissional,...). posto isto, esta dissertação tem como objetivo primordial estudar o impacto da referida mudança para teletrabalho, resultante da pandemia de covid-19, em profissionais de software em portugal. ao longo desta dissertação, são expostos todos os objetivos e questões de estudo, juntamente com o estado de arte relevante na temática. é evidenciada, igualmente, a construção e divulgação do método escolhido (inquérito), os resultados inerentes à recolha e tratamento dos dados obtidos no inquérito, bem como a sua análise, conclusões, limitações e vantagens. no total, foram recolhidas 176 respostas válidas de profissionais de software de portugal (maioritariamente, de braga). após a análise estatística efetuada na população alvo e focando-se nas 10 questões de estudo elaboradas, é possível concluir com certeza dois principais achados: (i) ter trabalhado em regime remoto antes do período pandémico tem uma relação forte com uma maior frequência de uso de ferramentas de teleconferência (microsoft teams, skype, zoom,...) após esse período, e (ii) os inquiridos que não se sentem seguros em regressar a um regime totalmente presencial têm maior probabilidade de preferir um regime totalmente remoto do que aqueles que se sentem seguros e o último grupo tem maior probabilidade de preferir um regime híbrido. adicionalmente, embora não estatisticamente significativo e, por consequência, sem certeza, pode-se sugerir que, por exemplo, (i) ter dependentes e apoio de alguém no seu cuidado pode afetar negativamente o trabalho dos inquiridos; (ii) ter dependentes pode evidenciar uma relação com uma preferência por um regime híbrido maioritariamente presencial, e (iii) a dimensão de trabalhadores da empresa pode evidenciar uma relação com a sensação de apoio dos inquiridos para manterem a produtividade. em suma, este projeto tem um forte perfil de investigação pioneira em portugal, com aplicação na área da engenharia de software."
    ],
    [
      "this document formally reports a m.sc. thesis project needed to obtain the master’s degree in informatics engineering, focusing on the scientific areas of digital humanities, social networks and inappropriate social discourse. the master’s work here presented was accomplished at universidade do minho in braga. the main objective of the referred master’s project was the development of an online editor that allows researchers to add their reflections and ideas to short sentences (usually called ’comments’) that belong to a social dialogue triggered by a ’post’ on a social network or a ’news’ on social media. those comments, to be analyzed by linguists or social science experts, are provided online and are extracted from the corpus created under the international project – netlang. netlanged, the editor developed and here reported, is mainly a tool to allow the analysts to create their own notes to be associated in the right place of each comment while reading it. basically, netlanged allows to highlight a multi-word term contained in the comment, using a color chosen by the user, and associate to that term an ’annotation’. an annotation is composed of two parts, a tag (also created and picked up at the user choice) and a text explaining the user idea. to make this ’annotation’ process truly dynamic, netlanged provides, through a simple and user-friendly set of menus, three basic operations for adding, editing and removing annotations. additionally, the editor also provides easy to use mechanisms to manage the tags so far created, as well as to view and locate the annotations. this master’s dissertation also describes how netlanged was tested for usefulness and usability. for that purpose, an experiment was designed and conducted with end-users. the results will be presented and discussed.",
      "este documento reporta formalmente um projeto de mestrado necessário para obter o grau de mestre em engenharia informática, focando-se nas áreas científicas das humanidades digitais, redes sociais e discurso social impróprio. o trabalho de mestrado aqui apresentado foi realizado na universidade do minho em braga. o objetivo principal do referido projeto de mestrado era o desenvolvimento de um editor online que permitisse aos investigadores adicionar as suas reflexões e ideias a frases curtas (normalmente denominadas por ’comentários’) que pertencem a um diálogo social desencadeado por uma ’publicação’ numa rede social ou uma ’notícia’ nas redes sociais. estes comentários, a serem analisados por linguistas ou especialistas em ciências sociais, são disponibilizados online e extraídos do corpus criado no âmbito do projeto internacional – netlang. netlanged, o editor desenvolvido e aqui relatado, é principalmente uma ferramenta para permitir que os analistas criem as suas próprias notas para serem associadas no lugar certo de cada comentário durante a sua leitura. basicamente, o netlanged permite destacar um termo com várias palavras contido no comentário, usando uma cor escolhida pelo usuário, e associar a esse termo uma ’anotação’. uma anotação é composta por duas partes, uma tag (também criada e escolhida de acordo com a preferência do utilizador) e um texto explicando a ideia do utilizador. para tornar este processo de ’anotação’ verdadeiramente dinâmico, o netlanged fornece, por meio de um conjunto de menus simples e de fácil utilização, três operações básicas para adicionar, editar e remover anotações. adicionalmente, o editor também oferece mecanismos fáceis de usar para gerir as tags criadas até o momento, bem como visualizar e localizar as anotações. esta dissertação de mestrado também descreve como o netlanged foi testado quanto à sua utilidade e usabilidade. para esse propósito, um experimento foi desenhado e conduzido com utilizadores finais. os resultados serão apresentados e discutidos."
    ],
    0.3
  ],
  [
    [
      "nos dias que correm existe um crescimento tecnológico que se traduz não só no número de dispositivos, mas também nas suas capacidades de processamento. cada vez mais é possível encontrar pedestres e veículos equipados com dispositivos capazes de comunicar sem fios. e graças a esses desenvolvimentos, é necessário criar aplicações que permitam otimizar a utilização destes dispositivos. de forma a ser possível prever o comportamento destas redes móveis, é necessário simular a utilização destes dispositivos em ambientes urbanos. no entanto, os simuladores existentes continuam a ser muito genéricos e incapazes de simular em grande escala ou simular protocolos específicos de encaminhamento. foi com o objetivo de responder a estes problemas que foi criado este simulador. esta dissertação descreve o desenvolvimento de um simulador de comunicações em ambientes urbanos. este simulador permite simular as movimentações de vários tipos de atores e permite simular a interação entre atores através de vários protocolos de encaminhamento. estes protocolos terão que realizar a comunicação da forma mais realista possível. foram então implementados vários protocolos de encaminhamento, de maneira a que seja possível existir essa interação. para ter a certeza que os protocolos implementados foram implementados da forma mais realista, estes protocolos foram testados a nível de performance e de resultados, e os resultados obtidos foram comparados com resultados obtidos por um dos simuladores mais usados para a simulação deste tipo de redes. assim, com esta comparação, é possível concluir que os objetivos propostos para este projeto foram atingidos.",
      "nowadays, the is a growing in the technological field that translates not only in the number of devices, but also in their processing abilities. it is more and more likely to find pedestrians and vehicles equipped with devices that are capable of wireless communication. and thanks to these developments, it is necessary to create applications that allow to optimize the use of these devices. in order to be possible to predict the behavior of this mobile networks, it is necessary to simulate the use of these devices in urban environments. however, existing simulators remain very generic and unable to simulate large scale scenarios or simulate specific routing protocols. it was with that purpose that this simulator was created. this thesis describes the development of a communication simulator in urban environments. this simulator allows to simulate the movements of several types of actors and allows to simulate the interaction between actors using several routing protocols. these protocols will have to perform the communication in the most realistic way. were then implemented several routing protocols in order to make possible that interaction. to be sure that the implemented protocols were implemented in the most realistic way, the protocols were tested in terms of performance and results, and the obtained results were compared with the results obtained by one of the most used simulators for this type of networks. therefore, with this comparison, it is possible to conclude that the objectives proposed for this project were attained."
    ],
    [
      "the high growth in the use of digital identities creates the need to develop mechanisms that can protect the personal data of each individual. the way identity is treated today prevents each of us from being able to control our personal information. this is due to the centralized architecture in which the personal data are inserted, that is, all these data are kept together and controlled by the entities responsible for providing the most varied services, which is wrong since the identity belongs to the person and thus it must be responsible for controlling that identity. centralized identity management brings within itself several problems, whether intentional (that is, data correlation for profiling) or unintentional (that is, data breach). to face this problem, multiple entities across the world are developing decentralized identity managment systems based on a self-sovereign identity architecture where each individual is responsible for managing and storing a set of credentials, each with parts of their personal information. a self-sovereign identity architecture allows users to provide only small parts of their personal information or even to omit any type of personal identification, using cryptographic techniques like selective disclosure and zero-knowledge proofs, which allows them to have more control over their privacy. taking into account the current problems of digital identity, this dissertation aims to explore the state of the art and develop a proof of concept, through the implementation of a system based on self-sovereign identity, which is able to cover the use cases for digital identity. thus, this document shows the architecture implemented, with a blockchain, responsible for the storage of all public data, and a user agent, responsible for facilitating all interactions of the various users with the developed system. the proof of concept developed allows not only to validate that it is possible to correct many of the problems associated with centralized identity management, but also to explore new cryptographic strategies in order to improve the way each of us manages our own identity.",
      "o elevado crescimento no uso de identidades digitais gera a necessidade de desenvolver mecanismos que sejam capazes de proteger os dados pessoais de cada indivíduo. o modo como a identidade é tratada atualmente impede cada um de nós seja capaz de controlar as suas informações pessoais. isto deve-se à arquitetura centralizada na qual os dados pessoais estão inseridos, ou seja, todos estes dados são mantidos em conjunto e controlados pelas entidades encarregues de fornecer os mais variados serviços, o que está errado por definição uma vez que a identidade pertence à pessoa e esta deverá ser responsável por controlá-la. a gestão centralizada de identidades traz consigo diversos problemas, sejam estes intencionais (i.e. correlação de dados para criação de perfis) ou não intencionais (i.e. data breaches). para enfrentar este problema, várias entidades em todo o mundo estão a desenvolver sistemas de gestão de identidade descentralizados com base numa arquitetura de self sovereign identity na qual cada indivíduo é responsável por gerir e armazenar um conjunto de credenciais que contêm partes da sua identidade digital. uma arquitetura de self-sovereign identity permite que os utilizadores partilhem apenas pequenas partes de suas informações pessoais ou até mesmo que omitam qualquer tipo de identificação pessoal, usando técnicas criptográficas como divulgação seletiva e provas de conhecimento zero, o que lhes permite ter mais controlo sobre sua privacidade. tendo em consideração os problemas atuais da identidade digital, esta dissertação tem como objetivo explorar o estado da arte e desenvolver uma prova de conceito, através da implementação de um sistema baseado em self-sovereign identity, que seja capaz de cobrir os vários casos de uso da identidade digital. deste modo, este documento apresenta a arquitetura implementada, com uma blockchain, responsável por armazenar todos os dados de caráter público, e um agente de utilizador, responsável por facilitar todas as interações dos vários utilizadores com o sistema desenvolvido. a prova de conceito desenvolvida permite não só validar que é possível corrigir muitos dos problemas associados à gestão centralizada de identidades mas também explorar novas estratégias criptográficas com o objetivo de melhorar o modo como cada um de nós gere a sua própria identidade."
    ],
    0.3
  ],
  [
    [
      "in the last few years, biological and biomedical research has been generating a large amount of quantitative data, given the surge of high-throughput techniques that are able to quantify different types of molecules in the cell. while transcriptomics and proteomics, which measure gene expression and amounts of proteins respectively, are the most mature, metabolomics, the quantification of small compounds, has been emerging in the last years as an advantageous alternative in many applications. as it happens with other omics data, metabolomics brings important challenges regarding the capability of extracting relevant knowledge from typically large amounts of data. to respond to these challenges, an integrated computational platform for metabolomics data analysis and knowledge extraction was created to facilitate the use of several methods of visualization, data analysis and data mining. in the first stage of the project, a state of the art analysis was conducted to assess the existing methods and computational tools in the field and what was missing or was difficult to use for a common user without computational expertise. this step helped to figure out which strategies to adopt and the main functionalities which were important to develop in the software. as a supporting framework, r was chosen given the easiness of creating and documenting data analysis scripts and the possibility of developing new packages adding new functions, while taking advantage of the numerous resources created by the vibrant r community. so, the next step was to develop an r package with an integrated set of functions that would allow to conduct a metabolomics data analysis pipeline, with reduced effort, allowing to explore the data, apply different data analysis methods and visualize their results, in this way supporting the extraction of relevant knowledge from metabolomics data. regarding data analysis, the package includes functions for data loading from different formats and pre-processing, as well as different methods for univariate and multivariate data analysis, including t-tests, analysis of variance, correlations, principal component analysis and clustering. also, it includes a large set of methods for machine learning with distinct models for classification and regression, as well as feature selection methods. the package supports the analysis of metabolomics data from infrared, ultra violet visible and nuclear magnetic resonance spectroscopies. the package has been validated on real examples, considering three case studies, including the analysis of data from natural products including bees propolis and cassava, as well as metabolomics data from cancer patients. each of these data were analyzed using the developed package with different pipelines of analysis and html reports that include both analysis scripts and their results, were generated using the documentation features provided by the package.",
      "nos últimos anos, a investigação biológica e biomédica tem gerado um grande número de dados quantitativos, devido ao aparecimento de técnicas de alta capacidade que permitem quantificar diferentes tipos de moléculas na célula. enquanto a transcriptómica e a proteómica, que medem a expressão genética e quantidade de proteínas respectivamente, estão mais desenvolvidas, a metabolómica, que tem por definição a quantificação de pequenos compostos, tem emergido nestes últimos anos como uma alternativa vantajosa em muitas aplicações. como acontece com outros dados ómicos, a metabolómica traz importantes desafios em relação à capacidade de extracção de conhecimento relevante de uma grande quantidade de dados tipicamente. para responder a esses desafios, uma plataforma computacional integrada para a análise de dados de metabolómica e extracção de informação foi criada para facilitar o uso de diversos métodos de visualização, análise de dados e mineração de dados. na primeira fase do projecto, foi efectuado um levantamento do estado da arte para avaliar os métodos e ferramentas computacionais existentes na área e o que estava em falta ou difícil de usar para um utilizador comum sem conhecimentos de informática. esta fase ajudou a esclarecer que estratégias adoptar e as principais funcionalidades que fossem importantes para desenvolver no software. como uma plataforma de apoio, o r foi escolhido pela sua facilidade de criação e documentar scripts de análise de dados e a possibilidade de novos pacotes adicionarem novas funcionalidades, enquanto se tira vantagem dos inúmeros recursos criados pela vibrante comunidade do r. assim, o próximo passo foi o desenvolvimento do pacote do r com um conjunto integrado de funções que permitem conduzir um pipeline de análise de dados, com reduzido esforço, permitindo explorar os dados, aplicar diferentes métodos de análise de dados e visualizar os seus resultados, desta maneira suportando a extracção de conhecimento relevante de dados de metabolómica. em relação à análise de dados, o pacote inclui funções para o carregamento dos dados de diversos formatos e para pré-processamento, assim como diferentes métodos para a análise univariada e multivariada dos dados, incluindo t-tests, análise de variância, correlações, análise de componentes principais e agrupamentos. também inclui um grande conjunto de métodos para aprendizagem automática com modelos distintos para classificação ou regressão, assim como métodos de selecção de atributos. este pacote suporta a análise de dados de metabolómica de espectroscopia de infravermelhos, ultra violeta visível e ressonância nuclear magnética. o pacote foi validado com exemplos reais, considerando três casos de estudo, incluindo a análise dos dados de produtos naturais como a própolis e a mandioca, assim como dados de metabolómica de pacientes com cancro. cada um desses dados foi analisado usando o pacote desenvolvido com diferentes pipelines de análise e relatórios html que incluem ambos scripts de análise e os seus resultados, foram gerados usando as funcionalidades documentadas fornecidas pelo pacote."
    ],
    [
      "na atualidade, cada vez mais informação pessoal é recolhida, armazenada, analizada e partilhada. tal é possível e encorajado devido a avanços na área tecnológica, nos métodos de armazenamento de dados e nas capacidades analíticas, incluindo machine learning. apesar deste novo paradigma de recolha de dados oferecer grandes oportunidades de negócio, a quantidade crescente de informações, que podem ser bastante sensíveis, levanta graves questões de privacidade e segurança. existe uma necessidade cada vez maior de adaptar os mecanismos existentes de exploração de dados, com garantias de segurança, a cenários reais. este trabalho explorará alguns desses mecanismos, tomamdo em consideração requisitos funcionais e de desempenho. para compreender melhor o contexto legal e os mecanismos de exploração de dados com garantias de segurança disponíveis, também conhecidos como privacy enhancing technologies (pets), realizou-se uma pesquisa preliminar. de seguida, vários cenários de exploração de dados adequados para a empresa altice/meo foram definidos e possíveis soluções apresentadas. entre esses cenários, um foi escolhido para implementação, devido aos desafios tecnológicos associados e à sua relevância a nível do negócio. o cenário escolhido enquadra-se na área de e-health, dizendo respeito à cifragem dos dados geridos por uma plataforma de vida assistida. estes dados só podem ser decifrados por utilizadores autorizados, o que não inclui o servidor da plataforma. assim sendo, este servidor deve ser capaz de realizar as operações necessárias sobre os dados cifrados. para solucionar este problema, um esquema foi proposto, um protótipo desenvolvido e uma avaliação comparativa de usabilidade efetuada. a avaliação de usabilidade revelou que o custo da aplicação do esquema proposto, em termos de desempenho, ocupação do espaço e gestão de chaves, é aceitável. devido à natureza muito sensível das informações envolvidas, a melhoria das garantias de privacidade e segurança, com manutenção da funcionalidade, supera o referido custo. apesar de ainda existir a necessidade de desenvolver novas pets e melhorar a eficácia das exis tentes, os resultados obtidos permitem concluir que atualmente existem implementações que podem ser aplicadas a cenários reais.",
      "personal information about individuals is currently being collected, stored, analyzed, and shared in increasingly greater quantities. this is possible and encouraged due to advances in technology, data storage methods, and analytical capabilities, including machine learning. although this new data collection paradigm offers great business opportunities, the growing amount of information, which can be quite sensitive, raises serious privacy and security issues. there is an increasing need to adapt existing data exploration mechanisms with security guarantees to real scenarios. this work will explore some of those mechanisms, taking into account their functionalities and performance. to better understand the legal context and the available data exploration mechanisms with security guarantees, also known as privacy enhancing technologies (pets), preliminary research was carried out. then, several data exploration scenarios suitable for the company altice/meo were defined, and possible solutions were presented. between these scenarios, one was chosen for implementation due to the related technological challenges and its relevance at the business level. the chosen scenario fits in the area of e-health, concerning the encryption of data managed by an assisted living platform. this data can only be decrypted by authorized users, which does not include the platform’s server. therefore, this server should be able to perform the needed operations over the en crypted data. to solve this problem, a scheme was proposed, a prototype implemented, and a comparative usability evaluation carried out. the usability evaluation revealed that the cost of applying the proposed scheme, in terms of per formance, space occupation, and key management, is acceptable. given the very sensitive nature of the handled information, the increase in privacy and security, while maintaining the functionality, far outweighs the mentioned cost. even if there is still the need to develop new pets and improve the existing ones’ effectiveness, the obtained results allow concluding that currently there are implementations that can be applied to real scenarios."
    ],
    0.0
  ],
  [
    [
      "over the past few years, the world has been witnessing a huge demographic change: the aging population has been growing at an alarming rate. this problem has been a matter of concern for many countries since it has been posing several challenges to healthcare systems worldwide. in portugal, which is one of the countries with the largest aging population, this demographic change has led to several issues. in fact, in portugal, the nursing homes have been getting a higher demand, and health professionals are overloaded with work. furthermore, the fact that nursing homes still use paper to record information and to clinically manage their residents is another tremendous problem since this method is more prone to errors and time-consuming. in this context, the present master’s dissertation emerged and consisted in the design and development of a mobile application for the health professionals, i.e. the nurses and doctors, working in a portuguese nursing home, more specifically in one of the nursing homes of santa casa da miseric´ordia de vila verde. this mobile application was developed to help the health professionals to clinically manage the residents and to assist them at the point-of-care, namely to schedule, perform, and record their daily tasks and to have access and manipulate information. additionally, the present dissertation also included the definition of clinical and performance indicators to assist the decision-making process. it is important to mention that a mobile solution was chosen since a hand-held device, which can be used anywhere and any time, is able to give access and store all the needed information at the point-of-care. thereby, this project was developed in order for the nursing home to shift from the paper-based to the computer-based management of data as well as to introduce technological improvements in the facility, more specifically, health information and communication technology. thus, by taking advantage of the benefits provided by these improvements, the mobile application could help the health professionals to provide better care, namely by reducing time-waste and errors, and, consequently, enhance elders’ quality of life. furthermore, the solution could relieve some of the workload of the health professionals and help them make more informed and evidence-based decisions and, hence, improve the decision-making process.",
      "ao longo dos últimos anos, o mundo tem vindo a testemunhar uma enorme mudança demográfica: o envelhecimento da população tem crescido a uma taxa alarmante. este problema tem sido um motivo de preocupação para muitos países, uma vez que tem vindo a provocar inúmeros problemas para os sistemas de saúde através o mundo. em portugal, que corresponde a um dos países com maior população envelhecida, esta mudança demográfica tem originado vários problemas. de facto, em portugal, os lares de idosos têm estado sobrelotados e os profissionais de saúde estão sobrecarregados com trabalho. além disso, o facto dos lares de idosos ainda usarem papel para registar informação e efetuar a gestão dos utentes é um enorme problema, visto que este método é mais demorado e propenso a erros. neste contexto, a presente dissertação de mestrado surgiu e consistiu na projeção e no desenvolvimento de uma aplicação móvel para os profissionais de saúde, isto é, para os enfermeiros e os médicos, que trabalham num lar de idosos português, nomeadamente num dos lares da santa casa da misericórdia de vila verde. esta aplicação móvel foi desenvolvida de modo a ajudar os profissionais de saúde a efetuar a gestão dos utentes e auxiliá-los no local de prestação de cuidados, nomeadamente para agendar, realizar e registar as suas tarefas diárias e para ter acesso e manipular informação. adicionalmente, a presente dissertação também incluiu a definição de indicadores clínicos e de desempenho, de forma a auxiliar o processo de tomada de decisão. importa mencionar que uma aplicação móvel foi escolhida visto que um dispositivo portátil, que pode ser usado a qualquer momento e em qualquer lugar, é capaz de dar acesso e armazenar toda a informação necessária no local de prestação de cuidados. assim sendo, este projeto foi desenvolvido de modo a permitir o lar de idosos a passar da gestão de dados em papel para a gestão de dados em computador, assim como para introduzir avanços tecnológicos no estabelecimento, mais especificamente, tecnologia de informação e comunicação na saúde. assim, tomando vantagem dos benefícios proporcionados por estes avanços, a aplicação móvel poderia ajudar os profissionais de saúde a fornecer melhores cuidados de saúde, nomeadamente a reduzir a ocorrência de erros e desperdícios de tempo, e, consequentemente, melhorar a qualidade de vida dos idosos. para além disso, a solução poderia ajudar a reduzir a carga de trabalho dos profissionais de saúde e ajudá-los a tomar decisões mais informadas e baseadas em evidências e, assim, melhorar o processo de tomada de decisão."
    ],
    [
      "in recent years, artificial intelligence has prospered to such an extent that it is present in our daily lives through the different technologies we use. this fact is evident, for example, in fraud detection in online purchases or games, which has been increasingly precise and has undergone an exponential evolution as never seen before. however, the use of decision-making oriented by artificial intelligence and machine learning generates transparency concerns. moreover, the need for explainability arises when machine learning models are manipulated in order to ensure fairness or to deliberate on decisions of greater importance. this paper proposes an approach to the general problem of explaining the decisions made by machine learning models through the use of explainable ai methods. the main goal is to develop a decision support solution applied to fraud detection based on human behaviour by exploring techniques that can help explain the results of machine learning models and also make them more transparent.",
      "nos últimos anos, a inteligência artifical prosperou de tal modo que se encontra presente no nosso dia a dia através das diferentes tecnologias que utilizamos. tal facto evidencia-se, por exemplo, na deteção de fraudes em compras ou em jogos online, que tem sido cada vez mais precisa e sofrido uma evolução exponencial como nunca antes visto. no entanto, a utilização de decisões orientadas por inteligência artificial e machine learning gera preocupações quanto à transparência. para além disso, a necessidade de explicabilidade surge quando os modelos de machine learning são manipulados com o intuito de garantir justiça ou deliberar sobre decisões com maior teor de importância. o presente documento propõe uma abordagem ao problema geral de explicar as decisões tomadas por modelos de machine learning, através da utilização de métodos de explainable ai. desta forma, o principal objetivo é o desenvolvimento de uma solução de suporte a decisões aplicada à deteção de fraudes baseada em comportamento humano, através da exploração de técnicas que possam ajudar a explicar e tornar mais transparentes os resultados dos modelos de machine learning."
    ],
    0.12857142857142856
  ],
  [
    [
      "com a crescente necessidade de armazenar dados sobre forma digital as ontologias tornam-se cada vez mais relevantes como maneira simples de expressar conhecimento. assim bases de dados capazes de guardar este tipo de estruturas de dados de modo eficaz, nomeadamente bases de dados orientadas a grafos, têm visto a sua utilização aumentar. nesta dissertação foram estudados dois motores de base de dados deste tipo: o graphdb (ontotext (2020a)) e o neo4j (neo4j (2020)). graphdb foi criado para armazenamento de ontologias web: owl (group (2012)), skos (group (2009)) e rdf (group (2014)), podendo estas ser interrogadas através de sparql (w3c (2013a)), enquanto neo4j foi desenhado para armazenar informação fortemente relacionada: grafos de informação dos quais as ontologias fazem parte, sendo cypher (neo4j (2020b)) a linguagem de query utilizada para a sua exploração. nesta dissertação estudou-se a viabilidade de armazenar ontologias em neo4j e explorá-las utilizando cypher. ao longo do processo da resolução deste problema foi determinado um segundo objectivo: criar uma camada tecnológica que permite o uso de sparql para interrogar o neo4j. nesse sentido foi realizado um estudo comparativo das duas linguagens, implementou-se um compilador capaz de traduzir um subconjunto de queries sparql em cypher e foi desenvolvida uma bateria de testes que permitem fazer o benchmark ing da tecnologia criada. finalmente foi construído um protótipo web que implementa uma frontend sobre o neo4j de modo a permitir não só armazenar ontologias como interrogá-las através de sparql.",
      "with the growing need of storing data in the web, ontologies are becoming more and more relevant as a simple way of expressing knowledge. as such, databases capable of storing such structures effectively, namely graph databases, have seen their use grown. in this dissertation we studied two graph database engines: graphdb (ontotext (2020a)) and neo4j (neo4j (2020)). graphdb was created to store web based ontologies: owl (group (2012)), skos (group (2009)) and rdf (group (2014)), allowing for their querying through sparql (w3c (2013a)), while neo4j was designed to store strongly linked data: information graphs of which ontologies are an example, with cypher (neo4j (2020b)) as the query language for their exploration. in this dissertation we studied the viability to store ontologies using neo4j and explore them through cypher. exploring this second problem led to a second objective: the creation of a technological layer that allows the use of sparql queries in a neo4j database. to do this a study was done to compare both languages, a compiler to translate a subset of sparql queries into cypher was implemented and a series of tests were made to allow the benchmarking of the developed tech. finally a web app prototype that implements a frontend over neo4j and allows the storage of ontologies and their interrogation by sparql was created."
    ],
    [
      "as organizações dispõem atualmente de standards e de guias de boas práticas de gestão de projetos bem estruturados e com ampla adoção. tal é o caso, por exemplo, da iso 21500 e do pmbok 5. não obstante, no contexto dos projetos de tecnologias e sistemas de informação realizados com recurso a metodologias ágeis, surgem novos desafios relacionados com a compatibilização das práticas ágeis com os paradigmas estruturados da gestão de projetos. a presente dissertação está focada em procurar respostas para a questão de investigação “como compatibilizar paradigmas estruturados da gestão de projetos com metodologias ágeis”, através de uma abordagem metodológica baseada na design science research (dsr). como resultado do trabalho realizado, apresenta-se um novo método de gestão de projetos desenvolvido com base no pmbok e no scrum. este método faz a conciliação das metodologias de gestão de projetos estruturadas com metodologias ágeis.",
      "today organizations and companies maintain well-structured and widely adopted project management standards and practices. such is the case of iso 21500 and pmbok 5. nevertheless, when using agile methodologies within the context of projects of information systems and technologies, new challenges arise regarding the conciliation of the agile practices with structured paradigms of project management. this thesis is focused on finding answers to the research question “how to combine structured paradigms of project management with agile methodologies”, through the usage of design science research (dsr) methodological approach. as a result of the work carried out, a new project management method based on pmbok and scrum is presented. this method combines the usage of structured project management methodologies with agile methodologies."
    ],
    0.3
  ],
  [
    [
      "improving customer experience is crucial in any industry, especially in telecommunications, where competition is a constant factor. today, all telecommunications companies rely on the massive amount of data generated daily to get to know the customer, study their behavior, and create new effective strategies for their business. the information collected may include network information, representing the status of hardware and software components in the network, and the client’s details and calls to support and customer assistance data, which describes problems, consumer complaints, and all other problem-solving interactions. by combining these mountains of raw data with data mining and machine learning techniques, companies can find solid patterns or systematic relationships that represent valuable information, that is, generate knowledge. within the most varied user experiences, the process of installing new services can be an event that raises doubts about their operation, degrade the user experience, or, in extreme cases, lead to maintenance interventions. therefore, the use of advanced predictive models that can predict such occurrences become vital. with this, the company can anticipate the cases that will be problematic and reduce the number of negative experiences. the main objective of this work is to create a predictive model that, through all the available data history, can predict which customers will contact the customer service with problems derived from the installation process and have a following maintenance intervention. after analyzing an imbalanced dataset with approximately 560k entries from a portuguese telecommunications company, and resorting to the crisp-dm methodology for modeling, the best results were found with lightgbm, which obtained an auprc of 0.11 and auroc of 0.62. the best tradeoff between precision and recall was found with a threshold model of 0.43 in order to maximize recall while still avoiding a large number of false negatives. the strategies explored in this work and the challenges found may help the company understand which details should improve in its service provision, and which data still need to be investigated in the future.",
      "melhorar a experiência do cliente é crucial em qualquer setor, principalmente nas telecomuni cações onde a concorrência é um fator constante. atualmente, todas as empresas de telecomu nicações recorrem à quantidade de dados colossal gerada diariamente para conhecer o cliente, estudar o seu comportamento e assim criar novas estratégias eficazes para o seu negócio. as informações recolhidas podem incluir relatórios de rede, que representam o estado dos componentes de hardware e software na rede, e detalhes sobre o cliente e contactos ao suporte e apoio ao cliente, que descrevem problemas, reclamações de consumidores e todas as demais interações para resolução de problemas. juntando estas montanhas de dados brutos a técnicas de data mining e machine learning, as empresas são capazes de encontrar padrões sólidos ou relacionamentos sistemáticos que representem informações valiosas, ou seja, gerar conhecimento e inteligência. dentro das mais variadas experiências do utilizador, o processo de instalação de novos serviços pode ser um evento que origina dúvidas sobre a sua operação, degrada a experiência de utilização ou, em casos extremos, leva a intervenções de manutenção. para tal, o uso de modelos avançados de previsão que consigam prever tais ocorrências torna-se vital. com isto, é possível à empresa antecipar os casos que serão problemáticos e reduzir o número de experiências negativas. esta dissertação tem como principal objetivo criar um modelo de previsão que, através de todo o histórico de dados disponível, consiga prever quais os clientes que irão contactar o serviço de apoio ao cliente com problemas decorrentes do processo de instalação e ter uma intervenção de manutenção de seguida. após analisar um conjunto de dados não balanceado com aproximadamente 560k entradas de uma empresa portuguesa de telecomunicações, e recorrendo à metodologia crisp-dm para modelação, os melhores resultados foram encontrados com o lightgbm, que obteve um auprc de 0.11 e auroc de 0.62. o melhor trade-offentre a precisão e a recallfoi encontrado com um thresholdde modelo de 0.43, de forma a maximizar a recall, evitando um grande número de falsos negativos. as estratégias exploradas neste trabalho e os desafios encontrados podem ajudar a empresa a entender que detalhes do seu provisionamento de serviços devem melhorar e quais os dados que ainda precisam de ser investigados no futuro."
    ],
    [
      "o tema desta dissertação de mestrado desenrolou-se no âmbito das humanidades digitais, pretendendo desenhar e construir um toolkit de apoio ao registo e processamento de histórias de família, micro-história, documentos, fotografias, elementos genealógicos e histórias de instituição, com especial interesse na inferência de indivíduos. foi desenvolvido um protótipo de aplicação web de uso pessoal, sobre flask/python, configurável (e reutilizável, salvo ajustes), como prova de conceito a suportar uma ontologia owl2 sobre o domínio do problema. anb = ancestors note book = ontologia ancestorsnb, para memorabilia e genealogia, relações humanas e ambiente + filosofia wiki + inferência interativa e dinâmica + sparql + geração de formulários o domínio do problema induziu naturalmente uma arquitetura de ontologia, à qual foi dada persistência num ficheiro de texto em sintaxe turtle, carregada pela aplicação num grafo rdflib. a ontologia é expansível, mas já inclui à partida relacionamentos de genealogia de pessoas, com diversos graus de parentesco, e ainda relacionamento no âmbito social, institucional e geográfico, para pessoas, organizações e lugares. ainda no âmbito da memorabilia, prevê-se o registo e referência de eventos, fotografias, documentos, multimédia, histórias de família, artigos sobre qualquer assunto. na aplicação, acolhe-se uma filosofia wiki, no sentido em que proporciona uma apresentação gráfica (suportada em mardkdown e html), navegação por hiperligações (internas ou externas), e edição. em virtualmente qualquer classe ontológica, a apresentação prettyprint de indivíduos e os formulários de ingestão permitem templating e alguma configuração. a apresentação prettyprint prevê documentos de variada morfologia, incluindo texto, pdf, markdown e multimédia. permite-se ao utilizador a ingestão de informação em lote (na sintaxe turtle, por edição direta ou via ficheiro), e crud interativo (via formulários ou sparql), num ambiente operacional rico, com pesquisas de utilizador combinadas com navegação, além de facilitar a inferência sobre indivíduos, sejam pessoas ou não. a inferência é interativa (i.e., a pedido interativo do utilizador) e/ou dinâmica (por etiquetagem no conteúdo de um campo configurado como elegível para este efeito), jogando com classes, id de indivíduo, nomes e datas. a inferência pode ter posicionamento genealógico: basta que seja encontrado um grau de parentesco. tanto a inferência como os motores de pesquisa interativa se baseiam num tratamento rico de nomes e moradas em português, em que a normalização prevê grafias antigas, além de acentos e partículas de ligação (de, da, do, etc.).",
      "the theme of this master's dissertation is born in the field of digital humanities, intending to design and build a toolkit to support the recording and processing of family histories, micro-history, documents, photographs, genealogical elements and institution histories, with special focus on the inference of individuals. it was developed a web application prototype of personal use, over flask/python, configurable (and reusable, except on adjustments), as a proof of concept to support an owl2 ontology over the problem domain. anb = ancestors note book = ontology for memorabilia and genealogy, human relations and environment + wiki philosophy + interactive and dynamic inference + sparql + form generation the problem domain naturally induced an ontology architecture, which was given persistence in a turtle syntax text file, loaded by the application in an rdflib graph. the ontology is expandable, but it already includes genealogical relationships of people, with different degrees of kinship, and also relationships in the social, institutional and geographic scope, for people, organizations and places. also within the scope of memorabilia, it is foreseen the registration and reference of events, photographs, documents, multimedia, family stories, articles on any subject. the application embraces a wiki philosophy, in the sense that it provides a graphical presentation (supported in mardkdown and html), navigation via hyperlinks (internal or external), and editing. in virtually any ontological class, the prettyprint presentation of individuals and the ingest forms allow for templating and some configuration. the prettyprint presentation provides documents of varying morphology, including text, pdf, markdown and multimedia. it allows the user to ingest information in batch (in turtle syntax, by direct editing or via file), and interactive crud (via forms or sparql), in a rich operational environment, with user searches combined with navigation, in addition to facilitating the inference about individuals, whether persons or not. the inference is interactive (i.e., at the user's interactive request) and/or dynamic (by tagging the content of a field configured as eligible for this purpose), playing with classes, individual id, names and dates. the inference can have a genealogical position: it is enough to find a degree of kinship. both inference and interactive search engines are based on a rich treatment of names and addresses in portuguese, in which the normalization foresees old spellings, in addition to accents and linking particles (de, da, do, etc.)."
    ],
    0.06666666666666667
  ],
  [
    [
      "many animal species are on the verge of extinction due to poaching and changes in land use, such as the destruction of forest areas to the detriment of agriculture, logging, and the construction of new towns and cities. therefore, regular monitoring of animal populations is necessary to ensure the protection of wildlife, especially when pressure on animals is high, and conservation management requires reliable and up-to-date data on land use, the size of animal populations, and the distribution of resources in highly variable ecosystems. in this context, we would like to contribute to the process of automating conventional methods, taking advantage of the internet of things (iot) paradigm, which has evolved rapidly in recent years, and emerging digital technologies such as low-power wide-area networks (lpwan) and the rapid growth of unmanned aerial vehicles (uavs), also known as drones. therefore, this study proposes a wildlife monitoring system based on acoustic detection, using lora networks assisted by uavs that act as mobile gateways to collect data from sensors on the ground. this aims to contribute to the protection of fauna in angola by helping forest rangers to obtain actionable data from their operations. to check that the proposed system works correctly, tests were carried out in a simulation environment, verifying the correct transmission of data from the sensors to the data center. however, this technique provides a more sustainable and scalable monitoring method, minimizing human effort.",
      "muitas espécies animais estão à beira da extinção devido à caça furtiva e às mudanças no uso da terra, como a destruição de áreas florestais em detrimento da agricultura, exploração madeireira e a construção de novas vilas e cidades. portanto, é necessário monitorizar regularmente as populações animais para assegurar a proteção da vida selvagem, especialmente quando a pressão sobre os animais é elevada, e a gestão da conservação requer dados fiáveis e atualizados sobre a utilização dos solos, a dimensão das populações animais e a distribuição dos recursos em ecossistemas altamente variáveis. neste contexto, gostaríamos de contribuir para o processo de automatização dos métodos conven cionais, aproveitando o paradigma da internet das coisas (iot), que evoluiu rapidamente nos últimos anos, e das tecnologias digitais emergentes, como as redes de área alargada de baixa potência (lpwan) e o rápido crescimento dos veículos aéreos não tripulados (uav), também conhecidos como drones. por tanto, este estudo propõe um sistema de monitorização da vida selvagem baseado na detecção acústica, usando redes lora assistidas por uavs que atuam como gateways móveis para coletar dados dos nós no solo. isso visa contribuir para a proteção da fauna em angola, ajudando os guardas florestais a obter dados acionáveis das suas operações. para verificar o correto funcionamento do sistema proposto, foram realizados testes em ambiente de simulação, verificando a correta transmissão dos dados dos sensores para o sistema central. no entanto, esta técnica fornece um método de monitoramento mais sustentável e escalável reduzindo no mínimo o esforço humano."
    ],
    [
      "the raft consensus algorithm allows multiple state machine replicas to behave as a single and robust system, capable of tolerating various faults, offering more resilience when compared to a monolithic system. raft is known for its ease of understanding and practical implementation, due to its utilization of strong leadership and division into three relatively independent sub-problems: log replication, leader election and safety. the combination of its simplicity, strong consistency, and fault tolerance guarantees in data replication has solidified raft as one of the most popular algorithms since its creation a decade ago. it has been widely adopted in production by various systems, including etcd, cockroachdb, mongodb, and neo4j. the leader in a raft cluster has the active role of replicating its log entries on a process-by-process basis. in contrast, the other processes play a passive role, waiting for the leader’s requests and responding accordingly. this approach limits raft in terms of scalability and performance: the larger number of processes in the system, the higher the leader’s replication workload, causing the leader to become the system bottleneck. therefore, the overall performance of a raft cluster is heavily dependent on the efficiency of the leader process. in terms of fault tolerance, a raft cluster with 2f+1 processes can tolerate f processes to crash, requiring only a majority of correct processes to ensure availability. furthermore, raft allows for messages to be delayed or lost and can tolerate total network partitions. however, it assumes that messages are eventually delivered, as the leader needs to maintain constant communication with all processes to retain its leadership status. if a process does not receive any communication from the leader within a specified timeout period, it will assume that the leader failed and initiate an election. therefore, raft needs a transitive network to operate efficiently. if there are processes that do not receive messages from the leader, it increases the likelihood of elections being triggered constantly, leading to a loss of system performance. a consequence of such an occurrence was the outage experienced by cloudflare on november 2, 2020, which lasted for six and a half hours [30]. in this thesis, we present driftwood, a novel algorithm that expands raft by incorporating gossip mechanisms to decentralize the leader replication effort. gossip protocols allow a process to deliver a message to all processes, even in non-transitive networks, with a high probability. the driftwood’s leader, instead of sending its log entries one-by-one, will initiate gossip rounds with a message containing uncommitted entries so that the processes propagate these entries among themselves and replicate the leader’s log. furthermore, this message serves as the leader’s heartbeat when first received, avoiding unnecessary elections in non-transitive networks. however, the leader still has to receive replication confirmations from the processes in order to commit entries. so, we also propose new replicated data structures, shared in the gossip rounds, to discover new committed log entries. driftwood is experimentally evaluated and compared with raft. to this end, we implemented three algorithms in the paxi framework: raft, which serves as the baseline comparison, and two versions of driftwood, one that uses the new replicated data structures while the other doesn’t. through our evaluation, we determine the differences in performance, scalability, and distributed resource usage between raft and driftwood. we also analyse the behaviour of these algorithms in simulated non-transitive network scenarios.",
      "o algoritmo de acordo raft permite que múltiplas réplicas de uma máquina de estado se comportem como um único sistema robusto, capaz de tolerar diversas faltas, oferecendo mais resiliência comparado a um sistema monolítico. raft é reconhecido pela sua facilidade de compreensão e implementação prática, devido ao uso de liderança forte e divisão em três sub-problemas relativamente independentes: replicação do registo, eleição do líder e segurança. a combinação de simplicidade, e garantias de coerência forte e tolerância a faltas na replicação de dados solidificou o raft como um dos algoritmos mais populares desde a sua criação há uma década, sendo adotado em produção por diversos sistemas, incluindo etcd, cockroachdb, mongodb e neo4j. o líder dum sistema de raft tem o papel ativo de replicar as entradas do seu registo processo a pro cesso. em contrapartida, os outros processos têm um papel passivo no sistema, esperando apenas pelos pedidos do líder e respondendo de acordo. esta abordagem limita o raft em termos de escalabilidade e desempenho. com um maior número de processos no sistema, maior será a carga de trabalho de replicação para o líder, fazendo com que o líder se torne o gargalo do sistema. assim, o desempenho dum sistema de raft é dependente da eficiência do processo líder. em termos de tolerância a faltas, um sistema de raft com 2f+1 processos consegue tolerar f crashes de processos, necessitando apenas uma maioria de processos corretos para assegurar disponibilidade. além disto, raft permite perda ou atraso de entrega de mensagens e tolera partições totais de rede. contudo, é assumido que mensagens são entregues inevitavelmente, uma vez que o líder tem que manter constante comunicação com todos os processos para manter a liderança. se um processo não recebe nenhuma comunicação do líder até um tempo limite, este assume que o líder falhou e inicia uma eleição. desta forma, raft necessita de uma rede transitiva para operar eficientemente. caso o líder não consiga comunicar com alguns processos, maior será a possibilidade que eleições aconteçam constantemente, resultando na perda de desempenho do sistema. uma consequência desta ocorrência foi uma interrupção de serviços da cloudfare a 2 de novembro de 2020 que durou seis horas e meia [30]. nesta tese apresentamos driftwood, um novo algoritmo que expande o raft com a incorporação de mecanismos de propagação epidémica para descentralizar o esforço da replicação pelo líder. protocolos de propagação epidémica permitem que um processo entregue uma mensagem a todos os processos, mesmo em redes não transitivas, com elevada probabilidade. o líder, no driftwood, em vez de enviar as entradas do seu registo um-a-um, vai iniciar rondas de propagação epidémica com uma mensagem contendo entradas não confirmadas para que os processos propagarem estas entradas entre si e replicarem o registo do líder. além disso, esta mensagem serve como heartbeat do líder quando recebida pela primeira vez, evitando eleições desnecessárias em redes não transitivas. contudo, o líder continua a ter que receber as confirmações de replicação dos processos para descobrir que entradas estão confirmadas. assim, propomos novas estruturas de dados partilhadas na propagação epidémica que permitirão aos processos descobrir as entradas do seu registo que são confirmadas. driftwood é avaliado experimentalmente e comparado com raft. para tal, implementamos três algo ritmos na framework paxi: raft que serve de base e duas versões de driftwood, que diferem no uso de novas estruturas de dados para avanço das entradas confirmadas. através da nossa avaliação pretendemos determinar a diferença de desempenho, escalabilidade e no uso de recursos distribuídos entre raft e driftwood. analisamos também o comportamento destes algoritmos em cenários de rede não transitiva simulada."
    ],
    0.3
  ],
  [
    [
      "this document is a master’s dissertation, within the scope of data analysis for visualisation and exploration of knowledge in the health area. the thesis herein described is part of the second year of the master’s in informatics engineering, and it was held at the university of minho in braga, portugal. the main objective of this project is to combine the data measuring the performance of a patient when he is playing a specific game with the data collected at the same time from some biometric sensors so that it is possible to apply data analysis algorithms in order to discover new relationships between the data. it is also intended that, through adequate visualisation, knowledge can be created. data collection was carried out in partnership with the centro neurosensorial de braga. for data collection, initially, it was used an emotion recognition activity, a test that measures the processing speed (quick naming) and two tests to better characterise the child in terms of memory and attention capacity. initially, a small analysis was made of the data that were extracted through the platform provided by the centro neurosensorial de braga. after a mass data collection, its analysis was carried out. it was possible to verify, analytically, that as the memory deficit increases, among others, the number of fixations, the number of regressions, the time taken to perform the rapid naming test increases. regarding the emotions expressed during the rapid naming test, it was possible to verify that respondents who expressed happiness during the test show a better memory capacity, while children who expressed emotions of surprise or sadness are subject to memory deficit.",
      "este documento é uma dissertação de mestrado, no âmbito da análise de dados para visualização e exploração do conhecimento na área da saúde. a tese aqui descrita insere-se no segundo ano do mestrado em engenharia informática e foi realizada na universidade do minho em braga, portugal. o principal objetivo deste projeto é combinar os dados que medem o desempenho de um paciente durante a realização um jogo específico com os dados recolhidos, ao mesmo tempo, de alguns sensores biométricos, para que seja possível aplicar algoritmos de análise de dados a fim de descobrir novas relações entre os dados. pretende-se também que, através de uma visualização adequada, o conhecimento possa ser criado. a recolha de dados foi realizada em parceria com o centro neurosensorial de braga. para a recolha de dados, inicialmente, opta-se por usar uma atividade de reconhecimento de emoções, uma prova que mede a velocidade de processamento (nomeação rápida) e duas provas para caracterizar melhor a criança quanto à sua capacidade de memória e de atenção. inicialmente, foi feito uma pequena análise dos dados que eram extraídos através da plataforma disponibilizada pelo centro neurosensorial de braga. depois de uma recolha de dados em massa, foi feita a sua análise. foi possível verificar, analiticamente, que à medida que aumenta o défice de memória aumenta, entre outros, o número de fixações, o número de regressões, o tempo de realização da prova da nomeação rápida. em relação às emoções expressas durante a prova de nomeação rápida, foi possível verificar que os respondentes que expressaram felicidade durante a prova mostram uma melhor capacidade de memória, enquanto as crianças que expressaram emoção de surpresa ou tristeza têm tendência a ter uma menor capacidade de memória."
    ],
    [
      "the constant technological evolution of the last decades makes more and more companies to focus on providing more resources to their customers, showing new perspectives for the development of solutions with high levels of performance, availability, scalability and flexibility. one of the biggest contributions in this regard was the appearance of application programming interfaces (apis), increasingly crucial as integration, automation and efficiency become more important. with the abrupt emergence of apis, api security has become a significant topic in the tech world. if an api does not have adequate security, it can be vulnerable to attacks that can compromise a company’s data or system. security should be considered from the beginning of any api development project and built into each step of the process to ensure that the api is adequately protected. in this dissertation we intended to investigate the functioning of apis, with special focus on the representational state transfer (rest) architecture and their security, allowing us to verify that, despite several techniques and tools for the creation of solid and robust rest apis have already been studied in detail and applied to a wide variety of domains, rest services still need practical approaches specialized in the design and security of their apis. it is proposed to fill this gap with the definition of a set of metrics capable of helping in the creation of a rest api with good design principles and absent of any vulnerabilities. in the context of un1qnx as a company that develops authenticity solutions, an it infrastructure capable of handling multiple customers and systems is essential for its business. bearing this need in mind, the opportunity arose to implement in practice the result of all the research carried out throughout the dissertation through the development of an application programming interface (api) that follows the principles of architectural style based on rest in order to allow managing the data flow of the un1qnx system together with the definition of mechanisms to integrate the entire un1qnx service with third-party applications and services in order to automate procedures for creating and changing data.",
      "a constante evolução tecnológica das últimas décadas faz com que cada vez mais as empresas se foquem em fornecer mais recursos para os seus clientes, evidenciando novas perspetivas para o desenvolvimento de soluções com altos níveis de performance, disponibilidade, escalabilidade e flexibilidade. um dos maiores contributos neste sentido foi o aparecimento das application programming interfaces (apis), cada vez mais cruciais à medida que a integração, automatização e eficiência se tornam mais importantes. com o surgimento abrupto das apis, a segurança nas apis tornou-se um tópico significativo no mundo da tecnologia. se uma api não tiver a segurança adequada, ela pode ficar vulnerável a ataques que podem comprometer os dados ou o sistema de uma empresa. a segurança deve ser considerada desde o início de qualquer projeto de desenvolvimento de uma api e construída em cada etapa do processo para garantir que a mesma tenha a proteção adequada. nesta dissertação pretendeu-se investigar o funcionamento de apis, com especial foco na arquitetura repre sentational state transfer (rest) e na segurança das mesmas permitindo-nos verificar que, apesar de várias técnicas e ferramentas para a criação de apis rest sólidas e robustas já tenham sido estudadas em detalhe e aplicadas a uma grande variedade de domínios, os serviços rest ainda necessitam de abordagens práticas especializadas no design e segurança das suas apis. propõe-se preencher esta vaziez com a definição de um conjunto de métricas capazes de auxiliar na criação de uma api rest com bons príncipios de design e ausente de quaisquer vulnerabilidades. no contexto da un1qnx, empresa que desenvolve soluções de autenticidade e que permitiu o desenvolvi mento desta dissertação em contexto empresarial, uma infraestrutura ti capaz de lidar com vários clientes e sistemas é essencial para o seu negócio. tendo esta necessidade em foco, surgiu a oportunidade de implemementar na prática o resultado de toda a investigação efetuada ao longo da dissertação através do desenvolvimento de uma interface de programação de aplicações (api) que siga os princípios do estilo arquitetural baseado em rest de forma a permitir gerir o fluxo de dados do sistema un1qnx a par com a definição de mecanismos para se integrar todo o serviço un1qnx com aplicações e serviços de terceiros no sentido de se automatizar procedimentos para criação e alteração de dados."
    ],
    0.0
  ],
  [
    [
      "green computing has an increasing importance in software engineering. unfortunately, there are lack of tools on this field to help developers to understand and fix issues related to unwanted energy consumption. the thesis project will provide software (sw) eng. with information about energy consumption of functions and methods. the codecompass system helps software developers to understand their source code, and it was developed by the hungarian team members of ericsson. thus, i will locate hot spots in the software’s source code responsible for abnormal energy consumption, and i will do a plug-in to extend the codecompass tool so that it can automatically locate such energy faults, helping software developers to optimize the energy consumption of their software.",
      "computação verde tem uma importância crescente em engenharia de software. infelizmente, há falta de ferramentas neste campo para ajudar os eng. de software a entender e corrigir problemas relacionados ao consumo de energia indesejados. o projecto desta tese fornecerá aos desenvolvedores de software informações sobre o consumo de energia de funções e métodos. a ferramenta codecompass ajuda os engenheiros de software a entender o código-fonte e foi desenvolvida pelos membros da equipa húngara da ericsson. por fim, localizarei zonas \"quentes\" no código fonte do software responsáveis pelo consumo anormal de energia e irei construir um plug-in para estender a ferramenta codecompass para que ela possa localizar automaticamente essas falhas de energia, ajudando os engenheiros de software na otimização do consumo de energia de seus programas."
    ],
    [
      "encryption has been essential to protect modern systems and services. it became the security foundation of databases, payment systems, cloud services, and others. cryptography enabled the creation and validation of digital signatures, where the protection of the private key is very important to prevent false signatures. cryptocur rencies rely on this mechanism. crypto wallets hold private keys used to sign transactions and prove ownership of a digital asset. these have to keep the private key secure, but accessible to its owner, as it may be needed frequently. with the increasing number of decentralized web applications that interact with a blockchain, this subject has become more prevalent, as they usually require frequent signatures from the user. the mass adoption of cryptocurrencies by non-technical users urged the creation of crypto wallets that are secure but prioritize usability. some of these are hosted services that store the private keys in their servers and others are non-hosted, where the user is responsible for storing it. when implemented as a browser plugin, these wallets allow the user to seamlessly interact with a web application. the rise of cloud technology brought forth multi-signature on the cloud, by combining different cloud services owned by the user. these give the user control of his private key and are less vulnerable to cyber attacks. in this work, it is presented a comprehensive analysis of existing crypto wallet approaches in usability and security to understand the existing problems. the next step was to propose multiple possible solutions to those problems and produce their implementations. these take advantage of previously studied multi-cloud technology and are used to attempt to improve usability and security. to evaluate the proposed solutions and to compare them to the existing ones, we have developed a framework that consisted of various objective tests based on previous work, which have the goal of evaluating security and usability. finally, the proposed and existing solutions were compared using the proposed framework.",
      "a encriptação tem sido fundamental para proteger serviços e sistemas modernos, tornando-se essencial para proteger bases de dados, sistemas de pagamento, serviços de nuvem, entre outros. a criptografia permitiu a criação e validação de assinaturas digitais, onde a proteção da chave privada é bastante importante para impedir assinaturas falsas. as criptomoedas dependem deste mecanismo. as carteiras digitais contém chaves privadas que são usadas para assinar transações e provar a posse de um artefacto digital. estas guardam a chave privada de forma segura e acessível ao seu proprietário, pois pode ser necessária frequentemente. com o aumento do número de aplicações web descentralizadas que interagem com blockchains, este assunto tem ganho relevância, pois estas podem necessitar assinaturas frequentes por parte do utilizador. a adoção em massa de criptomoedas por utilizadores não técnicos levou à necessidade de criar carteiras digitais seguras, mas que priorizam a usabilidade. algumas destas carteiras classificam-se como serviços hosted, que guardam as chaves privadas nos seus servidores, e outras como non-hosted, onde o utilizador é responsável por as guardar. quando implementadas como um browser plugin, estas carteiras permitem o uti lizador interagir fluidamente com uma aplicação web. a ascensão da tecnologia nuvem permitiu o aparecimento da múltipla assinatura na nuvem, através da combinação de diferentes serviços de nuvem já possuídos pelo utilizador. estes dão ao utilizador controlo total da sua chave privada e são menos vulneráveis a ciberataques. neste trabalho, é feita uma análise compreensiva à usabilidade e segurança dos vários tipos de carteiras digitais, para perceber os seus problemas existentes. o próximo passo foi propor várias possíveis soluções aos problemas encontrados e também fazer a sua implementação. estas utilizam trabalho previamente estudado sobre tecnologia multi-nuvem e são usadas para com intuito de melhorar a usabilidade e segurança. para fazer uma avaliação das soluções propostas e compará-las a produtos existentes, desenvolvemos uma framework para testar segurança e usabilidade com vários testes objetivos, baseados em trabalho previamente estudado. no final, as soluções propostas e as já existentes foram comparadas utilizando o framework proposto."
    ],
    0.3
  ],
  [
    [
      "data science has started to become one of the most important skills someone can have in the modern world, due to data taking an increasingly meaningful role in our lives. the accessibility of data science is however limited, requiring complicated software or programming knowledge. both can be challenging and hard to master, even for the simpler tasks. currently, in order to clean data you need a data scientist. the process of data cleaning, consisting of removing or correcting entries of a data set, usually requires programming knowledge as it is mostly performed using programming languages such as python and r (kag). however, data cleaning could be performed by people that may possess better knowledge of the data domain, but lack the programming background, if this barrier is removed. we have studied current solutions that are available on the market, the type of interface each one uses to interact with the end users, such as a control flow interface, a tabular based interface or block-based languages. with this in mind, we have approached this issue by providing a new data science tool, termed data cleaning for all (dca), that attempts to reduce the necessary knowledge to perform data science tasks, in particular for data cleaning and curation. by combining human-computer interaction (hci) concepts, this tool is: simple to use through direct manipulation and showing transformation previews; allows users to save time by eliminate repetitive tasks and automatically calculating many of the common analyses data scientists must perform; and suggests data transformations based on the contents of the data, allowing for a smarter environment.",
      "a ciência de dados tornou-se uma das capacidades mais importantes que alguém pode possuir no mundo moderno, devido aos dados serem cada vez mais importantes na nossa sociedade. a acessibilidade da ciência de dados é, no entanto, limitada, requer software complicado ou conhecimentos de programação. ambos podem ser desafiantes e difíceis de aprender bem, mesmo para tarefas simples. atualmente, para efetuar a limpeza de dados e necessário um data scientist. o processo de limpeza de dados, que consiste em remover ou corrigir entradas de um dataset, é normalmente efetuado utilizando linguagens de programação como python e r (kag). no entanto, a limpeza de dados poderia ser efetuada por profissionais que possuam melhor conhecimento sobre o domínio dos dados a tratar, mas que não possuam uma formação em ciências da computação. estudamos soluções que estão presentes no mercado e o tipo de interface que cada uma usa para interagir com o utilizador, seja através de diagramas de fluxo de controlo, interfaces tabulares ou recorrendo a linguagens de programação baseadas em blocos. com isto em mente, abordamos o problema através do desenvolvimento de uma nova plataforma onde podemos efetuar tarefas de ciências de dados com o nome data cleaning for all (dca). com esta ferramenta esperamos reduzir os conhecimentos necessários para efetuar tarefas nesta área, especialmente na área da limpeza de dados. através da combinação de conceitos de hci, a plataforma é: simples de usar através da manipulação direta dos dados e da demonstração de pré-visualizações das transformações; permite aos utilizadores poupar tempo através da eliminação de tarefas repetitivas ao calcular muitas das métricas que data scientist tem de calcular; e sugere transformações dos dados baseadas nos conteúdos dos mesmos, permitindo um ambiente mais inteligente."
    ],
    [
      "nowadays medical monitoring is very important and something that helps us a lot. some supporting tools are already used for this, such as patient reported outcome measures (proms) and patient reported experience measures (prems), which are standardized medical surveys used to evaluate the quality of care or patient experience from the patient’s viewpoint. they can often be transformed into computerized adaptive tests, which aim to determine the best set of questions to ask each patient based on their previous answers. with these tools, it becomes possible for most patients to finish their forms, because they no longer have extensive forms, which increases the response rate to them. this is beneficial for both the patients and the healthcare providers. in the case of the patients, they get a more careful follow-up that helps them to have a better lifestyle and well-being. on the provider side, with additional information from the patient, healthcare providers can provide better, more personalized care and have a larger data set for future research. so, this dissertation is based on these improvements at the medical level and also in other improvements at the business level. for this, a solution was developed that uses bpmn as a high level language, camunda as an engine, a user interface and the main component of the solution. this last one connects all the elements and makes it a workflow engine capable of processing forms coded in bpmn diagrams.",
      "nos dias de hoje a monitorização médica é muito importante e algo que nos ajuda bastante. e para isso já são usadas algumas ferramentas de apoio, tais como as medidas de resultados do reportadas pelo paciente (proms) e as medidas da experiência reportadas pelo paciente (prems), que são questionários médicos padronizados utilizados para avaliar a qualidade dos cuidados ou a experiência do paciente do ponto de vista do mesmo. podem muitas vezes ser transformados em testes adaptativos informatizados, que visam determinar o melhor conjunto de perguntas a fazer a cada paciente com base nas suas re spostas anteriores. com estas ferramentas, passa a ser possível que a maior parte dos pacientes acabem de responder aos seus formulários, pois deixam de ter formulários tão extensos, o que por sua vez au menta a taxa de resposta aos formulários. isto é algo benéfico tanto para os pacientes, como para os serviços de saúde, pois no caso dos pacientes, eles passam a ter um acompanhamento mais cuidadoso, o que ajuda a um melhor estilo de vida e bem estar. do lado do prestador, com informação adicional do paciente, os prestadores de cuidados de saúde podem fornecer melhores e mais personalizados cuidados e ter um conjunto de dados maior para investigação futura. do lado do prestador, com informação adi cional do paciente, os prestadores de cuidados de saúde podem fornecer melhores e mais personalizados cuidados e ter um conjunto de dados maior para investigação futura. assim sendo esta dissertação tem como base estas melhorias a nível médico e também outras melhorias a nível empresarial. para tal, foi desenvolvida uma solução que utiliza bpmn como linguagem de alto nível, camunda como motor, uma interface para o utilizador e foi implementado um componente da solução que liga todos os elementos e faz com que exista um workflow engine capaz de processar formulários codificados em diagramas de bpmn."
    ],
    0.3
  ],
  [
    [
      "emergency departments have a higher number of visits compared to other hospital de partments. technology has played a crucial role in promoting improvements in hospital management and clinical performance. the number of visits to emergency departments has increased considerably, giving rise to crowding situations that cause several adverse effects. this situation negatively affects the provision of emergency services, impairs the quality of health care and increases the time patients wait for medical check-up. one of the leading causes contributing to the crowding is the high number of patients with low severity clinical condition. these are referred to as non-urgent or inappropriate patients, whose clinical situation should be taken care through self-care or primary health care. it is the responsibility of the institutions to analyse and quantify the possible causes of crowding to find the best solution to mitigate the adverse effects caused. it is believed that non-urgent patients can use the time spent in the waiting room more productively, namely by using a self-service kiosk to which they can provide valuable information to facilitate and accelerate the clinical processing. this work proposes a solution to be used in the waiting room of emergency departments, which aims to reduce the period of medical check-up. the solution uses a self-service kiosk for the patient to provide relevant clinical data that would otherwise have to be collected by the physician during the clinical observation process. in particular, the kiosk will collect vital signs, past medical history, main complaint and usual medication. this data will be processed and provided to the physician in a structured and uniform way before each medical check-up. the primary purpose of this solution is to reduce the period of patients’ medical check-up and thus improve the response capacity of the emergency departments with the same resources. during the master’s work period, an android application was implemented for patients to enter the clinical data mentioned above, and a web application for physicians to access it. additionally, a data warehouse was implemented to store the data in a consolidated way to discover hidden relationships and patterns in the data. the first moment of evaluation, undertaken in a non-hospital facility, shows positive acceptability by participants, with a large majority considering the system user-friendly. due to the pandemic, it was impossible to perform the second planned evaluation moment in a real emergency environment.",
      "os serviços de urgência apresentam um número de visitas superior em comparação com outros serviços presentes nas instituições hospitalares. a afluência aos serviços de urgências tem vindo a aumentar consideravelmente, dando origem a situações de lotação que provocam diversos efeitos negativos nas instituições hospitalares. no geral, este fenômeno afeta negativamente a prestação dos serviços de urgência, prejudica a qualidade dos cuidados de saúde e faz aumentar o tempo que os doentes aguardam pela observação clínica na sala de espera. uma das principais causas apontadas para o surgimento da lotação é o elevado número de doentes com condição clínica de baixa gravidade. estes são designados como doentes não-urgentes ou inapropriados, cuja condição clínica poderia ser resolvida, idealmente, com recurso ao auto-cuidado ou a cuidados de saúde primários. é da responsabilidade das instituições analisar e quantificar as possíveis causas de lotação, de forma a encontrar a melhor solução para atenuar os efeitos negativos provocados. acredita-se que os doentes não-urgentes tenham a capacidade de utilizar o tempo na sala de espera de forma mais produtiva, através da utilização de um quiosque self-service. neste sentido e aliada à tecnologia, esta dissertação contextualiza uma solução para ser utilizada na sala de espera dos serviços de urgência, visando reduzir o período de observação clínico. esta solução vem complementar a realização do procedimento inicial efetuado pelo médico, no consultório, através do uso de um quiosque. assim, a recolha dos sinais vitais, história médica prévia, queixa principal e medicação habitual será efetuada pelos doentes no quiosque. estes dados vão ser fornecidos de forma estruturada e organizada ao médico antes da realização da consulta. o objetivo principal desta solução é reduzir o período de observação clínico e assim melhorar a capacidade de resposta dos serviços de urgência com os mesmos recursos hospitalares. durante o período da dissertação, foi implementada uma aplicação android para os pacientes registarem os dados clínicos acima mencionados, e uma aplicação web para os médicos acederem aos mesmos. foi implementado também um data warehouse para a descoberta de relações e padrões escondidos nos dados. o primeiro momento de avaliação, realizado num ambiente não hospitalar, mostrou uma aceitabilidade positiva pelos partici pantes, com grande maioria a considerar o sistema user-friendly. devido à pandemia, não foi possível realizar o segundo momento de avaliação planeado num serviço de urgências."
    ],
    [
      "é cada vez mais importante que os sistemas de informação hospitalar garantam uma melhoria na segurança e na qualidade dos cuidados médicos. os pacientes neonatais e pediátricos são mais vulneráveis que os pacientes adultos tornando essencial orientar as tecnologias de informação para as suas necessidades. erros na administração de medicamentos são os erros mais comuns e potencialmente mais nocivos nas instalações hospitalares, sendo a sua taxa de incidência maior na população pediátrica. neste sentido, torna-se essencial melhorar a segurança do paciente. para além disto, é essencial ao profissional de saúde uma interligação da informação do paciente pelos diferentes sistemas de informação que ele possui ao seu dispor. esta dissertação tem como principal objetivo a finalização e implementação de uma plataforma de apoio à decisão médica através do desenvolvimento de diversas ferramentas, que auxiliem os médicos nas suas atividades diárias e que contribuam para a diminuição da taxa de ocorrência de erro médico. este desenvolvimento foi acompanhado por um médico pediatra do centro hospitalar do porto. o sistema desenvolvido permite colmatar falhas existentes nos sistemas utilizados atualmente em algumas unidades hospitalares e, deste modo, obter um sistema que permitisse uma troca de informação e uma comunicação entre os serviços de pediatria e neonatologia e os serviços de farmácia. assim, é possível facilitar o trabalho diários dos profissionais de saúde e, ainda, provocar uma diminuição nos efeitos adversos causados por erros de medicação. sendo um processo acompanhado, a plataforma foi testada e melhorada ao longo do tempo de forma a obter um sistema final satisfatório. por fim, é lançada uma avaliação aos testes realizados na aplicação.",
      "it is increasingly important that health information systems ensure an improvement in the safety and quality of medical care. neonatal and pediatric patients are more vulnerable than adult patients, making it essential to steer information technology to their needs. errors in medication administration are the most common and potentially most damaging errors in hospital facilities, and their incidence rate is higher in the pediatric population. in this sense, it is essential to improve the patient’s safety. in addition, it is essential for the healthcare professionals to interconnect the information of the patient by the different information systems that he has at his disposal. the main objective of this dissertation is to finalize and implement a medical decision support platform through the development of several tools that assist physicians in their daily activities and contribute to the reduction of the rate of occurrence of medical error. this development was accompanied by a pediatric doctor of the centro hospitalar do porto. the system developed allows to bridge existing flaws in the systems currently used in some hospital units, and an exchange of information and communication between the pediatric and neonatology services and the pharmacy services. thus, it is possible to facilitate the daily work of health care professionals and also to decrease the adverse efects caused by medication errors. being an accompanied process, the platform was tested and improved over time in order to obtain a satisfactory final system. finally, it is made an evaluation of the tests carried out in the application."
    ],
    0.06666666666666667
  ],
  [
    [
      "this master thesis studies tor, an anonymous overlay network used to browse the internet. it is an open-source project that has gain popularity mainly because it does not hide its implementation. in this way, researchers and security experts can examine and confirm its security requirements. its ease of use has attracted all kinds of people, including ordinary citizens who want to avoid being profiled for targeted advertisements or circumvent censorship, corporations who do not want to reveal information to their competitors, and government intelligence agencies who need to do operations on the internet without being noticed. in opposite, an anonymous system like this represents a good testbed for attackers, because their actions are naturally untraceable. traffic characteristics are studied in detail, which can be used to detect tor. further, a detection mechanism was developed to prevent users from reaching the tor network. finally, some changes are proposed so that tor can better disguise its traffic with traditional web browsing traffic to overcome any intention of blocking it.",
      "esta tese de mestrado estuda o tor, uma rede overlay anonima que é usada para aceder a informações na internet. trata-se de um projeto open-source que ganhou popularidade principalmente porque não esconde a sua implementação. desta forma, investigadores e especialistas em segurança podem examinar e confirmar os requisitos de segurança especificados. a sua facilidade de uso atraiu diferentes tipos de pessoas, incluindo cidadãos comuns que pretendem evitar a publicidade e os anúncios direcionados ou a censura, empresas que não pretendem revelar informações aos concorrentes e agências de inteligência governamentais que precisam de realizar operações na internet sem serem vigiadas. por outro lado, um sistema anónimo como este representa um bom ambiente de teste para atacantes, porque as suas ações são difíceis de controlar. as características de tráfego são estudadas em detalhe, e podem ser usadas para detectar o tor. além disso, foi desenvolvido um mecanismo de deteção para impedir que os utilizadores alcancem a rede tor. finalmente, são propostas algumas alterações para que o tor possa ofuscar melhor o seu tráfego com o tráfego tradicional web, de modo a ultrapassar qualquer intenção de bloqueá-lo."
    ],
    [
      "com o crescimento significativo de utilizadores de dispositivos móveis, nos últimos anos, mais do que a necessidade apareceu a oportunidade de criar novas plataformas e serviços digitais que não só facilitam o quotidiano das pessoas, evitando deslocamentos, filas de espera e complicações desnecessárias, como tornam a comunicação das pessoas com as instituições num processo mais rápido e cómodo. como tal, estas alternativas estão gradualmente a complementar, em alguns casos mesmo a substituir, os métodos antigos. esta dissertação propõe uma solução digital, na forma de uma aplicação móvel, para aproximar as populações às suas instituições públicas recorrendo a gamification, isto é, transformando o processo de comunicação com uma câmara municipal ou junta de freguesia, por exemplo, recorrendo ao uso de elementos de jogos como conquistas, atribuição de recompensas, classificações, entre outras, com o objetivo de estimular a comunicação do utilizador com essas instituições permitindo que este possa ao mesmo tempo divertir-se e competir com os outros, enquanto explora e ajuda a sua cidade. para tal, além de um serviço de participação de ocorrências na cidade, os utilizadores têm à sua disposição toda uma narrativa que os leva a completar missões, participar em eventos e conhecer gente nova bem como a conhecer verdadeiramente a sua própria cidade. por sua vez, a instituição pública responsável pela manutenção e administração do sistema tem à sua disposição uma plataforma de administração para manter o conteúdo da aplicação atualizado para os seus utilizadores. no decorrer desta dissertação está documentado todo o processo de desenvolvimento da aplicação móvel e do servidor web, que a suporta, assim como todas as decisões tomadas e as razões que as justificam. o documento contém, ainda, exemplos e explicações do funcionamento da aplicação, considerações finais sobre o projeto e ideias para trabalho futuro. nas considerações finais é feita uma comparação dos objetivos inicialmente traçados para o projeto e o resultado alcançado, provando que os mesmos foram cumpridos com sucesso.",
      "the significant growth of mobile device users of the last few years, revealed the opportunity to develop new digital platforms and services which not only make people’s everyday lives easier, avoiding unnecessary trips, waiting lines and complications, but also making the process behind the communication between the populations and institutions in easier and smoother. consequently, these new alternatives are gradually complementing, in some cases even completely replacing, the old methods. this dissertation proposes a digital solution, in the form of a mobile application, to strengthen the communication between populations and their public institutions through gamification. turning that process into a game ensures people can have fun while they are exploring and helping their city. to this purpose, users have a report system that allows them to expose things they think are wrong with the city, such as broken structures, clogged sewers, among others. the game also has a lore that leads to extra motivation to complete missions, be part of player gatherings and events, meet new people and to better explore the cities’ wonders. the entity responsible for the maintenance and administration of the system have an admin platform to better help keeping the application’s content fresh and updated, allowing for a better user experience for the population. this document addresses all the steps taken in the application’s development process as well as the web server’s that support said application. all the decisions have been documented and the reasons that justify them, as well as examples and how tos about the application, ending with some considerations about the work that has been done and some ideas for the future of the system. in the final considerations is also made a parallel between the project’s goals and the final result, proving these goals were met successfully."
    ],
    0.3
  ],
  [
    [
      "a discalculia é uma desordem neurológica que dificulta a aprendizagem de diversos conceitos matemáticos. sendo uma desordem inerente à pessoa, pois nasce com ela, é crucial dedicar-lhe atenção desde cedo. uma forma de atenuar os seus efeitos é o uso de tecnologia moderna como aplicações móveis, nomeadamente jogos com foco na prática e ensino de diversos conceitos relacionados com matemática, nos primeiros anos de vida escolar em pessoas que sofrem de discalculia. este projeto tem como foco a criação de uma plataforma cujo objetivo é complementar estes jogos registando diversos parâmetros relacionados com o desempenho dos alunos em diversas tarefas e retirando conclusões baseadas nestes parâmetros de forma a detetar a discalculia e os seus sintomas, assim como acompanhar o progresso do aluno no que toca ao tratamento desta desordem. a criação desta plataforma permitiria a existência de uma nova e útil ferramenta no que toca à discalculia e ao tratamento dos seus sintomas o mais cedo possível na vida de uma pessoa.",
      "dyscalculia is a neurological disorder that hampers the ability to learn certain mathematical concepts. it is a disorder that exists in the person since the moment of birth and, as such, it should be dealt with as early as possible. it is possible to mitigate the damage done by this disorder by using modern technology, particularly mobile games which are focused on developing abilities in various fields of mathematics, on the first years of children’s scholar education. the main objective of this project is the creation of a platform that is meant to complement these games by registering performance parameters that were achieved by the students and drawing conclusions based on said parameters in order to detect dyscalculia and its symptoms, as well as register the student’s progress when it comes to its treatment. by creating this platform there is a new and helpful tool regarding dyscalculia and the treatment of its effects, as early as possible in a person’s life."
    ],
    [
      "ao longo dos últimos anos, a área da filatelia tem vindo a perder interessados, nomeadamente nas gerações mais jovens. essa questão, aliada ao facto do mercado online estar em plena afirmação, leva a que este desinteresse se torne evidente e cada vez mais acentuado, refletindo-se num decréscimo do negócio das lojas físicas de filatelia. neste contexto, o objetivo da dissertação era desenvolver uma aplicação web para filatelistas que permitisse e facilitasse a comunicação entre colecionadores para comprar, vender ou trocar artigos filatélicos. pretendia-se que a aplicação fosse desenvolvida segundo uma metodologia ágil. a adoção da metodologia ágil scrum foi fundamentada numa revisão bibliográfica das metodologias existentes mais representativas. embora fosse sugerida a utilização de uma metodologia ágil, também se analisaram algumas metodologias tradicionais. os resultados alcançados evidenciam que a falta de experiência na utilização da metodologia scrum pode criar alguns problemas à equipa de scrum, especialmente nas fases iniciais do desenvolvimento. no mesmo sentido, uma vez que a metodologia scrum se foca na produção de incrementos de produtos funcionais e menos na documentação, leva a que a documentação final da aplicação seja mais difícil caso não seja feita incrementalmente ao longo dos sprints. por outro lado, pelo facto da metodologia scrum funcionar em ciclos repetidos de desenvolvimento completo, permite que seja obtido feedback regular e valioso das várias partes interessadas, nomeadamente do product owner. deste modo, é muito mais garantido que o produto final agrada às partes interessadas. pode afirmar-se que a aplicação web desenvolvida é um produto viável mínimo, pois disponibiliza as funcionalidades suficientes para ser adotada por filatelistas e por outros interessados na área de negócio da filatelia.",
      "over the past few years, the area of philately has been losing interest, particularly among younger generations. this question allied with the fact that the online market is growing quickly, means that this disinterest becomes evident and even more accentuated, reflecting a business decline in physical philately stores. in this context, the goal of the thesis was to develop aweb application for philatelists that allows and facilitates the communication among collectors in order to buy, sell, or exchange philatelic products. it was intended that the application was developed according to an agile methodology. the adoption of the scrum agile methodology was based on a literature review of the most representative existing methodologies. although it was suggested to use an agile methodology, traditional methodologies were also analyzed. the achieved results indicate that the lack of experience in using the scrum methodology can create some problems to the scrum team, especially in the early phases of the development. in the same way, once the methodology scrum focuses on the production of functional product increments and less on documentation, makes the final application documentation more difficult if it is not done incrementally during the sprints. on the other hand, as the scrum methodology works in repeated cycles of full development, it allows to obtain regular and valuable feedback from the different stakeholders, including the product owner. thus, it is more assured that the final product will be approved by the stakeholders. it can be said that the developed web application is a minimum viable product, which provides sufficient functionality to be adopted by philatelists and other stakeholders in the business area of philately."
    ],
    0.06666666666666667
  ],
  [
    [
      "digitalization has taken over many businesses and industries, as technology is now widely used and preferred over manual work. therefore, technical devices nowadays contain several sensitive pieces of information that are prone to be shared with others. consequently, over the years there has been an increasing need to protect our data and to exchange it in a secure and sovereign way, as data are one of the most valuable assets of any industry and business. therefore, the need to implement systems that prioritize such values emerged, in order to grant continuous monitoring and control over data usage, to keep track of how data is used even after it has been exchanged with other entities. the motivation to consider data spaces relies on the fact that it should follow an interoperable and cross-sector approach, offering a reference implementation suitable to every industry and business, to ease the communication between different industries and to allow data sovereignty and usage control over data inside the ids ecosystem. building upon this groundwork, this study presents a data app designed to implement usage control, integrated inside an ids connector. this proposal ensures that customised usage policies are included and enforced on the user side of a connector and emphasizes the protection of sensitive data. moreover, the data app incorporates a cipher mechanism to safely exchange cryptographic keys used to protect data assets. by incorporating this data app, the ids initiative is able to stay truthful to its principle of ensuring data sovereignty and building a robust ids ecosystem.",
      "a digitalização apoderou-se de muitos negócios e indústrias, visto que, nos dias de hoje, a tecnologia é amplamente utilizada e preferida ao invés do trabalho manual. desta forma, os dispositivos eletrónicos da atualidade contêm cada vez mais informações pessoais e confidenciais que são propensas de ser partilhadas com outras entidades. consequentemente, tem havido uma necessidade crescente ao longo dos anos de proteger os nossos dados de forma a tornar a sua partilha mais segura tendo em conta o conceito de soberania de dados, visto se tratarem de um dos ativos mais valiosos de qualquer setor e negócio. desta forma, surgiu a necessidade de implementar sistemas que priorizem estes valores, de modo a fornecer monitorização contínua e controlo sobre o uso dos dados, para monitorizar como os dados são usados mesmo depois de serem transferidos para outras entidades. a opinião consensual sobre os data spaces assenta no facto de estes deverem seguir uma abordagem interoperável e inter setor, através de uma implementação de referência suscetível de ser personalizada e adequável a todas as indústrias e negócios, de forma a facilitar a comunicação entre diferentes indústrias e permitir a soberania e controlo de uso dos dados dentro do ecossistema ids. tendo em conta as ideias apresentadas, este estudo apresenta uma aplicação desenhada para im plementar controlo de uso sobre os dados, ao integrá-la num ids connector. esta proposta permite a costumização de politicas de acesso aos dados no plano de controlo do utilizador do connector. permite ainda a incorporação de mecanismos de cifra de dados e troca segura das chaves criptográficas consider adas, para troca de dados entre as apps. ao incorporar esta aplicação, a iniciativa é capaz de se manter fiel ao princípio de garantir a soberania de dados e construir um ecossistema robusto."
    ],
    [
      "the understanding of the rapid eye movement (rem) sleep structure is very limited. the rem stage presents periods with distinct characteristics, suggesting that it should be divided into sub-stages. at least two sub-stages are said to exist during rem, the tonic and phasic periods, mainly separated by the presence and absence of rapid eye movements. the main objective of this thesis was to evaluate the existence of patterns during the rem stage, based on the signals from the polysomnography. this evaluation focused on characterizing the phasic and tonic periods, through the development of algorithms. moreover, this study was extended using unsupervised learning to analyze the existence of other rem sub-stages. finally, this investigation was completed with the evaluation of the heart rate variability during the rem period, taking into consideration the rem sub-stages. this study corroborated the characteristics described in the literature for the phasic and tonic period from the rem stage. the limitations showed by the initial approach were surpassed with the application of a clustering technique. this resulted in a division of the rem period in 4 sub-stages, each presenting a specific pattern in the characteristics of the electroencephalography (eeg), electrooculography (eog), and electromyography (emg). the study of the heart rate variability during rem showed specific patterns between the changes in the heart rate variability and the control of the autonomic nervous system (ans) during this period and each rem sub-stage. moreover, the structure of the rem sleep changed in a specific pattern with the advance of the rem cycle, while there was a decrease of the heart rate and an increase in the parasympathetic activity. in this thesis, we presented a new hypothesis for the rem structure, which can be applied in future studies, allowing to be consolidated and possibly contributing to a better understanding of the rem stage. the findings emphasized the need to consider the rem stage a non-homogeneous stage. as for the study of the heart rate variability, this investigation led to a possible explanation for the variations occurring during rem sleep, bringing new knowledge that can benefit the unobtrusive methods for sleep monitoring.",
      "o sono rem (“rapid eye movement”) apresenta períodos com caraterísticas distintas, o que sugere que deveria ser separado em subfases. no entanto, o conhecimento da estrutura e função desta fase é bastante limitado, sendo descritas pelo menos duas subfases durante este período. estas duas subfases, denominadas fásica e tónica, são caraterizadas sobretudo pela presença ou ausência de movimento rápido dos olhos. o principal objetivo deste trabalho consiste em avaliar a existência de padrões durante o sono rem que sugiram a existência de subfases. utilizando os sinais obtidos através da polissomnografia, dois algoritmos foram desenvolvidos para identificar e caraterizar períodos fásicos e tónicos, baseados nos métodos utilizados na literatura. posteriormente, um método de aprendizagem não supervisionaa permitiu avaliar a existência de outras subfases. por fim, o sono rem foi caraterizado em termos da variabilidade do ritmo cardíaco, tendo em consideração as subfases anteriormente discutidas. os resultados obtidos confirmaram as caraterísticas para as duas subfases descritas na literatura. os algoritmos desenvolvidos inicialmente demonstraram limitações que foram ultrapassadas através da aplicação da técnica de clustering. este método resultou na separação do sono rem em 4 grupos, que apresentaram diferenças relevantes nas caraterísticas obtidas do eletroencefalograma (eeg), eletrooculograma (eog) e electromiograma (emg). observou-se uma alteração da estrutura do sono rem com o avanço da noite, acompanhado por uma diminuição do ritmo cardíaco e um aumento do domínio do sistema nervoso parassimpático. além destas alterações, cada subfase do sono rem apresentou um padrão específico em termos da variabilidade do ritmo cardíaco e do domínio do sistema nervoso autónomo. nesta tese é proposta uma nova hipótese para a estrutura do sono rem, podendo ser aplicada em futuros estudos, permitindo a consolidação e uma possível contribuição para uma melhor compreensão do sono rem. além disso, este estudo realçou a necessidade de considerar o sono rem como um período não homogéneo. no caso do estudo da variabilidade do ritmo cardíaco, esta investigação apresenta uma possível explicação para as variações descritas durante o sono rem, trazendo novo conhecimento que poderá beneficiar os métodos de monitorização do sono baseados no ritmo cardíaco."
    ],
    0.06666666666666667
  ],
  [
    [
      "o planeamento tem vindo a ganhar preponderncia entre a comunidade médica de cirurgiões. a elaboração de um plano para a cirurgia é fundamental para que esta se desenrole da melhor forma possível, encurtando assim os tempos de recuperação do paciente. no caso da cirurgia ortopédica, o planeamento tem uma importância ainda mais acentuada, devido à relação estreita entre os tempos de recuperação e fiabilidade a que o paciente fica sujeito e o sucesso da cirurgia. assim, é importante que os cirurgiões disponham de ferramentas que os auxiliem nessa tarefa, por forma a torná-la menos morosa e complexa. no entanto, isso não acontece. as soluções ao dispor dos cirurgiões revelam-se insuficientes, não possibilitando uma visão global da extensão da lesão e possíveis intervenções. dessas soluções, apenas um pequeno número permite a modelação tridimensional do estudo imagiológico de tomografia computorizada. porém, não possibilitam que a análise da intervenção desenvolvida pelo cirurgião possa ser feita no mesmo universo geométrico. isto deve-se essencialmente à dificuldade de interoperação entre tipos de formato imagem diferentes, dado que o estudo imagiológico é do tipo matricial e os templates representativos dos apoios físicos, vectorial. posto isto, o presente trabalho pretende apresentar uma solução para este problema de interoperação, bem como a sua implementação. através da solução apresentada, o cirurgião tem a possibilidade de manipular uma isosuperfície renderizada tridimensionalmente a partir do estudo imagiológico selecionado. de seguida, é-lhe permitido adicionar as representações digitais dos apoios físicos utilizados, por forma a avaliar a viabilidade da sua abordagem. em conjunto, tem a possibilidade de gerar novas isosuperfícies de valores diferentes, bem como cortar o modelo final num plano previamente definido, o que permite uma análise, agora em duas dimensões, da intervenção planeada. posto isto, é possível concluir que a solução apresentada auxilia o cirurgião no desenvolvimento de um planeamento mais adequado, podendo analisar tridimensionalmente o impacto da sua intervenção no paciente.",
      "planning has gained predominance among the medical community. the preparation of a surgery plan is essential for this to take place in the best way possible, thus shortening patient’s recovery times. in the case of orthopedic surgery, planning has an accentuated importance, due to the close relation between recovering times and reliability that the patient is subjected and the success of the surgery. thus, it is important that surgeons are provided with tools that help them in this task, in order to make it less time consuming and arduous. however, this doesn’t actually happen. available solutions for surgeons are insufficient and do not allow an overview of the injury’s extent and possible solutions. within these solutions, only a small number allows a three-dimensional modeling of the imaging study, disallowing however, that the solution developed by the surgeon can be analyzed in the same geometric universe. this is mainly due to the difficulty of interoperation among different types of image, since the medical imaging study is matrix type and the templates representing physical support, vectorial. this work aims to present a solution and its implementation for this problem. with the developed application, the surgeon is able to manipulate a three-dimensional isosurface rendered from the selected imaging study. then, he can add digital representations of the used physical supports, in order to evaluate the feasibility of his planning. he has the possibility to generate new isosurfaces of di↵erent values, and cut the final model in a pre-defined plan, which enables an analysis, now in two dimensions, of the planned solution. thus, it is possible to conclude that the presented application assists the surgeon in the planning process in a more suitable way, where he can three-dimensionally analyze the impact of his surgical intervention on the patient."
    ],
    [
      "os arquivos nacionais e regionais têm como objectivo principal recolher e preservar a herança arquivística nacional que de alguma forma possuam interesse histórico, como por exemplo, documentos originados em cartórios, tribunais, ou em organismos religiosos. estes documentos estão tipicamente relacionados com registos de nascimentos/óbitos, processos cíveis, entre outros. para além de preservar esta informação, os arquivos disponibilizam também a consulta pública destes documentos a quem assim o desejar. de uma forma geral, estes documentos são organizados, de forma hierárquica num modelo que comporta três níveis, orgânico, funcional e documental, e constituídos fisicamente em depósitos, que correspondem a uma determinada área física do(s) edifício(s) que representam o arquivo (por exemplo, salas). a aplicação digitarq da keep solutions é um software de gestão de arquivos, baseada em normas internacionais, que permite a produção e exportação de registos descritivos (informação recolhida nos documentos e outros objetos presentes nos fundos), disponibilizando-os em portais agregadores de conteúdos arquivísticos. atualmente, esta aplicação permite guardar e obter informação relativa aos documentos e à sua hierarquia lógica nos fundos em que estão inseridos, mas não permite conhecer detalhes sobre os depósitos em que estão inseridos, nem sobre os depósitos em si mesmos. é neste contexto que esta dissertação é realizada, acompanhando o processo de criação de um novo módulo experimental para o digitarq que permita realizar a manutenção de depósitos, incluindo a gestão do seu espaço, a identificação da localização física dos seus documentos, e que possua também funcionalidades de optimização dos seus espaços em termos de armazenamento.",
      "national and regional archives main purposes are to collect and maintain national archiving heritage that, in some way, may have some historical relevance, like notary’s office documents, courts or religious organizations. these documents are typically related with birth/death records, civil proceedings, among others. besides preserving this info, archives allows for records public consultation to whoever wishes so. generally, these documents are organized in a hierarchical way, in a model that supports three levels, organic, functional and documental, and then constituted physically in deposits that match to a given physical area of the building(s) that belongs to the archive (such as rooms). the software digitarq of keep solutions is an archival management application, based on international standards, that allows for production and exporting of descriptive records (data collect from the documents and other objects in the fonds), making them available on archival aggregation portals. currently, this application allows for saving and retrieving documents data and their logical organization in the fonds they belong, but it doesn’t provide any sort of information about the deposits where they are kept nor about the deposits themselves. it’s in this context that this dissertation will be developed, following the creation process of a new experimental software module for digitarq, that will allow for deposit management, including its space management, documents physical location identification, and that also have storage space optimization features."
    ],
    0.0
  ],
  [
    [
      "a nova versão do html traz melhorias significativas relativamente à construcão de aplicacões web mais ricas. contudo, com as novas funcionalidades vêm acoplados a elas sempre novos riscos de segurança que precisam ser analisados e colmatados. anteriormente ao html5 já existiam determinadas ameaças de segurança que afetavam as aplicacões web (tais como sqlinjection, xss, csrf, etc), e que ganham um novo potencial devido aos novos recursos do html. este estudo foca precisamente a análise dessas ameaças bem conhecidas, em conjunto com a análise dos riscos de segurança associados às novas funcionalidades do html5, assim como a apresentacão de regras para atenuacão das mesmas. adicionalmente são apresentados um conjunto de módulos para detecão de vulnerabilidades html5 em aplicacões web. as quais são originadas devido à má utilizacão do html5 durante a fase de desenvolvimento. esse conjunto de módulos corresponde a uma extensão adicionada a um black box web application security scanner bem conhecido da owasp designado zap. isso implicou adicionar também algumas funcionalidades html5 a uma aplicacão web também da owasp designada wave zap, cujo objetivo é ser utilizada para realizar testes de penetracão a fim de testar esses novos módulos do zap.",
      "the new version of html, offers impressive enhancements to develop rich web applications. but coupled with new features they always come new security issues that need to be analyzed and covered. prior to html5 already existed certain security threats that affecting the web applications (such as sqlinjection, xss, csrf, etc.), and that gain a new potential due to the new html features. this study focuses specifically on the analysis of these well known threats, together with the analysis of the security risks associated with the new features of html5, as well as, the presentation of mitigation rules. additionally are presented a set of modules for detecting html5 vulnerabilities in web applications, caused by inadequate use of html5 during the development phase. this set of modules corresponds to an extension added to a black box web application security scanner well known called zap, which is owned by owasp. this also implied the adding of some html5 features to the wave zap web application, also owned by owasp, with the intent to perform penetration tests against it, in order to test these new zap modules."
    ],
    [
      "este trabalho, ou este estudo, ou esta experiência tenta demonstrar que existem ferramentas diferentes das já utilizadas para ajudar os alunos a compreender o funcionamento exato da disciplina de matemática. assim sendo, todo este projeto concentra-se na obtenção de formas diferentes de ensinar matemática aos alunos. a robótica, como instrumento de ensino e demonstração, vai permitir aos alunos terem um contacto prático com as matérias que lhes são ensinadas. este projeto decorre numa escola básica com alunos do 5o ano. todos estes alunos são voluntários e eram alunos com notas medianas a esta disciplina no ano anterior. os robôs utilizados, são robôs da lego mindstorms nxt, com o respectivo software de programação. os alunos tinham de os programar consoante a matéria que iam dando nas aulas. nenhum aluno tinha tido contacto com estes robôs e muitos deles nem sequer nunca tinham mexido em computadores. esta articulação programa-aluno-professor foi feita com a diretora de turma e a professora de matemática, que iam revelando a planificação das aulas para que os alunos pudessem experimentar na prática o que tinham aprendido.",
      "this study or this work, tries to demonstrate that there are different tools from the ones used before, to help students understand the exact procedure of the mathematics subject. so, this whole project is focused in teaching mathematics in a different approach. robotics, as a teaching and demonstration tool, will allow the students to experience in a useful way the topics they’ll be learning. this project involves students of the 5th grade of a middle school. all the students were volunteers and had had a medium evaluation the previous school year. the robots used are lego mindstorms nxt, with the proper programming software. the students had to program them according to what they learned in class. none of the students had ever used these computers and some of them had never used a computer before. this purpose: program-student-teacher was done with the class director and the maths teacher, who explained their planning to the students so that they could put into practise their work."
    ],
    0.0
  ],
  [
    [
      "over the last years, there has been an increase in popularity of online platforms that enable users of the most varied economic activities (e.g. restaurants, bars, shops, shopping centers, nightlife, etc.) to evaluate the quality of the service received. thus, based on the community’s aggregate of opinions, the user can then make a more informed decision when opting between a wide variety of competing services. however, the field of medical services and health is one of the economic areas where this proce dure isn’t widely adopted. consequently, the goal of this work is to develop an intelligent technological solution centered on the user, that allows the registration, evaluation, and recommendation of healthcare institutions. in this work, we present a platform called healthadvisor. this platform is comprised of a web user interface, a recommender system that generates recommendations for the user, and a backend service that supports both of these components by providing and exposing multiple rest apis (representational state transfer application programming interfaces). by using this solution, it is possible for a user to search for healthcare institutions, read the reviews of institutions made by other users, provide their own reviews, and receive recommendations based on their user profile as well as their past usage of the service.",
      "nos últimos anos, têm vindo a ganhar popularidade várias plataformas online que possibilitam aos utilizadores de serviços das mais variadas áreas de atividade económica (e.g. restaurantes, bares, lojas, centros comerciais, diversão noturna) avaliar o serviço recebido. desta forma, com base no agregado das opiniões disponibilizadas pela comunidade, o utilizador poderá tomar uma escolha mais informada no momento da decisão entre os vários serviços disponíveis. no entanto, uma das áreas onde este processo não está a ser amplamente utilizado prende-se com a área dos serviços médicos e de saúde. assim sendo, o objetivo deste trabalho é o desenvolvimento de uma solução inteligente centrada no utilizador, que permita o registo, avaliação e recomendação de instituições de saúde. neste trabalho, apresentamos uma plataforma chamada healthadvisor. esta plataforma é composta por uma interface de utilizar web, um sistema de recomendação que gera recomendações para o utilizador, e um serviço de backend que suporta estes components através da disponibilização de rest apis (representational state transfer application programming interfaces). através da utilização desta solução é possível ao utilizador procurar instituições de saúde, ler as avaliações a instituições feitas por outros utilizadores, submeter as suas próprias avaliações e obter recomendações baseadas no seu perfil de utilizar e histórico de utilização do sistema."
    ],
    [
      "com a evolução da tecnologia cada vez mais aplicações de software são desenvolvidas para correrem sob um browser de internet, sendo normalmente designadas de aplicações web. como método de assegurar a qualidade destas aplicações, os testes atraem cada vez mais a atenção das comunidades académica e empresarial. ter uma estratégia de testes bem definida desde o início do projeto, e executar os mesmos durante a fase de desenvolvimento, além de assegurar a qualidade do software, reduz o risco de surgirem problemas imprevistos numa fase posterior do projeto, que aumentam o custo de implementação e consequentemente diminuem a rentabilidade do projeto para a empresa fornecedora, e ainda provocam atrasos que normalmente resultam na insatisfação do cliente. por outro lado, as constantes evoluções do software, que o mercado altamente competitivo de hoje obriga, aumentam o risco de destruir funcionalidades implementadas anteriormente (chamada regressão). por este motivo, é necessária que a estratégia de testes seja não só definida e implementada durante o projeto, mas também após o seu fecho. a repetição dos testes que cobrem todo o software em cada momento de entrega, são um desafio para as organizações, na medida em que o seu custo é elevado, e dado que existe a possibilidade de falha humana. por todos estes motivos, os responsáveis pela equipa de desenvolvimento web da eurotux apoiaram este trabalho de investigação e permitiram que o mesmo fosse aplicado em projetos reais desenvolvidos ao longo dos últimos meses, com o objetivo de ser definida uma estratégia de testes que permitisse tornar o processo de desenvolvimento web mais robusto, aumentando a qualidade das soluções desenvolvidas. pretendia-se essencialmente definir uma estratégia de testes automáticos que diminuísse a possibilidade de erro humano na execução dos testes, e que permitisse aumentar a rentabilidade dos projetos gastando menos horas na fase de verificação. foram assim estudadas várias metodologias, tendo sido os testes funcionais que asseguram a concordância da parte funcional do software face às suas especificações, e os testes de regressão que garantem que o novo código não introduz erros, o foco da presente dissertação. foram ainda realizados alguns estudos comparativos de ferramentas, tendo sido escolhido o selenium para definição de testes automáticos em conjunto com a ferramenta alloy analyser para avaliação da qualidade da especificação. após a escolha das ferramentas que mais se adequaram aos objetivos propostos neste estudo, foi desenvolvida uma ferramenta denominadawebtest que permite a programação automática de testes funcionais e produz relatórios que são enviados por email, permitindo à equipa de desenvolvimento web acompanhar o nível de qualidade do seu software. a webtest foi utilizada em projetos reais da empresa, e o feedback por parte da equipa e dos seus responsáveis foi bastante positiva. com a webtest foi possível reduzir o tempo gasto com os testes nos projetos e garantir a qualidade da solução apesar das alterações de requisitos e incremento de funcionalidades no software.",
      "with the evolution of technology more and more software applications are designed to run in a web browser, being commonly known as web applications. as a method to ensure the quality of these applications, the tests are attracting today more and more attention from both the academic and business communities. having a well-defined test strategy from the beginning of the project and applying them during the development phase, ensures the quality of the final software by reducing the risk of unforeseen problems at later stages. such problems translate into increased cost of implementation and reduced profitability of the project to the supplier. it may also postpone the projet delivery which result in customer dissatisfaction. on the other hand, the constant software upgrades, imposed by the highly competitive market of today, increase the risk of destroying previously implemented features (called regression). for this reason, it is required that the testing strategy is not only defined and implemented during the design, but also after each step of development in order to ensure that the development does not breaks already existing functionality. repeated tests, covering all the software at each delivery, is a challenge to organizations due to its high cost and increased possibility of human error. for all these reasons, the web development team supervisors at eurotux supported this research work and allowed the resulting prototype to be applied in real projects, developed over the last few months, thus defining a testing strategy aiming to increase customer satisfaction, profitability of the projects and the motivation of the team itself. several methodologies were studied to allow automate testing of web applications. automated testing ensures that the software is in accordance with all the defined specifications, maintaining the confidence of all stakeholders in the expected software quality throughout its life cycle. some comparative studies were evaluated, being analysed in detail the methodologies for functional testing and regression testing, selenium tools for defining automatic testing integrated with the alloy analyser tool, in order to evaluate the quality of the formal specification. after such analysis, a prototype tool namedwebtest was developed in order to build and run automated tests. a report containing the generated test results is then sent to the web application development team, to help keeping track of existing faults in software quality. lastly the prototype was used in several production projects at eurotux, receiving highly positive reviews from both developers and managers, leveraging the time spent testing software after each consecutive update."
    ],
    0.06666666666666667
  ],
  [
    [
      "numa era em que tudo está interligado, as organizações que mais antecipadamente adotam estraté-gias de conexão entre os seus produtos, dados e consumidores finais, são aquelas que frequentemente conseguem obter uma vantagem competitiva no mercado. a sonae mc, como presença dominante no mercado português, adotou uma nova evolução de uma arquitetura orientada a serviços, de modo a manter esse destaque. api-led connectivity visa estabelecer a conexão acima referida, com recurso às apis previamente existentes, cada uma com um único propósito, facilitando a sua reutilização e modularidade da arquite-tura. este conceito é simbólico da transformação digital na sonae mc e surge agora a oportunidade de estruturar, documentar e divulgar todas as apis existentes no seu portefólio. sendo assim, esta dissertação pretende expor um modo mais fácil e autónomo de implementação e manutenção, tanto de novos projetos, como daqueles já existentes dentro da empresa. isto será atingível através de um reforço da governação e visibilidade dos ativos digitais da organização, que terão como face um novo portal de apis. a dissertação acompanha a criação deste portal e descreve como todos os conceitos envolvidos dão origem a uma nova e mais acessível forma de adoção de apis pelas equipas que delas necessitam. neste processo de criação, são abordadas diversas fases do desenvolvimento do software, nomeadamente o levantamento e modelação dos seus casos de uso e a sua análise, conceção e implementação. é provado que é de facto vantajoso para uma empresa fornecedora de apis ter um portal para as apresentar e, por fim, são enumeradas formas de como comprovar e aumentar estas vantagens para o caso da sonae mc.",
      "in an era when everything is interconnected, the organizations that most early adopt connection strate-gies between their products, data and final consumers, are the ones that often obtain a competitive ad-vantage in the market. sonae mc, as a dominant presence in the portuguese market, has adopted a new evolution of service oriented architecture, in order to maintain this prominence. api-led connectivity aims to establish the connection mentioned above, using previously existing apis, each with a unique purpose, facilitating its reuse and modularity of the architecture. this concept is a symbol of the digital transformation at sonae mc and now there is an opportunity to structure, document and disseminate all the apis existing in its portfolio. this dissertation intends to expose an easier and autonomous way of implementation and mainte-nance, both for new projects, and for those already existing within the company. it will be attainable through a strengthening of the governance and visibility of the digital assets of the organization, which will be represented via a new api portal. the dissertation follows the creation of this portal and describes how all concepts involved result in a new and more accessible way of adopting apis by the teams that need them. several phases of the software development are addressed in the process, namely the elicitation and modelling of its use cases and its analysis, design and implementation. it is proved that it is indeed advantageous for an api provider to have a portal to present them. finally, some ways of how to prove and increase these advantages are enumerated for sonae mc's case."
    ],
    [
      "o tema da gestão do risco começou a ser debatido aquando da publicação do relatório do instituto de medicina, em 2000 “to err is human: building a safer health system”, que captou a atenção da população mundial, ao revelar que 44000 a 98000 pessoas morrem nos estados unidos da américa como consequência direta de erros nos cuidados de saúde. é a partir da última década que este tema passa a ser de extrema importância, e se começam a tomar medidas para o prevenir. as organizações de saúde voltaram as suas políticas para a gestão do risco, contudo, tem-se notado que estas têm grandes dificuldades a aprender com o erro. de forma a colmatar este problema é necessário registar todos os eventos adversos. no mínimo esses registos servem para identificar perigos, e minimizar a ocorrência do mesmo evento. o objetivo deste trabalho é o estudo e a concepção de um sistema de notificação e tratamento de eventos adversos integrável em sistemas de informação hospitalar existentes. além disso, dá a conhecer a importância que os sistemas de notificação de eventos adversos têm na sociedade atual, ao fornecer um estado de arte desta temática, e apresentando sistemas já utilizados, e com benefícios já reconhecidos. a avaliação do sistema desenvolvido foi efetuada através da sua integração na intranet de todos os hospitais do grupo trofa saúde, que no futuro se irá refletir como um indicador de qualidade dos cuidados prestados nesta organização de saúde.",
      "the topic of risk management began to be debated in the publication of a report by the institute of medicine in 2000 “to err is human: building a safer health system\", which caught the attention of the world population, revealing that 44,000-98,000 people die in the united states as a direct result of errors in health care. since the last decade, this issue has become extremely important, and steps began to be taken to prevent these errors. health organizations turned their policies to risk management. however, it has been noted that these organizations have great difficulties to learn from their mistakes. in order to overcome this problem it is necessary to record all adverse events. at a the least, these records are used to identify hazards and minimize the re-occurrence of an adverse event. the objective of this work is to study and design a notification and treatment system of adverse events, that can be integrated in existing hospital information systems. in addition, it shows the importance that adverse events reporting systems have on society today, by providing a state of art of this issue and presenting systems already in use, and which benefits have already been recognized. the evaluation of the developed system was carried out through integration with the intranet of all hospitals in the grupo trofa saúde, which, in the future, will reflect as an indicator of quality of care in this organization."
    ],
    0.0
  ],
  [
    [
      "in the past years, with the evolution of technology and the rise of the internet, personal health records appeared. these records are maintained by patients and turn them into a more active stakeholder in their own health management. they can be used to record medical parameters or give useful information to the patient, among others. however, this was not the only result from this evolution. with the internet, more medical information became available to everyone, most of it not from reliable sources, which brought additional problems. the obstetrics field was also impacted by these changes. this, combined with fact that a pregnancy is a delicate medical condition, brought the necessity to find solutions for this case. this dissertation aims to develop a personal health record for pregnant women that provides reliable information, and that has a set of features they find useful. with this goal in mind, a literature review on the technologies and methodolo gies that are used nowadays and on the use of technology by pregnant women is made. then, all the development process is presented, as well as the final result. this process was supervised by a medical institution, which had the advantage of facilitating the process of having feedback from pregnant women and it also provided all the medical information displayed in the application. the final result is an application that has a wide range of useful features, pro vides trustworthy information, is available in all devices and that was developed using modern technologies.",
      "nos últimos anos, com a evolução das tecnologias e o aparecimento da internet, surgiram personal health records. estes registos são mantidos pelos pacientes e tornam-os uma parte mais ativa na gestão da sua saúde. eles podem ser usados para registar parâmetros médicos ou dar informações úteis ao paciente, entre outros. no entanto, este não foi o único resultado desta evolução. com a internet, mais informação médica ficou disponível para todas as pessoas, sendo que a maioria não provém de fontes de confiança, o que trouxe problemas adicionais. a área da obstetrícia também foi influenciada por estas mudanças. isto, combinado com o facto de uma gravidez ser uma condição médica delicada, trouxe a necessidade de se encontrar novas soluções para este caso. esta dissertação pretende desenvolver um personal health record para grávidas que dê informação de confiança, e que tenha um conjunto de funcionalidades que elas considerem úteis. com este objetivo em mente, é feita uma revisão da literatura sobre as tecnologias e metodologias usadas atualmente e sobre o uso das tecnologias pelas grávidas. depois, todo o processo de desenvolvimento é apresentado, assim como o resultado final. este processo foi supervisionado por uma instituição de saúde, o que teve a vantagem de facilitar o processo de recolha de opiniões de grávidas e também de fornecer toda a informação médica disponibilizada na aplicação. o resultado final é uma aplicação que apresenta um grande conjunto de funcionalidades úteis, que da informação de confiança, está disponível em todos os dispositivos e que foi desenvolvida usando tecnologias modernas."
    ],
    [
      "os sistemas de informação representam uma parte importante dentro das organizações e o seu papel, para o fornecimento de informação a tempo e onde é necessária, é crítico. nos últimos anos, o recurso a estes sistemas tem aumentado exponencialmente, sendo possível encontra-los em praticamente todas as áreas da ação humana. a área da saúde não é alheia a esta tendência e tem-se observado um aumento bastante acentuado na quantidade de dados processados e armazenados. assegurar que a informação essencial para o correto funcionamento da instituição está disponível e tem um elevado grau de qualidade são missões da informática médica. as ferramentas de business intelligence entram nesta área com uma elevada relevância, pois permitem a análise de dados e da informação que estes potencialmente representam, através do recurso a diversas metodologias, aplicações e tecnologias. o registo clínico electrónico, para os dados do projeto “cirurgia segura salva vidas”, pode ser potencialmente vital para a qualidade e segurança do paciente no bloco operatório, uma vez que este define etapas e passos a serem realizados pelos profissionais envolvidos com o objetivo de reduzir o número de eventos adversos. avaliar a qualidade da informação deste registo torna-se, por isso, particularmente interessante, principalmente quando se ponderam as potenciais consequências que a informação extraída deste registo pode ter na instituição. os indicadores utilizados permitiram retirar conclusões sobre a qualidade da informação encontrada nos registo da cirurgia segura no chaa e considera-se que a ferramenta de business intelligence, pentaho community, foi aplicada com sucesso no sector da saúde. as necessidades e falhas de informação presentes no chaa foram identificadas.",
      "information systems represent a fundamental part of an organization e their part in providing information on time and where it is needed is critical. over the last few years, the use of these systems has increased exponentially and it is possible to find them in nearly every area of human intervention. healthcare is one of the areas in which this occurs. the amount of processed and stored data has increased by a lot. assuring that essential information is available and possesses an increased degree of quality is the role of medical informatics and it is crucial for the correct functioning of an institution. business intelligence tools appear with a high relevance in this area because they allow the analysis of high amounts of data and the information they potentially represent by using different methodologies, applications and technologies. electronic health record in the project “safe surgery saves lives” can be potentially vital for the quality and safety of the operating room because it defines stages and steps that health professionals need to go through to try and reduce the number of adverse events. assessing the quality of information in this record becomes especially interesting given the potential impact that the extracted information can have in the organization. the used indicators enabled the assessment of the quality of information in the safe surgery records. the business intelligence tool, pentaho community, was successfully applied to the healthcare area and the needs and flaws of information in chaa were successfully identified."
    ],
    0.09999999999999999
  ],
  [
    [
      "quantum computing has the potential to provide better time complexities (than those achieved with classical computers) for challenging problems solved by classical comput ers or even provide a solution for problems out of reach of classical computers in terms of time complexity (where the time consumed for the resolution of the problem is not prac tical, e.g. thousands of years). here we’re not solving an unsolvable problem but trying to improve its time complexity. there are several problems in rendering that are good can didates to being solved in a quantum fashion. although previous research has proposed of theoretical ways of doing this, here we present a practical solution. this work takes a first step in applying quantum computing to one of the most fundamental operations in rendering: ray casting. this technique allows computing visibility between two points in a 3d model of the world which is described by a collection of geometric primitives. the algorithm returns, for a given ray, which primitive it intersects closest to its origin. without a spatial acceleration structure, the complexity for this operation is o(n). the main goal of this work is to use the grover’s algorithm, a quantum search algorithm based on ampli tude amplification, to improve the complexity of this problem. this algorithm provides a quadratic speed up allowing for visibility evaluation for unstructured primitives in o( √ n) steps. due to technological limitations associated with current quantum computers we had to simplify our problem’s structure and in this work the geometrical setup is limited to rectangles and parallel rays (orthographic projection).",
      "a computação quântica tem o potencial de proporcionar melhores complexidades temporais, do que aquelas alcançadas por computadores clássicos, para problemas exigentes resolvidos por estes ou até proporcionar uma solução para problemas fora do alcance dos mesmos em termos de tempo consumido (onde o tempo necessário para a resolução do problema não é prático, como por exemplo milhares de anos). não estamos a tentar resolver um problema sem solução, mas sim a tentar melhorar a sua complexidade. existem vários problemas de renderização que são bons candidatos para serem resolvidos de forma quântica. apesar de existirem algumas propostas teóricas para alcançar isso mesmo, aqui é apresentada uma solução prática. este trabalho dá um primeiro passo em aplicar a computação quântica a uma das operações mais fundamentais da renderização: ray casting. esta técnica permite computar visibilidade entre z pontos num modelo 3d do mundo que é descrito por um conjunto de primitivas. o algoritmo retoma, para um dado raio, qual a primitiva que este interseta mais próxima da sua origem. sem uma estrutura espacial de aceleração, a complexidade desta operação é 0(n). o principal objetivo desta dissertação é usar o algoritmo de grover, um algoritmo quântico de procura, para melhorar a complexidade deste problema. o algoritmo permite uma aceleração quadrática possibilitando uma avaliação de visibilidade para primitivas não estruturadas em o(v) passos. devido a limitações tecnológicas associadas com os atuais computadores quânticos, tivemos a necessidade de simplificar a estrutura do nosso problema e, neste trabalho, a configuração geométrica é limitada a retângulos e raios paralelos (projeção ortográfica)."
    ],
    [
      "actualmente, uma larga variedade de produtos tais como antibióticos, proteínas, vacinas e outros compostos químicos são produzidos através de processos fermentativos. devido à subida dos preços do petróleo e aos fortes incentivos por parte das instituições para substituir os produtos derivados de petróleo por “produtos verdes”, muitos dos processos tradicionais têm vindo a ser substituídos por bioprocessos. consequentemente, tem existido um esforço para melhorar a produtividade dos processos biológicos. a optimização destes processos pode ser realizada em duas etapas: primeiramente, faz-se uma selecção e uma melhoria genética do microrganismo e num segundo passo são identificadas as melhores condições para realizar o processo fermentativo. nesta etapa, normalmente são realizados estudos experimentais através de tentativa-erro para obter as condições ambientais que propiciem o melhor crescimento e produtividade do microrganismo, manipulando as concentrações iniciais dos nutrientes, os perfis de alimentação de substrato ao reactor, os modos de operação, bem como a temperatura e o ph. nos últimos anos, têm sido desenvolvidas várias ferramentas informáticas para simulação e optimização de bioprocessos. porém, a maioria destas ferramentas está direccionada para estudar as vias metabólicas de um microrganismo de modo a optimizar a produtividade de determinado produto. numa fase posterior, é efectuada uma optimização genética do microrganismo. apesar de existir uma grande variedade de ferramentas informáticas verifica-se que nenhuma delas está desenhada especificamente para a optimização e simulação de processos fermentativos. assim, o objectivo deste trabalho foi desenvolver de raiz uma ferramenta direccionada para simulação, optimização e estimação de parâmetros de processos fermentativos. a aplicação optferm foi desenvolvida sobre uma plataforma denominada aibench, tendo-se utilizado a linguagem java como linguagem de programação. o optferm foi então desenvolvido de modo a ser uma ferramenta de fácil uso, extensível e que pudesse funcionar em qualquer sistema operativo, estando disponível como software livre em http://darwin.di.uminho.pt/optferm/. a aplicação foi desenhada de modo a que o utilizador pudesse realizar várias tarefas de simulação, optimização e estimação de parâmetros com diferentes condições no que se refere a variáveis de estado, parâmetros, perfis de alimentação, etc.. as tarefas de optimização foram focadas na determinação do melhor perfil de alimentação de uma corrente de substrato a alimentar ao reactor, dos melhores valores das variáveis de estado para iniciar uma fermentação e do tempo óptimo de duração para uma fermentação. foram realizados alguns estudos de optimização e estimação de parâmetros com o objectivo de verificar se a aplicação era suficientemente robusta. os estudos foram baseados na repetição das experiências por 30 vezes para obter significância estatística. após os estudos, verificou-se que as operações foram realizadas levando a resultados coerentes, não tendo sido detectados erros relevantes.",
      "nowadays, several products such as antibiotics, proteins, vaccines, aminoacids and other chemicals are produced using fermentation processes. due to the rise of petroleum prices and the strong incentive to replace petroleum derivatives by “green products”, many traditional processes have been replaced by new biotechnological ones. consequently, an effort to improve biotechnological techniques has been undertaken. in order to optimize the productivity of a biological process, in the majority of the cases, two different steps have to be addressed: firstly, a selection and genetic improvement of the microbial strain is accomplished; in a second step, the best conditions for the fermentation process are identified, such as the initial nutrient concentrations, operating modes, feeding profiles for fed-batch fermentations, temperature and ph. over the last few years, several tools have been developed for the simulation and optimization of biological processes. however, the majority of these tools was designed specifically to study metabolic pathways with the aim of increasing the productivity of a certain product. in a subsequent stage, a genetic optimization of the organism is conducted. although there are several tools available to study simulate and optimize cellular pathways, there is still a clear lack of specific tools to perform the optimization of fermentation processes. therefore, the aim of this work was to develop a specific computational tool to perform simulation and optimization of fermentation processes and estimation of unknown parameters. the optferm software was developed using the java programming language, with the aim of being a user-friendly, extensible and platform-independent computational tool. optferm is freely available in http://darwin.di.uminho.pt/optferm/. the tool was designed in order to allow the user to evaluate and compare several different methods for the tasks of simulation, optimization and parameter estimation, in the context of fermentation processes. the aim is to allow users to improve process productivity, achieving better results in reduced times. the optimization tasks available include the optimization of a substrate feeding trajectory, of the feeding trajectory plus initial conditions or of the feeding trajectory plus the final fermentation time. after developing the optferm tool, some studies on optimization and parameter estimation were performed, with the aim of verifying if the tool is sufficiently robust. the studies were based on repeating 30 times each type of experiment. it was verified that the operations were performed with coherent results and no relevant errors have been detected."
    ],
    0.0
  ],
  [
    [
      "a healthcare institution produces huge amounts of data on a daily basis. these data come from n sources and therefore have m different formats. as it is evident, this heterogeneity of information sources can be a huge obstacle, since increases the need for this information to be available and shared among the various information systems. the exchange of clinical information between health information system (his) is crucial for the effective provision of care, significantly improving the performance of the institutions. in the healthcare industry, the ability of different information and software systems to communicate and share data, as well as use the data exchanged, is called interoperability. this knowledge can be exchanged around the healthcare ecosystem thanks to the use of standards and data sharing models, regardless of the application being used. however, the lack of interoperability remains a concern. the agency for the integration, diffusion and archive (aida) proposes to achieve levels of interoperability never before implemented. for this purpose, web services are used for the processing, dissemination, and archiving of clinical information. in the context of this master’s dissertation, the aim is to develop and explore new information technology artefacts to help the administrative and accounting teams of the hospital da santa casa da misericórdia de vila verde. this solution aims to fill the existing gaps between the hospital and the assistência na doença aos servidores do estado (adse). the first gap occurs in the dentistry speciality, where it is not possible to verify the patient’s adse status to perform some dental medicine act under co-payment. the second gap occurs in the invoicing process through adse in real-time, in order to perform some medical appointments and/or acts resulting from them since the hospital’s accounting team cannot verify if a patient complies with the adse requirements. the gaps identified can make adse refuse to reimburse the hospital for the medical acts performed, which makes the administrative and accounting work unpleasant. in this situation, the hospital will have to find a solution that suits the patient and the adse. very often, the hospital has to bear the costs. in order to overpass these problems with the validation and invoicing process of medical acts through adse, this project consists of two web applications that enable the hospital’s information systems to interoperate with the adse web service.",
      "diariamente, numa instituição de cuidados de saúde é produzida uma grande quantidade de dados. esses dados derivam de n fontes e, consequentemente, têm m formatos diferentes. como é evidente, esta hetero geneidade de fontes de informação pode ser um grande obstáculo, uma vez que aumenta a necessidade de essa informação ter de estar disponível e ser partilhada entre vários sistemas de informação. a troca de in formação clínica entre sistemas de informação na saúde é crucial para uma prestação eficaz dos cuidados de saúde, aumentando significativamente o desempenho das instituições. na área da saúde, a capacidade de diferentes sistemas de informação e de diferentes sistemas informáticos para comunicar e partilhar dados, assim como de utilizar esses dados trocados, é chamada de interoperabili dade. este conhecimento pode ser trocado em todo o ecossistema de saúde graças ao uso de padrões e de modelos de partilha de dados, independentemente da aplicação que está a ser utilizada. contudo, a falta de interoperabilidade continua a ser preocupante. a agency for the integration, diffusion and archive (aida) propôs atingir níveis de interoperabilidade nunca antes atingidos. para isso, são utilizados web services para processar, disseminar e arquivar informação clínica. no contexto desta dissertação, o objetivo consiste no desenvolvimento e exploração de novos artefactos da tecnologia da informação para ajudar as equipas de gestão e contabilidade do hospital da santa casa da mis ericórdia de vila verde. esta solução visa preencher as lacunas existentes entre o hospital e a assistência na doença aos servidores do estado (adse). a primeira falha ocorre na especialidade de medicina dentária, onde não é possível verificar a situação de um paciente na adse para praticar um ato odontológico sob compartici pação. a segunda lacuna ocorre no processo de faturação através da adse para consultas médicas e/ou atos resultantes destas, uma vez que a equipa da contabilidade do hospital não consegue verificar se o paciente cumpre com os requisitos da adse. estas lacunas podem fazer com que a adse se recuse a reembolsar o hos pital pelos atos efetuados, o que torna o trabalho da gestão e contabilidade do hospital desagradável. quanto isto acontece, o hospital tem de encontrar uma solução que satisfaça o paciente e a adse o que, na maioria das vezes, passa por ser o hospital a acarretar com os custos. de maneira a ultrapassar estes problemas com a validação e faturação dos atos médicos através da adse, este projeto abrange duas aplicações web que permitem a interoperabilidade entre os sistemas de informação do hospital e o web service da adse."
    ],
    [
      "as arquiteturas monolíticas estão, em grande parte, presentes na maioria das plataformas de e-commerce, o que leva a um processo de modificação mais complicado e entregas demoradas ao cliente, uma vez que não está preparada para trabalho em paralelo. a arquitetura de microsserviços veio proporcionar outra forma de desenvolvimento destas plataformas, permitindo o trabalho em simultâneo por diferentes equipas, produzindo novas entregas para o cliente de forma mais acelerada e segura. todavia, esta possui alguns desafios e complexidades, o que leva muitas vezes à escolha de uma arquitetura monolítica para o desenvolvimento da aplicação. a maioria das aplicações não são imutáveis, pois mesmo estando entregues ao cliente são sujeitas a modificações. esta necessidade de modificar a aplicação leva a preocupações acerca da rapidez com que as novas funcionalidades são entregues ao cliente. é preciso tomar decisões no início do desenvolvimento sobre que arquitetura seguir, de modo a tomar a decisão mais vantajosa. no caso de aplicações monolíticas a mudança para uma arquitetura de microsserviços facilita este aspeto, bem como muitos outros. contudo, esta separação pode-se tornar quase impossível se o monolítico não for bem preparado para uma eventual futura mudança. uma das maiores dificuldades numa migração de um monolítico para microsserviços, relaciona-se com a definição do que deve ser cada microsserviço e na comunicação entre estes. a migração deve partir da identificação de partes do código que possam ser isoladas sem ter muito impacto no resto do código. com o desenho de um diagrama de packages é possível obter uma visão sobre a estrutura do sistema, percebendo que componentes são mais fáceis e mais difíceis de extrair. deve-se começar por extrair aqueles que contém menos dependências, adquirindo as vantagens de uma migração incremental que permite que sejam reduzidos os erros efetuados, porém, pode haver situações em que se queira extrair um componente com mais dependências. é necessário compreender o porquê da migração para uma arquitetura de microsserviços. esta decisão não deve ser tomada apenas porque a arquitetura de microsserviços está em voga, mas sim por razões fundamentadas. dentro destas razões encontra-se a rapidez com que as mudanças são efetuadas e colocadas em produção, pois é mais fácil realizar modificações e voltar a instalar os microsserviços sem que toda a aplicação tenha que reiniciar. isto permite uma melhor estruturação da equipa, possibilitando que várias equipas possam trabalhar em simultâneo para a mesma aplicação, não prejudicando em nada outros microsserviço. outra razão é a necessidade de escalar os microsserviços independentemente, providenciando maior robustez, pois a falha de um serviço não leva à falha de toda a aplicação ou então pela escolha de tecnologia, podendo-se implementar os microsserviço com a tecnologia que seja mais adequada e eficiente.",
      "monolithic architectures are largely present in most e-commerce platforms, which leads to a more complicated modification process and time-consuming deliveries to the customer as it is not prepared for work in parallel. microservices architecture has provided another way of developing these platforms, allowing different teams to work simultaneously, producing new deliveries for the client in a faster and safer way. however, this has some challenges and complexities, which often leads to the adoption of a monolithic architecture for the development of the application. most applications are not immutable, as they are subject to modification even when delivered to the client. this need to modify the application leads to concerns about how quickly new functionality is delivered to the customer. it is necessary to make decisions at the beginning of the development about what type of architecture to choose, in order to make the most advantageous decision. in the case of monolithic applications switching to a microservices architecture facilitates this aspect, as well as many others. however, this separation can become almost impossible if the monolith is not well prepared for possible future change. one of the greatest difficulties in a migration from a monolithic to microservices is related to the definition of what each microservice should be and the change of communication between them. with this, the migration should start from the identification of parts of the code that can be isolated without having much impact on the rest of the code. by drawing a package diagram it is possible to gain insight into the structure of the system, realising which components are easier and more difficult to extract. you should start by extracting those modules which contain fewer dependencies, acquiring the advantages of an incremental migration that allows the reduction of errors made and their size, however, there may be situations in which you want to extract a component with more dependencies. it is necessary to understand why migration to a microservices architecture is necessary. this decision should not be made just because microservices architecture is in vogue, but for well-founded reasons. one of these reasons is the speed with which changes are made and put into production, as it is easier to make modifications and reinstall the microservices without having to restart the whole application. this allows a better structuring of the team, enabling several teams to work simultaneously for the same application, without harming other microservices. another reason is the need to scale microservices independently, providing greater robustness, as the failure of one microservice does not lead to the failure of the whole application, or by the choice of technology, being able to implement microservices with the technology that is most appropriate and efficient."
    ],
    0.3
  ],
  [
    [
      "this thesis was developed as part of a curricular internship at bosch car multimédia sa, in collaboration with the university of minho, more specifically, an exploratory research thesis aligned with an r&d project that is being developed internally and whose objective is to detect impacts on vehicles that cause damage based on data obtained through sensors, the usefulness of the work developed in this thesis and the project in which it is inserted, in a real context, would be to help vehicle rental companies and car-sharing services to better monitor the conditions of vehicles in their fleets, this would be achieved by placing a device in vehicles that continuously monitored their status, reducing the need for validation and human interaction after use, the main focus of this thesis was to explore how the fusion of information from different sensors could improve the decision-rnaking capabilities of a system whose purpose is to determine whether impacts on the exterior of a vehicle, captured with a set of sensors, resulted in damage, this conjugation of sensory information is known as sensor fusion. ft is a process of combining information from different homogeneous and heterogeneous sensors to obtain a better representation of what is being observed, the approach chosen to achieve this goal consisted of training a set of machine learning (ml) algorithms with two distinct datasets, one based only on one data source and the other multiple sources combined. each pair of models was further evaluated on unseen data, and their performances were compared based on the va lues obtained, based on the results obtained, it can be said that the application of sensor fusion allowed for better learning by the models, which led to greater robustness in data never seen before. of the four chosen algorithms, xgboost, random forest (rf), support vector machine (svm), and artificial neural network (ann), all had at least one of the evaluation metrics, the matthews correlation coefficient (mcc) and number of false positive (fp)s in the test set, superior in model-based fused data. of these, xgboost and ann stand out where the results were significantly better in both metrics,",
      "esta tese foi desenvolvida no âmbito de um estágio curricular na bosch car multimédia s.a, em colabo-ração com a universidade do minho. mais concretamente, uma tese de pesquisa exploratória alinhada com um projeto de p&d que está a ser desenvolvido internamente e cujo objetivo é detetar impactos em veículos que causam dano com base em dados obtidos através de sensores, a utilidade do trabalho desenvolvido nesta tese e do projeto no qual está inserido, num contexto real, seria auxiliar empresas de aluguer de veículos e serviços de car-sharing a melhor monitorizarem as condições dos veículos das suas frotas. isto seria conseguido colocando um dispositivo nos veículos que continuamente monitorizava o seu estado reduzindo a necessidade de validação e interação humana após a sua utlização, o foco principal desta tese foi explorar de que forma a fusão de informação proveniente de diferentes sensores conseguiria melhorar as capacidades de tomada de decisão de um sistema cujo propósito é determinar se impactos no exterior de um veiculo, capturados com um conjunto de sensores, resultaram em dano. esta conjugação de informação sensorial é conhecida por fusão sensorial. é um processo de combinar informação de diferentes sensores homogéneos e hetogénos para obter uma melhor representa-ção do que está a ser observado, a abordagem escolhida para alcançar este objetivo consistiu em treinar um conjunto de algoritmos de ml com dois datasets distintos, um baseado apenas numa fonte de dados e o outro várias fontes, cada par de modelos foi posteriormente avaliado em dados nunca antes vistos, e as suas performances foram comparadas com base nos valores obtidos, com base nos resultados obtidos, pode ser dito que a aplicação de fusão sensorial permitiu a uma melhor aprendizagem por parte dos modelos o que levou a uma maior robustez em dados não vistos. dos quatro algoritmos ecolhidos, xgboost, rf, svm e ann, todos tiveram pelo menos uma das métricas de avaliação, mcc e número de fps no dataset de teste, superior no modelo baseado fusão sensorial. destes, destancam-se o xgboost e ann onde os resultados foram significavente melhores em ambas as métricas,"
    ],
    [
      "os telemóveis com os quais andamos hoje em dia já não são utilizados apenas para chamadas e mensagens. em vez disso utilizamo-los também para ver vídeos, ler o jornal, ou até consultar a nossa conta bancária. dada esta grande variedade de utilizações, os dispositivos móveis têm tido um enorme avanço tecnológico, o que leva a ainda mais possibilidades. por isso, porque não utilizá-los também como uma carteira dos nossos documentos de identificação? esta dissertação tem como objetivo apresentar alguns dos projetos que estão a ser desenvolvidos na área da identificação pessoal, assim desenvolver uma alternativa possível aos documentos de identificação físicos com que estamos acostumados. esta alternativa tem o nome de documentos de identificação desmaterializados, que são no fundo, versões digitais dos documentos de identificação que já utilizamos. durante este trabalho, foi desenvolvido um protótipo de um sistema de documentos de identificação desmaterializados, de forma a mostrar que estes sistemas funcionam e são seguros. este tipo de sistema pode vir a ser utilizado por qualquer pessoa, para o armazenamento de todos os documentos de identificação em dispositivos móveis.",
      "the mobile phones we use in the current days are no longer used just for calling or messaging. instead, we also use them for watching videos, read the newspaper or even check our bank account. given this high variety of uses, mobile phones have been suffering a huge technological advance, which leads to even more possibilities. so why not use them as a wallet for our identification documents too? this dissertation aims to present some of the projects that are currently being developed in the area of personal identification, and also develop a possible alternative to the physical ids we are currently accustomed with. this alternative is named dematerialized identification documents. which are basically digital versions of the identification documents we currently use. during the project, a prototype of a dematerialized identification document was developed, to show that these systems are functional and secure. this type of system might be used in the future by anyone, to store all the identification documents on mobile devices."
    ],
    0.3
  ],
  [
    [
      "no presente trabalho foi desenvolvida uma solução de apoio a pessoas idosas, doentes ou com limitações físicas, baseada em técnicas de aprendizagem automática e visão por computador. os modelos de aprendizagem profunda utilizados resolvem problemas de classificação de imagens. para desenvolver a solução proposta foi utilizada a linguagem python e as bibliotecas tensorflow, mediapipe e opencv. antes de treinar os modelos de aprendizagem automática, foram aplicadas técnicas de pré processamento às imagens, para as preparar para os modelos classificadores. a solução final desenvolvida combina duas tarefas de classificação diferentes: a estimação da pose humana e o reconhecimento de expressões faciais. para estimar a pose humana, primeiro utilizou-se um algoritmo que identifica a posição das articulações do corpo, sendo estas posições posteriormente classificadas com uma rede neuronal convolucional em três classes, queda, sentado e em pé/a andar. para efetuar o reconhecimento de expressões faciais utilizaram-se dois tipos de dados, os atributos de cor dos pixeis das imagens e os pontos de referência das faces previamente identificadas. estes dados foram depois classificados por uma rede neuronal híbrida, que inclui uma rede completamente ligada a uma rede convolucional. a solução final proposta combina estes dois modelos, o que permite a partir de uma imagem de uma pessoa, gerar um aviso se o modelo de estimação da pose detetar uma queda ou quando o modelo de reconhecimento de expressões faciais identificar uma expressão de dor. o modelo de estimação da pose identificou a classe queda com uma precisão de 97%, um recall de 98% e uma acurácia de 97%. a expressão de dor foi identificada pelo modelo de reconhecimento de expressões faciais com uma precisão de 82%, um recall de 86% e uma acurácia de 92%. o maior desafio no reconhecimento da expressão foi a deteção da face nas imagens.",
      "in the present work, a support solution for elderly, sick or physically limited people was developed, based on machine learning and computer vision techniques. the deep learning models used solve image classification problems. in order to develop the proposed solution, we used the python programming language and the tensorflow, mediapipe, and opencv libraries. before training the machine learning models, preprocessing techniques were applied to the images to prepare them for the classification models. the final solution developed combines two different classification tasks: human pose estimation and facial expression recognition. to estimate the human pose, we first applied an algorithm to identify the position of the body’s joints, and these positions were subsequently classified with a convolutional neural network into three classes, falling, sitting and standing/walking. to recognize facial expressions, two types of data were used: the color attributes of the image’s pixels and the reference points of previously identified faces. this data was then classified by a hybrid neural network, which includes a fully connected network and a convolutional network. the proposed solution combines these two models, which allows to generate a warning if the pose estimation model detects a fall or when the facial expression recognition model identifies an expression of pain, and does this from just one image of a person. the pose estimation model identified the falling class with a precision of 97%, a recall of 98%, and an accuracy of 97%. the pain expression was identified by the facial expression recognition model with a precision of 82%, a recall of 86%, and an accuracy of 92%. the biggest challenge in expression recognition was detecting the face in the images."
    ],
    [
      "nowadays, there are a lot of information services, many of them making their data available through apis (some free, others not) so that users can use the data at their own way. services related to the weather informa tion area are an example of this type of available services, which makes it possible to query weather information (such as temperature, humidity, rain, wind, etc.) in real time from the moment when the information provided by measuring stations (domestic or professional) can be accessed. the aim of this project is to research and work on aspects of application integration of this kind of information resorting to the development of an application that allows users to access information (obtained from various sources) about, for example, the meteorology of a given location of choice, by combining official sources with other sources, starting with the use of the infrastructure provided by netatmo as a test bed example. given the context of using multiple data sources, it is critical to study and develop an architecture that meets the expectations associated with service-based architectural environment. in addition, to establish this proof of concept, the system should be scalable and comply with a set of quality of service (qos) parameters established initially, such as the use of message brokers and caching.",
      "atualmente existe uma grande quantidade de serviços de informação, muitos deles disponibilizando os seus dados através de apis (sejam gratuitas ou não) de forma a que os utilizadores possam usar os dados à sua livre vontade. serviços ligados à área de informações meteorológicas são um exemplo deste tipo de serviços disponíveis, e que tornam assim possível a consulta de dados ligados à meteorologia (como temperatura, humi dade, pluviosidade, intensidade do vento, entre outros) em tempo real a partir do momento em que se consegue aceder à informação disponibilizada por estações de medição (domésticas ou profissionais). assim, este tipo de sistemas recolhem muita informação e originam um grande volume de dados, que podem ser utilizados e tratados de diversos modos de forma a construir novas aplicações. com isto, o objetivo deste projeto passa pela investigação e trabalho em aspetos de integração aplicacional destas informações recorrendo ao desenvolvimento de uma aplicação que permita aos utilizadores obter informações meteorológicas (obtidas a partir de diversas fontes). um exemplo de uma informação pode ser a meteorologia de um determinado local à escolha. para conseguir isto, é esperada então uma combinação de fontes oficiais com outras existentes, tendo como ponto inicial a utilização da infraestrutura disponibilizada pela netatmo como um exemplo de prova de conceito. tendo em conta o contexto de utilização de várias fontes de dados, torna-se fundamental estudar e perceber qual a melhor maneira de definir uma arquitetura a implementar que vá de encontro às expectativas a alcançar, sendo esperado um ambiente arquitetural baseado em serviços. para além disto, para estabelecer esta prova de conceito é ainda definido como objetivo que a aplicação construída seja escalável e que permita responder com parâmetros de quality of service (qos) definidos inicialmente, tais como a utilização de mecanismos de message brokers e de caching."
    ],
    0.3
  ],
  [
    [
      "the development and testing of ubiquitous environments (places enhanced with sensors, public displays and personal devices) usually presents high costs, both due to the need to acquire specific hardware (sensors, displays, etc.), and the need to use, or even to build, a space wherein the physical system will be implemented. consider, for example, the impact of testing a new ambient intelligence system to provide information in a hospital or in an airport. it is hardly feasible trying to prototype the system in the target environment due to the costs (e.g. of redesign) and problems associated with such approach. the use of three-dimensional virtual environments then arises as a solution to this problem. using them, it becomes possible to simulate the use of technology without needing to purchase hardware, and without interfering with the physical environments in which the final system will be installed. three-dimensional application servers such as secondlife (secondlife.com) or opensimulator (opensimulator.org) provide an easy way to develop virtual worlds. a platform for the prototyping of ubiquitous environments is being developed at the department of informatics of the university of minho, which is based on opensimulator: the apex (rapid prototyping for user experience) framework. at the moment, each new world has to be modelled manually, using an opensimulator compatible viewer, which makes this part of the process time-consuming and inefficient. this project’s objective is to study three-dimensional virtual environment modelling approaches, and to develop a module that integrates one of these approaches in the apex framework to streamline the virtual worlds generation process. the tool developed is presented in this dissertation. it has reduced significantly the environment generation development time in the apex framework. moreover, a case study was developed during the project where the tool was used to build the environment. despite the tool has been developed to meet the needs of the apex platform, it can be quite useful for other opensimulator users.",
      "o desenvolvimento e teste de ambientes ubíquos (locais enriquecidos com sensores, ecrãs públicos e dispositivos pessoais) está normalmente associado a custos elevados, quer seja pela necessidade de adquirir hardware específico (sensores, ecrãs, etc.), ou mesmo pela necessidade de usar, ou até construir um espaço onde o sistema ubíquo será implementado. considere, por exemplo, o impacto de testar um sistema inteligente de informação num hospital ou num aeroporto. é impraticável tentar prototipar o sistema no local destinado, devido aos custos (p.e. de redesenho) e dificuldades associadas. o uso de ambientes virtuais tridimensionais aparece como uma solução para este problema. utilizando este tipo de mecanismos, torna-se possível simular a instalação da tecnologia sem que seja necessário adquirir o hardware e sem interferir com o espaço físico onde o sistema final será instalado. os servidores aplicacionais 3d como o secondlife (secondlife.com) ou o open- simulator (opensimulator.org) proporcionam uma forma relativamente fácil de desenvolver mundos virtuais. está a ser desenvolvida no departamento de informática da universidade do minho uma plataforma de prototipagem de ambientes ubíquos, chamada apex (rapid prototyping for user experience) que se baseia no servidor aplicacional opensimulator. de momento, cada novo ambiente virtual tem de ser modelado manualmente, usando um viewer compatível com o open- simulator, o que torna o processo demorado e pouco eficiente. o objectivo deste projecto é estudar soluções para a modelação de ambientes virtuais tridimensionais, e desenvolver um módulo que integre uma dessas soluções na plataforma apex. a ferramenta desenvolvida é apresentada nesta dissertação. a ferramenta tem vindo a reduzir significativamente os tempos de desenvolvimento dos ambientes virtuais na platforma apex. para além disso foi desenvolvido um caso de estudo onde o módulo foi utilizado para gerar o ambiente. apesar da ferramenta ter sido desenvolvida para responder às necessidades da plataforma apex, ela pode ser bastante útil para outros utilizadores do opensimulator."
    ],
    [
      "ao longo dos últimos anos tem existido um elevado crescimento na utilização de dispositivos de comunicação sem fios, em que se destacam as redes de sensores sem fios, com elevado número de possíveis aplicações, nas mais diversas áreas, sendo cada vez mais uma realidade no quotidiano das pessoas e da indústria. as redes de sensores sem fios têm como principal objetivo o encaminhamento dos dados sensoriais recolhidos por determinados nós da rede para outros nós da rede (e.g., dados recolhidos pelos sensores de um dispositivo terminal e enviados para uma estação base). nesta dissertação, foi realizado um projeto que consiste no estudo, desenvolvimento e testes de uma rede sem fios zigbee, para controlar e monitorizar equipamentos em ambiente industrial. a implementação desta rede surge como um processo de substituição da rede cablada existente e dos dispositivos que controlam os equipamentos. esta implementação foi feita utilizando plataformas de hardware e software da texas instruments e o ide netbeans. numa fase inicial foram realizados testes para avaliação da taxa de erros na transmissão num ambiente industrial, de forma a poder viabilizar o projeto. foi desenvolvido código para dispositivos do tipo router zigbee com a capacidade de controlar equipamentos de refrigeração, recolher os dados dos sensores que lhe estão associados, e enviá-los pela rede até ao seu destino, diretamente ou por intermédio de outros routers. foi desenvolvido também um coordenador zigbee ligado a um pc que, através de uma interface gráfica com o utilizador, tem a capacidade de monitorizar e controlar todos os routers existentes na rede.",
      "over the past few years there has been a substantial increase in the use of wireless communication devices, in particular wireless sensor networks, with a wide range of possible applications in the most diversified areas, becoming a constant reality in people’s life and industry. wireless sensor networks are meant to forward the collected data from some network nodes to other nodes (e.g., data collected from the sensors of a terminal device and sent to a base station). in this dissertation, a project has been developed aiming at studying, develop and testing a zigbee wireless network, to control and monitor equipment in an industrial environment, replacing the existing cables and devices that control those equipments. the implementation was made using hardware and software platforms of texas instruments and the netbeans ide. in an early stage, evaluation tests of the transmission error rate were conducted in an industrial environment. zigbee router devices were developed with the capacity to control cooling equipments, collect data from the sensors associated with it, and send them through the network to their destination, directly or through other routers. a zigbee coordinator connected to a pc was also developed, enabling the monitoring and control of all existing routers in the network through a graphical user interface."
    ],
    0.0
  ],
  [
    [
      "imbalanced learning and small-sized datasets are present in machine learning problems, even with the increased data availability provided by recent developments. the performance of learning algorithms in the presence of unbalanced data and significant class distribution skews is known as the imbalanced learning problem. the models’ performance on such problems can drastically decrease for certain classes with an uneven distribution, because the models do not learn the distributive features of the data and present accuracy too favorable for a specific set of classes of data. this can have negative consequences when talking about cancer detection, for example, since the model may identify poorly unhealthy patients. hence, data augmentation techniques are usually conceived to evaluate how models would behave in nondata- scarce environments, generating synthetic data similar to real data. by applying those techniques, the amount of available data can be increased, balancing the class distributions. however, there is no standardized data augmentation process that can be applied to every domain of tabular data. therefore, this dissertation aims to identify which characteristics of a dataset provide a better performance when synthesizing samples by a data augmentation technique in a tabular data environment. moreover, if the data augmentation algorithm synthesizes more real samples, it is expected to increase the classifier’s performance as well. our results demonstrate that datasets whose features are mainly categorical have an associated difficulty in increasing the classifier results by adding new samples. furthermore, the technique that adapted best to those kinds of datasets was the more classical one, smote. as for the datasets with more continuous features, the variations of variational autoencoder, principally the vae with k-means and decay, as well as gan, demonstrated an increased capability when augmenting those kinds of datasets. this dissertation demonstrated that more categorical datasets could achieve better performance by including 25% synthetic samples, whereas continuous datasets could only do so by including minority samples.",
      "o desbalanceamento dos dados, juntamente com datasets de tamanho reduzido, estão presentes em muitos problemas de machine learning, apesar do aumento de recolha de dados atuais por consequência do desenvolvimento tecnológico. o desbalanceamento de dados é definido por uma diferença significativa na distribuição das suas classes dentro de um conjunto de dados. desta forma, a performance de um modelo pode diminuir drasticamente para certas classes com uma quantidade inferior de instâncias. isto deve-se ao modelo não aprender a distribuição dos atributos dos dados e apresenta uma performance demasiado focada na classe em maioria. este fenómeno compromete a performance dos modelos em problemas como por exemplo deteção de cancro em pacientes, uma vez que o modelo identifica poucos pacientes não saudáveis. assim, as técnicas de data augmentation podem colmatar este problema ao gerarem dados sintéticos similares aos reais, podendo simular um ambiente de aprendizagem sem escassez de dados para os modelos. com a aplicação destas técnicas, o número de dados disponíveis aumenta pelo que se consegue obter distribuições de classes mais equilibradas. contudo, não existe uma técnica comum de data augmentation que possa ser aplicada em qualquer domínio com bons resultados. desta forma, com esta dissertação pretende-se identificar quais características de um certo tipo de dataset beneficiam as diferentes técnicas para uma melhor performance na criação de dados sintéticos e, consequentemente, uma melhor performance dos modelos de machine learning. os resultados obtidos nesta dissertação demonstram que a adição de dados sintéticos a datasets, cujos atributos sejam na sua maioria categóricos, está associada a uma acrescida dificuldade em melhorar a performance dos classificadores. no entanto, a técnica que melhor se adaptava a estas características foi o smote, uma das técnicas mais clássicas de data augmentation. por outro lado, as variações do variational autoencoder, nomeadamente a que conjuga um decaimento na loss e o uso de k-means, e a gan geraram dados sintéticos capazes de melhorar a performance dos classificadores. para além disto, esta dissertação comprovou que a adição de mais 25% de dados sintéticos a um dataset maioritariamente categórico permitiria melhores resultados, enquanto num dataset com maior presença de atributos contínuos era beneficiada pela adição de apenas instâncias minoritárias."
    ],
    [
      "nesta dissertação propõe-se a realização de uma plataforma e ferramentas capaz de elevar informaticamente a modalidade em questão (atletismo). o objetivo desta dissertação é criar um ponto comum, capaz de coordenar diversas ferramentas existentes, com a inovação de aprimoramento de algumas destas. este projeto encontra-se com o apoio da fpa (federação portuguesa de atletismo), utilizando a aab (associação de atletismo de braga), como os primeiros utilizadores na fase de testes, extendendo gradualmente o número de intervenientes, até o sistema ser implementado a nível nacional. serão desenvolvidas 2 ferramentas distintas: • plataforma web • aplicação para concursos/provas fora da pista a primeira ferramenta será uma plataforma web, que terá a função de ser o ponto de controlo, terá acesso à base dados que contêm os atletas federados na fpa. esta deverá ser capaz de criar e tolerar todas as necessidades para a gestão de competição, desde a possibilidade para os clubes inscreverem os seus atletas nas competições até a geração dos comunicados de resultados e estatísticas das competições. numa fase final do projeto também deverá efetuar rankings dos melhores atletas por prova, assim possibilitando os atletas de visualizar o seu perfil, visualizando assim os seus resultados e melhores marcas. a segunda será uma aplicação desktop, com o objetivo de gerir os diversos concursos a ocorrer numa prova de pista ( lançamentos, saltos verticais e saltos horizontais), assim como provas de corta-mato e estrada fora da pista. esta aplicação também deverá ter funcionalidades específicas para que seja possível a utilização simultânea de placares electrónicos no local da prova e esteja integrada para possibilitar liveresults com a primeira ferramenta.",
      "in this dissertation we propose the realization of a platform and tools capable of elevating the modality in question (athletics). the objective of this dissertation is to create a common point, capable of coordinating several existing tools, with the innovation of improving some of these. this project is supported by fpa (portuguese athletics federation), using aab (braga athletics association), as the first users in the testing phase, gradually extending the number of stakeholders, until the system is implemented at national level. two different tools will be developed for this project: • web platform • application for off-track competitions / events the first tool will be a web platform, which will have the function of being the control point, will have access to the database containing the federated athletes in fpa. this should be able to create and tolerate all needs for competition management, from the possibility for clubs to enroll their athletes in competitions to the generation of reports of results and competition statistics. in a final phase of the project, you should also perform rankings of the best athletes per race, thus enabling athletes to view their profile, thus visualizing their results and best marks. the second will be a desktop application, with the objective of managing the various competitions taking place in a track event (launches, vertical jumps and horizontal jumps), as well as cross-country and off-track events. this application must also have specific functionalities so that it is possible to use electronic scoreboards simultaneously at the race site and be integrated to enable live results with the first tool."
    ],
    0.3
  ],
  [
    [
      "maps and geographic information system (gis) play a major role in modern society, particularly on tourism, navigation and personal guidance. however, providing geographical information of interest related to individual queries remains a strenuous task. the main constraints are (1) the several information scales available, (2) the large amount of information available on each scale, and (3) difficulty in directly infer a meaningful geographical context from text, pictures, or diagrams that are used by most user-aiding systems. to that extent, and to overcome the aforementioned difficulties, we develop a solution which allows the overlap of visual information over the maps being queried — a method commonly referred to as augmented reality (ar). with that in mind, the object of this dissertation is the research and implementation of a method for the delivery of visual cartographic information over physical (analogue) and digital two-dimensional (2d) maps utilizing ar. we review existing state-of-art solutions and outline their limitations across different use cases. afterwards, we provide a generic modular solution for a multitude of real-life applications, to name a few: museums, fairs, expositions, and public street maps. during the development phase, we take into consideration the trade-off between speed and accuracy in order to develop an accurate and real-time solution. finally, we demonstrate the feasibility of our methods with an application on a real use case based on a map of the city of oporto, in portugal.",
      "mapas e sistema de informação geográfica (gis) desempenham um papel importante na sociedade, particularmente no turismo, navegação e orientação pessoal. no entanto, fornecer informações geográficas de interesse a consultas dos utilizadores é uma tarefa árdua. os principais dificuldades são (1) as várias escalas de informações disponíveis, (2) a grande quantidade de informação disponível em cada escala e (3) dificuldade em inferir diretamente um contexto geográfico significativo a partir dos textos, figuras ou diagramas usados. assim, e para superar as dificuldades mencionadas, desenvolvemos uma solução que permite a sobreposição de informações visuais sobre os mapas que estão a ser consultados - um método geralmente conhecido como realidade aumentada (ar). neste sentido, o objetivo desta dissertação é a pesquisa e implementação de um método para a visualização de informações cartográficas sobre mapas 2d físicos (analógicos) e digitais utilizando ar. em primeiro lugar, analisamos o estado da arte juntamente com as soluções existentes e também as suas limitações nas diversas utilizações possíveis. posteriormente, fornecemos uma solução modular genérica para uma várias aplicações reais tais como: museus, feiras, exposições e mapas públicos de ruas. durante a fase de desenvolvimento, tivemos em consideração o compromisso entre velocidade e precisão, a fim de desenvolver uma solução precisa que funciona em tempo real. por fim, demonstramos a viabilidade de nossos métodos com uma aplicação num caso de uso real baseado num mapa da cidade do porto (portugal)."
    ],
    [
      "one of the major challenges that today’s society deals with the assurance of a sustainable future for present and coming generations. therefore, large cities and urban areas are becoming more committed to adopting new sustainability paradigms and are doing so by employing technological solutions. the increasing advancement of technology has made it possible to implement a number of innovative approaches that have a highly beneficial impact on both the environment and the administration of cities. this aspect enables the transition from conventional metropolises to smart cities. electric vehicles are among the most relevant and popular examples of technological innovations in recent years. the tran sition to a new electric transportation paradigm offers a viable approach to reducing humanity’s ecological footprint due to the low emissions of polluting gases and the substitution of fossil fuels with rechargeable lithium batteries. in light of this, this dissertation aims to optimize the placement of new charging stations in portuguese cities using various machine learning techniques. to do this, we can charge data was used to implement and evaluate multiple machine learning models in order to discover which one would determine more accurately the daily hours of usage of a potential charging station. overall, the random forest was the model with the best performance. additionally, artificial intelligence explainability techniques were implemented to identify the key factors influencing the charging stations usage. for the districts of portugal analysed, the number of times a charging station changes states daily, from being occupied by a vehicle to being free and vice-versa, was the most significant factor affecting the daily hours of usage of charging stations.",
      "um dos principais desafios que a sociedade enfrenta hoje é assegurar um futuro sustentável para as gerações presentes e vindouras. as grandes cidades e áreas urbanas estão cada vez mais comprometidas em adotar novos paradigmas de sustentabilidade, utilizando, para isso, soluções tecnológicas. o avanço da tecnologia tornou possível implementar diversas soluções inovadoras que têm um impacto altamente benéfico a nível ecológico e no planeamento e administração das cidades. este aspecto permite a transição das típicas metrópoles para cidades inteligentes. os veículos elétricos estão entre os exemplos mais relevantes e populares de inovações tecnológicas nos últimos anos. a transição para um novo paradigma de transporte elétrico são uma abordagem viável para reduzir a pegada ecológica da humanidade devido às baixas emissões de gases poluentes e à troca dos combustíveis fósseis por baterias de lítio recarregáveis. o objetivo desta dissertação é otimizar a instalação de novos pontos de carregamento em cidades portuguesas usando diversas técnicas de machine learning. para tal, os dados fornecidos pela we can charge foram usados para implementar e avaliar diversos modelos de machine learning de forma a determinar qual deles prevê mais corretamente as horas diárias de uso de um potencial ponto de carregamento. no geral, a random forest foi o modelo que obteve a melhor performance. técnicas de explicabilidade de inteligência artificial foram também implementadas para identificar os fatores chave que mais impactam o uso dos pontos de carregamento. para os distritos analisados, o número de vezes que ponto de carregamento muda de estado num dado dia, passando do estado de ocupado para livre e vice-versa, é o fator que mais influencia as horas diárias de uso dos pontos de carga."
    ],
    0.3
  ],
  [
    [
      "nas instituições de saúde, um dos serviços mais exigentes em termos de prestação de cuidados de saúde contínuos e atenção por parte dos profissionais, é o serviço de internamento hospitalar. aqui existe uma constante necessidade de informações atualizadas sobre o registo clínico eletrónico do paciente, através de sistemas de apoio ao ato clínico facilmente utilizáveis. neste contexto, a presente dissertação apresenta uma nova plataforma web para a monitorização diária de pacientes, projetada para ser usada por profissionais de saúde, especialmente enfermeiros. a aplicação é baseada em react, uma library open source de javascript, para criar user interfaces (ui). a nova plataforma é constituída por duas principais componentes: um quadro de enfermagem que contém informação de todos os pacientes internados, numa unidade de saúde, num determinado momento. este quadro baseia-se em indicadores, com o principal objetivo de alertar ações futuras (exames, análises, medicação, dietas, jejum e cirurgias), relativamente a cada paciente. a outra componente designa-se por registo clínico de internamento e contém todas as informações sobre sobre as ações supramenciadas. de notar que cada unidade de saúde deverá adaptar a plataforma às suas principais necessidades.",
      "hospital inpatient care compromises one of the most demanding services in health institutions when providing a careful and continuous healthcare assistance. such demands require constant update of the patients’ electronic health record allied with support systems responsible for monitoring their clinical information. in this context, this dissertation presents a new web platform for daily monitoring of patients, designed to be used by health professionals, especially nurses. the application is based on react, an open-source javascript library for building ui (user interfaces). the new platform consists of two main components: a nursing frame that contains information for all inpatients in a health unit. this component is based on indicators, with the main objective of alerting future actions (exams, analysis, medication, diets, fasting and surgeries), in relation to each patient. the other component is called a clinical internment registry and contains all the information about the above-mentioned actions. these actions can be passed or future, allowing the consultation of the history of a certain episode of hospitalization. it should be noted that the platform must adapt to each health unit and its main needs."
    ],
    [
      "atualmente a arquitetura na qual a internet se baseia assenta na ideia de comunicação fima-fim sempre disponível. é esperado que uma ligação entre a origem e o destino da informação esteja sempre disponível. contudo, existem situações onde tal não é possível, ou não é garantido. nessas situações é necessário ter uma abordagem diferente. foi com esse intuito que nasceram as redes tolerantes a atrasos (delay tolerant networks, dtn). neste tipo de redes, as ligações ponto a ponto são esporádicas, podendo acontecer em determinado momento, e no momento seguinte deixarem de estar disponíveis. por exemplo, numa rede em que os nós são móveis, quando dois nós se cruzam estabelecem ligação. no momento seguinte, afastam-se e a ligação perde-se. as dtns têm a capacidade de operar neste tipo de cenários, por exemplo. para além desta assunção de que existe sempre uma ligação fim-a-fim disponível, a internet, atualmente baseia-se também no conceito de ips, ou seja, endereço de origem e endereço de destino. a informação em si não é tida em conta. ultimamente tem sido tentada uma abordagem diferente. uma abordagem na qual é tido em conta o conteúdo das mensagens. um exemplo deste tipo de redes são as redes de dados nomeados (named data network, ndn). é aplicado o paradigma de produtor/consumidor, no qual existem dois tipos distintos de nós. o primeiro cria a informação e o segundo consome-a. um nó que produza determinado tipo de informação coloca-a na rede, os nós que tiverem interessados na mesma, devem então subscrevê-la. o principal objetivo desta dissertação é acoplar estes dois tipos de redes. mais precisamente tentar colocar uma abordagem semelhante às ndns em cenários onde se utilizam dtns. nomeadamente, é sugerida uma abordagem no que diz respeito à distribuição de dados nomeados em cenários onde as dtns operam. pretende-se melhorar um protocolo de encaminhamento probabilístico que encaminha mensagens tendo em conta encontros prévios. transportou-se para as redes de dados nomeados o paradigma das redes oportunistas de que se dois nós se encontram a probabilidade de se voltarem a encontrar aumenta. havia-se aplicado esse conceito no contacto com conteúdos, agora esse conceito é também aplicado no contacto de interesses. ou seja, os conteúdos são agora encaminhados consoante a probabilidade do nó recetor encontrar interessados no mesmo.",
      "nowadays the internet is based on the idea that end-to-end link is always available. it is expected that a link between the source and destination is always available. but there are some scenarios where that is not possible our at least is not granted. in these situations a different approach is necessary. for that reason the delay tolerant network (dtn) was born. in this kind of networks, links between nodes are exporadic, at some moment they are available, but after that they can no longer exist. for instance, in a mobile network, when nodes get physically close to each other they connect. in the next moment, one of the nodes move and the connection is lost. dtns have the capacity to work in this kind of scenarios, for example. besides this assumption, that an end-to-end link is always available, the internet is based in the ip concept, that is, source and destination addresses. the information itself is not taken into account. lately, a new approach has been tested. an approach where the content inside the messages is taken into account. one example of this kind of network is the named data networks (ndn). a publisher/subscriber paradigm is applied. there are two different types of nodes. the first type creates information and the second one consumes it. a node that creates information, makes it available to the network, the ones that are interested in that information, should subscribe it. the main goal of this thesis is to combine this two types of network. more precisely, try to apply a ndn approach to a dtn scenario. to do this, a new named content distribution in dtn scenarios approach is suggested. is intended to improve a probabilistic routing protocol that forwards messages based on previous encounters. the idea that if one node meets another node they are likely to meet again in the future, was transported from the oportunistic concept to the named data concept. that paradigm was first applied in contacts to contents, now it is also applied to interest contacts. that means, contents are now forwarded based on the other node’s probability to find interest in that content."
    ],
    0.06666666666666667
  ],
  [
    [
      "with the ever-increasing internet usage that is following the start of the new decade, the need to optimize this world-scale network of computers becomes a big priority in the technological sphere that has the number of users rising, as are the quality of service (qos) demands by applications in domains such as media streaming or virtual reality. in the face of rising traffic and stricter application demands, a better understand ing of how internet service providers (isps) should manage their assets is needed. an important concern regards to how applications utilize the underlying network infras tructure over which they reside. most of these applications act with little regard for isp preferences, as exemplified by their lack of care in achieving traffic locality during their operation, which would be a preferable feature for network administrators, and that could also improve application performance. however, even a best-effort attempt by applications to cooperate will hardly succeed if isp policies aren’t clearly commu nicated to them. therefore, a system to bridge layer interests has much potential in helping achieve a mutually beneficial scenario. the main focus of this thesis is the application-layer traffic optimization (alto) work ing group, which was formed by the internet engineering task force (ietf) to explore standardizations for network information retrieval. this group specified a request response protocol where authoritative entities provide resources containing network status information and administrative preferences. sharing of infrastructural insight is done with the intent of enabling a cooperative environment, between the network overlay and underlay, during application operations, to obtain better infrastructural re sourcefulness and the consequential minimization of the associated operational costs. this work gives an overview of the historical network tussle between applications and service providers, presents the alto working group’s project as a solution, im plements an extended system built upon their ideas, and finally verifies the developed system’s efficiency, in a simulation, when compared to classical alternatives.",
      "com o acrescido uso da internet que acompanha o início da nova década, a necessidade de otimizar esta rede global de computadores passa a ser uma grande prioridade na esfera tecnológica que vê o seu número de utilizadores a aumentar, assim como a exigência, por parte das aplicações, de novos padrões de qualidade de serviço (qos), como visto em domínios de transmissão de conteúdo multimédia em tempo real e em experiências de realidade virtual. face ao aumento de tráfego e aos padrões de exigência aplicacional mais restritos, é necessário melhor compreender como os fornecedores de serviços internet (isps) devem gerir os seus recursos. um ponto fulcral é como aplicações utilizam os seus recursos da rede, onde muitas destas não têm consideração pelas preferências dos isps, como exemplificado pela sua falta de esforço em localizar tráfego, onde o contrário seria preferível por administradores de rede e teria potencial para melhorar o desempenho aplicacional. uma tentativa de melhor esforço, por parte das aplicações, em resolver este problema, não será bem-sucedida se as preferências administrativas não forem claramente comunicadas. portanto, um sistema que sirva de ponte de comunicação entre camadas pode potenciar um cenário mutuamente benéfico. o foco principal desta tese é o grupo de trabalho application-layer traffic optimization (alto), que foi formado pelo internet engineering task force (ietf) para explorar estandardizações para recolha de informação da rede. este grupo especificou um protocolo onde entidades autoritárias disponibilizam recursos com informação de estado de rede, e preferências administrativas. a partilha de conhecimento infraestrutural é feita para possibilitar um ambiente cooperativo entre redes overlay e underlay, para uma mais eficiente utilização de recursos e a consequente minimização de custos operacionais. é pretendido dar uma visão da histórica disputa entre aplicações e isps, assim como apresentar o projeto do grupo de trabalho alto como solução, implementar e melhorar sobre as suas ideias, e finalmente verificar a eficiência do sistema numa simulação, quando comparado com alternativas clássicas."
    ],
    [
      "nowadays the success of a business is dependent on the ability to effectively integrate in an intricate network of entities that are connected by material and information flows, inventory management being one of the main concerns. these flows are characterized by decision-making processes that will vary depending on the environment, entities and business models in the network. so, these networks need a decision-making system capable of providing solutions that dictate the optimal way the network and its entities provide and collect inventory in order to reduce costs and maximize profit. in the context of this dissertation, the problem arises when there is a stock disruption in the network and outside entities can no longer answer the stores’ supply requests and these stores become the entities responsible for requesting and delivering products to each other. this problem is modeled as an inventory routing problem, since it encompasses inventory management and routing decisions. the main goal of the system can be described as maximizing the collection of products per travel distance, without causing stock-outs at any supplier, for the entire network. the problem at hand is an optimization problem. in order to solve this optimization problem, first, the structural characteristics and key aspects were identified and studied, followed by the mathematical conceptualization, which involved the definition of the objective function and the corresponding set of constraints. the mathematical formulation allows the problem to be translated into a specific and precise mathematical language, making it possible to evaluate solutions, by means of a fitness function, and apply optimization algorithms to solve the problem. these optimization algorithms can be approximate or exact methods and their suitability to the problem depends on many factors such as the size, structure and complexity of the problem. so, the choice of the optimization algorithms must be preceded by a careful analysis of the problem at hand and its characteristics. after this, in the implementation phase, two adaptations of the genetic algorithm, two adaptions of the simulated annealing algorithm, two adaptations of the tabu search algorithm were developed. additionaly, another algorithm responsible for generating reference solutions was also developed (dynamic2). in order to test and compare the developed optimization algorithms, three different sized scenarios were generated. each of these scenarios has a different amount of data associated with it, whether it be in the number of stores, types of products or number of requests. as to compare the results of the different instances of the algorithms fairly in each scenario, these were made to generate roughly the same number of solutions. in scenarios 1 and 3, all the optimization algorithms developed were successful in finding solutions with higher fitness values than the baseline dynamic2 solution. in scenario 2, due to time constraints and computational complexities, only the genetic algorithm and the genetic algorithm with elitism using an initial population consisting of solutions generated by the dynamic2 algorithm, managed to find solutions with higher fitness value than the baseline solution. in this scenario, the developed optimization algorithms were also tested using feasible solutions generated through random mechanisms as initial solutions. these instances also achieve solutions with improved fitness values when compared to their respective initial solutions or populations. furthermore, in scenarios 1 and 3, the genetic algorithm with an initial population consisting of feasible solutions generated through random mechanisms was the optimization algorithm that found the best solutions, with these solutions having fitness values 56.14% and 92.07% greater than the baseline dynamic2 solution’s, respectively. in scenario 2, the genetic algorithm with elitism, utilizing an initial population consisting of solutions generated by the mentioned dynamic2 algorithm, found the solution with the highest fitness value, being approximately 1.00% higher than the baseline solution.",
      "atualmente, o sucesso de um negócio depende da capacidade de se integrar de uma forma eficaz numa rede de entidades conectadas por fluxos de materiais e informações, sendo a gestão de inventário uma das principais preocupações. esses fluxos são caracterizados por processos de tomada de decisão que variam dependendo do ambiente, entidades e modelos de negócios na rede. estas redes precisam de um sistema de tomada de decisão capaz de fornecer soluções que otimizem a forma como a rede e as suas entidades fornecem e recolhem inventário para reduzir custos e maximizar lucros. no contexto desta dissertação, o problema surge quando há uma interrupção de stock na rede e as entidades externas já não conseguem responder às solicitações de abastecimento das lojas e essas lojas tornam-se as entidades responsáveis por solicitar e entregar produtos entre si. este problema é modelado como um inventory routing problem, pois engloba decisões de gestão de inventário e routing. o objetivo principal do sistema pode ser descrito como maximizar a recolha de produtos por distância percorrida, sem causar stock-outs nos fornecedores, para toda a rede. o problema em questão é um problema de otimização. para resolver este problema de otimização, primeiro foram identificadas e estudadas as características es truturantes do problema, seguido de uma conceptualização matemática do mesmo, através da definição da função objetivo e do conjunto de restrições associadas. a formulação matemática permite que o problema seja traduzido para uma linguagem específica e precisa, tornando possível avaliar e comparar soluções, através da função objetivo, e aplicar algoritmos de otimização para resolver o problema. esses algoritmos de otimização podem ser métodos aproximados ou exatos e sua adequação ao problema depende de muitos fatores, como tamanho, estrutura e complexidade do problema. então, a escolha dos algoritmos de otimização deve ser precedida por uma análise cuidadosa do problema em questão e suas características. posteriormente, na fase de implementação, foram desenvolvidas duas adaptações de algoritmos genéticos, duas de simulated annealing e duas de tabu search, juntamente com outro algoritmo, responsável por gerar soluções de referência (dynamic2). com o intuito de testar e comparar os algoritmos de otimização desenvolvidos, foram criados três cenários de tamanhos diferentes. cada um destes cenários possui diferentes quantidades de dados associados, seja em termos do número de lojas, tipos de produtos ou quantidade de pedidos. para garantir uma comparação justa entre as diferentes instâncias dos algoritmos em cada cenário, estes foram configurados para gerar aproximadamente o mesmo número de soluções. nos cenários 1 e 3, todos os algoritmos de otimização desenvolvidos tiveram sucesso em encontrar soluções com valores de fitness superiores à solução de referência do algoritmo dynamic2. no cenário 2, devido a limitações de tempo e complexidades computacionais, apenas o algoritmo genético e o algoritmo genético com elitismo, utilizando uma população inicial composta por soluções geradas pelo algoritmo dynamic2, conseguiram encontrar soluções com valores de fitness superiores à solução de referência. neste cenário, os algoritmos de otimização desenvolvidos foram também testados utilizando soluções possíveis geradas aleatoriamente como soluções iniciais. estes casos também alcançaram soluções com valores de fitness maiores quando comparadas às soluções iniciais ou populações respetivas. adicionalmente, nos cenários 1 e 3, o algoritmo genético com uma população inicial composta por soluções possíveis geradas aleatoriamente foi o algoritmo de otimização que encontrou as melhores soluções, sendo que estas soluções tinham valores de fitness superiores em 56.14% e 92.07%, respetivamente, à solução de referência do algoritmo dynamic2. no cenário 2, o algoritmo genético com elitismo, utilizando uma população inicial composta por soluções geradas pelo algoritmo dynamic2, encontrou a solução com o valor de fitness mais elevado, sendo este aproximadamente 1.00% superior à solução de referência."
    ],
    0.0
  ],
  [
    [
      "nos últimos anos os sistemas distribuídos têm sofrido um crescimento exponencial. estes sistemas, normalmente implementados na plataforma java, são compostos por um vasto conjunto de componentes de middleware, os quais desempenham várias tarefas de comunicação e de coordenação. esta tendência influencia a modelação e a arquitetura de novas aplicações cada vez mais complexas obrigando a um enorme esforço e a um custo elevado na avaliação do seu desempenho. a concorrência e a sua distribuição, bem como o facto de muitos problemas só se manifestarem pela grande escala em si, não permite que a sua avaliação seja feita com recurso a simples ferramentas que não tenham em conta estas características. avaliação realista e controlada de aplicações distribuídas é ainda hoje muito difícil de alcançar, especialmente em cenários de larga escala. modelos de simulação pura podem ser uma solução para este problema, mas criar modelos abstratos a partir de implementações reais nem sempre é possível ou mesmo desejável, sobretudo na fase de desenvolvimento na qual ainda podem não existir todos os componentes ou a sua funcionalidade estar incompleta. para colmatar esta falha, nesta dissertação é apresentada o minha, uma plataforma que permite uma avaliação realista das aplicações através da combinação de modelos abstratos de simulação e implementações reais num ambiente centralizado. esta plataforma combina a execução de código real sob análise, com modelos de simulação do ambiente envolvente, isto é, da rede e da aplicação. este sistema permite reproduzir as condições de um sistema em grande escala e através da manipulação de bytecode java, suporta componentes de middleware inalterados. a utilidade deste sistema é demonstrada aplicando-o ao ws4d, uma pilha que cumpre a especificação device profile for web services.",
      "in recent years, distributed systems have been su ering an exponential growth. these systems, typically implemented in java platform, are composed by a wide range of components of middleware, which perform several communication and coordination tasks. this trend influences the modelling and the architecture of the newest applications, which are increasing complexity and requiring an large e ort with high costs on the evaluation of their performance. concurrency and distribution, as well as the fact that many problems manifest only in large scale, would not allow doing an evaluation using simple tools which do not take in account these features. the realistic evaluation of distributed applications is still a dicult task, particularly for large scale scenarios. the use of simulation models can be a solution for this problem, but their creation based on real implementations can sometimes be impossible or undesirable, as the system can be incomplete and non functional. this problem can be solved with the minha platform, that allows a realistic evaluation of applications trough the combination of abstract models and the simulation of real implementations in a centralized environment. the main goal of this dissertation is the creation of a network model to be used by the minha platform. this model introduces new variables in the evaluation such as the needed time for message exchange, resulting in more accurate results. furthermore, it is presented a calibration method that improves the faithfulness of the model to the real environment. this allows the reproduction of a large scale system and through java bytecode manipulation it allows the usage of pre-existent middleware components. the usefulness of this system is demonstrated by applying it to ws4d, a stack that complies with the device profile for web services specification."
    ],
    [
      "neste momento, as aplicações móveis encontram-se cada vez mais presentes no quotidiano de cada individuo, permitindo desempenhar diferentes tarefas, tais como gerir contas bancárias e transação de fundos monetários. devido à rápida adoção e desenvolvimento de iot, é também importante garantir a proteção da segurança dos utilizadores de ataques cibernauticos impedindo o acesso destas contas e execução de determinadas operações não autorizadas pelos respetivos títulares de contas via aplicações de home-banking. tendo isto em consideração, esta dissertação tem como principal objetivo analisar e integrar uma camada de segurança para autenticação e validação de transações de fundos, designada por trustfactor, numa aplicação existente de home-banking. visto que a implementação será vocacionada para dispositivos móveis serão abordados temas relacionados com os paradigmas de desenvolvimento de aplicações e tecnologias usadas no ecossistema apple.",
      "at the moment, mobile applications are increasingly present in the daily lives of each individual, allowing them to perform different tasks, such as managing bank accounts and transaction of monetary funds. due to the rapid adoption and development of lot, it is also important to guarantee the protection of the users' security from cyber attacks, preventing the access of these accounts and execution of certain operations not authorized by the respective account holders via home-banking applications. with this in mind, this dissertation's main objective is to analyze and integrate a security layer for the authentication and validation of fund transactions, called trustfactor, in an existing home-banking application. since the implementation will be aimed at mobile devices, issues related to the application development paradigms and technologies used in the apple ecosystem will be addressed."
    ],
    0.0
  ],
  [
    [
      "ao longo dos últimos anos, tem-se assistido a um crescente avanço ao nível das tecnologias da informação (ti) e o caso da aplicabilidade das ti à área da saúde não é excepção, dando origem ao que se designa de tecnologias de informação da saúde (tis). assim, com o decorrer do tempo e dos ditos avanços, surgiram novas ferramentas, tecnologias e sistemas de informação hospitalar (sih) com o intuito de melhorar a qualidade da prestação dos serviços das instituições de saúde e, do lado do utente, com o objetivo de proporcionar um acesso cada vez mais eficiente aos cuidados de saúde. um dos avanços mais significativos na prossecução da interoperabilidade entre sistemas e na centralização da informação é o registo de saúde eletrónico (rse). este sistema integra dados do utente provenientes de várias fontes, tornando-se um ativo válido no que diz respeito ao suporte da decisão clínica. paralelamente a isso, permite ainda o acesso a aplicações para a realização de processos operacionais, tais como a prescrição de medicamentos e exames de forma eletrónica. partindo destes pressupostos, foi então estudado o estado atual dos avanços destes sih em portugal, por forma a perceber de que forma seria possível, com os recursos existentes atualmente, munir o utente de melhores e mais informações acerca da sua saúde. por isso, o principal objetivo deste projeto de dissertação é desenhar e desenvolver uma aplicação móvel capaz de apoiar o utente no cumprimento das suas obrigações de saúde, sejam elas consequência de eventos numa determinada instituição ou mesmo a toma de medicamentos prescritos. para além disso, é também pretendido que, para além do possível apoio conseguido através da aplicação criada, se consiga ainda criar uma comunidade de auxílio ao utente, através da criação de um agregado. a principal motivação é, portanto, uma melhoria na qualidade da saúde do utente, através de um acompanhamento monitorizado e o mais individualizado possível. metodologicamente, partiu-se de uma análise completa aos dados provenientes do portal do serviço nacional de saúde (sns) e de outras instituições de saúde, com o intuito de contornar a inexistência de uma api e conseguir extrair e tratar os dados e, posteriormente, carregá-los na base de dados (bd) que alimenta a aplicação. ultrapassada essa dificuldade, comprova-se então a possibilidade de agregar toda a informação de um mesmo utente numa só aplicação, com a devida autenticação.",
      "over the years, we have witnessed a growth in the information technologies (it) field and the case of the it’s applicability to healthcare is no exception, originating the so-called health information technologies. thus, with these advances, new tools, technologies and hospital information systems (his) have emerged with the aim of improving the quality of health care provided by health institutions and, on the user side, with the objective to provide increasingly efficient access to health care. one of the most significant advances in the pursuit of interoperability between systems and the centralization of information is the electronic health record (ehr). this system integrates user’s data from several sources, making it a valid asset with respect to clinical decision support. parallel to this, it also allows access for operational processes, such as medical prescription of medication and exams electronically. based on these assumptions, the current state of progress of these systems in portugal was studied in order to understand how, with the existing resources, it would be possible to provide the user with better and more information about their health. therefore, the main objective of this dissertation project is to design and develop a mobile app capable of supporting the user in the fulfillment of their health obligations. in addition, it is also intended that, besides the app support, the user can get extra support, by creating an aggregate. the main motivation is, therefore, an improvement in the user’s healthcare quality, monitoring it and as individualized as possible. methodologically, it started with a complete analysis of data provided by serviço nacional da saúde (sns) and other healthcare institutions, with the aim of overcoming the inexistence of an api, and to be able to extract, transform and then load data into the database (db) that provides the app. by overcoming this difficulty, with the proper authentication, it is proved to be possible to aggregate all patient’s ehr in a single app."
    ],
    [
      "a deteção de danos na estrutura externa de veículos representa um desafio para os fornecedores de serviços de viaturas de aluguer, especialmente os serviços mais recentes de mobilidade em que a inspeção de danos não é realizada no final de cada aluguer. à medida que estes serviços se tornam mais populares e que mais pessoas deixam de ter necessidade de possuir um carro pessoal, espera-se que as empresas procurem formas de facilitar este tipo de inspeção de danos. como tal, o objetivo principal desta dissertação é desenvolver uma solução que os fabricantes de automóveis poderiam potencialmente adotar. esta solução envolve a criação de um algoritmo para detetar riscos em carros usando dados de áudio obtidos a partir de microfones. este estudo explora a utilização de aprendizagem profunda na análise de imagens, por exemplo espectrogramas, que são representativas de eventos de riscos. para tal serão estudados e implementados métodos de transformação do sinal de áudio em imagens referentes a uma representação espectral do áudio e avaliar a capacidade de um algoritmo de aprendizagem profunda aprender a identificar este tipo de eventos. o algoritmo é treinado tendo em conta a enorme variedade de sons que podem ser captados pelo microfone, considerando os infinitos ambientes possíveis a que um carro pode estar sujeito, devido a uma condução diária.",
      "the detection of damage to the external structure of vehicles poses a challenge for rental car service providers, especially in cases where damage inspection is not carried out at the end of each rental period. as these services become more popular as more people move away from the need to own a personal car, the expectation is that companies will be on the lookout for ways to facilitate this type of damage inspection. therefore, the aim of this thesis is to create an urban mobility solution that car manufacturers could potentially adopt by developing an algorithm that detects scratches in vehicles, using audio data (obtained with microphones). to achieve this goal, methods for transforming the audio signal into images representing the spectral characteristics of the audio are studied and implemented. subsequently, the development and evaluation of a deep learning (dl) algorithm that can learn to identify such events is conducted. the algorithm undergoes training, using a vast array of sounds that a microphone can detect, within the countless everyday driving environments to which cars can be subjected."
    ],
    0.06666666666666667
  ],
  [
    [
      "a necessidade das empresas evoluírem as suas aplicações por forma a disponibilizarem mais recursos aos seus utilizadores é uma realidade da atualidade. a disponibilização de informação em tempo real é cada vez mais necessária, mesmo que isso implique a interação entre sistemas distintos, o que exige que essa comunicação seja completamente agnóstica de tecnologias. sendo uma das premissas da q-better - empresa que permitiu o desenvolvimento desta dissertação em contexto empresarial - proporcionar aos seus clientes uma melhor experiência de utilização aliado ao acompanhamento da evolução tecnológica, tornou-se imperativo a conceção de uma arquitetura que fornecesse suporte ao desenvolvimento de novas aplicações e também às já desenvolvidas, ainda que para tal seja necessária uma reformulação das mesmas. inicialmente foi feito um estudo sobre a temática das arquiteturas orientadas a serviços, incluindo os vários tipos de web services existentes, e também uma passagem pela temática da sincronização de dados para proporcionar a sincronização entre as várias aplicações da q-better. a viabilidade da solução final - uma arquitetura orientada a serviços composta por um conjunto de web services rest - foi testada com a criação da aplicação bloom appointments cujo objetivo passa pela gestão de agendamentos a partir de qualquer dispositivo que tenha ligação à internet ou à rede onde o sistema esteja instalado. foi possível concluir que a escolha deste tipo de arquitetura se revelou acertada, uma vez que além de permitir a interoperabilidade entre os vários sistemas existentes na q-better, permite uma maior expansão não só da aplicação usada como case study, mas também de todo o legacy software e de futuras aplicações.",
      "the market requirements increases the need of companies to update their applications in order to provide more resources to the users. the real time information availability is increasingly crucial even if it means interaction between different systems, which requires communication completely agnostic of technology. one of q-better premises – enterprise which allows the development of this dissertation in business context – is to provide to their customers a better use experience allied to technologic evolution. for that, it has become imperative the conception of an architecture to support the development of new applications and support too the older ones, even if they needed to be reformulated. initially was realized a study about services oriented architectures, included different types of web services, and about data synchronization to support the synchronization between q-better applications. the final solution’s viability – one services oriented architecture composed by rest web services – was tested with the creation of bloom appointments application which the main goal is manage appointments from any device with internet or local network (in case of a local network installation) connection. it was possible to conclude that the choice of this architecture was right because it allows the interoperability between different q-better systems, allows further expansion not only of the case study application but all legacy software and future applications too."
    ],
    [
      "nowadays, there currently exist many working program verification tools however, the developed tools are mostly limited to the verification of sequential code, or else of multi-threaded shared-memory programs. due to the importance that distributed systems and protocols play in many systems, they have been targeted by the program verification community since the beginning of this area. in this sense, they recently tried to create tools capable of deductive verification in the distributed setting (deductive verification techniques offer the highest degree of assurance) and claim to have achieved impressive results. thus, this dissertation will explore the use of the why3 deductive verification tool for the verification of dis tributed algorithms. it will comprise the definition of a dedicated why3library, together with a representative set of case studies. the goal is to provide evidence that why3 is a privileged tool for such a task, standing at a sweet spot regarding expressive power and practicality.",
      "nos dias de hoje, possuímos diversas ferramentas de verificação, ferramentas essas limitadas à verificação de código sequencial, ou então de programas multi-thread de memória partilhada. devido à importância que os sistemas e protocolos distribuídos desempenham em muitos sistemas, estes foram alvos por parte da comunidade de verificação de programas desde o início desta área. neste sentido, recentemente tentaram criar ferramentas capazes de realizar a verificação dedutiva no ambiente distribuído (técnicas de verificação dedutiva que oferecem o mais elevado grau de segurança) e afirmam ter alcançado resultados impressionantes. assim, esta dissertação irá explorar o uso da ferramenta de verificação dedutiva why3 com o propósito de verificar algoritmos distribuídos. irão ser desenvolvidos modos e modelos da biblioteca why3do, juntamente com um conjunto representativo de casos de estudos. o objetivo é fornecer evidências de que why3 é uma ferramenta privilegiada para esta tarefa, estando no ponto ideal na relação poder expressivo e praticabilidade."
    ],
    0.06666666666666667
  ],
  [
    [
      "this document presents the work developed to fulfill the requirements for a master thesis in software engineering, in the areas of virtual museums, and ontologies for knowledge representation and exploration. the first objective of this thesis work was the creation of a specific ontology for the document repository of the museum of the person (museu da pessoa), using a standard for museums, cidoc-crm (comit´e internacional pour la documentation - conceptual reference model), complemented with foaf (friend-of-a-friend) and dbpedia that provides specific concepts and relations to deal with persons. this abstract ontology was then populated with life stories collected previously through of interviews of common people. two different approaches have be proposed to create the web pages for the virtual museum (vm), but only the approach 1 was implemented. a triplestore was used as database to store all the information that constitutes the museum assets; the vm was created consulting the datastore through sparql (sparql protocol and rdf query language) queries. in the dissertation will be discussed the design decisions, and provided the technical details; the project outcomes will be illustrated. the npmp site created can be accessed at http://npmp.epl.di.uminho.pt/ and complements this reading.",
      "este documento apresenta o resultado do trabalho relativo a uma tese de mestrado em informática, nas áreas dos museus virtuais (vm, virtual museum), e ontologias para representação˜ do conhecimento e sua exploração. o primeiro objetivo desta tese foi a criação de uma ontologia específica para o repositório de documentos do museu da pessoa (mp, museum of the person), utilizando um padrão para museus, cidoc-crm (comité internacional pour la documentation - conceptual reference model), complementada com foaf (friend-of-a-friend) e dbpedia as quais que fornecem conceitos e relações específicas para lidar com pessoas. esta ontologia abstrata foi preenchida com histórias de vida recolhidas anteriormente através de entrevistas a pessoas comuns. duas abordagens diferentes foram concebidas para criar as páginas web para o museu virtual, mas apenas a primeira abordagem foi implementada. um triplestore foi usado como base de dados para armazenar todas as informações que constituem o património do museu; as salas de exposição do museu foram criadas através de consultas sparql (sparql protocol and rdf query language) enviadas a esse triplestore. na dissertação serão discutidas as decisões de design, e fornecidos os detalhes técnicos; os resultados do projeto serão também ilustrados. o site npmp criado, acessível em http: //npmp.epl.di.uminho.pt/, complementa esta leitura."
    ],
    [
      "a integração de módulos de serviços é aceite como uma necessidade no desenvolvimento de sistemas complexos, uma vez que garante que todos os componentes individuais de um sistema atinjam o propósito para o qual o sistema, como um todo, foi desenhado. essencialmente, construir um sistema não é nada mais que integrar diversas partes num todo. a divisão de um sistema em diversas partes garante uma maior produtividade e qualidade das operações de desenvolvimento e teste, permitindo realizar estas tarefas de forma mais eficiente e focada em cada funcionalidade específica. isso resulta numa maior produtividade, uma vez que equipas de desenvolvimento podem trabalhar de forma paralela e colaborativa em diferentes módulos, agilizando o processo de construção do sistema como um todo, bem como permite um menor custo de manutenção do próprio sistema. esta técnica não é apenas usada para conectar os diversos serviços de um sistema. permite também fazer a conexão de um sistema com outros sistemas externos. neste trabalho de dissertação identificou-se e caracterizou-se as diversas funcionalidades dos serviços que se pretendiam integrar num sistema de avaliação de conhecimento, de forma a implementar um sistema de elearning que operasse de forma consistente. grande parte do trabalho realizado envolveu a análise de um conjunto de módulos de serviços implementados anteriormente num novo sistema, com uma nova arquitetura, acolhendo os tradicionais serviços de autenticação e validação de credenciais. a nova versão do sistema que alcançámos, resultado de um processo de reengenharia bastante trabalhoso, permite definir e suportar processos de avaliação do conhecimento de estudantes em diversos domínios de estudo, bem como suporta todos os serviços de gestão e manutenção da informação associada como a avaliação de estudantes, nomeadamente, os domínios e subdomínios de estudo, as estruturas de conhecimento dos processos de avaliação e os serviços de análise relacionados com os resultados obtidos pelos alunos.",
      "the integration of service modules is widely accepted as a necessity in the development of complex systems, as it ensures that all individual components of a system achieve the purpose for which the system as a whole was designed. essentially, building a system is nothing more than integrating various parts into a whole. breaking down a system into various parts enhances productivity and the quality of development and testing operations, allowing these tasks to be carried out more efficiently and focused on specific functionalities. this results in increased productivity, as development teams can work in parallel and collaboratively on different modules, streamlining the overall system construction process, as well as reducing the maintenance cost of the system itself. this technique is not only used to connect various services within a system but also to connect a system to other external systems. in this dissertation work, the various functionalities of the services to be integrated into a knowledge assessment system were identified and characterized in order to implement a consistent elearning system. much of the work involved analyzing a set of previously implemented service modules in a new system with a new architecture, utilizing traditional authentication and credential validation services. the new version of the system that we achieved, as a result of a rather laborious reengineering process, allows for the definition and support of knowledge assessment processes for students in various study domains, as well as supporting all information management and maintenance services associated with student assessment. this includes study domains and subdomains, knowledge structures of assessment processes, and analysis services related to student performance results."
    ],
    0.3
  ],
  [
    [
      "ontologies are very powerful tools when it comes to handling knowledge. they offer a good solution to exchange, store, search and infer large volumes of information. throughout the years various solutions for knowledge-based systems use ontologies at their core. ontodl has been developed as a domain specific language using antlr4, to allow for the specification of ontologies. this language has already been used by experts of various fields has a way to use computer-based solutions to solve their problems. in this thesis, included on the second year of the master degree in informatics engineering, ontodl+ was created as an expansion of the original ontodl. both the language and its compiler have been improved. the language was extended to improve usability and productivity for its users, while ensuring an easy to learn and understand language. the compiler was expanded to translate the language specifications to a vaster array of languages, increasing the potential uses of the dsl with the features provided by the languages. the compiler and some examples of the dsl can be downloaded at the website https: //epl.di.uminho.pt/∼gepl/gepl ds/ontodl/ created for the application and presented in the final chapters of the thesis.",
      "as ontologias são formalismos muito poderosos no que toca a manipulação de conhecimento. estas oferecem uma boa solução para trocar, armazenar, procurar e inferir grandes volumes de informação. ao longo dos anos, várias soluções para sistemas baseados em conhecimento usaram ontologias como uma parte central do sistema. a ontodl é uma linguagem de domínio específico que foi desenvolvida através do uso de antlr4, para permitir a especificação de ontologias. esta linguagem foi já utilizada por especialistas de diversas áreas como forma de utilizar soluções informáticas para resolver os seus problemas. nesta tese, incluída no segundo ano do mestrado em engenharia informática, ontodl+ foi criado como uma expansão tanto à linguagem e como ao seu compilador. a linguagem foi extendida para melhorar a usabilidade e produtividade dos seus utilizadores, mantendo se fácil de aprender e perceber. o compilador foi expandido para ser capaz de traduzir as especificações de ontodl+ para um leque de linguagens mais vasto, aumentando os potenciais usos da dsl através das funcionalidades providenciadas pelas linguagens alvo. o compilador e alguns exemplos da dsl podem ser acedidos no sítio https://epl.di. uminho.pt/∼gepl/gepl ds/ontodl/ criado para a aplicação e mostrado nos capítulos finais da tese."
    ],
    [
      "a proximidade entre a informática e a saúde é cada vez maior a cada dia que passa. nos dias que correm é comum os hospitais guardarem eletronicamente todo o historial e relatórios clínicos dos utentes. o armazenamento digital destes dados traz vantagens aos sistemas de saúde como a acessibilidade, a otimização de recursos e redução de custos, a diminuição do erro médico e o auxílio nas tomadas de decisões. grande parte desses dados está em formato de texto livre, ou seja, são dados não estruturados. para os sistemas computacionais, este tipo de dados representa um maior desafio quer na análise, quer no seu processamento. sendo que, para este tipo de informação ser processada automaticamente é necessário recorrer ao processamento de linguagem natural, uma subárea da inteligência artificial. tarefas como classificação ou reconhecimento de entidades em textos requerem quase sempre textos anotados. o processo de anotação dos textos é demorado e pouco atrativo para o ser humano levando a que a quantidade disponível de dados anotados não seja em grande volume e consequentemente a que a aplicação de modelos de machine learning não seja a mais eficiente, resultado em problemas de over fitting e não generalizando como seria de desejar. devido a isto, a procura por uma solução de anotação automática dos dados em massa é necessária e extremamente útil. a principal contribuição desta dissertação é o desenvolvimento de uma aplicação para a anotação automática de informação clínica. esta aplicação permitirá a anotação de grandes quantidades de dados de forma automática comparativamente a outras ferramentas e abordagens existentes.",
      "the proximity between informatics and health is growing day by day. nowadays, it is common for hospitals to store all the history and clinical data electronically. the digital storage of these data brings advantages to health systems such as accessibility, optimization of resources and cost reduction, reduction of medical errors and help in decision-making. however, most of this data is in free-text format, that is, unstructured data. for computer systems, this type of data represents an enormous challenge both in analysis and processing. for this type of information to be processed automatically, it is necessary to resort to natural language processing, a sub-area of artificial intelligence. tasks such as classification or name entity recognition almost always require annotated text. the process of annotating texts is time-consuming and unattractive for human beings, leading to the fact that the available amount of annotated data is not large. consequently, the application of machine learning models is not the most efficient, resulting in overfitting problems and not generalizing as we would like. due to this, the search for a solution of automatic annotation of clinical data is necessary and extremely useful. the main contribution of this dissertation is the development of an application for the automatic annota tion of clinical information. this application will allow the annotation of large amounts of data automatically compared to other existing tools and approaches."
    ],
    0.3
  ],
  [
    [
      "computer vision is a vast knowledge subject responsible for traducing digital images and videos into a higher level of understandable information. image recognition is one of the several tasks that are inserted in this subject and it can be subdivided in object recognition (also called as object classification), segmentation, identification and detection. some of the available alternatives for image recognition are based on machine learning (ml) approaches. deep learning (dl) is a branch of ml that became very popular in the last years due to its success in previously considered hard tasks. the lack of large amounts of data and efficient computational resources a few years ago, were a barrier for the expansion of dl. however, thanks to the current easy data access and due to development of more powerful computational resources, including cpu and gpu too, the attention turned back on, and it became easier and faster to train a model than can distinguish different types of classes with a very low error rate. one interesting fact about dl is its ability to automatically learn from data and understand the most differentiable features of it. from the point of view of the industry, many artificial vision inspection lines still do their jobs relying on traditional computer vision methods/algorithms. yet, with more complex domains, for example like texture patterns, things can get more difficult. this is where dl comes in. this document begins with an introduction of dl for artificial vision. it starts by addressing the theoretical fundamentals of dl for image recognition and then focuses on the general aspects of convolutional neural networks (cnn). next, are reviewed the state of the art network configurations that stood out in recently. a high-level toolkit for image recognition was created to simplify the whole process of building dl models, from the data pre-processing to the trained model testing phase. it allowed to easily prepare a set of experiences that address some of the common practices used on cnns and highlight the power of dl on image recognition related tasks. this dissertation was developed under a business environment on a artificial vision company called neadvance, machine vision, sa. the neadvance, machine vision, sa is also interested in researching the new trends related to dl for image recognition in order to know how to apply them on their projects since it opens a new range of challenging opportunities.",
      "a visão por computador é uma área vasta de conhecimento responsável por traduzir imagens e vídeos digitais para um nível mais alto de informação compreensível. o reconhecimento de imagem é uma das várias tarefas que está inserida nesta área e pode ser sub-dividida em reconhecimento de objectos (também designada por classificação de objectos), segmentação, identificação e detecção. algumas das alternativas disponíveis para reconhecimento são baseadas em abordagens de ml. o dl é um ramo de ml e tornou-se muito popular nos últimos anos devido ao seu sucesso em tarefas consideradas difíceis, até ao momento. a falta de grande quantidade de dados e de recursos computacionais eficientes há uns anos atrás, foram uma barreira para a expansão do dl. contudo, graças à actual facilidade de acesso a dados e devido ao desenvolvimento de recursos computacionais mais potentes, incluindo cpu e gpu também, a atenção à volta do tema voltou a crescer, e tornou-se mais fácil e mais rápido treinar um modelo que consegue distinguir diferentes tipos de classes com uma taxa de erro baixa. um facto interessante sobre o dl, é a sua capacidade para aprender dos dados e compreender as suas características mais diferenciadoras. do ponto de vista da indústria, muitas linhas de inspecção via visão artificial ainda fazem o seu trabalho através de métodos/algoritmos tradicionais de visão por computador. todavia, com domínios mais complexos, como por exemplo padrões de texturas, as coisas podem tornar-se mais difíceis. é aí onde entra o dl. este documento inicia com uma introdução ao dl para visão artificial. começa por abordar os fundamentos teóricos de dl para reconhecimento de imagem e de seguida foca-se em aspectos gerais das convolutional neural network (cnn)s. depois, são revistas as configurações estado da arte das arquitecturas de rede que se destacaram recentemente. foi criado um conjunto de ferramentas para simplificar todo o processo de construção de modelos de dl, desde o pré-processamento dos dados até à fase de testes do modelo treinado. este permitiu ainda preparar facilmente uma série de experiencias que abordam algumas das práticas comuns usadas nas cnn e destacar o poder do dl em tarefas relacionadas com reconhecimento de imagem. esta dissertação foi desenvolvida sob ambiente empresarial numa empresa de visão artificial chamada neadvance, machine vision, sa. a neadvance, machine vision, sa também está interessada em investigar as novas tendências relacionadas com o dl de forma a saber como aplicá-las nos seus projectos, uma vez que lhe possibilita uma nova gama de desafios."
    ],
    [
      "industrial manufacturing is becoming highly reliant on automation developments, as they bring more efficient and accurate processes, with lower associated costs. consequently, robots are increasingly being deployed in a wide range of scenarios, especially where safety is demanded. in such cases, it is critical to employ appropriate procedures to verify both the system’s quality and safety. following the current growth of cyber-physical systems, as well as their usage in various technology domains, the development of software applications is becoming more demanding due to the complexity behind the integration of complementing services, beyond those provided by the operating system. one of the most popular open-source software platforms for building robotic systems is the robot operating system (ros) [53] middleware, where highly configurable robots are usually built by composing third-party modules. robot operating system 2 (ros2) is implemented using the data distribution service (dds) [49] communication protocol. ros2 implicitly makes use of the dds-security artefacts through the secure robot operating system 2 (sros2) security toolset. the present study focus on detecting security problems in ros2 networks, in which it is intended to verify, through formal techniques, security properties. however, security is a very broad subject, so this study focuses on a particular security property to show the viability of the proposed technique, namely observational determinism (od). this dissertation introduces a software tool, named security verification in ros (svros), that provides multiple functionalities to support this type of security analysis using alloy [32], a formal specification language and analysis tool.",
      "a crescente implementação de processos automáticos tem motivado a reestruturação nos mais diversos setores industriais, com o objetivo de aumentar a eficiência e precisão dos mesmos, e consequentemente, reduzir os custos associados. além disso, esta ideia levou à integração da robótica nos mais amplos domínios tecnológicos, especialmente onde a segurança é exigida. nestes casos, é fundamental adotar técnicas apropriadas de forma a verificar tanto a qualidade do sistema, como a segurança do mesmo. como resultado do atual crescimento dos sistemas ciber-físicos, nomeadamente sistemas robóticos, bem como a sua utilização em vários domínios tecnológicos, o desenvolvimento de aplicações tem vindo a ficar mais exigente, em particular devido à complexidade da integração dos serviços necessários, tipicamente não fornecidos pelo sistema operativo. uma das plataformas considerada como standard no que toca ao desenvolvimento de sistemas robóticos é o middleware ros [53], onde robôs altamente configuráveis são construídos através da composição modular de software externo, oferecendo características como flexibilidade e interoperabilidade aos sistemas integrados. o ros2 implementa um protocolo de comunicação, de nome dds [49], que, para além de garantir serviços de comunicação, implementa a especificação dds-security, que oferece diferentes métodos de adoção de segurança, através de uma metodologia de plugins. através do uso desta especificação, juntamente com o uso do toolset sros2, é possível configurar o ros2 de forma a proporcionar um ambiente seguro às aplicações integradas. o presente trabalho foca-se no estudo e deteção de problemas de segurança em topologias ros2, através da verificação formal de propriedades de segurança. no entanto, a segurança é um assunto extenso, pelo que o foco de interesse nesta tese é numa propriedade particular de segurança para mostrar a viabilidade da presente técnica, de nome od. esta dissertação introduz a uma ferramenta de verificação de nome svros, que contempla múltiplas funcionalidades para suportar este tipo de análise usando alloy [32], uma linguagem de especificação formal e respectiva ferramenta de análise."
    ],
    0.3
  ],
  [
    [
      "atualmente, java é uma das linguagens de programação mais populares. esta popularidade é parcialmente devida à sua portabilidade que advém do facto do código java ser compilado para bytecode que poderá ser executado por uma máquina virtual java (jvm) compatível em qualquer sistema. a jvm pode depois interpretar diretamente ou compilar para código máquina a aplicação java. no entanto, esta execução sobre uma máquina virtual cria alguns obstáculos à obtenção do perfil de execução de aplicações. perfis de execução são valiosos para quem procura compreender o comportamento de uma aplicação pela recolha de métricas sobre a sua execução. a obtenção de perfis corretos é importante, mas a sua obtenção e análise pode ser desafiante, particularmente para aplicações paralelas. esta dissertação sugere um fluxo de trabalho de otimização a aplicar na procura de aumentos na escalabilidade de aplicações java paralelas. este fluxo sugerido foi concebido para facilitar a descoberta dos problemas de desempenho que afetam uma dada aplicação paralela e sugerir ações a tomar para os investigar a fundo. o fluxo de trabalho utiliza a noção de possible speedups para quantificar o impacto de problemas de desempenho diferentes. a ideia de possible speedups passa por estimar o speedup que uma aplicação poderia atingir se um problema de desempenho específico fosse completamente removido. esta estimativa é calculada utilizando as métricas recolhidas durante uma análise ao perfil de uma aplicação paralela e de uma versão sequencial da mesma aplicação. o conjunto de problemas de desempenho considerados incluem o desequilíbrio da carga de trabalho, sobre carga de paralelismo devido ao aumento no número de instruções executadas, sobrecarga de sincronização, gargalos de desempenho no acesso à memória e a fração de trabalho sequencial. estes problemas foram considerados as causas mais comuns de limitações à escalabilidade de aplicações paralelas. para investigar mais a fundo o efeito destes problemas numa aplicação paralela, são sugeridos alguns modos de visualização do perfil de execução de uma aplicação dependendo do problema que mais limita a sua escalabilidade. as visualizações sugeridas consistem maioritariamente de diferentes tipos de flame graphs do perfil de uma aplicação. duas ferramentas foram desenvolvidas para ajudar a aplicar este fluxo de trabalho na otimização de aplicações java paralelas. uma destas ferramentas utiliza o async-profiler para recolher perfis de execução de uma dada aplicação java. a outra ferramenta utiliza os perfis recolhidos pela primeira ferramenta para estimar possible speedups e produzir todas as visualizações mencionadas no fluxo de trabalho sugerido. por fim, o fluxo de trabalho foi validado com alguns casos de estudo. o caso de estudo principal consistiu na otimização iterativa de um algoritmo k-means, partindo de uma implementação sequencial e resultando no aumento gradual da escalabilidade da aplicação. casos de estudo adicionais também foram apresentados para ilustrar possibilidades não abordadas no caso de estudo principal.",
      "java is currently one of the most popular programming languages. this popularity is, in part, due to the portability it offers which comes from the fact that java source code is compiled into bytecode which can be executed by a compatible java virtual machine (jvm) in a different system. the jvm can then directly interpret or compile into machine code the java application. however, this execution on top of a virtual machine creates some obstacles to developers looking to profile their applications. profilers are precious tools for developers who seek to understand an application’s behaviour by collecting metrics about its execution. obtaining accurate profiles of an application is important, but they can also be challenging to obtain and to analyse, particularly for parallel applications. this dissertation suggests an optimisation workflow to employ in the pursuit of reducing scalability bottlenecks of parallel java applications. the workflow is designed to simplify the discovery of the performance problems affecting a given parallel application and suggest possible actions to investigate them further. the suggested workflow relies on possible speedups to quantify the impact of different performance problems. the idea of possible speedups is to estimate the speedup an application could achieve if a specific performance problem were to completely disappear. this estimation is performed using metrics collected during the profile of the parallel application and its sequential version. the set of performance problems considered include workload imbalance, parallelism overhead due to an increase in the number of instructions, synchronisation overhead, memory bottlenecks and the fraction of se quential workloads. these were deemed to be the most common causes for scalability issues in parallel appli cations. to further investigate the effect of these problems on a parallel application, some visualisations of the application’s behaviour are suggested depending on which problem limits scalability the most. the suggested visualisations mostly consist of different flame graphs of the application’s profile. two tools were also developed to help in the application of this optimisation workflow for parallel java appli cations. one of these tools relies on async-profiler to collect profiles of a given java application. the other tool uses the profiles collected by the first tool to estimate possible speedups and also produce all visualisations mentioned in the suggested workflow. finally, the workflow was validated on multiple case studies. the main case study was the iterative optimisation of a k-means algorithm, starting from a sequential implementation and resulting in the gradual increase of the application’s scalability. additional case studies were also presented in order to highlight additional paths not covered in the main case study."
    ],
    [
      "in the last years, the widespread of cloud computing as the main paradigm to deliver a large plethora of virtualized services significantly increased the complexity of datacenters management and raised new performance issues for the intra-datacenter network. providing heterogeneous services and satisfying users’ experience is really challenging for cloud service providers, since system (it resources) and network administration functions are definitely separated. as the software defined networking (sdn) approach seems to be a promising way to address innovation in datacenters, the thesis presents a new framework that allows to develop and test new openflow–based controllers for cloud datacenters. more specifically, the framework enhances both mininet (a well–known sdn emulator) and pox (a openflow controller written in python), with all the extensions necessary to experiment novel control and management strategies of it and network resources. further more, the framework was validated by implementing and testing well known policies. hybrid allocation policies (considering both network and servers) were also implemented and scalability tests were performed. this work was developed under the erasmus student mobility program, in the telecommunication networks research group, dept. of information engineering, university of pisa, and resulted in the paper datacenter in a box: test your sdn cloud-datacenter controller at home that was accepted into ewsdn2013.",
      "nos últimos anos, a difusão da computação em nuvem como o principal paradigma para oferecer uma grande variedade de serviços virtualizados aumentou significativamente a complexidade da gestão de datacenters e trouxe novos problemas de desempenho para a sua rede interna. a prestação de serviços heterogêneos e a satisfação dos utilizadores tornou-se um desafio para os provedores de serviços em nuvem, uma vez que as funções de administração de rede e do sistema (recursos de ti) estão definitivamente separados. como o software defined networking (sdn) aparenta ser um caminho promissor para tratar a inovação em datacenters, a tese apresenta uma nova framework que permite desenvolver e testar novos controladores baseados em openflow para datacenters. mais especificamente, a framework melhora o mininet (um emulador sdn) e pox (um controlador openflow escrito em python), com todas as extensões necessárias para o desenvolvimento de novas estratégias de ti e de gestão das redes de controlo. além disso, a framework foi validada através da implementação e teste de políticas conhecidas. políticas híbridas de alocação de máquinas virtuais (considerando tanto os servidores como as redes) foram implementadas e testes de escalabilidade foram realizados. este trabalho foi desenvolvido no âmbito do programa de mobilidade de estudantes erasmus, no grupo de investigação em redes de telecomunicações tlcnetgrp, departamento de engenharia de informação da universidade de pisa, do qual resultou a elaboração do artigo datacenter in a box: test your sdn cloud-datacenter controller at home que foi aceite no ewsdn2013."
    ],
    0.0
  ],
  [
    [
      "the molecular dynamics simulation is a topic fairly investigated because it solves countless problems of physics, chemistry, or biology. from the computer engineering point of view it is an interesting case study because it is a computationally complex problem. the complexity arises when there are a high number of particles, thereby resulting in a high number of iterations to compute on each iteration. presently there are systems with millions of particles that need to be simulated in the shortest time possible. this led to the development of molecular dynamics packages that attempt to use all the resources available to improve the execution of simulations. the main goal of this thesis is to run efficiently molecular dynamics simulations on hybrid systems. instead of starting a molecular dynamics implementation from scratch, it was used the moil package. then it was developed an implementation based on moil with optimizations that allow the code to be automatically vectorized by the compiler. these optimizations focused on the calculation of forces and the data structures. new data structures were introduced to decompose the simulation domain into cells. the vectorization was used both in sequential and parallel implementations. in both cases, vectorization allowed a higher performance when used with cells. in order to achieve the best possible performance, the optimized code has been parallelized using different strategies, including shared memory, distributed memory, and a hybrid solution. in the execution of the parallel code several combinations of processes and threads were tested. among all the developed versions, the one that achieved the best performance was the hybrid version. all implementations were compared to gromacs, the reference in terms of performance of the molecular dynamics simulation.",
      "a simulação de dinâmica molecular é um tema bastante investigado porque permite resolver inúmeros problemas da física, química, ou biologia. do ponto de vista da engenharia informática é um caso de estudo interessante por ser um problema computacionalmente complexo. a complexidade surge quando se utiliza um elevado número de partículas, necessitando assim de se calcular um grande número de interações em cada iteração. atualmente há sistemas com milhões de partículas que se pretende que sejam simulados no menor tempo possível. este facto levou ao desenvolvimento de ferramentas de dinâmica molecular que procuram utilizar todos os recursos disponíveis para melhorar a execução das simulações. o principal objetivo desta tese é executar eficientemente simulações de dinâmica molecular em sistemas híbridos. em vez implementar a simulação de dinâmica molecular desde o inıcio, foi utilizado a ferramenta moil. depois foi desenvolvida uma implementação baseada no moil com otimizações que permitem que o código seja vetorizado automaticamente pelo compilador. as otimizações realizadas focaram-se no cálculo das forças e nas estruturas de dados. foram introduzidas novas estruturas de dados para decompor o domínio em células. a vetorização foi utilizada nas implementações sequenciais e paralelas. em ambos os casos a vetorização permitiu obter um desempenho melhor quando usada em conjunto com células. para obter o melhor desempenho possível, o código otimizado foi paralelizado usando diferentes estratégias, incluindo memória partilhada, memória distribuída e uma solução híbrida. na execução do código paralelo foram testadas varias combinações de processos e threads. de todas as implementações desenvolvidas a que permitiu melhores resultados foi a versão híbrida. todas as implementações foram comparadas com o gromacs que e uma referência em termos de desempenho das simulações de dinâmica molecular."
    ],
    [
      "in this dissertation, the efficiency of privacy protecting mechanisms in short-range vehicular communications, namely pseudonym change strategies, is investigated. to evaluate these strategies, a set of simulation tools is used, that allow for the assessment of several metrics, such as the privacy level obtained and the real pseudonym consumption, resulting from the use of a representative set of pseudonym change strategies. most importantly, hybrid strategies were considered, which combine schemes that were previously analysed separately. the results show that combining mix-zones with another scheme provides better privacy in most cases. lastly, we showcase and analyse the problems found in the process of trying to make the simulated scenarios more realistic, which easily comes into conflict with tool limitations and/or subtle and hard to anticipate interactions between different components.",
      "nesta dissertação investiga-se a eficácia de mecanismos de protecção da privacidade em comunicações veiculares de curto alcance, nomeadamente recorrendo a estratégias de alteração de pseudónimos. para a avaliação dessas estratégias, recorre-se a um conjunto de ferramentas de simulação que permitem aferir diferentes métricas, como o nível de privacidade obtido e o consumo efectivo de pseudónimos, decorrentes da utilização de um conjunto representativo de estratégias de alteração de pseudónimos. mais importante ainda, foram consideradas estratégias híbridas, que combinam esquemas antes analisados separadamente. os resultados mostram que combinar zonas mistas com outro esquema proporciona melhor privacidade na maioria dos casos. por último, apresentam-se e analisam-se problemas encontrados no processo de procurar tornar mais realistas os cenários das simulações realizadas, e que facilmente esbarra com limitações das ferramentas e/ou interações subtis e dificilmente antecipáveis de diferentes componentes."
    ],
    0.3
  ],
  [
    [
      "os lagos de dados, também conhecidos por data lakes, suportam a recolha de grandes quantidades de informação em ficheiros imutáveis para processamento analítico. no entanto, tem surgido a necessidade de modificar e atualizar esta informação de forma fiável, seja porque os dados são recebidos de forma incremental (por exemplo, de sensores e outras fontes de eventos) ou para eliminar os mesmos (por exemplo, devido ao rgpd (regulamento geral sobre a proteção de dados)). as soluções atuais para o fazer não são no entanto ideais: o armazenamento em sgbd (sistema de gestão de bases de dados) nosql (not only sql) tem um grande impacto no desempenho analítico, enquanto que sistemas baseados em ficheiros, como o delta lake, permitem apenas atualizações de granularidade grossa. neste trabalho aborda-se este problema propondo uma solução híbrida que combina o armazena mento de longo prazo em ficheiros com um armazenamento transitório num sgbd nosql de forma a obter as vantagens de ambos os sistemas. para o efeito, é implementado uma prova de conceito usando spark, com ficheiros parquet, e mongodb. assim, com a introdução deste sistema pretende-se possibi litar a execução de transações frequentes e de granularidade fina para suportar uma carga de trabalho oltp (online transaction processing). os resultados experimentais obtidos confirmam que esta proposta obtém desempenho analítico e transacional comparável a cada um dos sistemas isolados.",
      "data lakes support the collection of large amounts of information in immutable files for analytical processing. however, there has been a need to reliably modify and update this information, either because data is received incrementally (for example, from sensors and other event sources) or to eliminate them (for example, due to gdpr (general data protection regulation)). current solutions for doing this aren’t ideal: storage in nosql (not only sql) dbms (database management system) has a big impact on analytical performance, while file-based systems, such as delta lake, only allow coarse-grained updates. this work addresses this problem by proposing a hybrid solution that combines long-term file storage with transient storage in a nosql dbms in order to obtain the advantages of both systems. for this purpose, a proof of concept is implemented using spark, with parquet files, and mongodb. thus, with the introduction of this system, it’s intended to enable the execution of frequent and fine-grained transactions to support an oltp (online transaction processing) workload. the experimental results obtained confirm that this proposal obtains analytical and transactional performance comparable to each of the isolated systems."
    ],
    [
      "in the last few decades, an increasing growth of internet usage was witnessed worldwide. however, infrastructures do not always allow the existence of internet connectivity everywhere. therefore, to address this issue, the concept of delay tolerant networks (dtns) was developed. dtns purpose is to provide a different level of intermittent connectivity, dissimulating connection problems that arise in complex connectivity scenarios. examples of such scenarios are, for instance, cities, where cars exchange information about their location; in underdeveloped countries, where internet is inexistent; in freeways, where is not viable to provide infrastructures for a continuous connectivity, but cars, tolls, and services need to be aware of each other. thus, dtns constitute a possible solution for all the aforementioned communication environments. however, dtns still faces some obstacles in terms of delivering a service with quality as it lacks specific mechanisms, such as traffic differentiation. traffic differentiation is essential to provide different levels of service quality regarding delivering of messages. current proposals to improve service delivery through traffic differentiation on dtns are still under development or lack the proper testing and simulation. the main focus of these proposals is on buffer management mechanisms at each dtn node, instead of message prioritisation mechanisms. message prioritisation allows some messages to be prioritised over others, improving the delivery rate and, therefore, increasing the probability of a message being correctly delivered. the present thesis implements traffic differentiation in dtns based on prioritisation strategies, assuming a clear alternative to other buffer management proposals and message prioritisation. using the one simulation tool, three popular dtns routing protocols (epidemic, spray & wait, and prophet) are adapted to comply with traffic differentiation. the dtns traffic prioritisation objective is achieved by designing, implementing and testing four distinct algorithms that classify and order messages according to their priority levels. these algorithms are based and extend some traditional traffic differentiation mechanisms, namely the well-known priority queuing and weighted round robin strategies. results from the simulation tests corroborate that the delivery rate of the messages is affected according to their priorities. specifically, the simulation shows an increase in the delivery rate of high priority messages, with low impact on the total number of messages delivered, comparatively to the same scenario without differentiation capabilities. to conclude, dtns can effectively benefit from traffic differentiation based on message prioritisation techniques, being a promising approach to improve service quality levels in such scenarios.",
      "nas últimas décadas assistiu-se a um aumento crescente no uso da internet. contudo, as infra-estruturas nem sempre permitem uma ligação à internet. assim, para enfrentar este desafio, o conceito de delay tolerant networks (dtn) foi desenvolvido. o objetivo das dtn é providenciar diferentes níveis de ligação intermitente, atenuando os problemas de ligação que surgem em cenários de conectividade complexa. exemplos de tais cenários incluem, cidades, onde carros trocam informação da sua localização; países em vias de desenvolvimento, onde a internet é inexistente; em auto-estradas, onde não é viável conceber infra-estruturas que permitam uma conectividade permanente, mas onde carros, portagens e serviços necessitam de comunicar. deste modo, as dtns constituem uma solução possível para os ambientes indicados. contudo, as dtns ainda enfrentam alguns obstáculos na prestação de um serviço de qualidade, visto faltarem mecanismos específicos, como a diferenciação de tráfego. a diferenciação de tráfego é essencial para oferecer diferentes níveis de serviço de qualidade em termos de entrega de mensagens. as abordagens existentes para diferenciação de tráfego em dtns ainda estão em fase de desenvolvimento. estas focam-se principalmente nos mecanismos de gestão do buffer a cada nodo da dtn, em vez de ao nível de mecanismo de priorização das mensagens. a priorização de mensagens permite que algumas recebam prioridade em detrimento de outras, melhorando a taxa de entrega, aumentando a probabilidade desta ser entregue corretamente. esta tese implementa diferenciação de tráfego em dtns baseando-se em estratégias de priorização, assumindo-se como uma alternativa a outras abordagens de gestão de buffer e priorização de mensagens. usando a ferramenta de simulação “the one”, foram adaptados três protocolos de routing dtn (epidemic, spray & wait, and prophet) de modo a obedecerem à diferenciação de tráfego. este objetivo é alcançado pelo desenho, implementação e experimentação de quatro algoritmos que classificam as mensagens de acordo com o seu nível de prioridade, baseando-se em mecanismos tradicionais de diferenciação de tráfego, i.e. as estratégias de priority queuing e weighted round robin. os resultados demonstram que a taxa de entrega de mensagens é influenciada de acordo com as prioridades. nomeadamente, há um aumento na taxa de entrega de mensagens com prioridade alta, com pouco impacto no número total de mensagens entregues, comparativamente com o mesmo cenário sem mecanismos de diferenciação. em suma, as dtn podem beneficiar da diferenciação de tráfego baseado em técnicas de priorização de mensagens, representando uma abordagem à melhoria da qualidade de serviço bastante promissora."
    ],
    0.06666666666666667
  ],
  [
    [
      "apesar da evolução observada nas últimas décadas na utilização de sistemas de informação (si) na gestão de matérias primas (mps) e informação ao longo dos vários elos de uma cadeia de abastecimento, a rastreabilidade de mps em tempo real é ainda muito limitada devido à falta de integração dos si com tecnologias de identificação automática. a fábrica bosch car multimedia de braga (bosch brgp) não é indiferente a este problema e recorre com grande frequência a fornecedores que utilizam rotas dispendiosas em termos de tempo, custo e esforço de operacionalização, o que levanta diversos desafios relativamente aos processos logísticos (planeamento, receção, gestão e abastecimento de mps), ou seja, de todo o fluxo de mp desde os fornecedores até às linhas de produção. atualmente, a bosch brgp enfrenta problemas que decorrem da dificuldade em localizar, em tempo real e com elevado grau de precisão, todas as mps (envolvidas) nos diferentes fluxos da logística interna (na cadeia de abastecimento interna), o que dificulta o planeamento das necessidades de cliente. deste modo, a bosch brgp tem a necessidade permanente de ter acesso a informação relativa à localização da mp. é então pretendido otimizar a logística interna da bosch brgp, tornando-a mais competitiva no mercado e diminuir os desvios de stock originados pela dificuldade em localizar as mps. de forma a resolver os problemas de visibilidade e rastreabilidade de mps desde a sua receção até ao seu consumo, esta dissertação analisa e concebe uma solução para rastrear o fluxo das mps em todo o processo interno responsável pela gestão da cadeia de abastecimento interna, que recorre a tecnologia radio-frequency identification (rfid). os objetivos são alcançados através do desenvolvimento de um sistema de identificação e localização de mps, que garante uma maior visibilidade dos fluxos de mps, contribuindo para a redução do tempo de abastecimento às linhas de produção. para além disso, o sistema deve garantir que toda a movimentação/localização da mp em curso na cadeia de abastecimento interna é rastreável e efetuada de modo automático, em tempo real e com elevado grau de precisão, contribuindo para a disponibilização dessa informação sempre que solicitada. esta dissertação contribui para esse esforço de desenvolvimento com atividades de análise e conceção que se concretizam através do levantamento de requisitos (funcionais e não funcionais), da conceção da arquitetura de hardware e software, da definição dos pontos de interoperabilidade com os sistemas legados e da caracterização do novo fluxo de rastreabilidade. estas atividades são documentadas através de diversos esquemas e diagramas (de use-cases e de atividades) que seguem a notação unified modeling language (uml).",
      "despite the progress observed in recent decades on the use of information systems (is) to manage raw materials (rms) and information throughout the several links of the supply chain, the rms traceability in real-time remains very limited due to lack of is integration with automatic identification technologies. bosch car multimedia plant in braga is not indifferent to this problem and receives rms from different suppliers using expensive routes in terms of time, cost and operational effort, which raises many challenges regarding the logistics processes (planning, receipt, management and supply of rms), i.e., entire rm flow from the suppliers to the production lines. currently, bosch brgp faces problems to locate, in real-time and with high accuracy, the rms involved in the different internal logistics flows (internal supply chain), which difficulties the customer planning demands. thus, the bosch brgp has a constantly growing need of accessing the information related with rm location. it is intended to optimize the bosch brgp’s internal logistics, making it more competitive in the market and reducing stock deviations caused by the difficulty in locating the rms. in order to solve the rm visibility and traceability problems from reception to its consumption, this dissertation analyses and designs a solution to trace the rms flow in the internal process responsible for managing the internal supply chain, using the rfid technology. the objectives are achieved through the development of traceability system that provides a greater visibility of rms flows, contributing to the reduction of supply time to the production lines. furthermore, the system must ensure that the rms movements in the internal supply chain are traceable and performed automatically, in real-time and with high accuracy level, contributing to the information provision when requested. this dissertation contributes to this development effort through analysys and design activities like requirements elicitation (functional and non-functional), design of hardware and software architecture, definition of the interoperability points with legacy systems and characterization of the new traceability flow. these activities are documented throughout several diagrams (use-case and activity diagrams) based in the unified modeling language."
    ],
    [
      "durante longos períodos de atividade cognitiva é comum a sensação de cansaço e falta de energia, acompanhada de um decréscimo de desempenho. este estado, geralmente denominado de fadiga mental, é considerado uma das principais causas de erro humano. os efeitos da fadiga mental no desempenho de tarefas complexas e que requerem altos níveis de concentração devem ser estudados e antecipados de forma a minimizar erros. exemplo disto é o caso da aviação, ou medicina onde pequenas distrações podem gerar graves acidentes. o impacto negativo da fadiga mental na performance, saúde e bem estar dos indivíduos, torna-se, desta forma, um dos principais motivos que leva ao desenvolvimento de metodologias de deteção destes estados mentais, de forma a preveni-los. efetivamente, ao longo do tempo diversas metodologias de deteção de fadiga têm sido desenvolvidas, contudo a maioria é pouco objetiva ou requer um grande investimento económico. a presente monografia apresenta um estudo de um sistema de aprendizagem não supervisionada de casos de fadiga mental, utilizando padrões de interação homem-computador, recolhidos através da sensorização de rato e teclado. este estudo permitiu uma classificação correta de 83,3% de novos casos de fadiga mental.",
      "during long periods of cognitive activity it is common to have a feeling of tiredness and lack of energy, which is followed by a performance decrease. this state, commonly defined as mental fatigue, is considered one of the major causes of human error. mental fatigue effects on tasks that require high levels of concentration should be studied and anticipated to minimize errors. aviation and medicine are good examples of these tasks because small distractions could lead to severe consequences. the negative impact of mental fatigue on individuals performance, health and well-being, as well as the consequences of it, makes it necessary to develop strategies that detect these mental states. during the past years many methods have been developed. however, the majority of these methods are not very objective and requires a large economic investment. the following dissertation presents a non-invasive automatize system for unsupervised learning in mental fatigue, by using human-computer interaction patterns, collected through mouse and keyboard sensing.this study allowed a correct classification of 83.3 % of new cases of mental fatigue."
    ],
    0.06666666666666667
  ],
  [
    [
      "web-based videotelephony services currently occupy a prominent place in the wide range of services and generated network traffic. with the growing use of such services comes also an increasing need for the evaluation of user experience. depending on network conditions, there are relevant qos parameters (bandwidth, delay, loss ratio, jitter, etc.) that have an impact on the quality of experience of the service. the objective of this thesis is to find out which network parameters affect videocalls quality of experience, which are the threshold values for which the quality of experience levels are affected, in what way is the quality of experience impacted and what is their relation. the quality of experience was evaluated using objective methods for all video and audio samples collected during the experimentation phase. conclusions were made based on the results which are presented in this dissertation.",
      "os serviços de vídeo-telefonia na rede internet ocupam actualmente um lugar de relevo no leque de serviços e tráfego de rede gerado. com a crescente utilização deste tipo de serviços surge também uma maior necessidade de avaliar a qualidade de experiência que o utilizador experiência. dependendo do tipo de condições que uma rede apresenta, existem parâmetros de qos relevantes (largura de banda, atraso, perda de pacotes, jitter, etc.) que afectam a qualidade de experiência do uso do serviço. esta dissertação tem como objectivo descobrir quais os parâmetros de uma rede têm efeito na qualidade de experiência de uma videochamada, quais as condições limite para as quais os níveis de qualidade de experiência são afectados, de que forma é a qualidade de experiência afectada e qual a sua relação. a qualidade de experiência foi avaliada usando métodos objectivos em todas as sequências vídeo e áudio recolhidas durante a fase de testes. foram retiradas conclusões com base nos resultados obtidos que são apresentadas nesta dissertação."
    ],
    [
      "epidemic multicast protocols, also known as gossip protocols, offer fault tolerance and good performance at large scale. therefore, these are used in peer-to-peer (p2p) systems on the internet and in nosql data management systems. research has shown there are multiple variants of these protocols which are most efficient in certain environments and applications. some protocols, such as plumtree, even allow the application to configure to obtain different performance trade-offs. this dissertation aims at taking advantage of machine learning (ml) to configure these protocols, developing a solution that adapts in runtime to network conditions and evaluate it experimentally. the results obtained by using ml models to control the transmission strategy used when forwarding messages show that it is possible to achieve a better trade-off between bandwidth used and the time to reach the entire network. moreover, this does not endanger the characteristics of epidemic multicast protocols, maintaining their reliability while becoming even more scalable.",
      "os protocolos de difusão epidémica, também conhecidos como gossiping, oferecem tolerância a faltas e bom desempenho em grande escala. são por isso usados, por exemplo, em sistemas entre-pares (p2p) na internet e em sistemas de gestão de dados nosql. a investigação feita mostrou que existem múltiplas variantes destes protocolos, adaptadas a diferentes ambientes e aplicações. alguns protocolos concretos, como o plumtree, permitem até que a aplicação faça uma configuração das suas características, de forma a obter diferentes compromissos de desempenho. nesta dissertação apresenta-se uma abordagem que tira partido de tecnologias de aprendizagem automática para fazer a configuração destes protocolos, desenvolvendo uma solução capaz de se adaptar em runtime tendo em conta o estado atual da rede e posteriormente é feita uma avaliação da solução experimentalmente. os resultados obtidos com os modelos que controlam a estratégia de transmissão na distribuição de mensagens demonstram ser possível alcançar um melhor compromisso entre o número de mensagens enviadas e o tempo necessário para as distribuir. além disso, não compromete as características dos protocolos de difusão epidémica, mantendo a sua confiabilidade e tornando-se ainda mais escaláveis."
    ],
    0.0
  ],
  [
    [
      "recent evolution of high performance computing moved towards heterogeneous platforms: multiple devices with different architectures, characteristics and programming models, share application workloads. to aid the programmer to efficiently explore these heterogeneous platforms several frameworks have been under development. these dynamically manage the available computing resources through workload scheduling and data distribution, dealing with the inherent difficulties of different programming models and memory accesses. among other frameworks, these include gama and starpu. the gama framework aims to unify the multiple execution and memory models of each different device in a computer system, into a single, hardware agnostic model. it was designed to efficiently manage resources with both regular and irregular applications, and currently only supports conventional cpu devices and cuda-enabled accelerators. starpu has similar goals and features with a wider user based community, but it lacks a single programming model. the main goal of this dissertation was an in-depth evaluation of a heterogeneous framework using a complex application as a case study. gama provided the starting vehicle for training, while starpu was the selected framework for a thorough evaluation. the progressive photon mapping irregular algorithm was the selected case study. the evaluation goal was to assert the starpu effectiveness with a robust irregular application, and make a high-level comparison with the still under development gama, to provide some guidelines for gama improvement. results show that two main factors contribute to the performance of applications written with starpu: the consideration of data transfers in the performance model, and chosen scheduler. the study also allowed some caveats to be found within the starpu api. although this have no effect on performance, they present a challenge for new coming developers. both these analysis resulted in a better understanding of the framework, and a comparative analysis with gama could be made, pointing out the aspects where gama could be further improved upon.",
      "a recente evolução da computação de alto desempenho é em direção ao uso de plataformas heterogéneas: múltiplos dispositivos com diferentes arquiteturas, características e modelos de programação, partilhando a carga computacional das aplicações. de modo a ajudar o programador a explorar eficientemente estas plataformas, várias frameworks têm sido desenvolvidas. estas frameworks gerem os recursos computacionais disponíveis, tratando das dificuldades inerentes dos diferentes modelos de programação e acessos à memória. entre outras frameworks, estas incluem o gama e o starpu. o gama tem o objetivo de unificar os múltiplos modelos de execução e memória de cada dispositivo diferente num sistema computacional, transformando-os num único modelo, independente do hardware utilizado. a framework foi desenhada de forma a gerir eficientemente os recursos, tanto para aplicações regulares como irregulares, e atualmente suporta apenas cpus convencionais e aceleradores cuda. o starpu tem objetivos e funcionalidades idênticos, e também uma comunidade mais alargada, mas não possui um modelo de programação único o objetivo principal desta dissertação foi uma avaliação profunda de uma framework heterogénea, usando uma aplicação complexa como caso de estudo. o gama serviu como ponto de partida para treino e ambientação, enquanto que o starpu foi a framework selecionada para uma avaliação mais profunda. o algoritmo irregular de progressive photon mapping foi o caso de estudo escolhido. o objetivo da avaliação foi determinar a eficácia do starpu com uma aplicação robusta, e fazer uma análise de alto nível com o gama, que ainda está em desenvolvimento, para forma a providenciar algumas sugestões para o seu melhoramento. os resultados mostram que são dois os principais factores que contribuem para a performance de aplicação escritas com auxílio do starpu: a avaliação dos tempos de transferência de dados no modelo de performance, e a escolha do escalonador. o estudo permitiu também avaliar algumas lacunas na api do starpu. embora estas não tenham efeitos visíveis na eficiencia da framework, eles tornam-se um desafio para recém-chegados ao starpu. ambas estas análisos resultaram numa melhor compreensão da framework, e numa análise comparativa com o gama, onde são apontados os possíveis aspectos que o este tem a melhorar."
    ],
    [
      "quantum computing is a new and exciting field of research that, using the properties of quantum mechanics, has the potential to be a disruptive technology, being able to per form certain computations faster than any classical computer, such as shor’s factorization algorithm and grover’s algorithm. although there are several quantum computer with different underlying technologies, one of the main challenges of quantum computation is the occurence of errors, destroying the information and making computation impossible. errors may have several different sources namely, thermal noise, faulty gates or incorrect measurements. the present dissertation aims to study and employ methods for reducing the effects of errors during quantum computation and correct them using stabilizer codes, which are a very powerful tool to produce circuit encoding networks that can, in theory, protect quantum systems from errors during transmission. a proof of concept algorithm was implemented using qiskit, a python based program development language for the ibm q machines, and tested on both simulators and real systems. the algorithm is capable of, given any stabilizer in standard form, generate the circuit encoding network. due to technological limitations associated with current quantum computers the results obtained in ibmq_guadalupe fail to show the efficacy of stabilizer codes.",
      "a computação quântica é uma área de investigação recente que, usando as propriedades da mecânica quântica, tem o potencial de ser uma tecnologia disruptiva, sendo capaz de realizar alguns tipos de computação de forma mais rápida do que qualquer outro computador clássico atual, tais como, o algoritmo de fatorização de shor e o algoritmo de procura de grover. apesar de já existirem vários computadores quãnticos com tecnologias de diferentes modos de operação, um dos principais desafios que a computação quântica enfrenta é a existência de erros, destruindo a informação presente e impossibilitando a computação. os erros podem ser de várias fontes, nomeadamente, ruido térmico, operações deficientes ou medidas incorrectas. esta dissertação tem como objectivo estudar e aplicar métodos para reduzir os efeitos dos erros durante a computação quântica e corrigi-los usando códigos estabilizadores, que são uma ferramenta poderosa para produzir circuitos que podem, em teoria, proteger sistemas quânticos de erros ocorridos durante a transmissão. foi implementado um algoritmo usando qiskit, uma linguagem à base de python usada para desenvolver programas nas máquinas da ibm, que foi testado em simuladores e em sistemas físicos. o algoritmo é capaz de, dado um estabilizador na sua forma standard, gerar o circuito codificador. devido a limitações da tecnologia associadas aos atuais computadores quânticos, os resultados obtidos na máquina ibimq_guadalupe não demonstram a eficácia dos códigos estabilizadores."
    ],
    0.0
  ],
  [
    [
      "the traditional application format perfectly suited the necessities of the software industry a few years ago, but today’s needs often call for distributed applications, based on microservices, and containerized. this approach is advantageous at several levels: better usage of available resources, automatic deploy ment with integrated scaling and fault tolerance, as well as the inherent security granted by the isolation from the host’s operating system. further, in order to avoid the acquisition and maintenance costs associated with the physical equipment, companies are now migrating their workloads to the cloud, which can be further supported through a microservice based architecture, thus capitalizing on the benefits of both. microservice based architectures lead to an effort to rethink current applications, so that they can be migrated to a containerized solution and deployed in a cloud environment. one case is the application ksipadp, developed by altice labs, and the subject case of this dissertation. it is based on kamailio and acts as a session initiation protocol (sip) server with several components. over the course of this dissertation, ksipadp is first migrated into a containerized docker environment, followed by a single instance deployment in kubernetes, and finally into a scalable kubernetes implementation that integrates additional services like rtpengine or a media server. every aspect of the process that led to the project’s successful completion is detailed over the course of the dissertation, alongside all the tests that were performed in order to validate the solution’s correct functionality.",
      "se o formato de aplicação tradicional se enquadrava perfeitamente nas necessidades da indústria da informática há alguns anos, atualmente verifica-se uma migração para a implementação de soluções baseadas em microserviços em containers. esta abordagem traz vantagens a diversos níveis: melhor utilização dos recursos disponíveis, instanci ação automática com escalabilidade e resiliência integradas, além da segurança inerente à utilização de aplicações isoladas do sistema operativo instalado no servidor. ainda, uma das grandes preocupações para as empresas de hoje em dia é a de migrar os seus am bientes de desenvolvimento para a nuvem, de modo a evitar os custos de aquisição e manutenção do equipamento. esta tecnologia, aliada a arquiteturas baseadas em microserviços, permite obter os benefí cios associados a ambas. deste modo, existe um grande esforço em repensar a implementação de aplicações já existentes, por modo a transitar para uma solução que possa viver em containers e ser lançada num ambiente de nuvem. este é o caso da aplicação kamailio-ksipadp, que age como um servidor de sip. este é primeiramente migrado para uma arquitetura baseada em containers em docker, seguida de uma instalação num único nodo em kubernetes, e finalmente para uma implementação escalável, também em kubernetes. esta integra também serviços adicionais, como o rtpengine e um media server. todos os passos que levam a estas implementações são detalhados ao longo da dissertação, assim como os testes que validam que as funcionalidades pretendidas são atingidas."
    ],
    [
      "a aplicação de técnicas orientadas a objectos e de aprendizagem automática sobre imagens de satélite tem sido alvo de interesse nos últimos anos. o aumento da qualidade e quantidade de imagens, disponibilizadas por programas de observação da terra como por exemplo o programa cope rnicus, levou à geração de uma grande quantidade de dados. de entre as várias aplicações destes dados destaca-se a criação de mapas do coberto do solo. com a presente dissertação pretendia-se criar modelos de aprendizagem automática capazes de segmentar e classificar com precisão imagens de satélite, gerando automaticamente um mapa do coberto do território português. durante a dissertação foram realizadas várias experiências com as bandas espetrais do satélite sentinel-2, com índices espetrais e com diversos conjuntos de classes do coberto. foram testadas três arquiteturas nos modelos de aprendizagem automática treinados, que adotam duas técnicas diferentes para classificação das imagens. numa das técnicas a classificação é orientada ao objeto, e neste caso a arquitetura adotada nos modelos foi uma rede neuronal artificial u-net. na outra técnica, a classificação é orientada ao pixel e os modelos de aprendizagem automática testados foram a floresta aleatória e a máquina de vetores de suporte. a acurácia global dos resultados obtidos variou entre 82.32% e 94.75%, dependendo fortemente do número de classes em que se classifica o coberto. o resultado de 94.75% foi obtido quando se classifica o coberto em apenas 5 dasses. contudo conseguiu-se uma acurácia bem interessante de 92.37%, no modelo treinado para classificar 8 classes.",
      "the application of object-oriented and machine learning techniques on satellite imagery has been the subject of interest in recent years. the increase in quality and quantity of images, made available by earth observation programs, such as the copernicus program, led to the generation of a large amounts of data. among the various applications of these data is the creation of land cover maps. this dissertation aimed to create machine learning models capable of accurately segment and classify satellite images, automatically generating a land cover map of the portuguese territory. during the dissertation several experiments were carried out with the spectral bands of the sentinel-2 satellite, with vegetation indices, and with several sets of land cover classes. three architectures were tested in the trained machine learning models, which adopt two different techniques for image classification. one of the classification techniques follows an object-oriented approach, and in this case the architecture adopted in our models was a u-net artificial neural network. the other classification technique is pixel-oriented, and the machine learning models tested were random forest and support vector machine. the overall accuracy of the results obtained ranged from 82.32% to 94.75%, depending strongly on the number of classes into which the land cover was classified. the result of 94.75% was obtained when classifying the cover into only 5 classes. however, a very interesting accuracy of 92.37% was achieved by the model trained to classify 8 classes."
    ],
    0.3
  ],
  [
    [
      "human t cells are essential in the control of pathogen infections, cancer and autoimmune diseases. immune responses mediated by t cells are aimed at specific peptides, designated t cell epitopes, that are recognized when bound to human leukocyte antigen (hla) molecules. the hla genes are remarkably polymorphic in the human population driving a broad and fine-tuned capacity of their encoded proteins to bind a wide array of peptide sequences. amino acid variants in epitopes might impact the hla-peptide interaction and consequently the level and type of generated t cell responses. having the tools to effectively predict and measure the impact of amino acid variants in hla binding will be of great value for the future of personalized host directed therapies. multiple algorithms based on machine learning (ml) have been implemented to estimate hla-peptide binding. however, there is still no tool capable of performing integrated analyses, namely comparing wild type and mutant sequences by predicting all overlapping peptides including amino acid positions of interest. this requires that researchers have programming skills to analyse prediction data for hla peptide binding and to extract potentially meaningful conclusions from that data. the main objective of this thesis was to design and implement a web server, called hla binding intelligence (habit), that automates hla binding prediction and advanced interpretation of the impact of peptide variants in inducing adaptive immune responses. habit integrates the best overall predictors for peptide-hla binding (class i and ii), netmhcpan and netmhciipan, respectively. the application features an intuitive and user-friendly interface available online for academic users, including comparative studies of wild type and mutant sequences, statistical tests and calculations of human population coverage. the performance of the web server was tested with a case study representative of tuberculosis, the r233l mutation found in the acetyl-/propionyl-coa carboxylase beta chain (accd2) protein. the results obtained through the interactive graphical interface allowed the automated identification of the most promising peptide sequences to be used in the design of t cell-mediated immunotherapy approaches. overall, habit provides a quick and easy way to answer major questions of rational immunotherapy studies, namely “what is the influence of an amino acid variants on hla binding?”, “do wild type and mutant epitopes show statistically significant differences in predicted hla binding properties?” and “what is the predicted population coverage of the epitopes?”. thus, habit constitutes a promising and reliable advance in the discovery of molecular determinants that influence the variation in t cell mediated immune responses.",
      "no ser humano, as células t são essenciais no controle de infeções patogénicas, do cancro e das doenças autoimunes. as respostas imunológicas mediadas por células t destinam-se a péptidos específicos, designados por epítropos de células t, que são reconhecidos quando ligados previamente a moléculas de hla. os genes de hla são extremamente polimórficos na população humana, deste modo as proteínas que codificam, apresentam uma ampla e aperfeiçoada capacidade para se ligarem a um enorme conjunto de sequências peptídicas. as variantes de aminoácidos encontradas nos epítopos podem afetar a interação hlap éptido e consequentemente o nível e tipo de respostas geradas pelas células t. a existência de ferramentas capazes de prever e de medir o impacto destas variantes de aminoácidos na ligação das moléculas de hla será de grande valor para o futuro das terapias personalizadas. múltiplos algoritmos baseados em machine learning foram implementados para estimar a ligação do péptido ao hla. no entanto, ainda não existe uma ferramenta capaz de realizar análises integradas, nomeadamente comparar sequências do tipo selvagem e mutante através da previsão de todos os péptidos que incluem a posição do aminoácido de interesse. isso exige que os investigadores tenham conhecimentos de programação para analisar dados de previsão para a ligação do péptido ao hla e extrair conclusões potencialmente significativas desses dados. a presente tese teve como principal objetivo projetar e implementar uma aplicação, designada de habit, que automatiza o processo de previsão da ligação ao hla e a interpretação avançada do impacto de variantes peptídicas na indução de respostas imunes adaptativas. o habit integra as melhores ferramentas de previsão geral para a ligação dos péptidos `as moléculas hla (classe i e ii), o netmhcpan e o netmhciipan, respetivamente. o servidor apresenta uma interface intuitiva e user-friendly disponível online para os utilizadores académicos, incluindo estudos comparativos das sequências selvagem e mutante, testes estatísticos e cálculos de cobertura de população humana. o desempenho do servidor web foi testado com um caso de estudo representativo da tuberculose, a mutação r233l encontrada na proteína accd2. os resultados obtidos através da interface gráfica interativa permitiram de forma automatizada identificar as sequências de péptidos mais promissoras a serem utilizadas em projetos no contexto da imunoterapia. em geral, o habit fornece uma maneira rápida e fácil de responder às principais questões de estudo de imunoterapia racional, nomeadamente “qual ´e a influência de uma variante de aminoácido na ligação aos alelos?”, “os epítopos selvagem e mutante apresentam diferenças estatisticamente significativas nas propriedades de ligação ao hla?” e “qual ´e a cobertura populacional prevista para os epítopos?”. assim, o habit constitui um avanço promissor e confiável na descoberta de determinantes moleculares que influenciam a variação nas respostas imunes mediadas por células t."
    ],
    [
      "due to the security threats and complexity of network services, such as video conferencing, internet telephony or online gaming, which require high qos guarantees, the need for monitoring and evaluating network performance, in order to promptly detect and face security threats and malfunctions, is crucial to the correct operation of networks and network-based services. as the internet evolves in size and diversity, these tasks become difficult and demanding. moreover, administrative limitations can restrict the position and the scope of the links to be monitored, while legislation imposes limitations on the information that can be collected and exported for monitoring purposes and almost all organization can't monitor or have knowledge or evaluate the performance of the entire network. they only can do this to part of the network, which corresponds to their own network. in this thesis, we propose the use of tomographic techniques for network topology discovery and performance evaluation. network tomography studies the internal characteristics of the network using end-to-end probes, ie, it does not need the cooperation of the internal nodes of the network and can be successfully adopted in almost all scenarios. thus, it is possible to have knowledge of the network characteristics out of the administrative borders. in this thesis we propose a new approach to probe packet sandwich, where we use ttl-limited probes to infer the delay of a path hop-by-hop. we have shown that this approach is more effective than existing ones. this work was developed under the erasmus student mobility program, in the telecommunication networks research group, dept. of information engineering, university of pisa.",
      "devido às ameaças de segurança e complexidade dos serviços de rede, tais como videoconferência, telefonia via internet ou jogos on-line, que exigem altas garantias de qos, a necessidade de monotorização e avaliação de desempenho da rede, a fim de detectar prontamente e enfrentar as ameaças de segurança e mau funcionamento, é crucial para o correto funcionamento das redes e serviços baseados em rede. à medida que a internet evolui em tamanho e diversidade, essas tarefas tornam-se difíceis e exigentes. além disso, as limitações administrativas podem restringir a posição e o alcance dos links a serem monitorizados, enquanto a legislação impõe limitações sobre as informações que podem ser coletadas e exportadas para fins de monotorização e quase todas as organizações não podem controlar ou ter conhecimento ou avaliar o desempenho de toda a rede. eles só podem fazer isso a parte da rede, o que corresponde à sua própria rede. neste trabalho, nós propomos o uso de técnicas tomográficas para a descoberta da topologia da rede e avaliação de desempenho. a tomografia de rede estuda as características internas da rede usando medições fim-a-fim, ou seja, não necessita da ajuda dos nós internos da rede, podendo ser adoptadas com sucesso em quase todos os cenários. desta maneira é possível obter conhecimento das características da rede para além dos limites administrativos. neste tranalho propomos uma nova abordagem do packet sandwich probe, onde utilizamos pacotes ttl-limited para inferir o delay de um path hop-by-hop. nós mostramos que esta abordagem é mais eficaz que outras já existentes. este trabalho foi desenvolvido no âmbito do programa de mobilidade de estudantes erasmus, no grupo de investigação em redes de telecomunicações, departamento de engenharia de informação da universidade de pisa."
    ],
    0.0
  ],
  [
    [
      "esta dissertação de mestrado examina a transformação dinâmica da indústria da moda, desde as práticas tradicionais até a combinação de tecnologias digitais avançadas. o foco principal está na reinvenção da moda através da adoção de programas digitais inovadores em 3d, que são cada vez mais proeminentes no cenário da moda atual. esta pesquisa apresenta uma análise abrangente no papel do design de moda, com vertente no 3d, na indústria da moda contemporânea, particularmente no desenvolvimento e apresentação de coleções de moda. o contraste entre as apresentações físicas tradicionais de moda e a abordagem moderna é orientada para a tecnologia que destaca os benefícios da tecnologia 3d no aumento da acessibilidade, da relação custo-benefício e da liberdade criativa. o estudo aborda o design 3d que integra sistemas de simulação virtual para oferecer um novo upgrade na apresentação de coleções de moda no processo de desenvolvimento de design e na vertente comercial. a autenticidade da representação material e a criação de gémeos digitais realistas também são exploradas, com foco em plataformas que melhoram a fidelidade visual dos ativos digitais. a pesquisa inclui uma revisão ampliada da literatura e um estudo das marcas de moda que adotaram esta inovação tecnológica, selecionando 3 casos de estudos proeminentes para análise detalhada. o estudo sugere que para que as empresas de moda prosperem nas vendas e cativar novos clientes, é imperativo o investimento estratégico na tecnologia 3d e na integração digital. esses avanços são fundamentais para promover uma indústria da moda inclusiva, sustentável e inovadora. concluindo, esta dissertação serve como um recurso essencial para profissionais e entusiastas da moda, oferecendo insights sobre o potencial transformador da tecnologia digital 3d no futuro das apresentações de moda.",
      "this master's dissertation examines the fashion industry's dynamic transformation, tracing its transition from traditional practices to the combined use of advanced digital technologies to develop fashion products and enhance collections. the core focus is the reinvention of fashion by adopting innovative 3d digital programs, which are increasingly prominent in today's fashion landscape. this research presents a comprehensive analysis of the role of 3d design in the contemporary fashion industry, particularly in developing and showcasing fashion collections. the contrast between traditional physical fashion presentations and the modern technology-driven approach highlights the benefits of 3d technology in enhancing accessibility, cost-effectiveness, and creative freedom. the study approaches 3d design that integrates virtual simulation systems to offer a new upgrade in showcasing fashion collections in the design development process and commercial proposals. it also explores the authenticity of material representation and the creation of realistic digital twins, focusing on platforms that enhance the visual loyalty of digital assets. the research includes a thorough literature review and examination of fashion brands that have embraced this technological innovation, selecting three case studies for detailed analysis. the study suggests that for fashion companies to thrive in sales and customer attraction, strategic investment in 3d technology and digital integration is imperative. such advancements are key to fostering an inclusive, sustainable, innovative fashion industry. conclusively, this dissertation serves as an essential resource for fashion professionals and enthusiasts, offering insights into the transformative potential of 3d digital technology in the future of fashion presentation."
    ],
    [
      "particle based methods have been been increasing in popularity in fluid simulations as of late due to the increases seen in computational power in the form of various many core devices like graphics-cards (gpus) or high-core count cpus. the increasing popularity of particle based fluid simulations resulted in a need for higher fidelity renders which portray the particle data in a cohesive substance rather than isolated particles. rendering the particle data in real-time however is a difficult problem due to the fact that the simulations tend to be computationally intensive and, as a result, the rendering has to deal with computational constraints that make it difficult to achieve visual quality while, at the same time, keeping the interactive aspect of the simulation. currently there are various approaches to rendering particle data which thread a balance between visual qual- ity and performance. these techniques can vary widely by having entirely different algorithms/approaches and having different objectives, some of them focusing on having the best looking surfaces, while others pretending to achieve the shortest render times, or even trying to have a low memory footprint. computational constraints have kept certain techniques from being usable in the interactive realm. this con- straints have, however, been loosening up due to hardware advances which has, in turn, been lowering the gap between high-quality renders and interactive rendering and thus opening the door to new rendering approaches. this thesis aims to explore the implementation of real-time fluid rendering in conjunction with a preexisting particle-based simulation. the thesis will be divided in two halves with the first half covering a screen-space implementation, a family of techniques which aims to render the fluid surface while having a low computational cost, while the second half explores a volumetric render implementation based on a voxel grid while leveraging the computational power of a modern gpu. the screen-space implementation is able to represent the frontal surface of the fluid, and also implements the extraction of a back surface. this enables it to enhance its visual fidelity at a low computational cost. this work also compares different approaches used in the process of smoothing these surfaces, which enable a higher surface cohesion while tackling the preservation of the fluid’s edges. the volumetric implementation uses a voxel grid to represent the fluid, enabling it to render multiple refractions thus achieving a more realistic render. the volumetric implementations is also able to represent occluded fluid features such as air bubbles by leveraging the gpu memory, and a more accurate colouring of the fluid.",
      "métodos baseados em partículas têm estado a aumentar em popularidade em simulações de fluidos devido ao aumento em poder computacional nas placas gráficas (gpus) e nos processadores. o aumento da popularidade das simulações de fluidos com partículas resultou na necessidade de renderiza- ção de alta qualidade, os quais representam as partículas como uma substância coesa. renderizar as partículas em tempo real, no entanto, é um problema difícil devido ao facto de que estas simulações são computacional- mente intensivas e, consequentemente, o renderizador tem de lidar com restrições computacionais adicionais as quais dificultam a obtenção de qualidade visual enquanto se mantém a componente interativa. atualmente existem várias técnicas de renderizar as partículas, cada uma com um diferente equilíbrio entre qualidade visual e desempenho. estas técnicas podem variar bastante devido à utilização de diferentes algorit- mos e devido a terem diferentes objetivos. algumas técnicas focam-se em obter as superfícies com o melhor aspeto, enquanto que outras tentam obter a renderização mais rápida, ou a minimização do uso de memória. algumas técnicas têm sido impedidas de ser usadas em tempo real devido a restrições computacionais. estas restrições têm, no entanto, estado a relaxar devido aos avanços no hardware, o que tem levado ao abrir das portas a novos métodos de renderização os quais se aproximam às renderizações de alta qualidade. esta tese tem como objetivo explorar a implementação de um renderizador de fluidos em tempo real uti- lizando uma simulação de fluidos pré-existente como base. a tese será dividida em duas metades. a primeira metade cobre a implementação de um método screen-space, uma família de métodos o qual tem como objetivo renderizar a superfície do fluido com pouco custo computacional. a segunda metade cobre a implementação de um renderizador volumétrico baseado numa grelha de voxeis o qual utiliza o poder computacional de uma placa gráfica moderna. a implementação screen-space representa a superfície frontal do fluido e extrai a superfície traseira para poder aumentar o realismo da imagem com pouco custo computacional adicional. esta implementação também compara diferentes métodos de suavização de superfície, os quais são responsáveis pela coesão da superfície e da preservação das bordas desta mesma. a implementação volumétrica utiliza uma grelha de voxeis para representar o fluido. esta grelha torna pos- sível a renderização de múltiplas refrações as quais aumentam o realismo do resultado final e permitem a representação de propriedades do fluido ocluídas, como bolhas de ar, e a colorização mais realista do fluido."
    ],
    0.0
  ],
  [
    [
      "ao longo das últimas duas décadas a utilização de sistemas domóticos, num mundo cada vez mais conectado e tecnológico, tem vindo a revelar-se cada vez mais atrativo e com maior aceitação do público em geral. novos produtos suportados por novos protocolos e tecnologias estão constantemente a ser introduzidos no mercado. no entanto, este desenvolvimento foi quase sempre efetuado sem grande preocupação em definir regras e normas para que fosse possível a interoperacionalidade entre produtos de diferentes fabricantes, originando soluções pouco modulares, de elevado custo e forçando os clientes a escolher um ecossistema de um mesmo fabricante sem ser possível de forma rápida e facilitada integrar tecnologias de vários fabricantes num mesmo sistema. o principal objetivo desta dissertação foi a definição de uma arquitetura integrada para sistemas domóticos baseada no protocolo de gestão snmp e que permitisse ultrapassar algumas das mais importantes limitações das soluções atuais para este tipo de sistema. nesse sentido foi criada uma nova mib domótica para implementação num agente snmp integrador. além disso, foi desenvolvido um novo protocolo de gestão para dispositivos domóticos, mais simples que o snmp e mais adequado para gestão de pequenos equipamentos sensores ou atuadores utilizados em sistemas domóticos. este protocolo, designado por light snmp (l-snmp), será utilizado na comunicação entre o agente snmp e os dispositivos domóticos que implementam uma light mib (l-mib) de domótica. no decorrer do projeto foi criado um sistema protótipo com dispositivos domóticos implementando a l-mib de domótica, um agente snmpv2 implementando a mib domótica e uma aplicação gestora snmp que contém um simples interface com o utilizador. as experiências realizadas com este protótipo permitiram confirmar a correção funcional da solução e a sua viabilidade como alternativa tecnológica válida, potencialmente de baixo custo e com elevados níveis de interoperabilidade.",
      "over the past two decades the use of domotic systems in an increasingly connected and technological world, has been becoming increasingly attractive and generally with greater public acceptance. new products supported by new protocols and technologies are constantly being introduced into the market. however, this development was almost always carried out without major concerns to define rules and standards to allow inter-operationality between different manufacturers, resulting in unmodular, high-cost solutions and forcing customers to choose an ecosystem from the same manufacturer without being possible to integrate technologies from multiple manufacturers in the same system in a quickly and easily way. the main objective of this dissertation was the definition of an integrated architecture based on the snmp management protocol and which would allow the overcoming of some of the most important limitations of current solutions for this type of system. in this sense, a new mib for home automation for implementation in an integrative snmp agent. in addition, it has been developed a new management protocol for domotic devices, simpler than snmp and more appropriate for management of small sensor or actuators devices used in domotic systems. this protocol, called light snmp (l-snmp), will be used in communication between the snmp agent and the domotic devices that implement a light mib (l mib). during the project, a prototype system was created with domotic devices implementing the l-mib (l-mib), an snmpv2 agent implementing the domotic mib and an snmp management application that contains a simple user interface. the experiments carried out with this prototype allowed to confirm the functional correction of the solution and its viability as a valid technological alternative, potentially low-cost and with high levels of interoperability."
    ],
    [
      "the analysis of the player’s behaviour is a requirement with growing popularity in the traditional computer games segment and has been proven to aid the developers create better and more profitable games. there is now interest in trying to replicate this attainment in a less conventional genre of games known as web browser games. the main objective of this work is to analyse and create a technique for the analysis of the behaviour of the players inside a web browser game. for this analysis a system to automatically collect, process and store the relevant data for the referred analysis was developed. the web browser game used as a case study for this work is developed by 5dlab and is called wack-a- doo. the work developed focused on creating short-term prediction models using the information collected during the first days of playing for each player. the objectives of these models are to predict the time played or the conversion state of the players. with the study of the created models it was possible to extract results that provide potentially useful information to increase the profitability of wack-a-doo.",
      "a análise do comportamento de jogadores é uma prática com crescente popularidade no segmento dos jogos de vídeo tradicionais. esta técnica foi já aprovada como capaz de ajudar os criadores a desenvolver melhores e mais lucrativos jogos. existe agora interesse em tentar replicar este sucesso num género de jogos de vídeo menos convencionais normalmente referidos como jogos de browser web. o objetivo deste trabalho é analisar e criar uma técnica para essa análise do comportamento dos jogadores de um jogo de browser web. para isto um sistema automático de recolha, processamento e armazenamento dos dados relacionados com o comportamento dos jogadores foi desenvolvido. o jogo de browser web usado para este estudo foi criado pela empresa 5dlab e dá pelo nome de wack-a-doo. o trabalho desenvolvido centrou-se em fazer modelos de previsão de curto prazo usando as informações recolhidas durante os primeiros dias de jogo de cada jogador. estes modelos têm como objetivo prever o tempo jogado e o estado de conversão do jogador. estudando os modelos criados foi possível extrair resultados que fornecem informação potencialmente útil para melhorar a rentabilidade do wack-a-doo."
    ],
    0.0
  ],
  [
    [
      "uma ontologia pode ser definida como um modelo de dados, que representa uma descrição de conceitos num determinado domínio. esta tem como principal objetivo, aumentar a compreensão partilhada num determinado domínio, eliminando as diferenças, sobreposições e incompatibilidades em conceitos, estruturas, entre outros, criando assim uma especificação formal legível por um computador, e explícita, no sentido em que as entidades da ontologia são claramente definidas, distintas e inter-relacionadas entre si. tendo em conta o aumento exponencial de dados presentes na web, ontologias têm sido cada vez mais usadas como modelo de armazenamento de dados. este aumento de usabilidade leva a que seja necessário o desenvolvimento de ferramentas que nos permitam criar/editar/analisar ontologias, ou seja, que nos permitam não só a interagir com elas, mas também realizar um tratamento dos dados consequentemente recolhidos. assim, pretende-se nesta dissertação, desenvolver uma aplicação web capaz de gerar, através de uma especificação, um relatório, referente a uma determinada ontologia acessível através de um determinado endpoint. por outras palavras, pretende-se a criação de uma aplicação que permita ao utilizador obter, da ontologia, apenas os dados que pretende, em vez de uma quantidade enorme de dados sem qualquer tratamento prévio.",
      "an ontology can be defined as a data model, which represents a description of concepts in a given domain. its main objective is to increase the shared understanding in a given domain, eliminating differences, overlaps and incompatibilities in concepts, structures, among others, thus creating a formal specification readable by a computer, and explicit in the sense that the entities of ontology are clearly defined, distinct and interrelated. given the exponential increase in data present on the web, ontologies have been increasingly used as a data storage model. this increase in usability makes it necessary to develop tools that allow us to create / edit / analyse ontologies, that is, that allow us not only to interact with them, but also to perform a treatment of the consequently collected data. thus, this dissertation intends to develop a web application capable of generating, through a specification, a report, referring to a certain ontology accessible through a given endpoint. in other words, it is intended to create an application that allows the user to obtain from the ontology only the data he wants instead of a huge amount of data without any previous processing."
    ],
    [
      "biomedical literature is composed of a large and ever increasing number of publications, written in natural language. patents are a relevant fraction of these publications, considered important sources of information due to all the curated information available in the documents, from the granting process. although being real technological libraries, their unstructured data turns the search of information within these documents a challenging task. biomedical text mining is a scientific field that explores this task, creating methodologies to search and structure the information in the biomedical literature. information retrieval is one of the biomedical text mining tasks, in which the relevant information is obtained from an extensive collection of documents using several text retrieval methodologies. getting all the information available on a patent document requires the download of the respective pdf document, that is then converted into a machine-readable text by technologies as optical character recognition (ocr). in this project, an information retrieval, and a pdf to text conversion system were developed building a “patent pipeline” which was integrated into @note2, an open-source computational framework for biomedical text mining. the patent pipeline can be disintegrated into four different tasks: the patent search, the retrieval of patent metadata, the retrieval of their pdf files, and the extraction of all the information from these documents. a set of patents from the biocreative v chemdner task was used to test the developed pipeline, evaluating the framework performance and the real capacity to retrieve the requested patents and extract their unstructured information. the results were promising, bringing to the scientific community the published patent information and allowing the posterior implementation of other biomedical text mining processes over these documents.",
      "a literatura biomédica é constituída por um número alargado e em crescimento de publicações escritas em linguagem natural. as patentes, uma fração integrante das referidas publicações, têm vindo a ser consideradas importantes fontes de informação, uma vez que possuem informação curada resultante do seu processo de atribuição. apesar de serem consideradas verdadeiras bibliotecas tecnológicas, a sua informação não estruturada transforma a procura de informação nesses textos uma tarefa deveras desafiante. a mineração de textos biomédicos é um campo científico que explora esta tarefa, criando metodologias para a pesquisa de informação estruturada em literatura biomédica. a obtenção de informação é uma tarefa integrante do processo de mineração de textos biomédicos, na qual a informação relevante é obtida de uma extensa coleção de documentos usando diversas metodologias. o processo de obtenção de toda a informação contida numa patente requer o download do respetivo ficheiro pdf que posteriormente é convertido em texto passível de ser lido por máquinas recorrendo a tecnologias de processamento tais como o reconhecimento ótico de carateres (ocr). neste projeto, um sistema de obtenção de informação e um sistema de conversão de pdf em texto foram desenvolvidos dando origem a uma ferramenta de tratamento de patentes que foi integrada no @note2, uma plataforma computacional de código aberto usada para a mineração de textos biomédicos. a pipeline elaborada pode ser desintegrada em quatro diferentes funções: pesquisa de patentes, obtenção de meta-informação das mesmas, obtenção dos seus ficheiros em formato pdf e a extração de todo o texto desses documentos. um conjunto de patentes do desafio biocreative v chemdner foi usado para testar a ferramenta desenvolvida, avaliando o seu desempenho e a sua real capacidade de obtenção das patentes e todo o processo de extração de informação das mesmas. os resultados são promissores, aproximando a comunidade científica da informação disponibilizada nas patentes publicadas, permitindo a posterior implementação de outros processos da mineração de textos biomédicos a esses documentos."
    ],
    0.3
  ],
  [
    [
      "a presente dissertação baseia-se na expectativa de diminuir o trabalho e tempo necessário para o desenvolvimento de uma loja online por programadores. com a evolução do comércio, começaram a surgir cada vez mais negócios com lojas online, sendo por isso cada vez maiores as exigências e condições para que estas se tornassem um sucesso. todas as lojas online costumam ter aspetos em comum como o carrinho de compras, a autenticação, entre outros componentes básicos sobre os quais se poderia salvar tempo de desenvolvimento se não se tivesse de reescrever sempre o mesmo código. esta dissertação teve início com a investigação dos parâmetros necessários para que uma loja on-line funcionasse, pelo que em seguida foi desenvolvido uma loja online de venda de jogos e produtos relacionados. após a investigação foi possível identificar vários aspetos e parâmetros necessários para o desenvolvimento web tanto no frontend como no backend, conseguindo-se assim fazer a distinção entre componentes e as suas relações. de seguida foi abordado o principal objetivo da dissertação que consistia em desenvolver uma ferramenta de criação de lojas online automática seguindo as configurações de diferentes programadores. esta ferramenta pretende diminuir o tempo despendido pelos programadores em cada novo projeto, ao lhes dar a oportunidade de obter o código de lojas online com diferentes layouts e diferentes parâmetros definidos por eles. após a criação deste projeto template usando a configuração dos programadores, o objetivo é que eles o adaptem aos pedidos dos seus clientes de uma forma simples, tendo sido usada uma programação modular ao longo deste projeto para facilitar a sua utilização. o projeto desenvolvido nesta dissertação focou-se na autenticação, algumas páginas relacionadas com o produto, na organização do código, na programação modular e na capacidade de os inputs dos programadores alterarem o resultado da ferramenta. no final deste trabalho, foi possível obter uma framework funcional com os pontos anteriormente mencionados, pelo que se atingiu o principal objetivo de desenvolver uma ferramenta que simplificasse o trabalho dos programadores.",
      "this dissertation is based on the premise of reducing the work and time needed by programmers to develop an online store. with the evolution of commerce, more and more businesses have started to set up online stores, and the requirements and conditions for them to become successful have become ever greater. all online stores tend to have aspects in common, such as the shopping cart, authentication, and other basic components that could save development time if the same code didn’t have to be rewritten over and over again. this dissertation began by investigating the parameters needed for an online store to work, and then developed an online store selling games and related products. by carrying out this step, it was possible to identify various aspects and parameters necessary for the development of both the web and the backend, thus distinguishing between components and their relationships. after this step, the main objective of the dissertation was addressed, which was to develop a tool for creating automatic online stores following the configurations of different programmers. this tool aims to reduce the time spent by programmers on each new project by giving them the opportunity to obtain the code for online stores with different layouts and different parameters defined by them. after creating this template project using the programmers’ configuration, the aim is for them to adapt it to their customers’ requests in a simple way, and modular programming has been used throughout this project to make it easier to use. the project developed in this dissertation focused on authentication, some product-related pages, code organization, modular programming and the ability of programmers’ inputs to alter the tool output. at the end of this work, it was possible to complete a functional framework with the points mentioned above, so the main objective of developing a tool that would simplify the work of programmers was achieved."
    ],
    [
      "computer vision is usually used as the perception channel of robotic platforms. these platforms must be able of visually scanning the environment to detect specific targets and obstacles. part of detecting obstacles is knowing their relative distance to robot. in this work different ways of detecting the distance of an object are analyzed and implemented. extracting this depth perception from a scene involves three different steps: finding features in an image, finding those same features in another image and calculate the features’ distance. for capturing the images two approaches were considered: single cameras, where we capture an image, move the camera and capture another, or stereo cameras, where images are taken from both cameras at the same time. starting by susan, then sift and surf, these three feature extraction algorithms will be presented as well as their matching procedure. an important part of computer vision systems is the camera. for that reason, the procedure of calibrating a camera will be explained. epipolar geometry and the fundamental matrix are two important concepts regarding 3d reconstruction which will also be analyzed and explained. in the final part of the work all concepts and ideas were implemented and, for each approach, tests were made and results analyzed. for controlled environments the relative distance of the objects is correctly extracted but with more complex environment such results are harder to obtain.",
      "a visão por computador é, normalmente, usada como o canal de percepção do mundo em plataformas robóticas. estas plataformas têm de ser capazes de rastrear, visualmente, o ambiente para detectar objectivos e obstáculos específicos. parte da detecção de obstáculos envolve saber da sua distância relativa ao robot. neste trabalho, são analisadas e implementadas diferentes formas de extrair a distância de um objecto. a extracção desta noção de profundidade de uma cena envolve três passos diferentes: encontrar características numa imagem, encontrar estas mesmas características numa imagem diferente e calcular as suas distâncias. para a captura de imagens foram considerados dois métodos: uma única câmara, onde é tirada uma imagem, a câmara é movida e é tirada a segunda imagem; e câmaras estéreo onde as imagens são tiradas de ambas as câmaras ao mesmo tempo. começando pelo susan, depois o sift e surf, estes três algoritmos de extracção de características são apresentados, assim como os seus métodos de emparelhamento de características. uma parte importante dos sistemas de visão por computador é a câmara, por este motivo, o procedimento de calibrar uma câmara é explicado. geometria epipolar e matriz fundamental são dois conceitos importantes no que refere a reconstrução 3d que também serão analisados e explicados. na parte final do trabalho, todos os conceitos e ideias são implementados e, para cada método, são realizados testes e os seus resultados são analisados. para ambientes controlados, a distância relativa é correctamente extraída mas, para ambientes mais complexos, os mesmos resultados são obtidos com mais dificuldade."
    ],
    0.0
  ],
  [
    [
      "it is clear that the trend towards higher levels of abstraction in programming methods, as well as the effort to make software design more of a scientific, engineering discipline, has led to the development of various programming paradigms and the use of rigorous proof methods to ensure the reliability and safety of critical software systems. however, the implementation of these formal methods can be challenging due to their reliance on inductive proofs following the invent-and-verify method. despite this, some in the field continue to seek out and use these theoretical foundations in an attempt to produce high-quality software. therefore, this study presents the potential for the correct-by-construction method, using galois connections and theoretical concepts from computer science to develop a methodology for constructing practically applicable software systems whose correctness is guaranteed from the outset.",
      "é clara a tendência em direção a níveis mais elevados de abstração nos métodos de programação, bem como o esforço para tornar o design de software mais uma disciplina científica e de engenharia, levando ao desenvolvimento de vários paradigmas de programação e ao uso de métodos rigorosos de prova para garantir a confiabilidade e segurança de sistemas de software críticos. no entanto, a implementação desses métodos formais pode ser desafiadora devido à sua dependência de provas indutivas uma vez seguido o método de “inventar-e-verificar”. apesar disso, alguns na área continuam a procurar e a utilizar tais fundamentos teóricos numa tentativa de produzir software de alta qualidade. assim, este estudo apresenta o potencial do método “correção-por-construção”, utilizando conexões de galois e conceitos teóricos das ciências da computação para desenvolver uma metodologia para construção de sistemas de software praticamente aplicáveis e cuja correção é garantida desde o início."
    ],
    [
      "nestes últimos anos, a importância da eficiência energética nos sistemas informáticos tem vindo a crescer exponencialmente desde a área móvel até à computação de alto desempenho. o crescimento do mercado dos gadgets e a sua crescente dependência dos serviços de computação em cloud são algumas das razões para este rápido avanço neste ramo tecnológico. a redução do consumo de energia e o aumento da produtividade de um sistema é, por várias razões, uma preocupação, tanto por parte do cliente como do fabricante. a exploração aprofundada do tópico pode contribuir para, por exemplo, o aumento da autonomia dos terminais móveis e a redução dos custos energéticos dos data centers e do cliente particular. neste sentido, a monitorização do consumo de energia de um sistema desempenha um papel fundamental para se aprimorar a eficiência energética do mesmo. no entanto, o grande desafio atual da monitorização passa por categorizar o consumo de energia a vários níveis como, por processo, por máquina virtual ou por subsistema de hardware. esta escala de granularidade na análise energética permite o desenvolvimento de relatórios mais incisivos e conclusivos sobre a distribuição de consumo de energia do sistema. no âmbito desta dissertação e tendo por base estes motivantes fatores, foi desenvolvido um modelo simples que visa estimar o consumo de energia do sistema na sua totalidade, categorizado por subsistema e, no caso do armazenamento secundário, também categorizado por processo. este documento apresenta um estudo sobre diferentes metodologias de medição assim como sobre as abordagens possíveis para o modelo em sistemas de teste com diferentes tipos de armazenamento secundário e com uma das ultimas gerações de processador da intel. numa fase inicial foi feita uma investigação a vários aspetos referentes à energia de um sistema, incluindo modelos de estimação do consumo de sistemas, métodos de medição de consumo e a precisão desses mesmos métodos. no desenvolvimento do modelo, recorreu-se apenas a recursos existentes no sistema em causa para viabilizar um mais fácil investimento a larga escala. por isso, o modelo recorre a interfaces de gestão e monitorização de energia como o rapl e acpi. foram analisados os mais importantes aspetos energéticos de um sistema como a distribuição do consumo de energia estático e dinâmico pelos subsistemas e avaliouse a eficiência e o desempenho dos mesmos nas mais diversas atividades. desta forma, garantiu-se uma maior polivalência do modelo e, de um modo geral, uma maior precisão do mesmo. o modelo foi validado com base em ferramentas disponibilizadas pelo sistema e através de medições físicas. os resultados obtidos parecem satisfatórios, tendo sido registadas taxas de erro máximas de 5% no consumo total e de 10% no consumo do armazenamento secundário. este modelo desenvolvido pode ser adaptado a outro sistema, no entanto, necessita de executar uma ferramenta de calibragem que realiza todas as etapas que foram executadas na configuração usada para este projeto. isto acontece essencialmente no subsistema de armazenamento secundário onde não se recorre a qualquer ferramenta existente para a estimação da sua energia. desta forma, há uma etapa inicial que consiste na exercitação do subsistema de armazenamento secundário através de ferramentas de benchmarking e na recolha de dados estatísticos do custo das operações. em seguida, é feito um estudo sobre esses mesmos dados e são atribuídos diferentes pesos energéticos para cada operação executada no subsistema. depois, constrói-se o modelo e este é calibrado com recurso a interfaces de energia como o rapl e acpi. no fim, este modelo deve ser capaz de apresentar a fatura energética de cada processo que utiliza o subsistema de armazenamento secundário. além disso, o modelo deve também estimar o consumo de energia total do sistema e a sua distribuição pelos principais subsistemas.",
      "in the past few years, the importance of energy efficiency in computer systems has been growing exponentially from the mobile area to the high-performance computing. the gadgets market growth and their increasing dependence on cloud computing services are some of the reasons for this fast progress in this technological field. the power consumption reduction and the increasing productivity of a system is, for multiple reasons, a concern both for the customer and the manufacturer. for example, the in-depth research of this topic can contribute for increasing the mobile terminals autonomy and for reducing the energy costs on data centers and even on the costumer devices. altogether, the power consumption monitoring of a system plays an important role to enhance its energy efficiency. some of the most challenging aspects of this research field are accurate power consumption measurement, estimation and categorization at different levels of the system, from hardware components to processes. this meticulous scale in the energy analysis allows the development of more incisive and conclusive reports about the system power consumption distribution. within this thesis and based on these challenging factors, it has been developed a simple model that aims to estimate the system power consumption in its entirety, categorized by subsystem and, in the case of secondary storage, also categorized by process. this document presents an analysis of different measurement methods as well as about possible approaches for a model that run over experimental systems with different configurations of secondary storage and using a last generation intel processor. initially a research was made concerning many aspects about a system energy, including power consumption estimation models, consumption measurement methods and those methods accuracy. for the model development, it has been used only the existing resources in the tested system to enable an easier investment on a large scale architecture. therefore, the model implements energy management and monitoring interfaces like rapl and acpi. the most important system energy concerns were analysed, such as the power consumption breakdown of static and dynamic power subsystems. the efficiency and performance of some system activities were evaluated. this process ensured the development of a more versatile model, and globally, with a greater accuracy. the model validation process was carried out based on tools provided by the system and by physical measurements. the obtained results seem adequate, having been recorded a maximum error rate of 5% concerning the total system consumption and 10% regarding the secondary storage consumption. this power consumption model can actually be applied to any typical computer system through a calibration tool also developed in this project. this calibration step will estimate relevant power consumption parameters regarding secondary storage since we cannot rely on existing tools or apis concerning this subsystem. this calibration will exercise the secondary under different usage patterns. the collected statistics will be processed and the cost of relevant operations regarding this secondary storage will be estimated. the next step involves the model development and its calibration using power interfaces as rapl and acpi. in the end, this model should be capable of presenting the energy bill of each process that uses the secondary storage subsystem. additionally, the model should also be able to estimate the total system power consumption and its breakdown by the major subsystems."
    ],
    0.3
  ],
  [
    [
      "televisions nowadays are shipped with more and more processing power. this allows the development of applications that run in the television. with the evolution of the different smart tv development frameworks, application development for televisions will become more usual. the aim of the project here reported is to study the viability of the available technologies for smart tv development, research the means to gather and classify news articles, study techniques of similar document detection and finally to implement a system divided in two parts: the back-end, where the news aggregation and management will occur; and the front-end, a smart tv application that will present to the user the news filtered according to the rating based on standard or customized criteria. as a result of this project, a system for news collection, management, and presentation was implemented. the back-end collects, classifies, and detects similar news articles, and also obtains news related images. then, and according to user preferences news are rated and served to the front-end. the front-end is a samsung smart tv application. samsung smart tv was chosen as the best suited smart tv framework for the project. while news are presented on the front-end, feedback about each news article is being sent to the back-end, which will cause changes in the news presentation order.",
      "hoje em dia as televisões têm cada vez mais poder de processamento. isto permite que se desenvolvam aplicações que correm na televisão. com a evolução das diferentes frameworks das smart tvs, o desenvolvimento de aplicações para televisão vai começar a ser mais comum. o objectivo do projecto aqui relatado é estudar a viabilidade das tecnologias disponíveis de desenvolvimento para smart tv, pesquisar os meios para obter e classificar artigos noticiosos, estudar técnicas para detetar documentos semelhantes e finalmente implementar um sistema dividido em duas partes: o back-end, onde vão ser obtidos e geridos os artigos noticiosos; e o front-end, que será uma aplicação para smart tv, irá apresentar notícias filtradas de acordo com o rating, que é baseado em critérios padronizados ou pessoais. como resultado deste projecto, foi implementado um sistema para obtenção, gestão e apresentação de notícias. o back-end reúne, classifica e deteta notícias semelhantes, obtendo ainda imagens relacionadas com as mesmas. depois, e de acordo com as preferências de utilizador, as notícias são avaliadas e enviadas para o front-end. o front-end é uma aplicação samsung smart tv. a samsung smart tv foi escolhida como a framework de smart tv mais apropiada para o projecto. enquanto as notícias estão a ser apresentadas no front-end, é enviado para o back-end feedback acerca de cada artigo noticioso, o que vai causar alterações na ordem em que as notícias são apresentadas."
    ],
    [
      "the success of a manufacturing company is linked to the efficiency of its supply chain. more specifically, procure ment, which refers to the scheduling and quantity planning of raw materials to order from suppliers, can have a big impact on the company’s operating costs. to be able to avoid stock shortages and assure the necessary stock levels for production, there are a few safety measures such as: safety time and safety stock. in bosch’s plant located in braga, the assignment of these values is done infrequently and the data taken into consideration is not very specific. this dissertation proposes a dynamic solution that should be able to provide a set of optimal pair solutions of safety time and safety stock according to certain objective measures, for each of the raw materials currently being used. this calculation is based on logistic data, which of some are static (master data) and other is historic data. this solution was developed using distributed data processing frameworks, such as apache, for execution speedup. more specifically, hdfs (hadoop distributed file system), a technology from apache hadoop to store data in a distributed manner and apache spark to load and process the data, by splitting processing tasks and assigning them to different servers inside a cluster.",
      "o sucesso de uma empresa de fabrico depende da eficiência da sua supply chain.em particular, a atividade de procurement, que consiste na calendarização e decisão de quantidades de matéria prima a encomendar aos fornecedores, pode ter um impacto elevado nos custos da empresa, e como tal é alvo de muito estudo. de modo a evitar quebras de inventário e garantir as quantidades necessárias de matérias primas para produção, existem algumas medidas de segurança como: safety time (st) e safety stock (ss). na fábrica da bosch em braga, o cálculo destas medidas é feito com pouca regularidade e com pouco nível de detalhe tomado em consideração. esta dissertação propõe uma solução dinâmica, que calcule um conjunto de pares ótimos de safety time e safety stock segundo certas medidas objetivas, para cada uma das matérias primas em uso. este cálculo é feito com base em dados logísticos, alguns estáticos(denominados dados mestre), outros históricos. de modo a acelerar o processo, foi desenvolvida uma solução informática, baseada em tecnologias de pro cessamento de dados distribuídos, mais concretamente o ecossistema apache. em particular, foi utilizado o hdfs (hadoop distributed file system), que faz parte de apache hadoop para armazenamento de dados em ambiente distribuído, e a ferramenta apache spark para leitura e processamento distribuído de dados, dividindo tarefas de processamento entre diferentes servidores dentro de um cluster."
    ],
    0.0
  ],
  [
    [
      "a resolução de problemas no âmbito de um domínio específico pode adotar técnicas e ideologias distintas. para tal, é vital e imperativo elaborar uma análise contextual a todos os elementos pertencentes à teia de relações entre conceitos. nesse sentido, o uso de uma ontologia permite construir uma rede semântica, no qual a mais importante premissa é a correta identificação dos conceitos e respetivos atributos. a automatização do processo de extração de ontologias permite construir ontologias mais escaláveis e uniformes, extraindo conhecimento assente nas mesmas premissas e padrões. no plano geral, uma extração automática facilita a análise e a leitura de informação de um problema apresentado numa linguagem própria. o trabalho desta dissertação focou-se na extração de conhecimento em textos não estruturados, mais concretamente, textos de culinária, com o intuito de disponibilizar uma ontologia que espelhasse o conhecimento interligado entre receitas. o verdadeiro desafio passa pela correta identificação de termos relevantes, com base em análise sintática, semântica, e linguística em geral, e pela formalização de relações entre os mesmos. a utilização de mecanismos de controlo e de automatização permitiu a extração do conhecimento presente nos textos não estruturados. estes mecanismos foram aplicados conforme as características linguísticas inerentes aos documentos e restrições de domínio. a ontologia gerada pode ser consultada através de uma plataforma web, na qual o utilizador pode pesquisar os documentos importados no sistema e analisar a interligação entre receitas através da pesquisa por termos e por hiperligações que se encontram nos detalhes de cada registo de culinária.",
      "the resolution of problems within a specific domain may adopt distinct approaches. as such, it is vital and imperative to elaborate a context analysis to each and every single existing element in the domain. for that matter, the use of an ontology allows the construction of a semantic environment where the most important factor is the correct identification of the concepts and its attributes. the automation of the whole process enables the ability to create more scalable and sustainable ontologies while extracting knowledge based on the same premises and patterns. an automatic extraction eases the analysis and understading of the information presented in a problem, usually written in natural language. this dissertation takes focus on the knowledge extraction in unstructured texts — culinary texts to be precised — with the sole goal of generating an ontology that exposes the knowledge intertwined between recipes. the main challenge presents itself as identifying the correct relevant terms, based upon context analysis and linguistics, and formalizing the relations among them. using the proper control and automation mechanisms ensure the best results when retrieving knowledge from unstructured texts. those mechanisms are chosen regarding linguistic characteristics and the corpus domain. the generated ontology will be used as the backend of a web platform, where the user may search for the desired recipes imported in the system. thus, the connection between recipes is highlighted when searching for a specific term and the hyperlinks embedded in recipe detailed information."
    ],
    [
      "nowadays, one of the major worries about a network is security. since the network has become the big platform it is, the number of attacks or attempts to steal information or just harm someone or something is getting bigger to handle or harder to find. sampling techniques help to solve these problems as they are used to reduce the scope of the analysis, as well as the resources needed to perform it. by using sample techniques to search and find the attacks in the network traffic it will become easier to detect attacks and keep the network secure. as will be seen in the following sections, joining sampling and security is not an easy task to do. questions such as, what are the best techniques to be used, what are the best methods to be implemented, are inevitable when using sampling. however, sampling can bring more advantages than disadvantages. besides that, depending on the chosen measurement method, sampling technique or algorithm performed to analyse the samples, the results can change a lot according to the target for the technique. to achieve results for evaluation, a network-based intrusion detection system (nids) will be used to identify anomalous events present in the samples.",
      "hoje em dia, uma das maiores preocupações com uma rede é a segurança. como a rede se tornou a grande plataforma que é, o número de ataques ou tentativas de roubar informações ou apenas prejudicar alguém ou algo está cada vez maior ou mais difícil de encontrar. as téc nicas de amostragem ajudam a resolver esses problemas visto que são utilizadas para reduzir o escopo da análise assim como os recursos necessários para realizar a mesma. usando técnicas de amostra para procurar e localizar os ataques no tráfego da rede, facilita prevenir ataques e manter a rede segura. como será constatado nas próximas secções, juntar amostragem e segurança não é uma tarefa fácil. questões como, quais são as melhores técnicas a serem utilizadas, quais os melhores métodos a serem implementados, são inevitáveis aquando da utilização de amostragem. contudo, amostragem pode trazer mais vantagens do que desvan tagens. além disso, dependendo do método de medição escolhido, técnica de amostragem ou algoritmo usado para analisar as amostras, os resultados podem variar muito consoante o alvo da técnica. para alcançar resultados para avaliação vai ser utilizado um network-based intrusion detection system (nids) de forma a identificar os eventos anómalos presentes nas amostragens."
    ],
    0.0
  ],
  [
    [
      "para que as interfaces de sistemas críticos possuam um nível de qualidade que permita o seu uso em segurança, devem passar por um processo rigoroso de análise. a verificação formal de interfaces é uma das formas de realizar essa análise. para tal, é importante que os desenvolvedores dessas interfaces consigam editar e criar os modelos que acharem mais adequados para as suas interfaces. tanto os desenvolvedores mais experientes como os menos experientes. a ivy workbench é uma ferramenta que permite descrever o funcionamento das interfaces e verificar propriedades sobre o seu comportamento, de forma a identificar potenciais problemas na interação. deste modo, fornece informação relevante para os desenvolvedores que utilizem o ivy, para que se possa melhorar o software sem ter de necessariamente passar por um processo de teste manual longo e exaustivo. o atual editor do ivy é difícil de manter e não fornece ajuda suficiente nem guia novos utilizadores adequadamente. por isso, é necessário que haja uma melhor forma de editar os modelos na linguagem model action logic (mal), a linguagem de programação da ivy workbench. o objetivo desta dissertação é construir uma solução que permita que todos os tipos de desenvolvedores consigam construir os seus modelos através de orientações do próprio editor. é bastante desafiante desenvolver uma solução deste gênero, que permita alcançar o nível de apoio pretendido, dado que precisamos de ter em conta com o que é que os utilizadores estão mais confortáveis e quais as ferramentas que usam com maior regularidade, para que seja possível desenvolver uma solução o mais abrangente possível. para que se concretize o principal objetivo, enquanto também se alcança o máximo número de utilizadores, optou-se por desenvolver uma extensão de vs code. trata-se do editor de código mais utilizado e fornece várias ferramentas para desenvolvedores de extensões, assim como uma vasta documentação. é possível tirar partido das funcionalidades que esta ferramenta já apresenta, típicas de um integrated development environment (ide) comum, que nos permitem criar novas formas para os utilizadores da ivy escreverem modelos mal, e fazendo isso, aumentar a sua produtividade. depois da extensão estar concluída, é expectável que esta solução seja mais fácil de manter no futuro, e mais utilizadores achem esta nova solução menos complexa para trabalhar, levando a que estes se sintam mais satisfeitos a utilizar a ferramenta e a própria linguagem, ajudando assim o crescimento da utilização da ivy workbench assim como da qualidade do software.",
      "in order for the interfaces of critical systems to have a quality level of security that allows for its safe usage, they should be subject to rigorous analysis process. formal verification is one of the alternatives to perform that analysis. so, it is important that developers can edit or create the models which they find the most suitable for their interfaces. both the most experienced developers as well as the least ones. the ivy workbench is a tool that allows for the modeling of user interfaces, and for properties about the interface behaviour to be verified, so that potential problems in the interaction can be identified. by doing this, it provides information for the developers who use ivy, so that their software can be enhanced without having to perform extensive manual testing. ivy’s current editor is difficult to maintain, and does not provide enough help nor guidance to inexpe rienced users. so, there is the need of a better way for users to write in the mal language, the modeling language of the ivy workbench. the goal of this thesis is to build a solution that allows every level of developer to build their own models based on guidance by the editor itself. it can be challenging to put together an editor or code editor extension that would allow such goal, because there is the need to con sider what the users are comfortable with, and what their most often used tools are, in order to build the more embracing solution. in order to achieve the main goal, while also reaching as many users as possible, it was considered that the best option would be to develop a vs code extension. vs code is the most widely used code editor and provides various tools for extension developers, with a vast documentation about their development. also, it is possible to make use of the features this code editor already presents, common amongst the most used ide, to build new ways for the users to write mal, and in doing so, increase their productivity. after the extensions is completed, it is expected that this new solution will be easier to maintain in the future, and that more users will find it less complicated to work with, leading users to get more satisfied when using the editor and the language itself, thus helping the growth of ivy workbench as well as the quality of the software."
    ],
    [
      "nosql databases opt not to offer important abstractions traditionally found in relational databases in order to achieve high levels of scalability and availability: transactional guarantees and strong data consistency. these limitations bring considerable complexity to the development of client applications and are therefore an obstacle to the broader adoption of the technology. in this work we propose a middleware layer over nosql databases that offers transactional guarantees with snapshot isolation. the proposed solution is achieved in a non-intrusive manner, providing to the clients the same interface as a nosql database, simply adding the transactional context. the transactional context is the focus of our contribution and is modularly based on a non persistent version store that holds several versions of elements and interacts with an external transaction certifier. in this work, we present an implementation of our system over apache cassandra and by using two representative benchmarks, ycsb and tpc-c, we measure the cost of adding transactional support with acid guarantees.",
      "as bases de dados nosql optam por não oferecer importantes abstrações tradicionalmente encontradas nas bases de dados relacionais, de modo a atingir elevada escalabilidade e disponibilidade: garantias transacionais e critérios de coerência de dados fortes. estas limitações resultam em maior complexidade no desenvolvimento de aplicações e são por isso um obstáculo à ampla adoção do paradigma. neste trabalho, propomos uma camada de middleware sobre bases de dados nosql que oferece garantias transacionais com snapshot isolation. a abordagem proposta e não-intrusiva, apresentando aos clientes a mesma interface nosql, acrescendo o contexto transacional. este contexto transacional e o cerne da nossa contribuição e assenta modularmente num repositório de versões não-persistente e num certificador externo de transações concorrentes. neste trabalho, apresentamos uma implementação do nosso sistema sobre apache cassandra e, recorrendo a dois benchmarks representativos, ycbs e tpc-c, medimos o custo do suporte do paradigma transacional com garantias transacionais acid."
    ],
    0.0
  ],
  [
    [
      "os exames endoscópicos são prescritos em grandes quantidades, pois são eficazes no diagnóstico, baratos quando comparados com outros exames e estarem generalizados há muito tempo, pois podem ser realizados em quase todos os hospitais. o resultado deste exame é normalmente um relatório que inclui anotações médicas complementadas com algumas imagens retiradas durante o exame. alguns dos exames realizados são apenas feitos para confirmar informação já recolhida, o que leva a uma duplicação de esforços desnecessária e desperdício de recursos. os profissionais de saúde podem descartar informação relevante ao não conseguirem anotar em pormenor uma região de interesse para posterior análise mais cuidada. o objetivo deste trabalho consiste na criação de um sistema que consiga resolver o problema apresentado anteriormente, usando tecnologia de reconhecimento de voz. este sistema deve reconhecer um pequeno vocabulário, independentemente do falante, usado para anotar regiões de interesse nos exames. o sistema myendoscopy atua como uma cloud privada, que contém vários dispositivos que usam e providenciam serviços entre si. o dispositivo central deste sistema é a mivbox, que se liga ao endoscópio e permite a captura digital do sinal de vídeo que este gera. a principal funcionalidade providenciada por este sistema é a capacidade de armazenar indefinidamente os vídeos completos que são produzidos durante exames endoscópicos, bem como disponibilizar estes vídeos e outros dados para outros profissionais de saúde que os necessitem de consultar. nesta dissertação apresenta-se um módulo de reconhecimento de voz para línguas portuguesa e inglesa, denominado mivcontrol, totalmente integrado no sistema myendoscopy. este módulo reconhece um pequeno vocabulário, que consiste em comandos usado para controlar os outros módulos. o mivcontrol é apresentado como uma alternativa a sistemas similares baseados na cloud, que resolve certos problemas relacionados com proteção de dados e segurança. foi realizado um estudo sobre o módulo desenvolvido para determinar a sua eficácia em comparação ao estado da arte. na sequência desse estudo conclui-se que o sistema tinha uma taxa de erro comparável a sistemas similares para outras línguas, e que como resultado é passível de ser usado em ambientes reais.",
      "endoscopic procedures are prescribed in large quantities, since they are effective in diagnostics, cheap when compared to other exams and are generalized for a long time, as almost all hospitals can perform them. the result produced by this exam is usually a report which includes medical annotations, complemented with some images produced during the exam. some exams have as only purpose confirming previously gathered information, which leads to unnecessary duplication of effort and waste of scarce resources. health professionals might discard important information if they can not mark with a reasonable detail level certain interesting regions, for further analysis. the objective of this thesis consists in creating a system that is able to solve the problem posed before, using voice recognition technology. this system should be able to recognize a small vocabulary, speakerindependent, used to annotate interesting regions during endoscopic exams. the myendoscopy system acts as a private cloud, which contains several devices that both use and provide services. the central device of this system is the mivbox, which connects to the endoscope and allows capturing the digital video signal it generates. the main functionality provided by the system is the ability of indefinitively store the complete video files produced during endoscopic procedures, as well make these videos and other data available to other healthcare professionals who need them. in this thesis it is presented a voice recognition module for portuguese and english, named mivcontrol, completely integrated in the myendoscopy system. this module recognizes a small vocabulary which consists of commands used to control other modules of the system. mivcontrol is presented as an alternative to similar cloud-based systems, which solves certain problems related to data protection and security. the module was studied to determine its efficiency compared to the state-of-the-art. that study concluded that the system had an error rate comparable to that of other similar systems developed for other languages, and thus can be used in the field."
    ],
    [
      "no âmbito de uma rede de um provedor de internet sem fios, faz todo o sentido afirmar que é essencial a existência de um sistema de monitorização com capacidades de acesso remoto e funcionalidades automatizadas. desta forma, consegue-se reduzir a carga nos administradores da rede, bem como melhorar o tempo de resposta a vários eventos, tais como perda de rendimento da rede e aumento de colisões. procura-se também que este sistema tenha baixas percentagens de uso da largura de banda. para atingir esta finalidade, recorre-se a tecnologias normalizadas facilmente disponibilizadas como o snmp ou netconf. depois de um breve estudo comparativo entre as tecnologias referidas, serão analisadas em detalhe as mibs mais relevantes relativamente a pontos de acesso sem fios. a existência de nodos escondidos, pela sua importância na degradação da largura de banda de redes sem fios, foi estudada em particular. um dos algoritmos mais relevantes para a mitigação deste problema utiliza dinamicamente o mecanismo rts/cts através da monitorização de parâmetros, tais como o número de retransmissões e número de tramas com erros, activando-o tendo em conta os valores dos parâmetros monitorizados, evitando a introdução de overhead na rede devido ao seu uso desnecessário. tal algoritmo foi introduzido na aplicação de gestão implementada e testada, sendo que os resultados obtidos não permitiram concluir da relevante bondade deste mecanismo quando aplicado somente do lado ponto de acesso.",
      "on the scope of an internet service provider’s wireless network, it makes all sense to declare that it is essential the existence of a monitoring system with remote access and automated capabilities. with such system it is possible to reduce the network administrators workload, as well as improve the response time to several events, like loss of throughput of the network or increasing collisions. ideally that system would have low percentages of bandwidth usage. to achieve this, standardized technologies like snmp or netconf will be used. then it will take place a brief comparative study between those two technologies, followed by a detailed analysis of the most relevant mibs present on wireless access points. the existence of hidden nodes, for its importance in bandwidth degradation in a wireless network, was studied in particular. one of the most relevant algorithms used for mitigation of this problem dynamically uses the rts/cts mechanism through parameter monitoring, such as number of retransmissions and number of frames with errors, activating the mechanism accordingly with the monitored parameters, avoiding overhead addition to the network caused by the unnecessary utilization of the mechanism. such algorithm was introduced on the implemented and tested management application, though the obtained results didn’t allow to achieve any conclusions relatively to the mechanism when applied only to the access point’s end."
    ],
    0.0
  ],
  [
    [
      "parallel programming is becoming increasingly common, given the evolution of hardware into multicore and manycore architectures. to take advantage of these architectures it is necessary to develop parallel code, since the automatic generation of parallel code from a sequential base code is not a viable option. parallel programming tends to be complex and therefore the developed code tends to be difficult to maintain or even difficult to reuse. this dissertation aims to explore the use of algorithmic skeletons in the development of parallel applications in order to allow faster development as well as the production of higher quality final software. a skeleton framework was developed. the skeletons were developed through a technique called ”mixin layers”, which makes use of generic classes of c++. a great emphasis wass given to the use of generic classes and other alternatives in the construction of the skeletons. finally, the performance of these solutions were evaluated in comparison with solutions already consolidated within parallel computing. for that, the codes of a set of applications were restructured in order to make use of the skeletons provided by the library",
      "a programação paralela está a tornar-se cada vez mais comum, dada a evolução do hardware para arquiteturas multicore e manycore. para tirar partido destas arquitecturas é necessário desenvolver código paralelo, uma vez que a geração automática de código paralelo a partir de um código base sequencial não é uma opção viável. a programação paralela tende a ser complexa e por isso o código desenvolvido tende a ser de difícil manutenção ou até mesmo de difícil reutilização. esta dissertação pretende explorar utilização de esqueletos algorítmicos no desenvolvimento de aplicações paralelas de forma a permitir um desenvolvimento mais célere assim como a produção de um software final de maior qualidade. foi desenvolvida uma biblioteca de esqueletos. os esqueletos foram desenvolvidos através de uma técnica denominada “mixin layers”, que faz uso de classes genéricas de c++. foi dada uma grande ênfase à utilização destas classes genéricas e outras alternativas na construção dos esqueletos. por fim foi avaliada a performance desta solução em comparação com soluções já consolidadas dentro da computação paralela. para isso foram reestruturados os códigos de um conjunto de aplicações de modo a fazerem uso dos esqueletos disponibilizados pela biblioteca."
    ],
    [
      "the technological advances and the massification of information technologies have allowed a huge and positive proliferation of the number of libraries and apis. this large offer has made life easier for programmers in general, because they easily find a library, free or commercial, that helps them solve the daily challenges they have at hand. one area of information technology where libraries are critical is in computer graphics, due to the wide range of rendering techniques it offers. one of these techniques is ray tracing. ray tracing allows to simulate natural electromagnetic phenomena such as the path of light and mechanical phenomena such as the propagation of sound. similarly, it also allows to simulate technologies developed by men, like wi-fi networks. these simulations can have a spectacular realism and accuracy, at the expense of a very high computational cost. the constant evolution of technology allowed to leverage and massify new areas, such as mobile devices. devices today are increasingly faster, replacing and often complementing tasks that were previously performed only on computers or on dedicated hardware. however, the number of image rendering libraries available for mobile devices is still very scarce, and no ray tracing based image rendering library has been able to assert itself on these devices. this dissertation aims to explore the possibilities and limitations of using mobile devices to execute rendering algorithms that use ray tracing, such as progressive path tracing. its main goal is to provide a rendering library for mobile devices based on ray tracing.",
      "os avanços tecnológicos e a massificação das tecnologias de informação permitiu uma enorme e positiva proliferação do número de bibliotecas e apis. esta maior oferta permitiu facilitar a vida dos programadores em geral, porque facilmente encontram uma biblioteca, gratuita ou comercial, que os ajudam a resolver os desafios diários que têm em mãos. uma área das tecnologias de informação onde as bibliotecas são fundamentais é na computação gráfica, devido à panóplia de métodos de renderização que oferece. um destes métodos é o ray tracing. o ray tracing permite simular fenómenos eletromagnéticos naturais como os percursos da luz e fenómenos mecânicos como a propagação do som. da mesma forma também permite simular tecnologias desenvolvidas pelo homem, como por exemplo redes wi-fi. estas simulações podem ter um realismo e precisão impressionantes, porém têm um custo computacional muito elevado. a constante evolução da tecnologia permitiu alavancar e massificar novas áreas, como os dispositivos móveis. os dispositivos são hoje cada vez mais rápidos e cada vez mais substituem e/ou complementam tarefas que anteriormente eram apenas realizadas em computadores ou em hardware dedicado. porém, o número de bibliotecas para renderização de imagens disponíveis para dispositivos móveis é ainda muito reduzido e nenhuma biblioteca de renderização de imagens baseada em ray tracing conseguiu afirmar-se nestes dispositivos. esta dissertação tem como objetivo explorar possibilidades e limitações da utilização de dispositivos móveis para a execução de algoritmos de renderização que utilizem ray tracing, como por exemplo, o path tracing progressivo. o objetivo principal é disponibilizar uma biblioteca de renderização para dispositivos móveis baseada em ray tracing."
    ],
    0.3
  ],
  [
    [
      "drug combination therapies are commonly used to overcome tumor drug resistance. computational methods can be helpful tools in drug combination discovery, but there are currently no e stablished methods for the prediction of drug combination effects. this work, integrated in the astrazeneca -sanger drug combination prediction challenge launched by the dialogue for reverse engineering assessments and methods (dream) community, aimed to develop machine learning methods to estimate the effects of drug combinations on cancer cell lines. the challenge was divided into three subchallenges (1a, 1b, and 2) addressing different clinical scenarios. a variety of machine learning models were devel oped and evaluated using cross-validation. tree-based ensembles, particularly gb, performed best for this problem. among the different the genomic datasets provided, the monotherapy, mutation and cnv datasets were the most informative and were the only ones used in the final models. the best model, submitted to 1a, was an ensemble of gradient boosting (gb), random forest (rf), and partial least squares (pls) regression models, having achieved an average weighted pearson correlation of 0.30, and ranking 24th among 76 submissions. the 1b model (average weighted pearson correlation of 0.18; 47th/62 submissions) was also an ensemble of gb, rf, and pls models. for subchallenge 2, a gb model was selected. it had a performance score (based on a three-way analysis of variance (anova) ) of 5.15 and ranked 20th out of 39 submissions. the strategies explored in this work and by the dream challenge community will help to further the development of computational methods for the rational design of effective drug combinations for cancer therapy.",
      "a utilização de múltiplos fármacos em combinação é uma estratégia comum para superar a resistência a medicamentos em tumores. métodos computacionais podem ser ferramentas valiosas na descoberta de novas combinações de interesse, mas atualmente não existe nenhum método estabelecido para este propósito. este trabalho, integrado na iniciativa astrazeneca-sanger drug combination prediction challenge proposta pela comunidade dream, tinha como objetivo o desenvolvimento de métodos de aprendizagem máquina para prever os efeitos de combinações de fármacos em linhas celulares tumorais. o problema encontrava-se dividido em três desafios (1a 1b e 2) que abordav am cenários clínicos distintos. vários modelos foram desenvolvidos, sendo avaliados atr avés de validação cruzada. conjuntos de modelos baseados em árvores de decisão conseguiram um melhor desempenho. de todos os conjuntos de dados, os dados de monoterapia, de mutações e de variação do número de cópia foram os mais informativos, tendo sido utilizados pelos mode los finais. o estimador utilizado para a tarefa 1a (média ponderada da correlação de pearson de 0.30; 24º em 76 submissões) foi um conjunto composto por modelos de gradient boosting (gb), random forest (rf) e regressão por mínimos quadrados parciais (pls). para o problema 1b foi utilizado outro conjunto de modelos com gb, rf e pls (0.18; 47º em 62 submissões). para a questão 2, foi desenvolvido um modelo de gb que conseguiu um desempenho (calculado com base nos resultados de uma anova) de 5.15, tendo sido o 20º melhor modelo num total de 39. as estratégias exploradas neste trabalho e pelas outras equipas que participaram neste desafio da comunidade dream são um contributo útil para o desenvolvimento futuro de métodos computacionais para o desenho racional de combinações de fármacos eficazes para o tratamento de tumores."
    ],
    [
      "ao longo dos anos, tem aumentando o número de tecnologias da informação e comunicação(tic) em todas as organizações e também nas organizações de saúde, como é o caso do registo de saúde eletrónico (rse). este sistema integra dados provenientes de diferentes fontes, reunindo registos clínicos dos utentes, informação que suporta a decisão clínica, repositórios de dados, aplicações de gestão hospitalar. permite ainda o acesso a aplicações para a realização de processos operacionais, como é o caso da prescrição eletrónica de exames e de medicamentos. as organizações de saúde, têm como principal objetivo, uma boa prestação de cuidados de saúde aos utentes, garantindo a prestação de serviços de qualidade. para isto, a tomada de decisão tem que ser rápida e e caz. assim, o conceito de business intelligence (bi) vem contribuir para a melhoria da prestação de cuidados de saúde na medida em que se baseia na organização e análise de dados, de forma a disponibilizar informação útil para formar conhecimento, o que consequentemente, facilita a tomada de decisão. o objetivo deste projeto é estudar e analisar, através de ferramentas de bi, o tempo de prescrição de meios complementares de diagnóstico e terapêutica (mcdt). tal estudo é feito através da criação de uma plataforma de auditoria, com a ferramenta de bi pentaho community que, de forma simples e atrativa mostra informação relativa ao tempo que os pro ssionais de saúde levam a preencher os formulários de prescrição. esta auditoria é necessária, para obter informação útil, formar conhecimento sobre a prescrição e sobre os médicos e enfermeiros, do centro hospitalar do porto (chp) e implementar mudanças. este sistema permite avaliar os tempos de prescrição, tanto por médico bem como por especialidade, com o intuito de, posteriormente, melhorar todo o processo de prescrição eletrónica médica (pem). veri cou-se que é a especialidade de radiologia que executa mais pedidos e são as tarefas de gastrenterologia que, em média demoram mais tempo a serem executadas.",
      "over the years, the number of information technologies used in all organizations, including healthcare systems, in which electronic health record( ehr) are a good example, have increased. this type of system integrates data from di erent sources, i.e., the clinical records of patients, clinical decision support applications, data repositories and other hospital management systems as the same time that allows access to applications for performing operational processes, such as the electronic prescription of medical exams and drugs. good provision of healthcare to patients ensuring the quality of services is healthcare organizations main goal. for this, decisions have to be made quickly and e ectively. thus, the concept of bi contributes to improve the delivery of healthcare and that it's based on the analysis of data in order to provide useful information to form knowledge, which consequently facilitates decision making. the objective of this project is to study and analyze through the bi tools, the time that physicians take to prescribe complementary diagnostic and therapeutic procedures. such study is done by creating a bi auditing platform using the pentaho community, which shows a simple and attractive way of information on the time that health professionals take to ll out the forms of prescription. this audit is useful to gain information about the prescription system and as well it's relation with the physicians and nurses of the centro hospitalar do porto (chp) and ultimately, implement changes. this system allows evaluating the time prescription, by physician and by specialty, in order to improve the whole process of electronic medical prescription (emp). thanks to the this study, it has been found that is the specialty of radiology that performs more requests of complementary diagnostic and therapeutic procedures and the tasks of gastroenterology, on average, that take longer to perform."
    ],
    0.05454545454545454
  ],
  [
    [
      "the assembly of miniature electronic components requires an adequate scale of the size of the welding terminators in printed circuit boards to minimize the stresses due to deformation. an optimum terminator layout minimizes the surface tension of the liquid solder, but requires efficient simulation algorithms to compute the results in an acceptable time slot. current surface evolver is a software tool to study surfaces, shaped by surface tension and other energies, and its execution efficiency can be improved to take advantage of shared memory systems based on multi-core and many-core computing devices. this dissertation aims to analyze the surface evolver, identifying the computational bottlenecks and working on solutions to improve the overall performance of the application. parallel algorithms were developed to explore the architectural features of current multi-core and many-core computing devices namely the xeon phi, and including the growing vectorization features of newer processing devices. after an analysis of the application and its profiling, the original data structure was identified as the critical bottleneck for software performance: it is implemented with linked lists, which prevents the use of the vectorization features of current devices and leads to inefficient parallel algorithms, both key elements to improve the performance of the surface evolver. the modification of the data structure was a key task in this dissertation. the calculation force was identified as one of the most time consuming tasks of surface evolver and it was the target function of this work. this algorithm iterates over all vertices, edges and faces so is a good example to conclude how vectorization and parallelism affects the performance of simulation software used in the variety fields of science and engineering. in the end of this work it is possible to see that vectorization can greatly improve the performance of an application, bringing significant speedups to surface evolver. the measured execution times are presented and discussed, throughout the various development stages of the application, aiming to analyze the impact of the application of high performance techniques on the surface evolver, suggesting yet further future improvements that were well identified in the end of this work.",
      "a montagem de componentes eletrónicos mais pequenos requer um tamanho adequado da solda nas placas de circuito impresso para minimizar as tensões devido às deformações. uma disposição ótima do terminal minimiza a tensão superficial da solda líquida, mas requer algoritmos eficientes para calcular os resultados num intervalo de tempo aceitável. o surface evolver é uma ferramenta de software para estudar superfícies, moldadas pela tensão de superfície e outras energias, e a sua eficiência pode ser melhorada para tirar proveito dos atuais sistemas paralelos. esta dissertação tem como objetivo analisar o surface evolver, identificando os estrangulamentos computacionais e trabalhando em soluções para melhorar o desempenho global da aplicação. algoritmos paralelos foram desenvolvidos para explorar as características das arquiteturas multi-core e dispositivos de computação many-core, nomeadamente xeon phi, e também as novas características de vetorização presente nos dispositivos mais recentes. depois da análise da aplicação, a estrutura de dados original foi identificada como o principal problema da aplicação: é implementada com listas ligadas, o que não possibilita o uso de vetorização e leva a algoritmos paralelos ineficientes, dois elementos cruciais para o aumento de performance no surface evolver. a alteração da estrutura de dados foi o trabalho mais importante ao longo desta dissertação. o cálculo das forças foi identificado como uma das tarefas mais pesadas do surface evolver e foi por isso o alvo principal deste trabalho. o algoritmo itera sobre todos os vértices, arestas e faces, sendo por isso um bom exemplo para se tirar conclusões sobre como a vetorização e o paralelismo pode melhorar a performance de aplicações de simulação usadas nos vários campos da ciência e engenharia. no final deste trabalho será possível constatar que a vetorizacao consegue trazer melhorias de performance a uma aplicação, trazendo essas mesmas melhorias ao surface evolver. os tempos de execução foram medidos e discutidos, durante os vários períodos de desenvolvimento da aplicação, tendo como objetivo analisar o impacto da aplicação das técnicas de alto desempenho no surface evolver, sugerindo ainda futuras melhorias que foram identificadas e explicadas no final deste trabalho."
    ],
    [
      "o envelhecimento populacional é uma realidade dos nossos tempos. o envelhecimento provoca muitas vezes o problema do sedentarismo, que é associado à obesidade e doenças cardiovasculares. o sedentarismo é por isso um problema preocupante que para além de afetar as idades avançadas afeta todas as outras faixas etárias. é nos tempos livres que as pessoas são mais sedentárias. a prática de atividades físicas combate o sedentarismo e é benéfica em termos psicológicos, sociais e económicos. é necessário incentivar as pessoas a praticá-la e uma das formas de o conseguir é através de um sistema que recomende tais atividades. a recomendação deverá contudo ter em consideração as preferências das pessoas pelas atividades, para que estas tendam a aceitar as atividades que são recomendadas. as preferências de uma pessoa são associadas ao conceito de perfil e é através deste, que um sistema de recomendação infere sobre que atividades recomendar. é por isso imperativo que o perfil esteja constantemente atualizado e consistente com a pessoa que representa. garantir estas propriedades num perfil não é computacionalmente claro, devido às propriedades naturais de um perfil e devido à capacidade das pessoas em evoluir e variar nas suas preferências. este trabalho tem como objetivo desenvolver um sistema que recomende às pessoas atividades que não sejam sedentárias e que incentivem o convívio social. para que o sistema seja preciso nas atividades que recomenda, é também objetivo deste trabalho desenvolver um mecanismo para obtenção, representação e modulação de perfis que garanta a sua dinamicidade, que garanta que os perfis sejam eficientemente adaptáveis e atualizáveis. realizaram-se observações acerca da taxa de aceitação de utilizadores, em relação às atividades recomendadas pelo sistema desenvolvido. estas observações assistiram na avaliação da precisão do sistema em recomendar e a sua capacidade em acompanhar os utilizadores e aprender acerca das suas preferências. consultando periodicamente a quantidade de atividades recomendadas aceites ou rejeitadas, verificou-se que o número de atividades aceites tende a aumentar e as rejeitadas a diminuir. estes resultados demonstram que o sistema possui a capacidade de aprender e aperfeiçoar o perfil dos seus utilizadores, tornando-se mais preciso nas atividades que recomenda. acredita-se desta forma que o mecanismo desenvolvido para a obtenção e modulação de perfis dinâmicos é eficiente e robusto.",
      "the population aging is a known reality. retirement often causes the inactivity, that is related to obesity and cardiovascular diseases. physical inactivity is a serious problem that affects all people, not merely older people. leisure activities can become sedentary, when people choose to practice activities like television and computer. the practice of physical activity addresses the inactivity and is helpful in psychological, social and economic terms. it is essential to encourage people to practice them. an approach to achieve this need is through a system that recommend activities to people. the recommendation should take into account the people preference's for the activities, so that they tend to accept the recommendation. the preferences of a person are related to the concept of profile. the recommendation system uses the profile to infer about the activities to recommend. therefore, it is vital that the profile is constantly updated and consistent with the person who it represents. ensuring these properties in a profile is not a clear task because of the natural properties of a profile, and because of the person's capability to evolve and change their preferences. this work aims to develop a system that recommends physical and socialization activities. it is also aimed to develop a mechanism to obtain, represent and module profiles that ensure their dynamism, that ensure that the profiles are efficiently adapted and updated. based on these profiles, the system can be accurate in the activities that it recommends. observations regarding the rate of acceptance of users for the activities recommend by the developed system were performed. these observations helped in the evaluation of the system accuracy in recommending and its ability to follow the users and learn their preferences. by periodically checking the number of recommended activities accepted or rejected, it was found that the number of accepted activities tends to raise, and the number of rejected to decrease. these results have proven that the system is capable of learning and improve the user profile, becoming more precise in the recommended activities. it is believed that the developed mechanism to confine and module dynamic profiles is efficient and robust."
    ],
    0.0
  ],
  [
    [
      "characterizing network traffic is a very important process for network planning, management, and analysis. despite having received attention over the years, there are still many improvements to be developed, for example, how to accurately classify secure network traffic. the research community has already presented numerous characterization methodologies, and in this dissertation, one of the approaches for the characterization of secure network traffic is investigated. first, the most common encrypted traffic protocols on the internet are presented. its architecture and operating mode are shown to carry out data traffic safely. next, the methods for capture network traffic are examined and the most used and efficient methods of classification of network traffic are pointed out in the study of the characterization of secure traffic. the advantage of each method, the use of hybrid methods, the accuracy of characterizing certain application protocols are discussed. after selecting the desired method of characterization of secure traffic from among the several that were presented, an analysis of the accuracy of this method was made with several datasets. in addition to the tests carried out with data capture in an experimental environment, where all generated traffic was controlled, tests with public datasets were also accomplished. finally, the results obtained from the precision achieved in each environment are revealed and the results were synthesized with a brief explanation and highlighting their characteristics.",
      "caracterizar tráfego de rede é um processo de grande importância para o planeamento, gerenciamento e análise da rede. apesar de receber atenção ao longo dos anos, ainda é possível encontrar muitas melhorias a ser desenvolvidas, por exemplo, como classificar o tráfego de rede seguro de forma precisa. a comunidade já apresentou inúmeras metodologias de caracterização, e nesta dissertação é investigado uma das abordagens para a caracterização de tráfego de rede seguro. primeiro são apresentados os protocolos de tráfego criptografado mais utilizados na internet. mostra-se a sua arquitetura e o modo de operação para realizar o tráfego dos dados de forma segura. na sequência, foram examinados os métodos de captura de tráfego de rede e apontados os métodos de classificação de tráfego de rede mais utilizados e eficientes no estudo da caracterização de tráfego seguro. a vantagem de cada método, a utilização de métodos híbridos, a precisão de caracterizar determinados protocolos aplicacionais são discutidas. após selecionar o método de caracterização de tráfego seguro desejado dentro dos diversos que foram apresentados, foi realizado uma análise da precisão desse método com diversos datasets. além dos testes realizados com a captura dos dados em um ambiente experimental, onde foi controlado todo o tráfego gerado, também realizou-se testes com datasets públicos. finalmente, são revelados os resultados obtidos da precisão alcançada em cada um dos ambientes e foram sintetizados os resultados com uma breve explicação, junto as características."
    ],
    [
      "relational database engines associated to the widely used structured query language (sql) are suffering unsatisfactory performance results in complex business queries, due to ever increasing volumes of stored data. to retrieve and process data in a more efficient way, online analytical processing (olap) models have been proposed with an increased focus on attributes (measures and dimensions) over records. olap is based on a row-oriented theory, while a columnar-oriented theory could considerably improve the performance of analytical systems. the typed linear algebra (tla) approach is an example of such theory: it encodes each database attribute in a distinct matrix. these matrices are combined in a single linear algebra (la) expression to obtain the result of a query. this dissertation combines concepts of relational databases, olap, tla and performance engineering to design, implement and validate an efficient tla-db engine: sql queries are converted into its equivalent la expression, using type diagrams (tds), which represent each matrix as an arrow pointing from the number of columns to the number of rows, tds are converted to a la expression encoded in linear algebra query language (laq) and the laq script of a query is automatically coded in c plus plus (c++). an efficient tla-db engine required the encoding of the sparse matrices in an adequate format, namely compressed sparse column (csc), while the operations specified in laq expressions had their performance improved by optimised algorithms and an optimised query processor. the functionality of the resulting laq engine was validated with several tpc benchmark h (tpc-h) queries for various dataset sizes. a comparative evaluation of the tla-db with two popular database management systems (dbmss), postgresql and mysql, showed that the developed framework outperforms both dbmss in most tpc-h queries.",
      "as melhorias de desempenho dos sistemas de gestão de bases de dados relacionais não têm sido suficientes para acompanhar o crescimento do volume de dados com que são utilizados. para colmatar a consequente necessidade de soluções mais eficientes, a teoria olap foi proposta. esta introduz as noções de medidas e dimensões, guardando préagregações das medidas baseadas nas últimas, de forma a acelerar o processo de análise de dados. contudo, ainda que com regras mais restritas, o olap está assente em álgebra relacional. a proposição de uma teoria orientada à coluna pode abrir portas a grandes melhorias de desempenho em consultas analíticas. a álgebra linear tipada é um bom exemplo. segundo esta teoria, cada um dos atributos é convertido numa matriz independente, as quais são posteriormente combinadas através de uma expressão de álgebra linear que define o resultado da consulta. esta dissertação combina conceitos de bases de dados relacionais, olap, álgebra linear, teoria de tipos, e computação eficiente para projetar, implementar e validar um motor olap robusto e eficiente. para tal, consultas em sql são convertidas para a expressão de álgebra linear equivalente, usando diagramas de tipo que representam cada matriz como uma seta a apontar do número de colunas para o número de linhas da matriz. a expressão que deles resulta é então codificada em laq e automaticamente implementada em c++. para garantir a eficiencia da ferramenta desenvolvida, todas as matrizes foram guardadas num formato adequado, nomeadamente o csc. por sua vez, as operações especificadas na laq foram implementadas recorrendo a algoritmos optimizados. a correção do sistema implementado foi garantida através da validação dos resultados de um grupo de consultas extraidas do tpc-h, executadas sobre bases de dados de multiplos tamanhos. finalmente, a comparação com dois sistemas de bases de dados convencionais (o postgresql e o mysql) nas métricas de tempo de execução e memória utilizada, demonstrou a maior eficiencia da ferramenta desenvolvida na maioria das consultas."
    ],
    0.06666666666666667
  ],
  [
    [
      "every major database management system (dbms) and most components in a distributed system in use today, closed or open source, comprise a set of configuration parameters which have substantial influence over the performance of the system. the correct configuration and tuning of these parameters often leads to a performance level that is orders of magnitude greater than that achieved by default configurations. the number of parameters tends to increase as new versions are released. moreover, the optimal values for these parameters vary with the environment, namely the workload to which the system is being subjected to, and the physical characteristics of the hardware it is running on. it is common to delegate the responsibility of parameter tuning to a system administrator. the problem with this approach is that it requires both extensive prior experience with the specific system and workload at hand, and a large amount of the administrator’s time. moreover, variables may establish extensive and non-trivial correlations between them that are very difficult to identify and tune. this dissertation introduces an automated and dynamic approach to parameter tuning using a reinforcement learning approach, while also adopting the use of deep neural networks to tackle the fact that complex relations between variables may exist. two use cases were implemented to showcase our approach, in the context of a distributed database. one where we adjust tuning variables specific to each replica and another where we adjust the shard configuration of the cluster (i.e. what shard is allocated to what replica). the reinforcement learning agents act at the middleware level, where all replication logic is held. the performance was measured in terms of the reward achieved by those agents as well as the values for the individual performance metrics that make up that reward. for the use case that concerns individual replica configurations, a maximum gain in reward of 105.41% was observed in one of the replicas as well as a maximum gain of 484.31% in one of the individual performance metrics. in the second scenario, of shard reallocation, we saw improvements in reward value up to 28.72% and of up to 69.92% for individual metrics.",
      "todos os principais sistemas de gestão de bases de dados, bem como a maioria dos componentes que são parte constituinte de um sistema distribuído em uso atualmente, licenciados ou abertos, incluem um conjunto de parâmetros de configuração que demonstram ter uma influência substancial sobre o desempenho do sistema. a correta configuração e ajuste destes parâmetros leva frequentemente a níveis de desempenho que podem ser ordens de magnitude acima do que os que são atingidos por configurações pré-definidas. o número de parâmetros tem tendência a aumentar à medida que novas versões são lançadas. para além disso, os valores ótimos para estas variáveis tendem a variar com o contexto de execução, nomeadamente a carga de trabalho a que o sistema está a ser sujeito, e as características do hardware em que está a ser executado. é comum delegar a tarefa de ajuste dos parâmetros de configuração ao administrador do sistema. o problema com esta abordagem é que esta tarefa requer, por um lado, uma vasta experiência com o workload e sistema a ser configurado, e por outro, uma porção considerável do tempo do administrador. para além disso, as variáveis podem estabelecer correlações complexas entre si que podem ser muito difíceis de identificar e compreender. esta dissertação apresenta uma abordagem automatizada e dinâmica para o ajuste de variáveis de configuração, recorrendo para isso a técnicas de aprendizagem por reforço e combinando estas com o uso de redes neuronais para abordar o problema de identificação de correlações entre variáveis. dois casos de estudo foram implementados para demonstrar a abordagem no contexto de uma base de dados distribuída. um, em que são ajustados parâmetros de configuração individuais a cada réplica e outro onde se ajusta a configuração de shards do cluster (i.e. a que réplica está alocado cada shard). os agentes de aprendizagem por reforço atuam ao nível de um middleware, onde é tratada toda a lógica de replicação. o desempenho foi medido em termos da recompensa alcançada pelos agentes assim como pelos valores das métricas individuais de desempenho que compõem essa recompensa. para o caso de estudo relativo às configurações individuais de cada réplica foi observado um ganho máximo de 105.41% no valor da recompensa e um ganho máximo de 484.31% no valor de uma das métricas individuais. no caso de estudo de realocação de shards, foram observados ganhos no valor de recompensa de até 28.72% e 69.92% para métricas individuais."
    ],
    [
      "a escassez de água é atualmente uma grande preocupação. a reutilização de águas residuais tratadas, seja por descarga em ambientes hídricos (por exemplo, rios) ou pela utilização em irrigação, é apontada como umas das principais soluções. no entanto, é importante monitorizar o possível impacto desta reutilização, sobretudo ao nível da disseminação de contaminantes emergentes como as bactérias resistentes a antibióticos (arb) e os seus genes de resistência a antibióticos (args). este estudo teve como objetivo determinar e comparar os resistomas de diferentes amostras de água (afluente, lamas ativadas, efluente e água doce), com base em metagenomas de diferentes geografias, de bases de dados públicas. o objetivo final foi identificar padrões e caraterísticas distintas entre amostras. estes permitirão identificar args como possíveis biomarcadores para monitorizar a contaminação de ambientes aquáticos com agentes biológicos de origem antropogénica. no total, 139 metagenomas (30 afluente, 30 lamas ativadas, 21 efluente, 58 de água doce) de 24 países foram analisados, usando métodos baseados em assembly e em reads. os resultados mostraram que diferentes tipos de água partilham um grande número de args. uma nova abordagem foi usada para combinar a anotação de duas das bases de dados de args mais abrangentes (card e resfinder), superando a dificuldade que é lidar com anotações distintas provenientes de bases de dados diferentes. esta abordagem permitiu determinar o resistoma core dos diferentes tipos de água, com o objetivo de obter genes biomarcadores para rastrear a contaminação em termos de resistência a antibióticos provocado pela descarga de águas residuais em ambientes recetores, como água doce. no final foram obtidos 60 possíveis biomarcadores, para os quais foram desenhadas sequências consenso que poderão ser usadas, por exemplo, para o desenho de primers. além disso, 7 modelos de deep learning foram desenvolvidos para classificar a transferibilidade de args (genes adquiridos versus intrínsecos), dada a falta de informação sobre transferibilidade. esta distinção é muito importante quer na monitorização quer na predição do risco, visto que os args adquiridos são mais propensos à disseminação entre bactérias. o modelo de redes neurais convolucionais superou os restantes com destaque (mcc de 0.881 e roc-auc de 0.906), o que é considerado um desempenho consistente.",
      "water scarcity is a major concern nowadays. the reuse of treated wastewater, by discharging in surface water bodies (e.g. rivers) or by utilization for irrigation, is pointed as one of the main solutions. however, it is important to monitor the possible impact this reuse, namely in terms of dissemination of contaminants of emerging concern such as antibiotic resistant bacteria (arb) and their antibiotic resistance genes (args). this study aimed to determine and compare the resistomes (set of genes associated with antibiotic resistance) of different water samples (urban wastewater influent, sewage sludge, final effluent and freshwater), based on metagenomes collected worldwide and available in public databases. the final goal was to identify overlaps and distinctive features among those compartments. this approach will permit the identification of args to be used as possible biomarkers to monitor the contamination of aquatic environments with these biological contaminants of anthropogenic origin. a total of 139 metagenomes (30 influent, 30 sludge, 21 effluent, 58 freshwater) from 24 countries were analysed, using assembled-based and reads-based methods. the results shown that different water types share a large number of args. a new approach was used to combine the annotation of two of the most comprehensive args databases (card and resfinder), surpassing the difficulty that is to deal with different annotations coming from different databases. this approach allowed to determine the core resistomes, aiming to obtain biomarker genes to trace antibiotic resistance contamination from wastewater in receiving environments, such as freshwater. at the end 60 putative biomarkers were obtained, for which were designed consensus sequences that can be used, for example, to design primers for the genes monitoring. additionally, 7 deep learning models were developed and compared for classifying args transferability (acquired versus intrinsic genes), motivated by the lack of information regarding transferability. this distinction may be very important in the monitoring and prediction of risk, since acquired args are more prone to spread among bacteria. after validation, a convolutional neural networks model outperformed the remaining with a 0.881 mcc and a 0.906 roc-auc, which is considered very consistent performance."
    ],
    0.06666666666666667
  ],
  [
    [
      "o universo das tecnologias de informação está a assistir a grandes mudanças desde o surgimento do conceito de cloud computing. a cloud computing revela-se como um meio que possibilita a fácil aquisição e liberação (elasticidade) de recursos computacionais, que disponibiliza infraestruturas altamente escaláveis e dispendiosas com o mínimo de configuração possível e, ainda, pelo facto de ser um serviço com custos reduzidos comparativamente a uma solução in-house, pois tipicamente utiliza um modelo “pay-as-you-go”. claro que com a delegação de toda a infraestrutura e dos dados para um provedor de clouds, questões como a segurança e privacidade dos dados começaram a ser equacionadas, apresentando-se assim como desvantagens para soluções em cloud. no entanto, além da cloud computing, outras variáveis, como o grande aumento do volume de dados nas empresas e os avanços tecnológicos alcançados nas redes de banda larga, têm “exigido” a adaptação das bases de dados para um ambiente em cloud, o que originou, pouco a pouco, o paradigma de database as a service. atualmente ainda existem dúvidas relativamente às bases de dados sql, sobre se estas serão as mais indicadas para ambientes cloud. este modelo de bases de dados tem dominado o mercado mas apresenta diversas limitações (por exemplo a nível de escalabilidade e garantia das propriedades acid) quando confrontadas com implementações num ambiente cloud. por outro lado, o ecossistema de aplicações desenvolvidas com bases de dados sql é demasiado grande para ser modificado para outro modelo. apesar desta indefinição, a cloud parece ser um cenário ideal para data warehouses pois são bases de dados que albergam usualmente enormes volumes de dados e são essencialmente de leitura. com esta dissertação pretendeu-se estudar a viabilidade da implementação e migração de um sistema de data warehousing para um ambiente cloud e apresentar um protótipo que expusesse a utilidade do mesmo face a uma típica implementação in-house.",
      "the world of information technology is witnessing major changes since the appearance of the cloud computing concept. cloud computing reveals itself as a means to allow easy acquisition and release, in other words elasticity, of computing resources, provides highly scalable and costly infrastructures with minimal configuration and also because it is a service with reduced costs compared to an in-house solution, because it typically uses a \"pay-as-you-go\" model. of course, with the delegation of the entire infrastructure and the data to a cloud provider, issues such as security and privacy of data began to be addressed, becoming drawbacks to cloud solutions. however, in addition to cloud computing, other variables, such as the large increase of enterprise data volumes and the technological advances in broadband networks, have required the adaptation of databases to a cloud environment, which led, step by step, to the database as a service paradigm. currently there are still doubts whether sql databases will be the most suitable model for cloud environments. while this database model has dominated the market, it has several limitations (eg. in terms of scalability and assurance of the acid properties) when confronted with implementations in a cloud environment. on the other hand, the ecosystem of applications developed under sql databases is too large to be changed to another model. despite this uncertainty, the cloud seems to be an ideal environment for data warehouses because these are databases that usually house huge volumes of data and are essentially used for reading purposes. the purpose of this dissertation was to study the feasibility of implementation and migration of a data warehousing system to a cloud environment and to develop a prototype that would expose its usefulness compared to a typical in-house implementation."
    ],
    [
      "com a constante expansão de sistemas informáticos nas diferentes áreas de aplicação, a quantidade de dados que exigem persistência aumenta exponencialmente. assim, por forma a tolerar faltas e garantir a disponibilidade de dados, devem ser implementadas técnicas de replicação. atualmente existem várias abordagens e protocolos, tendo diferentes tipos de aplicações em vista. existem duas grandes vertentes de protocolos de replicação, protocolos genéricos, para qualquer serviço, e protocolos específicos destinados a bases de dados. no que toca a protocolos de replicação genéricos, as principais técnicas existentes, apesar de completa mente desenvolvidas e em utilização, têm algumas limitações, nomeadamente: problemas de performance relativamente a saturação da réplica primária na replicação passiva e o determinismo necessário associado à replicação ativa. algumas destas desvantagens são mitigadas pelos protocolos específicos de base de dados (e.g., com recurso a multi-master) mas estes protocolos não permitem efetuar uma separação entre a lógica da replicação e os respetivos dados. abordagens mais recentes tendem a basear-se em técnicas de repli cação com fundamentos em mecanismos distribuídos de logging. tais mecanismos propor cionam alta disponibilidade de dados e tolerância a faltas, permitindo abordagens inovado ras baseadas puramente em logs. por forma a atenuar as limitações encontradas não só no mecanismo de replicação ativa e passiva, mas também nas suas derivações, esta dissertação apresenta uma solução de replicação híbrida baseada em middleware, o sqlware. a grande vantagem desta abor dagem baseia-se na divisão entre a camada de replicação e a camada de dados, utilizando um log distribuído altamente escalável que oferece tolerância a faltas e alta disponibilidade. o protótipo desenvolvido foi validado com recurso à execução de testes de desempenho, sendo avaliado em duas infraestruturas diferentes, nomeadamente, um servidor privado de média gama e um grupo de servidores de computação de alto desempenho. durante a avaliação do protótipo, o standard da indústria tpc-c, tipicamente utilizado para avaliar sistemas de base de dados transacionais, foi utilizado. os resultados obtidos demonstram que o sqlware oferece uma aumento de throughput de 150 vezes, comparativamente ao mecanismo de replicação nativo da base de dados considerada, o postgresql.",
      "with the constant expansion of computational systems, the amount of data that requires durability increases exponentially. all data persistence must be replicated in order to provide high-availability and fault tolerance according to the surrogate application or use-case. currently, there are numerous approaches and replication protocols developed supporting different use-cases. there are two prominent variations of replication protocols, generic protocols, and database specific ones. the two main techniques associated with generic replication protocols are the active and passive replication. although generic replication techniques are fully matured and widely used, there are inherent problems associated with those protocols, namely: performance issues of the primary replica of passive replication and the determinism required by the active replication. some of those disadvantages are mitigated by specific database replication protocols (e.g., using multi-master) but, those protocols do not allow a separation between logic and data and they can not be decoupled from the database engine. moreover, recent strategies consider highly-scalable and fault tolerant distributed logging mechanisms, allowing for newer designs based purely on logs to power replication. to mitigate the shortcomings found in both active and passive replication mechanisms, but also in partial variations of these methods, this dissertation presents a hybrid replication middleware, sqlware. the cornerstone of the approach lies in the decoupling between the logical replication layer and the data store, together with the use of a highly scalable distributed log that provides fault-tolerance and high-availability. we validated the prototype by conducting a benchmarking campaign to evaluate the overall system’s performance under two distinct infrastructures, namely a private medium class server, and a private high performance computing cluster. across the evaluation campaign, we considered the tpcc benchmark, a widely used benchmark in the evaluation of online transaction processing (oltp) database systems. results show that sqlware was able to achieve 150 times more throughput when compared with the native replication mechanism of the underlying data store considered as baseline, postgresql."
    ],
    0.0
  ],
  [
    [
      "in recent years more and more complex structures have been built. buildings and locations which users must navigate efficiently so they can reach their appointments in a timely fashion, such as hospitals, universities and airports. unfortunately technologies such as gps are not well adapted to indoor locations and therefore do not provide a solution to this problem. indoor mapping has been subject to increased amounts of research in the past few years and a plethora of different solutions have started to arise although none completely fulfill every requirement this problem presents. this thesis is done in conjunction with others with the final objective of creating the prototype of a mobile application and system that will be able to precisely locate where a user is inside of an indoor location through showing them their location on a floorplan. it will more specifically focus on the modeling aspect of the space geometry in an efficient way that can be used by this application. the purpose of this dissertation is to document the research done to choose the most appropriate data format, the development of a conversion method of the available data to the chosen format, and the development of web services and mobile application components that will provide this information to the end user. additionally the development of a web application that, with the results obtained throughout this investigation process, helps keep track of the progress of the radio mapping will also be documented.",
      "nos últimos anos estruturas cada vez mais complexas tem sido construídas. edifícios e localizações que devem ser eficientemente percorridos pelos utilizadores para que estes possam chegar aos seus destinos e marcações atempadamente. estas localizações podem incluir instituições como hospitais, universidades e até mesmo aeroportos. infelizmente tecnologias como o gps não funcionam corretamente dentro destes edifícios devido ao seu sinal ser bloqueado pelas paredes dos edifícios. por esta razão medidas alternativas de mapeamento indoor tem vindo a ser estudadas nos últimos anos e uma variedade de diferentes soluções tem começado a surgir, no entanto, até à data nenhuma destas soluções resolve completamente o problema em questão. a presente tese é feita em conjunto com outras com o objetivo final de produzir o protótipo de uma aplicação móvel e sistema que sejam capazes de localizar precisamente onde um utilizador se encontra dentro de uma localização interior, através do suporte de plantas dos edifícios em questão. no caso desta dissertação, o foco será, mais especificamente, o aspeto da modelação da geometria do espaço tendo como seu propósito a documentação da pesquisa do formato de dados mais apropriado para este use case, o desenvolvimento de um método de conversão de formatos que possibilite a conversão dos dados existentes para o formato escolhido, e todo o desenvolvimento e implementação dos web services e da aplicação móvel. adicionalmente também é feita a documentação do desenvolvimento de uma aplicação web que, utilizando as funcionalidades do web service das plantas, facilita a visualização do progresso do processo de radio mapping."
    ],
    [
      "nowadays, we are living in a time where sensors and applications that take advantage of them are increasingly taking part of our daily lives. thus, it is increasingly common to be surrounded by sensors, like image, sound or even luminosity or motion sensors, among several other types. in this context arises the challenge of how to connect sensors and applications that use them. the basic approach is to have each sensor bound to an application with its own private interface. the desirable approach is the oposite: a sensor should serve any application that requires its data using a well known interface. for this purpose a middleware solution is needed. nowadays, two trends are emerging in automotive industry: electrification and au tonomous driving. both cases means more sensors and software to deal with these sensors. also in this context the concept of a middleware makes sense to connect sensors and ap plications. more specifically, in this dissertation it is shown how robot operating system (ros) can be used to bridge the gap between the in-vehicle sensors and the applications that process the data from those sensors. through a distributed architecture, remote inter action between different components is possible, thus facilitating resources allocation and management. the project on which the work developed during this dissertation focuses, is part of the easyride program, the result of a partnership between the university of minho and bosch.",
      "atualmente, vive-se uma época em que os sensores e as aplicações que tiram partido deles são cada vez mais parte integrante do nosso quotidiano. tornando-se natural contactar com uma vasta gama de dispositivos no nosso dia-a-dia, o crescimento da área aplicacional facilita o processo de intenção entre os diferentes dispositivos. assim, é cada vez mais comum estar-se rodeado por vários sensores, como sensores de imagem, som ou ainda sensores de luminosidade ou movimento, entre diversos outros tipos. é neste contexto que surge o desafio de criar soluções distribuídas para lidar com esta grande variedade de dispositivos, tecnologias e aplicações. no caso concreto do setor automóvel, o processo de monitorização dos passageiros no veículo é muito similar. então, o objetivo deste projeto consiste em criar um sistema de aquisição de dados, para o setor automóvel, para monitorizar o interior do veículo, desde o seu estado de conservação até ao estado e comportamento dos passageiros. mais concretamente, o objetivo é mostrar como o robot operating system (ros) pode ser utilizado para fazer a ponte entre os sensores dentro do veículo e as aplicações que processam os dados desses mesmos sensores. através de uma arquitectura distribuída, é possível uma intenção remota entre diferentes componentes, facilitando assim a alocação e a gestão de recursos. o projeto sobre o qual incide o trabalho desenvolvido durante esta dissertação, faz parte do programa ensyride, fruto de uma parceria entre a universidade do minho e a bosch."
    ],
    0.3
  ],
  [
    [
      "this document presents a masters thesis in software engineering, in the area of academic management, support tools and web software. in a given higher education institution, the teaching service is assigned by it’s department twice a year, following a model of their own self-re-creation. initially, it is intended to standardize the information in a transversal way to all the departments and create a model that will serve as the basis for the implementation of a web application. this application will allow it’s users to enter and verify information, generating database entries in a dsl of their choice (such as sql), feeding a timetable construction platform.",
      "este documento tem como propósito apresentar uma tese do mestrado em engenharia informática, na área de gestão académica, ferramentas de apoio e software web. numa determinada instituição de ensino superior, o serviço docente é atribuído por cada departamento aos docentes em funções, duas vezes por ano, seguindo um modelo da sua própria auto-recriação. pretende-se, numa primeira instância, normalizar a informação de forma transversal a todos os departamentos e criar um modelo que servirá como base para a implementação de uma plataforma web de especificação da distribuição de serviço docente. a aplicação permitirá aos seus utilizadores inserir e verificar informação, gerando entradas para uma base de dados usando uma dsl (domain-specific language, ou, em português, linguagem de domínio específico) da sua escolha (como sql). as tabelas geradas servirão de base a plataformas de construção de horários."
    ],
    [
      "dados de alta qualidade sobre tratamentos médicos e de informação técnica tornaram-se acessíveis, criando novas oportunidades de e-saúde para a recuperação de um paciente. a implementação da aprendizagem automática nestas soluções provou ser essencial e eficaz na elaboração de aplicações para o utilizador para aliviar a sobrecarga do sector de saúde. atualmente, muitas interações com os utentes são realizadas via telefonemas e mensagens de texto. os agentes de conversação podem responder a estas questões, fomentando uma rápida interação com os pacientes. o objetivo fundamental desta dissertação é prestar apoio aos pacientes, fornecendo uma fonte de informação fidedigna que lhes permita instruir-se e esclarecer dúvidas sobre os procedimentos e repercussões dos seus problemas de saúde. este propósito foi concretizado não apenas através de uma plataforma web intuitiva e acessível, composta por perguntas frequentes, mas também integrando um agente de conversação inteligente para responder a questões. para este fim, cientificamente, foi necessário conduzir a investigação, implementação e viabilidade dos agentes de conversação no domínio fechado para os cuidados de saúde. constitui um importante contributo para a comunidade de desenvolvimento de chatbots, na qual se reúnem as últimas inovações e descobertas, bem os desafios actuais da aprendizagem automática, contribuindo para a consciencialização desta área.",
      "high-quality data on medical treatments and facility-level information has become accessible, creating new ehealth opportunities for the recuperation of a patient. machine learning implementation in these solutions has been proven to be essential and effective in building user-centred applications to relieves the burden on the healthcare sector. nowadays, many patient interactions are handled through healthcare services via phone calls and text message exchange. conversation agents can provide answers to these queries, promoting fast patient interaction. the underlying aim of this dissertation is to assist patients by providing a reliable source of information to educate themselves and clarify any doubts about procedures and implications of their health issue. this purpose was achieved not only through an intuitive and accessible web platform, with frequently asked questions, but also by integrating an intelligent chatting agent to answer questions. to this end, scientifically, it was necessary to conduct the research, implementation and feasibility of closed-domain conversation agents for healthcare. it is a valuable input for the chatbot development community, which assembles the latest innovations and findings, as well as the current challenges of machine learning, contributing to the awareness of this field."
    ],
    0.3
  ],
  [
    [
      "matrix algorithms often deal with large amounts of data at a time, which impairs efficient cache memory usage. recent collaborative work between the numerical algorithms group and the university of minho led to a blocked approach to the matrix square root algorithm with significant efficiency improvements, particularly in a multicore shared memory environment. distributed memory architectures were left unexplored. in these systems data is distributed across multiple memory spaces, including those associated with specialized accelerator devices, such as gpus. systems with these devices are known as heterogeneous platforms. this dissertation focuses on studying the blocked matrix square root algorithm, first in a multicore environment, and then in heterogeneous platforms. two types of hardware accelerators are explored: intel xeon phi coprocessors and nvidia cuda-enabled gpus. the initial implementation confirmed the advantages of the blocked method and showed excellent scalability in a multicore environment. the same implementation was also used in the intel xeon phi, but the obtained performance results lagged behind the expected behaviour and the cpu-only alternative. several optimizations techniques were applied to the common implementation, which managed to reduce the gap between the two environments. the implementation for cuda-enabled devices followed a different programming model and was not able to benefit from any of the previous solutions. it also required the implementation of blas and lapack routines, since no existing package fits the requirements of this application. the measured performance also showed that the cpu-only implementation is still the fastest.",
      "algoritmos de matrizes lidam regularmente com grandes quantidades de dados ao mesmo tempo, o que dificulta uma utilização eficiente da cache. um trabalho recente de colaboração entre o numerical algorithms group e a universidade do minho levou a uma abordagem por blocos para o algoritmo da raíz quadrada de uma matriz com melhorias de eficiência significativas, particularmente num ambiente multicore de memória partilhada. arquiteturas de memória distribuída permaneceram inexploradas. nestes sistemas os dados são distribuídos por diversos espaços de memória, incluindo aqueles associados a dispositivos aceleradores especializados, como gpus. sistemas com estes dispositivos são conhecidos como plataformas heterogéneas. esta dissertação foca-se em estudar o algoritmo da raíz quadrada de uma matriz por blocos, primeiro num ambiente multicore e depois usando plataformas heterogéneas. dois tipos de aceleradores são explorados: co-processadores intel xeon phi e gpus nvidia habilitados para cuda. a implementação inicial confirmou as vantagens do método por blocos e mostrou uma escalabilidade excelente num ambiente multicore. a mesma implementação foi ainda usada para o intel xeon phi, mas os resultados de performance obtidos ficaram aquém do comportamento esperado e da alternativa usando apenas cpus. várias otimizações foram aplicadas a esta implementação comum, conseguindo reduzir a diferença entre os dois ambientes. a implementação para dispositivos cuda seguiu um modelo de programação diferente e não pôde beneficiar the nenhuma das soluções anteriores. também exigiu a implementação de rotinas blas e lapack, já que nenhum dos pacotes existentes se adequa aos requisitos desta implementação. a performance medida também mostrou que a alternativa usando apenas cpus ainda é a mais rápida."
    ],
    [
      "a primavera bss é uma empresa tecnológica portuguesa ao serviço da gestão empresarial. empresa pioneira, em portugal, a desenvolver soluções de gestão para windows. a nível tecnológico, a primavera bss disponibiliza um vasto leque de serviços, nomeadamente softwares de gestão. o desenvolvimento dos micro-serviços na empresa assenta numa framework proprietária — a framework lithium. esta assegura uma arquitetura comum e padrões de desenho aplicando práticas de model-driven development (mdd). os micro-serviços são cada vez mais uma arquitetura muito utilizada na indústria. grandes empresas, adotaram esta arquitetura e muitas outras seguem a tendência, ao migrar as suas aplicações para esta arquitetura. contudo, existe ainda uma dificuldade em construir um sistema neste estilo muito devido à falta de informação ou conhecimento acerca dos padrões disponíveis, e por isso mesmo, este estilo arquitetural necessita de ser amplamente estudado. posto isto, um dos objetivos desta dissertação é suprir esta lacuna, através do estudo de elementos importantes, que devem ser considerados durante o desenvolvimento de aplicações/sistemas baseados na arquitetura de micro-serviços. outro objetivo passa por estudar a framework acima referida bem como respetivas alternativas. em suma, para além de se ter procedido à pesquisa e estudo de arquiteturas concorrentes, bem como de frameworks alternativas à lithium, também foi desenvolvida uma aplicação baseada na arquitetura dos micro-serviços.",
      "primavera bss is a portuguese technological company at the service of business management. pioneer company, in portugal, to develop management solutions for windows. in terms of technology, primavera bss provides a wide range of services, namely management software. the development of microservices in the company is based on a proprietary framework — the framework lithium. it ensures a common architecture and design standards by applying design practices mdd. microservices are increasingly a widely used architecture in the industry. large companies have adopted this architecture and many others follow the trend by migrating their applications to this architecture. however, there is still a difficulty in building a system in this style due to the lack of information or knowledge about the available patterns, and for this reason, this architectural style needs to be widely studied. that said, one of the objectives of this dissertation is to fill this gap, through the study of important elements, which must be considered during the development of applications/systems based on microservices architecture. another objective is to study the above-mentioned framework as well as its alternatives. in short, in addition to researching and studying competing architectures, as well as frameworks in alternatives to lithium, an application based on the microservices architecture was also developed."
    ],
    0.0
  ],
  [
    [
      "uma infraestrutura de comunicação nem sempre é fácil de gerir e o nível de dificuldade aumenta com a dimensão e o número de dispositivos presentes na infraestrutura de rede. a monitorização de uma rede é um processo fundamental na medida em que previne/deteta eventuais problemas e dispõe de diversas ferramentas auxiliadoras do trabalho de um administrador de redes de computadores. este trabalho tem como objetivo primordial o desenvolvimento de uma plataforma de monitorização de infraestruturas de rede de modo a que seja viável observar os tipos de aplicações que estão a gerar tráfego na rede, analisar o respetivo impacto do tráfego gerado, bem como apresentar detalhes sobre os dispositivos que estão a utilizar essas mesmas aplicações, entre outras diversas funcionalidades. esta informação será exposta num dashboard intuitivo de forma a que um utilizador, mesmo sem ter conhecimentos técnicos aprofundados na área das redes de computadores, seja capaz de interpretar com facilidade o estado da infraestrutura de rede. o dashboard será desenvolvido de forma a possibilitar uma rápida perceção dos pontos críticos que existem na rede (e.g. pontos de congestão) e aplicações e dispositivos que estão na origem dos mesmos. assim, será possível detetar em tempo real possíveis anomalias e tomar medidas que as contenham, tendo por base as aplicações e os dispositivos que as originaram. também serão emitidos alertas referentes a essas mesmas anomalias. posteriormente, será estudado e desenvolvido um módulo de otimização da rede que integrará na plataforma desenvolvida, consistindo na determinação de alterações que aproveitem ao máximo os recursos da rede e, consequentemente, aumentem o desempenho da infraestrutura, de acordo com o tráfego que nela circula. estas alterações poderão ser implementadas pela plataforma, nos casos em que haja essa possibilidade, ou assumir a forma de recomendações enviadas ao administrador da rede (e.g. horários em que se devam realizar determinadas operações na rede, tais como backups entre outras possibilidades). a plataforma será desenvolvida, aplicada e testada tento em conta o contexto específico de uma intranet de uma empresa do sector têxtil. esta empresa necessita deste tipo de plataforma de forma a melhorar o desempenho da sua infraestrutura de rede.",
      "a communication infrastructure is not easy to manage and the level of difficulty increases with the network size and the number of existing devices. thus, monitoring a network is essential to prevent potential problems and there are many monitoring tools to assist the work of a network administrator. this work aims to design and create a platform allowing to monitor a network infrastructure and discover what applications are generating network traffic and the devices that are using such applications. this information will be presented in a dashboard so that a computer technician can understand the current state of the comunication network, even if he do not have special technical skills in the computer networks field. the dashboard will be structured to perceive quickly the critical points (e.g. point of congestion) that exist in the network and which applications and devices are in the origin of such problems. thus, it will be possible to detect, in real time, critical situations and make decisions to avoid them, based on the applications and devices responsible for these situations. with the proposed system it will also be possible to generate alerts of these situations. additionally, it will be studied and developed a module for network optimization. this module will determine changes that can increase the network performance, according to the traffic that flows in it. these changes can be triggered by the platform (if possible) or recommend to the administrator (e.g. hour to execute backups of information from users of the infrastructure, or other examples). this platform will be developed and tested in a textil company scenario. this company requires this kind of platforms to improve the performance of company network infrastructure and overcome situations where there is a low quality of service in the network."
    ],
    [
      "logistics services, including express mail delivery areas, have been growing significantly by the increase in the volume of e-commerce activity worldwide. it is expected that the rise in the level of digital competencies of companies and citizens will not only promote considerable growth in this sector over the next few years, but also demand higher levels of efficiency, quality, and modernization of digital platforms for interaction with customers. in terms of continuous monitoring, new technologies offer potential, namely the use of gps devices to collating coordinates. with system integration, the collected coordinates can be temporarily saved and then sent to a remote server. door-to-door service requires exact locations, so there are certain technologies, which allow us to collect that information accurately without the minimum margin of error. in the context of door-to-door distribution, most companies have simple technology that provides a piece of insufficient information regarding the status of their order, they only present information that the postal service may be delivered, refused, or the addressee may not be found. regarding door-to-door distribution, technologies can be implemented to improve the current industry solutions, providing more detailed information about the order status. thus, a solution was developed based on international standards, that allow live tracking application ensuring also data security through blockchain technologies.",
      "os serviços de logística, onde se inserem as áreas de entrega expresso de correspondência postal, tem vindo a crescer significativamente, muito impulsionados pelo aumento do volume da atividade e-commerce em todo o mundo. perspetiva-se que o aumento do nível de competências digitais das empresas e cidadãos venha não só a promover um forte crescimento deste setor nos próximos anos, como também a exigir níveis mais elevados de eficiência, qualidade, e modernização das plataformas digitais de interação com os clientes. no que se enquadra a uma monitorização contínua, as novas tecnologias oferecem potencialidades nomeadamente o uso de dispositivos gps, para o efeito de recolha de coordenadas. com a integração de sistemas, as coordenadas recolhidas podem ser guardadas temporariamente e de seguida enviadas para um servidor remoto. como se pretende um serviço de porta-a-porta a localização atual deve de ser exata, por isso existem determinadas tecnologias, que nos permitem recolher essa informação de maneira precisa sem que haja uma mínima margem de erro. no que concerne à distribuição porta a porta atualmente a informação apresentada ao utilizador proporciona apenas simples mensagens acerca do estado atual tais como: entregue, recusado, ou destinatário não encontrado. de acordo com a distribuição porta-a-porta, tecnologias podem ser implementadas de modo a melhorar as soluções da indústria, providenciando informação mais detalhada acerca do estado da encomenda. assim, foi desenvolvida uma solução baseada em standards internacionais, que permitem um rastreamento em tempo real assegurando também a segurança dos dados através da tecnologia blockchain."
    ],
    0.3
  ],
  [
    [
      "the main objective of this dissertation was the development of a secure web application with the django framework, through the implementation of a set of selected security mea sures. the application that was selected to apply the security measures was an e-commerce platform. another objective of the present work was to analyze the support, offered by django, to the development of secure web applications. the literature review helped us to identify web application security threats as well as their possible organization in 5 classes: (i) code injection threats, (ii) authentication control threats, (iii) access control threats, (iv) threats to data confidentiality, and (v) threats to the availability of the service. this kno wledge was crucial for the selection of the protection measures to implement in the web application. the application was developed according to the scrum agile methodology. the main problems encountered in using scrum were (i) the need to adapt the methodo logy to a context in which the scrum team has few members, and (ii) managing the conflict between the need to document sprints and the scrum practice that favors productivity over documentation. the adopted compromise was to document each sprint before starting the next one. on the other hand, using scrum improved the definition and fulfillment of the objectives, and allowed for the improvement of the development process itself. these be nefits result from scrum following an iterative approach geared to the rapid production of functional product increments. from the set of functionalities identified during the re quirement elicitation phase, 18 user stories were successfully implemented, resulting in a minimum viable product, i. e., a functional product that implements the top most priority requirements for potential users. after implementing and testing the application, it was found that the level of security achieved is high, since 16 of the 19 implemented security measures are effective in protecting against the respective attacks. the support provided by django, to the implementation of the selected security measures, reached a value close to 68 %. code injection protection is the security measure best supported by django. at the opposite extreme are the threats to the availability of the service, against which django does not offer any support.",
      "o principal objetivo desta dissertação era o desenvolvimento de uma aplicação web segura com a framework django, através da implementação de um conjunto de medidas de segu rança selecionado. a aplicação escolhida para aplicar as medidas de segurança foi uma plataforma de comércio eletrónico. outro objetivo do trabalho consistia em analisar o su porte, oferecido pelo django, ao desenvolvimento de aplicações web seguras. a revisão da literatura ajudou a identificar as ameaças que a segurança das aplicações web enfrenta, bem como uma possível organização destas em 5 classes: (i) ameaças de injeção de código, (ii) ameaças ao controlo de autenticação, (iii) ameaças ao controlo de acesso, (iv) ameaças à confidencialidade dos dados e (v) ameaças à disponibilidade do serviço. este conhecimento foi crucial para a seleção das medidas de proteção a implementar na aplicação web. a apli cação foi desenvolvida de acordo com a metodologia ágil scrum. os principais problemas encontrados na utilização do scrum foram (i) a necessidade de adaptar a metodologia a um contexto em que a equipa scrum é de dimensão reduzida e (ii) a gestão do conflito entre a necessidade de documentar os sprints e a prática do scrum, que favorece a rentabili dade do trabalho em detrimento da documentação. a solução de compromisso encontrada passou por documentar cada sprint antes de se iniciar o próximo. em contrapartida, a adoção do scrum melhorou a definição e cumprimento dos objetivos, e permitiu o aperfei çoamento do próprio processo de desenvolvimento. estes benefícios resultam de o scrum seguir uma abordagem iterativa e orientada à produção rápida de incrementos de produto funcionais. do conjunto de funcionalidades identificadas durante a fase levantamento de requisitos, foram implementadas com sucesso 18 histórias de utilizador, resultando num produto minimamente viável, ou seja, um produto funcional que satisfaz os principais requisitos que interessam aos potenciais utilizadores. após a implementação e teste da aplicação, constatou-se que o nível de segurança atingido é elevado, visto que 16 das 19 medidas de segurança são eficazes na proteção contra os respetivos ataques. o suporte oferecido pelo django, à implementação das medidas de segurança selecionadas, atingiu um valor próximo de 68%. a proteção contra injeção de código é a medida de segurança melhor suportada pelo django. no extremo oposto estão as ameaças à disponibilidade do serviço, contra as quais o django não oferece qualquer apoio."
    ],
    [
      "na atual era digital, a cibersegurança surgiu como um aspeto essencial da tecnologia da informação que tem uma importância significativa a nível mundial. a crescente dependência das plataformas digitais e o volume cada vez maior de dados sensíveis trocados através da internet fizeram com que a cibersegurança deixasse de ser um elemento opcional e passasse a ser uma necessidade fundamental. a importância da cibersegurança vai para além das medidas de proteção. desempenha um papel crucial para garantir a continuidade das operações comerciais, proteger a privacidade das pessoas e manter a integridade dos sistemas e dos dados. um dos problemas atualmente é o excesso de informação de segurança que se tem de analisar. além disso, nem sempre a relação entre eventos é direta e, por vezes, apenas é possível detetar algo malicioso quando são observados eventos que à partida parecem não relacionados. em resposta a este desafio, o tema desta dissertação é o desenvolvimento de um gestor de eventos no contexto da cibersegurança. este gestor tem como objetivo simplificar o processo de identificação e compreensão das relações entre vários eventos de segurança, mesmo aqueles que podem parecer não relacionados à primeira vista. o sistema proposto fornecerá uma vasta quantidade de dados, permitindo aos utilizadores identificar padrões, anomalias e correlações que possam significar potenciais ameaças à segurança. a interface pro posta não é apenas uma ferramenta para interpretar dados de segurança, mas uma solução abrangente que aumenta a eficiência das operações de cibersegurança. esta tese tem como objetivo demonstrar a implementação deste tipo de gestor de eventos, explorando os seus potenciais benefícios e implicações para as práticas de cibersegurança.",
      "in today’s digital age, cybersecurity has emerged as an essential aspect of information technology that is of significant importance worldwide. the growing dependence on digital platforms and the ever-increasing volume of sensitive data exchanged over the internet has meant that cybersecurity is no longer an optional element, but a fundamental necessity. the importance of cybersecurity goes beyond protection measures. it plays a crucial role in guaranteeing the continuity of business operations, protecting people’s privacy and maintaining the integrity of systems and data. one of the problems today is the excessive amount of security information that has to be analyzed. in addition, the relationship between events is not always direct and sometimes it is only possible to detect something malicious when events are observed that at first seem unrelated. in response to this challenge, the subject of this dissertation is the development of an event manager in the context of cybersecurity. this manager aims to simplify the process of identifying and understanding the relationships between various security events, even those that may seem unrelated at first glance. the proposed system will provide a vast amount of data, allowing users to identify patterns, anomalies and correlations that may signify potential security threats. the proposed interface is not just a tool for interpreting security data, but a comprehensive solution that increases the efficiency of cybersecurity operations. this thesis aims to demonstrate the implementation of this type of event manager, exploring its potential benefits and implications for cybersecurity practices."
    ],
    0.3
  ],
  [
    [
      "this document presents a master thesis in the integrated master’s in informatics engi neering focused on the automatic identification of vulnerabilities, that was accomplished at universidade do minho in braga, portugal. this thesis work aims at developing a machine learning based tool for automatic iden tification of vulnerabilities on programs (source, high level code), that uses an abstract syntax11tree representation. it is based on fastscan, using code2seq approach. fastscan is a recently developed system aimed capable of detecting vulnerabilities in source code using machine learning techniques. nevertheless, fastscan is not able of identifying the vulnerability type. in the presented work the main goal is to go further and develop a method to identify specific types of vulnerabilities. as will be shown, the goal will be achieved by changing the method of receiving and processing in a different way the input data and developing an architecture that brings together multiple models to predict different specific vulnerabilities. the best f1 metric obtained is 93% resulting in a precision of 90% and accuracy of 85%, according to the performed tests and regarding a trained model to predict vulnerabilities of the injection type. these results were obtained with the contribution given by the optimization of the model’s hyperparameters and also the use of the search cluster from university of minho that greatly diminished the necessary time to perform training and testing. it is important to refer that overfitting was detected in the late stages of the tests, so this results do not represent the true value in real context. also an interface is presented, it allows to better interact with the models and analyse the scan results.",
      "este documento apresenta uma dissertação do mestrado integrado em engenharia infor mática, que tem como foco a automação da deteção de vulnerabilidades e foi concluída na universidade do minho em braga, portugal. o trabalho apresentado nesta tese pretende desenvolver uma ferramenta que utiliza machine learning e que seja capaz de identificar vulnerabilidades em código. utilizando para isso a representação do mesmo numa abstract syntax tree. tem como base fastscan que utiliza a abordagem do code2seq. fastscan é um projeto recentemente desenvolvido que é capaz de detetar vulnerabilidades em código utilizando técnicas de machine learning, sendo que tem algumas lacunas como o facto de não ser capaz de identificar vulnerabilidades específicas. no trabalho apresentado o objetivo é ir mais além e desenvolver um método capaz de identificar qual o tipo específico de vulnerabilidade presente. como será apresentado ao longo do documento, este objetivo será alcançado pela alteração do método de receção e processamento dos dados recebidos, assim como o desenvolvimento de uma arquitetura que junte os vários modelos de maneira a cooperarem e a ferramenta ser capaz de detetar e prever a presença de vulnerabilidades específicas. a melhor métrica de f1 obtida foi de 93%, com precisão de 90% e accuracy de 85%, de acordo com os testes efetuados sobre um modelo treinado para prever a presença de vulnerabilidades do tipo de injection. os resultados foram obtidos devido à otimização dos hiper-parâmetros dos modelos e o cluster search da universidade do minho diminuiu consideravelmente o tempo necessário para efetuar o traino e testes dos modelos. é importante referir que foi detetado overfitting na fase final do desenvolvimento deste trabalho, sendo que os resultados apresentados não representam o valor real dos modelos em contexto real. para além disso é apresentada uma interface que permite interagir e analisar os resultados de um scan feito pelos modelos."
    ],
    [
      "o recurso a ferramentas de geração automática de código permite economizar tempo quando se desenvolvem soluções de software, factor importante em questões de produtividade. existe um conjunto de padrões de conceção [gamma et al., 1995] que representam soluções genéricas para problemas relativos ao desenvolvimento de aplicações de software, numa perspetiva orientada aos objetos. para cada um deles pode ser vista a sua estrutura de classes, métodos e relacionamentos, bem como as situações mais adequadas para a sua utilização. bastará consultar o catálogo de padrões de conceção [gamma et al., 1995] e utilizar aquele que mais se adequar à resolução de determinado problema que surja no desenvolvimento de um novo programa. a existência de uma aplicação de software capaz de fazer a geração automática do código associado aos padrões de conceção, agiliza o desenvolvimento de novas aplicações, porque fornece de imediato o respetivo código. o que se propõe com o desenvolvimento desta dissertação é uma solução de software, capaz de efetuar a geração automática de código para os padrões de conceção catalogados em [gamma et al., 1995]. juntamente com o programa desenvolvido, é também apresentado um levantamento do estado da arte sobre os padrões de conceção, considerando também situações atuais da sua aplicabilidade. em seguida, é descrita a especificação da aplicação elaborada, bem como o seu processo de desenvolvimento, acompanhado de um exemplo de utilização. por fim, encontram-se dois casos de estudo, servindo para provar que o programa elaborado pode ser utilizado em contextos reais.",
      "automatic code generation tools are very important when developing software, since they generate code very quickly, the software can be released earlier, which is a key factor nowadays. there is a set of design patterns [gamma et al., 1995] that represent generic solutions to software development problems, regarding an objectoriented perspective. for each design pattern there is a class diagram with some methods and relationships between classes, and some examples of use. to solve a problem that arises when developing a new software program, it is enough searching for the appropriate design pattern [gamma et al., 1995]. so, a software application that automatically generates code for design patterns eases developing new software, once the patterns' code is immediately provided. in this master dissertation it is proposed a software solution to automatically generate code for design patterns [gamma et al., 1995]. it is also presented the state of the art about design patterns, as well as some recent examples using them. the design of the developed program is also approached, and its implementation process too. finally, there are two case studies proving the developed program can be used in real contexts."
    ],
    0.2571428571428571
  ],
  [
    [
      "numa era de grandes evoluções tecnológicas e de fácil acesso a todo o tipo de informação a nível global, gerou-se uma oportunidade há muito idealizada para que fossem libertados dados adquiridos pelos governos para uso livre, pela e para a comunidade em geral. no entanto, o acesso aos dados por parte de um cidadão por si só pode não ter grande impacto social. posto isto, a utilização livre dos dados transformandoos em informação, seja de forma aberta ou comercial, por parte de indivíduos ou equipas de desenvolvimento especializadas poderá fornecer ao cidadão todo um leque de serviços até então inexistentes. assim, o principal objetivo deste trabalho passa por disponibilizar uma aplicação móvel que se torne uma mais-valia para os cidadãos, auxiliando-os na resposta a necessidades inerentes aos serviços públicos e privados mais procurados. de forma a concretizar o objetivo proposto, foi efetuado um levantamento do funcionamento real da estrutura a representar. posto isto, verificou-se a necessidade de criação de um back-office para gestão e centralização da informação associada ao modelo originado. como resultado, foi criada uma aplicação para android que servirá como primeira ferramenta de medida dos benefícios que a solução pretendida poderá levar à vida dos cidadãos. espera-se ainda que a aplicação sirva de incentivo à criação de soluções do mesmo âmbito. os objetivos foram alcançados e brevemente os cidadãos poderão usar a aplicação criada, assim como terão acesso a todo o código fonte para melhorias que possam ser acrescentadas ao sistema.",
      "in an era of great technological advances and easy, global access to all kinds of information, there is now an opportunity, idealized long-ago, to release data acquired by the governments to the public, rendering it free to use by the general community. however, a single citizen accessing this data might not make any significant social impact; thus, freely using this data and turning it into information, whether openly or commercially, by individuals or specialized development teams, could provide citizens with a new range of services that did not exist before. this way, the main objective of this work is to create a mobile application that can be an asset to citizens and help them deal with the inherent necessities of public and private services that are more commonly and often used. in order to achieve the proposed goal, we observed the inner workings of the structure to be represented; therefore, there was the need to create a back-office to manage and centralize the information associated with the devised model. as a result, an application for the android system was created which will serve as the first tool of measurement of the benefits that the proposed solution will bring to citizens. it is also expected that this application will function as an incentive to create solutions in the same spectrum. the objectives were achieved and soon citizens will be able to use the application, as well as have access to the source code so that improvements can be made and features added to the system."
    ],
    [
      "o absentismo é um problema que tem vindo a crescer ao longo dos últimos anos e que afeta gravemente a economia das empresas. para além disso, esta questão está intrinsecamente relacionada com a saúde dos trabalhadores, dado que doenças e acidentes de trabalho são duas das maiores causas de absentismo por todo o mundo. de modo a perceber melhor a razão por detrás da crescente taxa de absentismo, assim como a relação entre o absentismo e a saúde e estilo de vida dos colaboradores de uma empresa portuguesa, tornou-se necessário recorrer às tecnologias de informação (ti), nomeadamente ao business intelligence (bi). esta tecnologia permite uma rápida análise de dados e uma melhor compreensão da informação existente. assim sendo, esta dissertação tem como objetivo desenvolver uma aplicação web, recorrendo a ferramentas de bi, que permita o estudo dos dados de absentismo através de indicadores clínicos de forma a identificar as principais causas, bem como a implicação que o trabalho e o estilo de vida possa ter na saúde dos colaboradores e, consequentemente, no absentismo. por conseguinte, esta aplicação fornece apoio à tomada de decisão e prática clínica por parte dos profissionais de saúde e permite também, através da análise dos dados, que sejam encontradas soluções para a diminuição da taxa de absentismo da empresa. esta solução pretende ainda apresentar toda a informação relativa aos colaboradores e às suas ausências de uma forma organizada e de fácil leitura, sendo, deste modo, menos suscetível a erros, assim como mais rápido e mais eficiente que o atual sistema utilizado para a análise do absentismo, facilitando assim o trabalho dos profissionais.",
      "absenteeism is a problem that has been increasing over the past few years and it severely affects the economy of industries. moreover, it is also related to the health of the employees since the most common causes of absenteeism all over the world are sickness leaves and work accidents. in order to understand the main motives for the increasing absenteeism rate, as well as the relation between absenteeism and the health and lifestyle of the collaborators of a portuguese company, it was necessary to resort to health information and communication technology (ict), namely to business intelligence (bi). this technology allows a fast analysis of the data and a higher comprehension of the information available. thus, the main purpose of this dissertation is to develop a web application based on bi that enables the study of the data related to the absences through clinical indicators so that it is possible to identify the main causes, as well as how the workload and the lifestyle of the employees may affect their health and, consequently, the absenteeism of the company. furthermore, this platform gives support to the decision-making process and to the healthcare provided by the health professionals and it also permits to discover solutions, through the analysis of the data, in order to decrease the absenteeism rate of the company. in addition, this solution is intended to present all of the information related to the collaborators and to their absences in an easier to interpret and more organized way so that it is less prone to errors and also faster and more efficient comparing to the method that is currently being used for the analysis of the absenteeism, in order to facilitate the work done by health and human resources professionals."
    ],
    0.0
  ],
  [
    [
      "phobia is a type of anxiety disorder defined by a persistent and excessive fear of an object or situation. currently, exposure therapy is the most practiced method to treat phobias, although it comes with limitations. we can reduce these limitations by combining augmented reality techniques with exposure therapy. its benefits are a decrease in costs, versatility of the process, and full control of the procedure by the therapist. as shown in multiple research, augmented reality has obtained interesting results in the therapy of psychological disorders serving as a foundation for the development of this project. the recent technological advances in the field also allowed for easier access to augmented reality which is accessible to use even in old smartphones. the goal of this master’s dissertation was to develop an artefact in conjunction with psychologists who treat phobic patients, to create a program to support the therapy of phobias with a gradual exposure system. their help was essential to understand the most important features needed for the platform. the platform was deployed in the informatics department servers, which could be accessed by everyone that had internet connection. multiple psychologists were invited to test the platform by following a user guide created and give their technical feedback in the end. the results gathered were positive, which proves the viability of this system as an extension to the current methods by providing comfort and efficiency.",
      "a fobia é um tipo de transtorno de ansiedade definido por um medo persistente e excessivo de um objeto ou situação. atualmente, o terapia de exposição é o método mais praticado para tratar fobias, embora com limitações. estas limitações são reduzidas combinando técnicas de realidade aumentada com a terapia de exposição. os seus benefícios são uma redução de custos, versatilidade do processo e controle total do procedimento pelo terapeuta. como foi demostrado em várias pesquisas, a realidade aumentada obteve resultados interessantes no tratamento de distúrbios psicológicos, servindo de base para o desenvolvimento deste projeto. os recentes avanços tecnológicos no campo permitem também um acesso fácil à realidade aumentada, acessível para uso mesmo em smartphones antigos. nosso objetivo é desenvolver um artefato em conjunto com psicólogos que tratam de pacientes fóbicos, para criar um programa para apoiar o tratamento de fobias com um sistema de exposição gradual. a ajuda deles foi essencial para entender os recursos que são mais importantes para a plataforma. a plataforma foi colocada nos servidores do departamento de informática e podia ser acedida por qualquer pessoa que tivesse conexão à internet. vários psicólogos foram convidados a testar a plataforma seguindo um guião criado e dando seu feedback técnico no final. os resultados recolhidos foram positivos, o que comprova a viabilidade deste sistema como uma extensão dos métodos atuais providenciando conforto e eficiência."
    ],
    [
      "microservices emerged as one of the most popular architectural patterns in the recent years given the increased need to scale, grow and flexibilize software projects accompanied by the growth in cloud computing and devops. many software applications are being submitted to a process of migration from its monolithic architecture to a more modular, scalable and flexible architecture of microservices. this process is slow and, depending on the project’s complexity, it may take months or even years to complete. this dissertation proposes a new approach on microservices identification by resorting to topic modelling in order to identify services according to domain terms. this approach in combination with clustering techniques produces a set of services based on the original software. the proposed methodology is implemented as an open-source tool for exploration of monolithic architectures and identification of microservices. an extensive quantitative analysis using the state of the art metrics on independence of functionality and modularity of services was conducted on 200 open-source projects collected from github. cohesion at message and domain level metrics showed medians of roughly 0.6. interfaces per service exhibited a median of 1.5 with a compact interquartile range. structural and conceptual modularity revealed medians of 0.2 and 0.4 respectively. further analysis to understand if the methodology works better for smaller/larger projects revealed an overall stability and similar performance across metrics. our first results are positive demonstrating beneficial identification of services due to overall metrics’ results.",
      "os microserviços emergiram como um dos padrões arquiteturais mais populares na atualidade dado o aumento da necessidade em escalar, crescer e flexibilizar projetos de software, acompanhados da crescente da computação na cloud e devops. muitas aplicações estão a ser submetidas a processos de migração de uma arquitetura monolítica para uma arquitetura mais modular, escalável e flexivel de microserviços. este processo de migração é lento, e dependendo da complexidade do projeto, poderá levar vários meses ou mesmo anos a completar. esta dissertação propõe uma nova abordagem na identificação de microserviços recorrendo a modelação de tópicos de forma a identificar serviços de acordo com termos de domínio de um projeto de software. esta abordagem em combinação com técnicas de clustering produz um conjunto de serviços baseado no projeto de software original. a metodologia proposta é implementada como uma ferramenta open-source para exploração de arquiteturas monolíticas e identificação de microserviços. uma análise quantitativa extensa recorrendo a métricas de independência de funcionalidade e modularidade de serviços foi conduzida em 200 aplicações open-source recolhidas do github. métricas de coesão ao nível da mensagem e domínio revelaram medianas em torno de 0.6. interfaces por serviço demonstraram uma mediana de 1.5 com um intervalo interquartil compacto. métricas de modularidade estrutural e conceptual revelaram medianas de 0.2 e 0.4 respetivamente. uma análise mais aprofundada para tentar perceber se a metodologia funciona melhor para projetos de diferentes dimensões/características revelaram uma estabilidade geral do funcionamento do método. os primeiros resultados são positivos demonstrando identificações de serviços benéficos tendo em conta que os valores das métricas são de uma forma global positivos e promissores."
    ],
    0.3
  ],
  [
    [
      "o segment routing (sr) é uma implementação de software defined networking (sdn) para encaminhamento de tráfego. baseado num paradigma de source routing, cada nodo na fronteira da rede adiciona um conjunto de etiquetas ao cabeçalho dos pacotes e define explicitamente o caminho que cada fluxo de tráfego deverá percorrer até ao destino. cada etiqueta é um segmento que identifica uma instrução topológica, um serviço ou mesmo uma instrução de processamento. embora o sr seja muito parecido com o multi protocol label switching (mpls) no que respeita ao encaminhamento simples de pacotes, a sua utilização simplifica os processos de gestão da rede. por outro lado, o sr também proporciona soluções para problemas de escalabilidade existente em implementações sdn como o openflow. esta tecnologia está rapidamente a tornar-se um standard e é já suportada por empresas importantes, como a cisco e a huawei, desempenhando um papel importante na arquitetura das futuras redes 5g. face ao desenvolvimento desta nova tecnologia, surgiram novos tipos de serviço que tiram partido das capacidades da mesma. devido ao crescente número de dispositivos 5g, surgiu a necessidade de disponibilizar os serviços em diversos pontos da rede, fornecendo assim resposta às necessidades requeridas. como tal, as operadoras de telecomunicações desenvolveram o conceito de network function virtualization (nfv). todavia existem questões importantes que necessitam ser tratadas, nomeadamente, como distribuir os serviços pela topologia de forma a reduzir o nível de utilização das ligações e nodos da mesma. este trabalho tem como objetivo a exploração de soluções baseadas em sr e em nfv, recorrendo a técnicas de otimização como algoritmos evolucionários e machine learning.",
      "segment routing (sr) is an implementation of software-defined networking (sdn) for traffic routing. it derives from a source routing paradigm where each node, at the network’s periphery, adds a list of labels to the packet’s header defining explicitly the path each traffic flow should follow to its destination. each label is a segment that identifies a topological instruction, a service, or even a processing instruction. although sr can be very similar to multi-protocol label switching (mpls), concerning the simple routing of packets, its utilization simplifies the network management tasks. on the other hand, sr provides solutions to existing scalability problems on sdn implementations, for example, openflow-based. this technology is rapidly becoming a standard and is already supported by big players such as cisco and huawei, performing a fundamental role in the architecture of future 5g networks. also, due to these new technology capabilities, there is an increasing number of novel services. this growth, allied with the rise of 5g capable devices, motivated the deployment of services on crucial points of the network. consequently, mobile operators created the network function virtualization (nfv) concept. however, some important questions should be considered, such as how to optimize network resources regarding service locations to reduce the congestion level on the network. the main contribution of this work consists of exploring solutions based on sr and nfv. thus, relying on optimization techniques to pursue optimal resource allocation for traffic routing and service processing requirements."
    ],
    [
      "nos últimos anos temos assistido a grandes evoluções nas aplicações web, proporcionando uma interação cada vez mais apelativa, tanto no seu aspeto como na sua usabilidade. com estas evoluções foi surgindo o conceito de rich internet applications (ria). as ria são aplicações web que tem características e funcionalidades que eram usualmente desempenhadas por software para desktop dada a sua complexidade. o conceito passa por transferir processamento para o cliente (browser), permitindo uma interação mais próxima com o utilizador, respostas mais rápidas e uma menor sobrecarga sobre o lado do servidor. esta ideia permite uma abordagem ao desenvolvimento de aplicações empresariais no contexto web, sendo possível manter a capacidade de processamento das aplicações desktop tirando partido do melhor do mundo web e permitindo chegar mais facilmente aos clientes. a primavera, como interessada neste tipo de aplicações, já desenvolveu a framework athena assente nas tecnologias microsoft silverlight e wcf ria services, mas dado que a evolução do microsoft silverlight foi descontinuada por parte da microsoft, existe a necessidade de procurar uma alternativa. o html5 surge como principal alternativa, mas é preciso perceber se este pode efetivamente constituir-se como tal. assim, esta dissertação será inicialmente focada na análise comparativa entre as tecnologias microsoft silverlight e html5, passando depois pela implementação de protótipos, que no final, permitam à primavera tomar uma decisão sobre a substituição da tecnologia. o caso de estudo será uma aplicação desenvolvida em html5, javascript e css, que poderá ser instalada em dispositivos móveis com qualquer sistema operativo. estas aplicações, denominadas aplicações híbridas, implicam também uma elevada complexidade no lado do cliente e envolvem vários dos conceitos relacionados com o desenvolvimento de ria em html5.",
      "in the last years we have seen many improvements in web applications, providing an increasingly attractive interaction in its appearance and usability. with these improvements has grown the concept of rich internet applications (ria). ria are web applications that have features and functionalities that were usually performed by desktop applications due to its complexity. the concept involves transferring processing to the client’s side (browser), providing a closer interaction with the user, faster responses and lower load on the server’s side. this idea allows an approach to the development of business applications in a web context, in order to keep the processing power of desktop applications and to reach customers more easily by taking advantage of the web world. primavera, as an interested party in this kind of applications, already developed the athena framework based on microsoft silverlight and wcf ria services, but since the development of microsoft silverlight was discontinued by microsoft, it is necessary to find an alternative. html5 ascends as the main alternative but we must realize if it can effectively be considered as such. initially, this dissertation will be focused on the comparative analysis between microsoft silverlight and html5 technologies, then by the implementation of prototypes, which in the end, allow primavera to make a decision about the technology’s replacement. the case study will be based on an application developed with html5, javascript and css, which can be installed on mobile devices running any operating system. these applications, denominated hybrid applications, also include a high complexity on client’s side and involve several concepts related to ria development with html5."
    ],
    0.0
  ],
  [
    [
      "monitoring of internet services reveals that there is a growing trend in the use of cloud services. considering the strong growth on the access to these services in a relatively short period of time, it is estimated that shortly they will be responsible for a significant amount of internet flows. in this context, this project aims to identify and quantify the use of cloud services at the university of minho (um). achieving this goal involves identifying appropriate techniques for traffic classification and the definition of a model for processing the collected traces. attending to the available set of cloud services, this study focuses on characterizing cloud storage services, identifying the most accessed cloud storage providers and the characteristics of the corresponding traffic. cloud storage services present several characteristics that turn the current classification methods insufficient or too complex to apply, namely the use of dynamic communication ports and security protocols encrypting the traffic. this motivates the use of a new classification approach based on tstat tool, which uses the technique of extracting signatures of servers during the handshake of secure sockets layer (ssl) protocol. the obtained results provide global statistics regarding the most used services at um, focusing subsequently on cloud storage services. for these, the top cloud storage providers within um users preferences are identified and the characteristics of the traffic associated to each one are discussed.",
      "a monitorização dos serviços que fluem na internet revela que existe uma tendência crescente na utilização de cloud services. pelo forte crescimento de acessos a estes serviços num espaço de tempo relativamente curto, estima-se que brevemente estes sejam responsáveis pela grande parte dos fluxos da internet. neste contexto, este projeto tem por objetivo identificar e quantificar a utilização de cloud services por parte dos utilizadores da universidade do minho (um). a concretização do objetivo envolve a identificação de técnicas apropriadas de classificação de tráfego e a definição do modelo de processamento dos dados coletados. do conjunto de cloud services, este estudo incide nos serviços de armazenamento na nuvem, os cloud storage. como tal, o estudo realizado tem em conta o prestador do serviço de armazenamento e as características do tráfego associado. as características do tráfego destes serviços, que recorrem a portas de comunicação dinâmicas e utilizam protocolos de segurança, cifrando desta forma o seu tráfego, tornam os métodos de classificação atuais insuficientes ou muito complexos de aplicar. desta forma, foi necessário utilizar uma nova abordagem de classificação, recorrendo à ferramenta tstat, que utiliza a técnica de extração de assinaturas dos servidores durante a fase de handshake do protocolo secure sockets layer (ssl). os resultados apresentados reportam, inicialmente, as estatísticas sobre os serviços mais acedidos na um de forma global, incidindo posteriormente nos serviços cloud storage. relativamente a estes, discutem-se os cloud storage providers mais acedidos pelos utilizadores e as características do tráfego de cada um deles."
    ],
    [
      "the procedural generation of geometry within the space of computer graphics has been a topic of study for quite some time, benefiting from a more unpredictable brand of randomness. similarly, the exploration of lighting as a phenomenon within virtual space has been a field of study of comparable age. despite its age and early adoption, there is a surprising lack of research in emulating the phenomenon of lighting past its interactions with the world. most implementations of procedurally generated lightning within video games are based on randomized data trees. when part of the skybox, 2d meshes or textures are randomly selected from a pre-made pool. there are, however, methods based entirely on the dielectric breakdown model, using approximations to solve a laplacian equation. this dissertation aims to present an alternative approach to the randomized and procedural generation of lightning bolts based on the space colonization algorithm. while the algorithm was first conceived for use in botanical applications, modeling the growth of biological structures, the similarities between the results produced by the dielectric breakdown model and botanic modeling algorithms coupled with the visual likeness of a lightning bolt and certain trees, made for solid groundwork upon which to establish this unique approach. as such, this work largely aims to be a first step into this particular realm, showing space colonization as a suitable algorithm for this specific purpose. that being said, a large portion of time was spent iterating, modifying and experimenting with ideas that were either discarded or adapted, an effort primarily dedicated towards controlling and stifling the possible growth of branches in ways beyond the reduction of attractors. the original algorithm was altered, focus put especially on the creation of a singular channel at a time, mixing discoveries from previous research with the work done on manipulating space colonization. instead of the venation patterns observed with the original work, the stifling of any growth means that each node has a chance, when created, of sprouting a branch and each branch is, in turn, a different, modified instance of the same underlying concept providing an additional level of control. effort was equally placed on showcasing different properties inherent to a lightning strike, such as its iterative construction when descending from its origin. in the rendering section, along with recreating the bloom and glow effect seen in previous works, effort was put into recreating the strobing observed in capturing slow-motion footage of lightning bolts with special detail given to this. in addition, parameters were joined with a waypoint system to allow for a great degree of freedom when generating new bolts.",
      "a geração iterativa de geometria no contexto de computação gráfica é um tópico de estudo à já algum tempo apesar de usado em apenas contextos específicos, um ramo que benefícia de um tipo de aleatoriedade imprevisível. similarmente, a exploração de relâmpagos como um fenómeno em espaço virtual é uma faceta de idade comparável. apesar disto, o foco quando tratando relâmpagos tem caído marioritariamente nos seus efeitos após impacto. estudos têm sido conduzidos no âmbito de mitigar o dano causado por estes em fuselagem de aeronaves e analizar o impacto de trovoada em estruturas críticas. no entanto, existe uma falta de investigação sobre a emulação deste fenómeno barra as suas interações com o mundo. a maioria das implementações iterativas em video jogos são baseadas em árvores de dados. quando fazem parte do cenário, são marioritariamente meshes ou texturas 2d selecionas aleatoriamente de um conjunto. existem, no entanto, métodos baseados num modelo de colapso elétrico usando apróximações a uma equação de laplace. esta dissertação tem como foco apresentar uma alternativa para a geração aleatória e iterativa de relâmpagos baseada no algoritmo de space colonization. apesar deste algoritmo ter sido concebido para uso botânico, modelando o crescimento de estruturas biológicas, as similaridades entre os resultados obtidos pelo modelo de colapso elétrico e estes algoritmos de modelagem, quando considerados com a semelhança entre certos relâmpagos e árvores, constroem uma fundação sólida para o tópico. neste âmbito, este trabalho é um primeiro passo que tem o intuito de mostrar a capacidade do algoritmo de space colonization em simular relâmpagos. dito isto, uma grande porção do tempo de desenvolvimento dobrou-se sobre a iteração modificação e experimentação de ideias que foram discardadas ou adaptadas, um esforço primariamente dedicado em controlar o crescimento de ramos sem reduzir o número de atratores. o algoritmo original foi alterado, focando especialmente na criação de um único canal e fazendo uso de conhecimento prévio, oriundo de trabalho e investigação feita sobre manipulação de space colonization. em vez de padrões de venação, observados no trabalho original, o impedimento de qualquer crescimento significa que cada nodo tem uma probabilidade, quando criado, de dar origem a um ramo e que cada ramo é uma instância diferente e modificada do mesmo conceito, algo que cria um nível de controlo mais profundo. um esforço extra foi, também, realizado com o intuito de mostrar todas as propriedades diferentes, inerentes a um relâmpago tal como a construção iterativa durante a sua travessia. na parte de renderização, foram recriados efeitos de brilho e bloom vistos em trabalhos prévios. foi também dada especial atenção à recriação do efeito estroboscópico observado durante a análise de imagens em câmera lenta, algo que se tornou no foco principal desta parte. adicionalmente, a adição de parâmetros foi conjugada com um sistema de pontos que dá um grau superior de liberdade ao utilizador."
    ],
    0.0
  ],
  [
    [
      "a gestão do produto de software é um dos principais mecanismos a usar durante o desenvolvimento e a exploração de um produto de software. a forma como o produto é gerido durante o seu ciclo de vida, toma em conta várias áreas de negócio, que, infelizmente, nem sempre fazem parte da experiência do gestor de produto, devido também ao facto de ainda existir pouca formação para o exercício deste cargo. como forma de avaliar as práticas de gestão do produto de software na região de braga, é apresentada a matriz de maturidade para a gestão do produto de software, como instrumento de avaliação das práticas implementadas pelas empresas. uma análise individual e coletiva das empresas é realizada, com a finalidade de analisar a maturidade das empresas em cada uma das áreas de foco da matriz. para tal, foram realizadas 13 entrevistas junto de colaboradores de empresas do ramo de software, que permitiram recolher dados essenciais na construção das matrizes de maturidade de cada uma destas empresas. a partir dos resultados obtidos destas matrizes, uma análise mais geral foi realizada, identificando pontos comuns de falha, bem como de áreas de foco nas quais as empresas direcionam mais os seus esforços. foi possível concluir que apesar de existir uma grande diversidade de resultados dentro de cada uma das áreas de foco, bem como o facto de que por vezes o nível de maturidade atingido, não está diretamente relacionado com o número de competências implementadas em cada uma desta áreas. também se constatou que o uso de ferramentas e metodologias adotadas por estas empresas, facilitam a gestão do produto, o que se traduz no aumento do nível de maturidade para as empresas em determinadas áreas de negócio.",
      "software product management is one of the core mechanisms to use on the development and exploration of a software product. the way a product is managed during its lifecycle takes into account several business functions, which unfortunately not always are part of the product manager’s professional experience. this is due as well to the fact that there is little education and training for this role. as a way of evaluating software product management practices in the braga region, the software product management maturity matrix is presented, as an instrument to evaluate these practices implemented by the companies. an individual and collective analysis is performed, with the intent to analyse the maturity of each one of the company’s focus areas. to do so, thirteen interviews were conducted together with employees of the software companies, which allowed to collect essential data to construct their maturity matrixes. with the collected results from these matrixes, a broader analysis was performed, identifying common points of failure, as well as focus areas that companies direct their efforts towards. it was possible to conclude that although there is a big diversity of results within each focus area and they reach maturity most of the times, they’re not directly connected to the amount of implemented capabilities in these areas. it was also found that the use of software platforms as well as methodologies adopted by the companies, facilitate the product management, which resulted in a maturity level increase in certain business functions."
    ],
    [
      "de entre os diversos tipos de exames de endoscopia, a esofagogastroduodenoscopia assume um papel preponderante devido a ser o método ideal para examinar a mucosa do trato digestivo alto, bem como para detetar inúmeras patologias gastrenterológicas. o resultado deste tipo de exames é, geralmente, um relatório composto por um conjunto de frames capturados durante o exame, eventualmente acompanhado por um vídeo. hoje em dia, apenas as imagens juntamente com o relatório endoscópico, são arquivadas. o facto de o vídeo não ser arquivado pode conduzir a um incómodo no bem-estar do paciente, assim como a um acréscimo de custos e tempo despendido, pois frequentemente o mesmo é necessário para revisão e validação da hipótese de diagnóstico, bem como para comparação de segmentos do vídeo com exames futuros. mesmo nos casos em que a informação é arquivada, a falta de reutilização e partilha de informação e vídeos entre entidades contribui, mais uma vez, para uma repetição desnecessária de exames. a existência de um arquivo de vídeos endoscópicos seria uma mais-valia, pois além de resolver os problemas referidos ainda possibilitaria a sua utilização para fins de pesquisa e investigação, além de disponibilizar exames para servirem como referência para estudo de casos similares. neste trabalho é proposta uma solução abrangente para a aquisição, tratamento, arquivo e difusão de exames de endoscopia. o objetivo passa por disponibilizar um sistema capaz de gerir toda a informação clínica e administrativa (incluindo conteúdo audiovisual) desde o seu processo de aquisição até ao processo de pesquisa de exames antigos, para comparação com novos casos. de forma a garantir a compatibilidade lexical da informação partilhada no sistema, foi utilizado um vocabulário endoscópico estandardizado, o minimal standard terminology (mst). neste contexto foi planeado um dispositivo (mivbox) orientado à aquisição do vídeo endoscópico, independentemente da câmara endoscópica utilizada. toda a informação é armazenada de forma estruturada e normalizada, possibilitando a sua reutilização e difusão. para facilitar este processo de partilha, o vídeo sofre algumas etapas de processamento, de forma a ser obtido um vídeo reduzido e as respetivas características do conteúdo. deste modo, a solução proposta contempla um sistema de anotação que habilita a pesquisa por conteúdo, servindo assim como uma ferramenta versátil para a investigação nesta área. este sistema é ainda dotado de um módulo de streaming, no qual é transmitido, em tempo real, o exame endoscópico, disponibilizando um canal de comunicação com vídeo unidirecional e áudio bidirecional, permitindo que os profissionais ausentes da sala do exame deem a sua opinião remotamente.",
      "among the different kinds of endoscopic procedures, esophagogastroduodenoscopy plays a major role because it is the ideal method to examine the upper digestive tract, as well as to detect numerous gasteroentologic diseases. the result of such procedures is usually a written report that comprises a set of frames captured during the examination, sometimes complemented with a video. nowadays only the images are stored along with the endoscopic report. not storing the video may lead to discomfort concerning the patient’s well-being, as well as an increase of costs and time spent, because it is often necessary to review and validate the diagnostic hypothesis, and compare video segments in future exams. even in the cases in which the information is stored, the lack of reutilization and share of information and videos among entities contributes, once again, for an unnecessary repetition of exams. besides solving the problems mentioned above, the existence of an endoscopic video archive would be an asset because it would enable research and investigation activities. furthermore it would make available exams to serve as a reference for the study of similar cases. in this work, an extended solution of acquisition, processing, archiving and diffusion of endoscopic procedures is proposed. the aim is to provide a system capable of managing all the administrative and clinical information (including audiovisual content) from its acquisition process to the searching process of previous exams, for comparison with new cases. in order to ensure compatibility of lexical information shared in the system, a standardized endoscopic vocabulary, the minimal standard terminology (mst) was used. in this context, a device for the acquisition of the endoscopic video was designed (mivbox), regardless of the endoscopic camera that is used. all the information is stored in a structured and standardized way, allowing its reuse and sharing. to facilitate this sharing process, the video undergoes some processing steps in order to obtain a summarized video and the respective content characteristics. the proposed solution provides an annotation system that enables content querying, thus becoming a versatile tool for research in this area. this system is also provided with a streaming module in which the endoscopic video is transmitted in real time. this process uses a communication channel with one-way video and two-way audio, allowing professionals absent from the exam room to give their opinion remotely."
    ],
    0.0
  ],
  [
    [
      "transversal a qualquer indústria, a retenção de clientes é um aspeto de elevada importância e a que se deve dar toda a atenção possível. o abandono de um produto ou de um serviço por parte de um cliente, situação usualmente denominada por churn, é cada vez mais um indicador a ter em atenção por parte das empresas prestadoras de serviços. juntamente com técnicas de customer relationship management (crm), a previsão de churn, oferece às empresas uma forte vantagem competitiva, uma vez que lhes permite obter melhores resultados na fidelização dos seus clientes. com o constante crescimento e amadurecimento dos sistemas de informação, torna-se cada vez mais viável a utilização de técnicas de data mining, capazes de extrair padrões de comportamento que forneçam, entre outros, informação intrínseca nos dados, com sentido e viável no domínio do negócio em questão. o trabalho desta dissertação foca-se na utilização de técnicas de data mining para a previsão de situação de churn dos clientes no ramo das seguradoras, tendo como o objetivo principal a previsão de casos de churn e, assim, possibilitar informação suficiente para a tomada de ações que visem prever o abandono de clientes. nesse sentido, foi desenvolvido nesta dissertação um conjunto de modelos preditivos de churn, estes modelos foram implementados utilizando diferentes técnicas de data mining. com esta implementação de vários modelos, pretende-se realizar uma avaliação comparativa dos mesmos, de forma a analisar qual o mais eficaz na previsão de casos churn.",
      "transversal to any industry, customer retention is a highly important aspect and that we should give all possible attention. the abandonment of a product or a service by a customer, a situation usually referred to as churn, is an indicator that the service provider company should take in attention. along with techniques of customer relationship management (crm), the churn prediction offers to companies a strong competitive advantage since it allows them to get better results in customer retention. with the constant growth and maturity of information systems, it becomes more feasible to use data mining techniques, which can extract behavior patterns that provide intrinsic information hided in the data. this dissertation focuses on using data mining techniques for predicting customer churn situations in insurance companies, having as main objective the prediction of cases of churn and thereby allow information gathering that can be used to take actions to avoid the customer desertion. in this dissertation we develop a set of predictive churn models using different data mining techniques. we studied the following techniques: decision trees, neural networks, logistic regression and svm. the implementation of various models using this set of techniques allowed us to conclude that the most suitable techniques to predict churn in an insurance company are decision trees and logistic regression, in addiction we did a study about the most relevant churn indicators."
    ],
    [
      "with the rise of data, the creation of algorithms capable of using that data is an evolution that appears naturally. taking advantage of those algorithms, impressive advances have been made in the ability for a computer to recognize objects. nevertheless, even after all those advances, further ones can still be achieved. with the reduction of infrared cameras prices and at the same time the increase in the picture quality of those same cameras, they are becoming reliable solutions for commercial applications. these images provide an all new kind of information that is not available with the use of only the traditional visible light images. as such, in this work, it is tested if the additional usage of infrared images, in complement with the visible image, has any kind of influence in the results for object detection for different levels of illumination, in the interior of a vehicle. in order to test this influence, several tests are done in equivalent conditions and the results between using infrared images and visible light images compared. in addition to that, there were also experiments done in the usage of both types of images at the same time as a way to improve detection. it was also documented the influence of some more traditional modifications over the images of the training set, such as data augmentation and changes in the number of classes. to keep the results of the experiments as comparable as possible, a training methodology was planned and used in all of the training processes of the algorithms.",
      "com o aumento da quantidade de dados, a criação de algoritmos capazes de os utilizar é uma evolução que surge com naturalidade tirando partido desses algoritmos, foram feitos avanços impressionantes na capacidade de reconhecimento de objetos por parte dos computadores. mesmo com todos esses avanços ainda existe grande capacidade para melhorar. tendo os preços das câmaras de infravermelhos diminuído ao mesmo tempo que a qualidade de imagem dessas mesmas câmaras aumentado, elas têm atraído atenção como soluções comercialmente válidas. estas imagens proporcionam todo um novo tipo de informação que não é disponibilizada através de a utilização individual de imagens visíveis tradicionais. como tal vai ser testado se a utilização adicional das imagens de infravermelhos têm influencia nos resultados apresentados na deteção de objetos para diferentes níveis de iluminação, no interior de um automóvel. de modo a testar esta influência são realizados vários testes em condições equivalentes e comparados os resultados obtidos entre a utilização de imagens infravermelhos e a utilização de imagens visíveis. para além disso são também feitas experiências na utilização de ambos os tipos de imagens ao mesmo tempo como forma de melhorar a deteção. para além das experiências em ambos os espetros foram documentados também a influência de algumas modificações mais tradicionais sobre as imagens de treino, tais como o aumento de dados e alteração de número de classes. para manter os resultados das experiências o mais fidedignos possível de modo a comparar os resultados entre elas uma metodologia de treino foi utilizada em todos os treinos de algoritmos."
    ],
    0.0
  ],
  [
    [
      "the recent advances in different analytical techniques able to produce spectral data, including raman, infrared (ir) or ultraviolet-visible (uv-vis) spectroscopies, have provided novel approaches for many research issues in the biological and chemical fields. indeed, they have allowed to address tasks in functional genomics, sample characterization and classification, or drug discovery. to take full advantage of these data, advanced bioinformatics methods are required for data analysis and mining. a number of methods and tools for spectral data analysis have been put forward recently, being one of the major limitations still faced the lack of integrated frameworks for extracting relevant knowledge from these data and being able to integrate these data with previous biochemical knowledge. also, the lack of reproducibility in many data analysis or data mining processes is a strong obstacle for biological discovery, being common the lack of data and data analysis pipelines in the published work. in recent work from the host group, specmine, a metabolomics and spectral data analysis/ mining framework, in the form of a package for the r system, has been developed to address some of these issues. in this thesis, the main aim was to design and develop an integrated web-based platform for spectral data analysis and mining, based on the specmine package, providing an easier and more user friendly interface, but also addressing some of the package’s current limitations. the developed platform contains features that cover the main steps of the metabolomics data analysis workflow, with modules for data reading and dataset creation, data preprocessing and a variety of analysis types. it includes an authentication system, allowing the user to have his own personal workspace where projects can be stored and accessed later, with the option to share projects with other users. the different modules were validated using real data from previously published studies in the host group, related to the analysis of the characteristics and potential of natural products, addressing as well the exploration and integration of data from distinct experimental techniques, attesting the platform’s robustness and utility.",
      "recentes avanços nas diferentes técnicas analíticas capazes de produzir dados espectrais, incluindo as espectroscopias de raman, infravermelho e ultravioleta-visível, têm contribuído com novas abordagens em vários problemas nos campos da biologia e química. de facto, tais avanços permitiram abordar tarefas em genómica funcional, caraterização e classificação de amostras, ou na descoberta de fármacos. de modo a obter o máximo de informação a partir deste tipo de dados, são necessários métodos avançados de bioinformática para a análise e extração de conhecimento dos dados. recentemente, vários métodos e ferramentas para análise de dados espectrais têm surgido, sendo que uma das maiores limitações enfrentadas é a falta de estruturas integradas que permitam a extração de conhecimento relevante a partir deste tipo de dados, integrando-os com conhecimento bioquímico prévio. a falta de reprodutibilidade em muitos processos de análise e extração de conhecimento a partir de dados é também um forte obstáculo na descoberta biológica, sendo comum a falta de pipelines de análise nos trabalhos atualmente publicados. num trabalho recente do grupo anfitrião foi desenvolvido o specmine, uma ferramenta para análise e extração de conhecimento de dados espectrais, sob a forma de uma biblioteca para o sistema r, de modo a abordar os problemas mencionados. no presente trabalho, o principal objetivo consistiu na projeção e desenvolvimento de uma plataforma baseada em web para análise e extração de conhecimento a partir de dados espectrais, baseada no specmine, fornecendo assim uma interface agradável e de fácil utilização para o utilizador, abordando também algumas das atuais limitações desta ferramenta. a plataforma desenvolvida contém funcionalidades que cobrem as principais etapas numa análise de dados de metabolómica, com módulos para leitura de dados e criação de datasets, pré-processamento de dados e uma variedade de tipos de análise. inclui ainda um sistema de autenticação que permite ao utilizador ter o seu espaço pessoal onde projetos podem ser armazenados e acedidos posteriormente, com a opção de partilha destes projetos com outros utilizadores. os diferentes módulos foram validados utilizando dados reais de estudos previamente publicados no grupo anfitrião, relacionados com a análise das características e potencial de produtos naturais, abordando também a exploração e integração de dados de distintas técnicas experimentais, atestando assim a robustez e utilidade da plataforma desenvolvida."
    ],
    [
      "contrarily to most conventional programming languages where certain symbols are used so as to create non-ambiguous grammars, most recent programming languages allow ambiguity. this results in the necessity for a generic parser that can deal with this ambiguity without loss of performance. currently, there is a glr parser generator written in haskell, integrated in the biyacc system, developed by departamento de informática (di), universidade do minho (um), portugal in collaboration with the national institute of informatics, japan. in this thesis, this necessity for a generic parser is attacked by developing disambiguation filters for this system which improve its performance, as well as by implementing various known optimizations to this parser generator. finally, performance tests are used to measure the results of the developed work.",
      "contrariamente às linguagens de programação mais convencionais em que certos símbolos eram utilizados por forma a criar gramáticas não ambíguas, as linguagens mais recentes permitem ambiguidade, que por sua vez cria a necessidade de um parser genérico que consiga lidar com esta ambiguidade sem grandes perdas de performance. atualmente, existe um gerador de parsers glr em haskell integrado no sistema biyacc, desenvolvido pelo di, um, portugal, em colaboração com o national institute of informatics, japão. nesta tese, são desenvolvidos filtros de desambiguidade para este sistema que aumentam a sua performance, assim como são feitas otimizações a vários níveis e se implementa um gerador de parsers usando um algoritmo gll, que poderá trazer várias vantagens a nível de performance comparativamente com o algoritmo glr atualmente implementado. finalmente, são feitos testes de performance para avaliar os resultados do trabalho desenvolvido."
    ],
    0.06666666666666667
  ],
  [
    [
      "one of the most accurate personality assessments available is the goldberg’s ’the big five personality test’, which measures the five ocean dimensions: openness, conscientiousness, extraversion, agreeableness and neuroticism. this assessment is performed by presenting a total of forty adjectives requesting the subject to rate each word using a scale of 1 to 9 indicating whether it accurately (9) describes herself or not (1). nonetheless, scientific research has shown that this test may, accurately, suggest personality traits such as aggressive reactions, work performance, fitness on specific expertise areas and also mental illnesses. however, one big disadvantage of this test, it simply takes too much time to perform, which can result on undesirable measurements. indeed, several developments have been done in order to reduce the required effort to perform this test, an example is the mini marker test by saucier. this study aims to propose a viable shorter alternative to this by applying machine learning techniques, i.e., although measurement precision may be reduced, is it possible to build a much shorter version losing as little precision as possible by just requiring the subject to select the adjectives that characterise him the most? for this study, it was developed a platform to collect data, requesting both the subject to rate each adjective but also to select those he most identifies with. with this, the available data contains both ratings and the selections of the words that most characterise the subject. three different machine learning architectures are developed and tested. both regression and classification approaches are considered. the main input for these architectures are the words selected by each evaluated subject. data collected by this work showed to be insufficient, requiring the use of data augmentation techniques. for this, different versions are proposed, one including the use of frequent itemset mining techniques. the proposed machine learning architectures shown a very high precision, with an rmse of around 7%. the results show the proposed solutions to be able to perform a shorter version of this test with a minimum precision loss. it was also possible to define a list of common sets of selected words. further research can be performed mainly on two different streamlines, i.e., strength the data collection process and develop an even shorter version of this test.",
      "uma das avaliações de personalidade mais precisas foi criada por goldberg, chamada 'the big five personality test', que mede um total de cinco dimensões denominadas de ocean: openness, conscientiousness, extraversion, agreeableness and neuroticism. a avaliaçao em causa é realizada apresentando um total de quarenta adjetivos a um individuo solici-tando lhe que classifique cada uma das palavras usando uma escala de a a 9, indicando se esta o descreve de forma exata (9) ou não (1). assim sendo, estudos científicos sugerem que este teste poderá, de forma precisa, indicar outros traços da personalidade, tais como reações agressivas, desempenho no trabalho, aptidão para áreas de especialidade e doenças mentais. no entanto, uma grande desvantagem deste teste, é que este pode ser demasiado extenso e demorado, podendo gerar resultados indesejados. na verdade, múltiplos desenvolvimentos foram feitos de modo a reduzir o esforço necessário para a realização do mesmo. este estudo pretende assim propor uma alternativa mais curta e viável aplicando técnicas de machine learning, isto é, apesar da precisão dos resultados poder ser degradada, é possível construir uma versão muito mais curta com o mínimo possível de degradação da qualidade dos resultados apenas solicitando ao sujeito que este selecione os adjetivos que melhor o caracterizam? para este estudo, foi desenvolvida uma plataforma para recolha de dados, solicitando ao individuo tanto para classificar cada adjetivo, usando a escala, como também para selecionar aqueles com que este mais se identifica. assim, os dados disponíveis contém tanto as escalas como a seleção das palavras que mais caracterizam cada um dos sujeitos. três diferentes arquiteturas de machine learning são desenvolvidas e testadas. tanto abordagens de regressão como classificação são consideradas. o principal input para estas arquiteturas é a seleção de cada uma das palavras por parte dos sujeitos avaliados. os dados recolhidos durante este estudo demonstraram ser insuficientes, exigindo o uso de técnicas de data augmentation. nesse sentido, diferentes versões são propostas, sendo que uma delas incluí o uso de técnicas de frequent itemset mining. as arquiteturas de machine learning propostas apresentaram uma precisão bastante elevada nos resultados, com um rmse de cerca de 7%. os resultados obtidos mostram que as soluções propostas são capazes de gerar uma versão reduzida do teste em causa com uma degradação mínima dos resultados. foi também possível definir uma lista de conjuntos frequentes de palavras selecionadas. desenvolvimentos futuros podem ser feitos em duas direções distintas, isto é, melhorar o processo de recolha de dados ou desenvolver uma versão ainda mais reduzida deste teste."
    ],
    [
      "recent endeavours over the past few years have been applying generative deep learning (dl) models to generate novel proteins using an array of different approaches. such initiatives represent a specially important development towards major contributions to the field of protein engineering. to contribute to this, various dl architectures can be applied to the different datasets to generate proteins with a particular set of properties. the field of dl applied to the generation of novel molecules has been presenting results that encourage further research on this subject. an increasing number of novel, computationally generated, molecules being synthesized with successful results creates grounds for stimulation of new endeavours and diversification of the current applications. the goal of the work presented in this dissertation is to apply different generative dl architectures to the design of novel protein sequences for a targeted set of optimized properties. the developed framework, termed genprotea, stands as the main contribution of this work. the framework envisages the implementation of generative dl architectures for the design of novel proteins and leverages the use sampling techniques and evolutionary computation to steer the generative process towards a specific set of properties. evolutionary algorithms (eas) can be applied both to single and multi-objective optimization problems which in itself presents an added advantage. the optimization problems were designed considering the literature concerning protein design. the problems ranged from a simple maximization of the average hydrophobicity of the protein sequence to more complex problems such as minimizing two sets of events in a sequence or maximizing a probability of a protein being generated by a defined profile hidden markov model (hmm). the results of the proposed case studies and the respective analysis accompany the framework in this endeavour. two different generative dl architectures were deployed, trained, and evaluated, using loss and accuracy metrics to perform the analysis.: a generative adversarial network (gan) and a variational autoencoder (vae). for the gan architecture, new proteins are sampled varying the latent seed used in the generative process and then selecting the best candidates for each of the case studies. besides following a same sampling approach to obtain new protein designs, the vae latent space is explored using eas. the results of this work show that the use of eas in the optimization, steering the generative process, can produce the best results, allowing for more variability in the experiments designed and resulting in a much greater set of possibly functional novel proteins.",
      "ao longo dos últimos anos têm sido desenvolvidas várias iniciativas para aplicar modelos generativos de dl para gerar novas proteínas, usando uma variedade de abordagens. estas iniciativas representam um desenvolvimento bastante importante especialmente no campo da engenharia proteica. para formular essa contribuição, vários modelos de dl podem ser aplicados, usando diferentes conjuntos de dados com o objetivo de gerar proteínas com um determinado conjunto de propriedades. a vertente de aplicação de modelos generativos de dl a geração de novas moléculas tem apresentado resultados que incentivem ao aprofundamento de trabalhos de investigação relacionados com este tópico. o número crescente de novas moléculas geradas computacionalmente bem como a subsequente bem-sucedida sintetização, estimulam uma diversificação das abordagens atuais. o objetivo do trabalho apresentado nesta dissertação consiste em aplicar diferentes modelos generativos de dl para a geração de novas proteínas com um conjunto optimizado de propriedades específicas. o framework desenvolvido, denominado de genprotea, é apresentado como a principal contribuição deste trabalho. esta framework tenciona acomodar a implementação de modelos generativos de dl para a formação de novas proteínas, beneficiando do uso de computação evolutiva para guiar o processo de generativo de acordo com o conjunto específico de propriedades desejado. os algoritmos evolucionários são aplicados em problemas de otimização com um só ou vários objetivos, que por si só representa uma vantagem adicional. os problemas de otimização foram estabelecidos de acordo com apresentado na literatura referente ao design de proteínas. esses problemas variaram de uma simples maximização da hidrofobicidade média da sequência proteica para problemas mais complexos, como minimizar a ocorrência de dois conjuntos de eventos numa sequência ou maximizar a probabilidade de uma proteína ser gerada por um perfil de hmm. 0s resultados obtidos nos casos de estudo propostos, bem como as respetivas análises, acompanham a framework desenvolvida neste trabalho. foram implementados, treinados e avaliados dois modelos generativos diferentes, usando métricas de perda e precisão na avaliação: uma gan e uma vae. para o modelo referente à gan, novas proteínas são amostradas variando a iatent seed no processo generativo selecionando as melhores amostras para o conjunto de casos de estudo implementados. além de aplicar esta mesma abordagem com a vae, os espaço latente da vae foi explorado usando eas. os resultados deste trabalho mostram que o uso de eas na otimização, guiando o processo generativo para um objetivo específico, pode produzir melhores resultados e permite uma major variabilidade de casos de estudo para possível avaliação. este método apresenta ainda um conjunto muito maior de proteínas possivelmente funcionais."
    ],
    0.3
  ],
  [
    [
      "a sequenciação de próxima geração veio permitir a sequenciação em paralelo de milhões de pares de bases de dna / rna, tendo tido desde o início um grande impacto, ao ponto de se tornar o método escolhido em projetos de grande escala, em detrimento do método de sanger. entre as principais aplicações desta tecnologia encontram-se a análise em larga escala da metilação de dna, o chip-seq para análise da interação entre proteínas e dna ou rna, e o mapeamento de rearranjos estruturais. destacam-se, especialmente, a sequenciação de novos organismos ou indivíduos, o estudo de polimorfismos de nucleótido único (dna-seq) e a análise de expressão genética (rna-seq). neste trabalho, foi desenvolvido um sistema onde foram integradas ferramentas necessárias para estudos de dna-seq e rna-seq. inicialmente, foi efetuado um estudo das aplicações existentes, tendo de seguida sido selecionadas as que se destacaram em parâmetros como a facilidade de utilização, documentação e possibilidade de integração com as restantes ferramentas do sistema. o sistema foi desenvolvido utilizando-se as linguagens de programação ruby, java e r, sendo as principais funcionalidades o estudo de polimorfismos, a assemblagem de novo e a análise de expressão genética a partir de dados de rna-seq. este permite uma utilização simplificada e semiautomática dos vários programas, sendo acessível a utilizadores com poucos conhecimentos informáticos. o sistema foi testado em três casos de estudo: caracterização de duas estirpes de mycobacterium tuberculosis, assemblagem de novo da pseudomonas str. m1 e o estudo da expressão genética em amostras de saccharomyces cerevisiae.",
      "next-generation sequencing has enabled the sequencing of millions of base pairs of dna and rna, in parallel. this technology had, from the beginning a great impact to the point of becoming the method of choice for large-scale projects, replacing the sanger method. among the many applications of this technology we can include the analysis of dna methylation, the analysis of the interaction between proteins (chip-seq) and dna or rna, and the mapping of structural rearrangements. however, the sequencing of new organisms or individuals, the study of single nucleotide polymorphisms (dna-seq) and gene expression analysis (rna-seq) are the main fields of study with this technology. in this work, a system integrating tools to study dna-seq and rna-seq data has been developed, starting by studying existing applications. then, taking into account parameters such as ease of use, documentation and possibility of integration with other system tools, an optimal set of tools has been selected. the system was developed using the ruby, java and r programming languages, and its main features are the study of polymorphisms, de novo genomes assemblies and gene expression analysis. the developed system allows a simplified and semiautomatic use of the implemented tools making them accessible to users with limited computer knowledge. the system was tested on three case studies: characterization of two strains of mycobacterium tuberculosis, de novo assembly of pseudomonas str. m1 and a study of gene expression in saccharomyces cerevisiae samples."
    ],
    [
      "o protocolo de máquinas de estado replicadas (mer) é uma peça fundamental dos sistemas distribuídos. no centro deste protocolo estão os algoritmos de consenso, como o paxos, usados para manter a consistência das mer. todavia, os sistemas modernos não podem depender estritamente das técnicas de mer, estes devem também implementar estratégias de reconfiguração. estas estratégias consistem em alterar a configuração do sistema, adicionando, removendo ou substituindo os processos que o compõem. dada a sua complexidade, a implementação de protocolos de reconfiguração é muito suscetível a erros, daí que seja aconselhável a especificação, validação e verificação dos mesmos. no presente trabalho apresentamos uma especificação em linguagem alloy do protocolo de reconfiguração vertical paxos e do protocolo de consenso paxos. além destes, modelamos o protocolo multi-paxos, o qual implementa uma mer. estes protocolos estão intrinsecamente relacionados e a compreensão do primeiro é facilitada com o conhecimento dos demais. atualmente, o alloy é uma das linguagens de especificação mais populares, mas pouco explorada na modelação de algoritmos distribuídos e, tanto quanto sabemos, não existe ainda nenhuma especificação dos referidos protocolos em alloy. o presente trabalho visa modelar e validar os referidos protocolos, bem como verificar as suas propriedades de safety, de modo a obtermos confiança nas especificações. ademais, realizamos uma avaliação de desempenho de diferentes solvers e estratégias de decomposição nativas do alloy, bem como uma breve análise comparativa com o tla+.",
      "state machine replication (smr) protocols have a crucial role in distributed systems. at the heart of these protocols are the consensus algorithm, such as paxos, responsible for smr’s consistency. however, modern systems cannot only rely on smr thecniques, they must implement reconfiguration strategies, which consist in changing their configurations by adding, removing or replacing their processes. due to its complexity, implementing a reconfiguration algorithm is error-prone, therefore its specification, validation and verification is advisable. in this work, we present a specification, in alloy, of the reconfiguration protocol vertical paxos and the consensus protocol paxos. besides, we model the multi-paxos protocol which implements a smr. these three protocols are intrinsically related and, once we are familiar with paxos and multi-paxos, understanding vertical paxos becomes straigthfoward. nowadays, alloy is one of the most popular specification languages, but littleexplored in modeling and analyzing distributed algorithms. as far as we know, there is still no specification of these protocols in alloy. the aim of this work is to model and validate these protocols, as well as to verify their safety properties, in order to obtain confidence in our specifications. furthermore, we evaluate the performance of different solvers and decomposition strategies. finally, we carry out a brief comparative analysis with tla+."
    ],
    0.0
  ],
  [
    [
      "this dissertation aims to evaluate and improve the performance of deep learning (dl) algorithms to autonomously drive a vehicle, using a remo car (an rc vehicle) as testbed. the rc vehicle was built with a 1:10 scaled remote controlled car and fitted with an embedded system and a video camera to capture and process real-time image data. two different embedded systems were comparatively evaluated: an homogeneous system, a raspberry pi 4, and an heterogeneous system, a nvidia jetson nano. the raspberry pi 4 with an advanced 4-core arm device supports multiprocessing, while the jetson nano, also with a 4-core arm device, has an integrated accelerator, a 128 cuda-core nvidia gpu. the captured video is processed with convolutional neural networks (cnns), which interpret image data of the vehicle’s surroundings and predict critical data, such as lane view and steering angle, to provide mechanisms to drive on its own, following a predefined path. to improve the driving performance of the rc vehicle, this work analysed the programmed dl algorithms, namely different computer vision approaches for object detection and image classification, aiming to explore dl techniques and improve their performance at the inference phase. the work also analysed the computational efficiency of the control software, while running intense and complex deep learning tasks in the embedded devices, and fully explored the advanced characteristics and instructions provided by the two embedded systems in the vehicle. different machine learning (ml) libraries and frameworks were analysed and evaluated: tensorflow, tensorflow lite, arm nn, pyarmnn and tensorrt. they play a key role to deploy the relevant algorithms and to fully engage the hardware capabilities. the original algorithm was successfully optimized and both embedded systems could perfectly handle this workload. to understand the computational limits of both devices, an additional and heavy dl algorithm was developed that aimed to detect traffic signs. the homogeneous system, the raspberry pi 4, could not deliver feasible low-latency values, hence the detection of traffic signs was not possible in real-time. however, a great performance improvement was achieved using the heterogeneous system, jetson nano, enabling their cuda-cores to process the additional workload.",
      "esta dissertação tem como objetivo avaliar e melhorar o desempenho de algoritmos de deep learning (dl) orientados à condução autónoma de veículos, usando um carro controlado remotamente como ambiente de teste. o carro foi construído usando um modelo de um veículo de controlo remoto de escala 1:10, onde foi colocado um sistema embebido e uma câmera de vídeo para capturar e processar imagem em tempo real. dois sistemas embebidos foram comparativamente avaliados: um sistema homogéneo, um raspberry pi 4, e um sistema heterogéneo, uma nvidia jetson nano. o raspberry pi 4 possui um processador arm com 4 núcleos, suportando multiprocessamento. a jetson nano, também com um processador arm de 4 núcleos, possui uma unidade adicional de processamento com 128 núcleos do tipo cuda-core. o vídeo capturado e processado usando redes neuronais convolucionais (cnn), interpretando o meio envolvente do veículo e prevendo dados cruciais, como a visibilidade da linha da estrada e o angulo de direção, de forma a que o veículo consiga conduzir de forma autónoma num determinado ambiente. de forma a melhorar o desempenho da condução autónoma do veículo, diferentes algoritmos de deep learning foram analisados, nomeadamente diferentes abordagens de visão por computador para detecção e classificação de imagens, com o objetivo de explorar técnicas de cnn e melhorar o seu desempenho na fase de inferência. a dissertação também analisou a eficiência computacional do software usado para a execução de tarefas de aprendizagem profunda intensas e complexas nos dispositivos embebidos, e explorou completamente as características avançadas e as instruções fornecidas pelos dois sistemas embebidos no veículo. diferentes bibliotecas e frameworks de machine learning foram analisadas e avaliadas: tensorflow, tensorflow lite, arm nn, pyarmnn e tensorrt. estes desempenham um papel fulcral no provisionamento dos algoritmos de deep learning para tirar máximo partido das capacidades do hardware usado. o algoritmo original foi otimizado com sucesso e ambos os sistemas embebidos conseguiram executar os algoritmos com pouco esforço. assim, para entender os limites computacionais de ambos os dispositivos, um algoritmo adicional mais complexo de deep learning foi desenvolvido com o objetivo de detectar sinais de transito. o sistema homogéneo, o raspberry pi 4, não conseguiu entregar valores viáveis de baixa latência, portanto, a detecção de sinais de trânsito não foi possível em tempo real, usando este sistema. no entanto, foi alcançada uma grande melhoria de desempenho usando o sistema heterogeneo, jetson nano, que usaram os seus núcleos cuda adicionais para processar a carga computacional mais intensa."
    ],
    [
      "the combination of operational technology of industrial networks with information technologies has en abled the increase of attacks on industrial networks, causing professionals and researchers linked to the security area to study tools, mechanisms and techniques capable of detecting and blocking malicious activities in industrial network environments. this dissertation sets out to comprehensively explore and analyze anomaly-based intrusion detection sys tems (ids) as a central focal point. the primary ambition is to meticulously investigate the efficacy of such systems in identifying, monitoring, and recording abnormal behaviors, detecting malicious activities, and pinpointing potential attacks that exploit remote services and orchestrate denial of service incidents. through an in-depth examination and critical evaluation, this study aims to contribute to the existing body of knowledge in the realm of cybersecurity, advancing our understanding of ids capabilities and their sig nificance in safeguarding digital and industrial environments against emerging threats. in this way, it is intended to create a model relating the attack techniques, their indicators, and the fields, logs, and events generated by the ids, in order to help detect such attacks, in a way, to evaluate the efficiency of the ids in detecting malicious activities.",
      "a combinação da tecnologia operacional de redes industriais com as tecnologias da informação tem possi bilitado o aumento de ataques a redes industriais, fazendo com que profissionais e pesquisadores ligados à área de segurança estudem ferramentas, mecanismos e técnicas capazes de detectar e bloquear ativi dades maliciosas em ambientes de redes industriais. esta dissertação se propõe a explorar e analisar os sistemas de detecção de intrusão (ids) baseados em anomalias como um ponto focal central. a principal ambição é investigar meticulosamente a eficácia de tais sistemas na identificação, monitoramento e registro de comportamentos anormais, detecção de atividades maliciosas e identificação de possíveis ataques que exploram serviços remotos e orquestram incidentes de negação de serviço. através de um exame aprofundado e avaliação crítica, este estudo visa contribuir para o corpo de conhecimento existente no domínio da segurança cibernética, avançando nossa compreensão dos recursos de ids e sua importância na proteção de ambientes digitais e industriais contra ameaças emergentes. desta forma, pretende-se criar um modelo relacionando as técnicas de ataque, seus indicadores e os campos, logs e eventos gerados pelo ids, a fim de detectar tais ataques, de forma a avaliar a eficiência do ids na detecção de atividades maliciosas."
    ],
    0.3
  ],
  [
    [
      "neurodegenerative diseases impair the functioning of the brain and are characterized by alterations in the morphology of specific brain regions. some of the main disorders include alzheimer's, parkinson's, and huntington's diseases, and the number of cases increases exponentially since ageing is one of the main risk factors. trying to identify the areas in which this type of disease appears is something that can have a very positive impact in this area of medicine and can guarantee a more appropriate treatment or allow the improvement of the quality of life of patients. with the current technological advances, computer tools are capable of performing a structural or functional analysis of neuroimaging data from magnetic resonance images(mri). therefore, medical informatics uses these techniques to create and manage medical neuroimaging data to improve the diagnosis and management of these patients. mri is the image type used in the analysis of the brain area and points to a promising and reliable diagnostic tool since it allows high-quality images in various planes or strategies and mri methods are fundamental diagnostic tools in clinical practice, allowing the diagnosis of pathologic processes such as stroke or brain tumours. however, structural mri has limitations for the diagnosis of neurodegenerative disorders since it mainly identifies atrophy of brain regions. currently, there is increased interest in informatics applications capable of monitoring and quantifying human brain imaging alterations, with potential for neurodegenerative disorders diagnosis and monitoring. one of these applications is radiomics, which corresponds to a methodolog ythat allows the extraction of features from images of a given region of the brain. specific quantitative metrics from mri are acquired by this tool, and they correspond to a set of features, including texture, shape, among others. to standardize radiomics application, specific libraries have been proposed to be used by the bioinformatics and biomedical communities, such as pyradiomics, which corresponds to an open source python package for extracting radiomics of mris. therefore, this dissertation was developed based on magnetic resonance images and the study of deep learning (dl) techniques to assist researchers and neuroradiologists in the diagnosis and prediction of neurodegenerative disease development. two different main tasks were made: first, a segmentation, using freesurfer, of different regions of the brain and then, a model was build from radiomic features extracted from each part of the brain and interpreted for knowledge extraction.",
      "as doenças neurodegenerativas estão associadas ao funcionamento do cérebro e caracterizam-se pelo facto de serem altamente incapacitantes. são exemplos destas, as doenças de alzheimer, parkinson e huntington, e o seu número de casos tem vindo a aumentar exponencialmente, uma vez que o envelhecimento é um dos principais factores de risco. tentar identificar quais são as regiões cerebrais que permitem predizer o seu aparecimento e desenvolvimento é algo que, sendo possível, terá um impacto muito positivo nesta área da medicina e poderá garantir um tratamento mais adequado, ou simplesmente melhorar a qualidade de vida dos pacientes. com os avanços tecnológicos atuais, foram desenvolvidas ferramentas informáticas que são capazes de efetuar uma análise estrutural ou funcional de ressonâncias magnéticas (mri), sendo essas ferramentas usadas para promover a melhoria e o conhecimento clínico. deste modo, as constantes evoluções científicas têm realçado o papel da informática médica na neuroimagem para criar e gerenciar dados médicos, melhorando o diagnóstico destes pacientes. a mri é o tipo de imagem utilizada na análise de regiões cerebrais e aponta para uma ferramenta de diagnóstico promissora e fiável, uma vez que permite obter imagens de alta qualidade em vários planos, permitindo assim, o diagnóstico de processos patológicos, tais como acidentes vasculares ou tumores cerebrais. atualmente, existem inúmeras aplicações informáticas capazes de efetuar análises estruturais e funcionais do cérebro humano, pois é este o principal órgão afetado pelas doenças neurodegenerativas. uma dessas aplicações é o radiomics, que permite fazer a extração de features de imagens do cérebro. a biblioteca a utilizar será pyradiomics, que corresponde a um package open source em python para a extração de features radiomics de imagens médicas. as features correspondem a características da imagem. assim sendo, a presente dissertação foi desenvolvida com base em imagens de ressonância magnética e no estudo das técnicas de deep learning para investigar e auxiliar os médicos neurorradiologistas a diagnosticar e a prever o desenvolvimento de doenças neurodegenerativas. foram feitas duas principais tarefas: primeiro, uma segmentação, utilizando o software freesurfer, de diferentes regiões do cérebro e, de seguida, foi construído um modelo a partir das features radiómicas extraídas de cada parte do cérebro que foi interpretado."
    ],
    [
      "the evolution of software products that interact with the physical world has led to a greater need to simulate their behavior in order to verify their effectiveness and safety in different scenarios. this dissertation project aims to enhance a simulation tool for hybrid programs called lince, more specifically to provide more powerful simulation capabilities to hybrid programs regulated by newtonian mechanics. these include the addition of new language constructs (such as the division operator and the trigonometric functions), the implementation of non-linear expressions, grammar relaxation and organization, improved error detection, and the mitigation of existing tool-related issues. throughout this dissertation, it is discussed how the implementation of these improvements benefits the simulation of hybrid programs and are explained the key methods adopted for their conception. finally, this new version of lince is put to the test by handling case studies related to autonomous driving (for example, adaptive cruise control and a missile targeting a moving object) and other types of systems as well, such as purely physical systems and the so-called on-off systems. the results obtained in the treatment of these case studies attest to the enhanced capabilities of this tool and the contribution of this dissertation to the scientific community, demonstrating its relevance in simulating integrated systems in everyday life.",
      "a evolução de produtos de software que interagem com o mundo físico levou a uma maior necessidade de simular o seu comportamento como forma de verificar a sua eficácia e segurança em diferentes cenários. este projeto de dissertação tem como objetivo melhorar uma ferramenta de simulação para programas híbridos denominada lince, mais especificamente para fornecer capacidades de simulação mais poderosas a programas híbridos regulados pela mecânica newtoniana. estas incluem a adição de novas construções linguísticas (como o operador de divisão e as funções trignométricas), a implementação de expressões não-lineares, relaxamento e organização da gramática, melhoria da deteção de erros e a mitigação de problemas existentes associados à ferramenta. ao longo desta dissertação, é discutida a forma como a implementação dessas melhorias benefi cia a simulação de programas híbridos e são explicados os principais métodos adotados para a sua conceção. finalmente, esta nova versão do lince é testada através do tratamento de casos de estudo relacionados com a condução autónoma (por exemplo, o adaptative cruise control e um míssil que visa um objeto em movimento) e também outros tipos de sistemas, como sistemas puramente físicos e os chamados sistemas on-off. os resultados obtidos no tratamento destes casos de estudo aferem para as capacidades melhoradas desta ferramenta e o contributo desta dissertação para a comunidade científica, demonstrando a sua relevância na simulação de sistemas integrados no quotidiano."
    ],
    0.02727272727272727
  ],
  [
    [
      "nowadays, the most widely used yeast in wine, beer, and bread fermentations is saccharomyces cerevisiae. however, in the past years, torulaspora delbrueckii attracted interest due to its properties, from flavor and aroma-enhanced wine to the ability to be preserved longer in frozen dough. the main objective of this thesis was to explore t. delbrueckii genomes publicly available and the ones belonging to our project’s collection, exploring their genomic information and establishing its relationship with their origins and biotechnological applications. in the first phase, publicly available genomes of t. delbrueckii were explored, and their annotation was improved. eggnog-mapper was used to perform functional annotation of the deduced t. delbrueckii coding genes, offering insights into its biological significance, and revealing 24 clusters of orthologous groups (cog), gathered in three main functional categories: information storage and processing (28% of the proteins), cellular processing and signaling (27%) and metabolism (23%). small intra-species variability was found when considering functional annotation of the four t. delbrueckii available genomes. a comparative study was also conducted between t. delbrueckii genome and those from 386 fungal species, revealing high homology with species of zygotorulaspora and zygosaccharomyces genera, but also with lachancea and s. cerevisiae. lastly, the phylogenetic placement of t. delbrueckii was assessed using the core homologues found across 204 common protein sequences of 386 fungal species and strains. in a second phase, the genome of fifty-four t. delbrueckii strains were sequenced and data was explored. the alignment, snp statistics, annotation, among other steps, were attempted, for the first time, for those strains. pca analysis was performed with those strains and the ones publicly available, to better understand the connection between the strains’ technological groups. the present work represents a successful effort to increase and improve the annotation of t. delbrueckii’s genome. overall, this work provides a starting point to unravel the diversity of potential biotechnological applications of t. delbrueckii.",
      "hoje em dia, a levedura mais utilizada na fermentação de vinho, cerveja e pão é a saccharomyces cerevisiae. no entanto, nos últimos anos a torulaspora delbrueckii tem despertado interesse, devido às suas propriedades, desde o sabor e aroma do vinho até a capacidade de ser preservado por mais tempo em massa congelada. o principal objectivo desta tese foi explorar os genomas de t. delbrueckii publicamente disponíveis, assim como os que constituem a coleção do nosso projeto, analisando as suas informações genómicas e estabelecendo relação com suas origens e uso biotecnológico. na primeira fase, genomas publicamente disponíveis de t. delbrueckii foram explorados e sua anotação foi aprimorada. eggnog-mapper foi usado para realizar a anotação funcional dos genes codificantes de t. delbrueckii, oferecendo uma perspetiva sobre seu significado biológico, o que revelou 24 grupos de grupos ortólogos (cog), reunidos em três categorias funcionais principais: armazenamento e processamento de informações (28% das proteínas), processamento e sinalização celular (27%) e metabolismo (23%). pouca variabilidade intra-espécies foi encontrada quando se considerou a anotação funcional dos quatro genomas disponíveis de t. delbrueckii. foi ainda realizado um estudo comparativo entre o genoma de t. delbrueckii e o de 386 espécies de fungos, o que revelou uma elevada homologia com espécies dos géneros zygotorulaspora e zygosaccharomyces, mas também com lachancea e s. cerevisiae. por último, foi avaliado o posicionamento filogenético da t. delbrueckii usando os homólogos encontrados em 204 sequências de proteínas comuns de 386 espécies e estirpes de fungos. na segunda fase, cinquenta e quatro estirpes de t. delbrueckii foram sequenciadas na novogene e os dados recebidos foram explorados. procedeu-se ao alinhamento, análise de snp, anotação, entre outras análises, pela primeira vez, para essas estirpes. com o objetivo de compreender a relação entre as estirpes estudadas, foi realizado um pca. o presente trabalho representa um esforço, bem-sucedido, para melhorar a anotação do genoma de t. delbrueckii. no geral, este estudo fornece um ponto de partida para desvendar as potenciais aplicações biotecnológicas da t. delbrueckii."
    ],
    [
      "nowadays, mouse and keyboard are crucial perpherals for interacting with a computer. these peripherals allow computer users to navigate the windows, select items, type int text, among others. however, the use of such peripherals, which is considered elementary for most of us, is sometimes an obstacle to computer interaction, especially for users with physical limitations or fine motor skill problems. this dissertation presents a system that will allow the interaction with a computer using only the voice, without the need for a physical/mechanical interaction. this system also allows the user to, besides controlling the keyboard and the mouse cursor, carry out the most common tasks by using voice com mands, such as creating a task for a date or conduct a web search. lastly, this dissertation presents the efficiency tests carried out and the system acceptance in a real context.",
      "nos dias de hoje o controlo do rato e teclado apresentam-se como uma forma crucial de interação com um computador. é através do rato que podemos controlar o cursor que nos permite a navegação nas janelas, seleção de itens entre outros. a utilização de tais periféricos, como por exemplo o rato, que tantas vezes consideramos como elementares, é por vezes um obstáculo à interação com o computador, especialmente para utilizadores com limitações físicas ou de motricidade fina (capacidade da execução de movimento finos com controlo e destreza). nesta dissertação será apresentado um sistema que permita ao utilizador controlar um computador recorrendo apenas a voz, sem a necessidade de existir uma intervenção física/mecânica. este sistema também permitirá que o utilizador, para além de controlar o teclado e o cursor do rato, consiga efetuar tarefas mais básicas, recorrendo a comandos de voz, como por exemplo criar uma tarefa para um dado dia ou uma simples pesquisa web. por fim serão apresentados testes de eficiência e aceitação do sistema realizados em contexto real."
    ],
    0.0
  ],
  [
    [
      "com a evolução tecnológica, as aplicações web têm um papel crucial na comunidade. nesse sentido é essencial que estas acompanhem o desenvolvimento tecnológico e sejam cada vez mais plataformas confiáveis e disponíveis. um dos componentes imprescindíveis para o sucesso de um sistema interativo é a interface gráfica com o utilizador (gui, em inglês graphical user interface), que, neste caso, são acedidas através de web browsers. com o aumento das capacidades dos browsers, cada vez mais aplicações fazem uso dessas capacidades, existindo uma componente lógica que é executada no próprio browser. desse modo, é fundamental analisar o impacto, a nível computacional, resultante da execução da componente lógica no próprio browser. uma forma de o fazer é através de testes de carga que são executados a partir da interface com o utilizador, permitindo identificar possíveis falhas, tais como problemas de implementação, tempos de resposta elevados ou gargalos de desempenho. no entanto, é indiscutível que as aplicações são cada vez mais complexas e, por sua vez, o processo de testes torna-se mais difícil e demorado, existindo uma necessidade crescente da automatização do mesmo. os testes baseados em modelos (mbt, em inglês model-based testing) suportam a geração e execução automática de testes a partir de um modelo do sistema. o mbt aplicado às interfaces gráficas permite uma avaliação mais exaustiva da aplicação, dado que permitem uma simulação da interação do utilizador com o sistema. esta dissertação tem como objetivo desenvolver uma solução que, tendo como componentes principais o processo de testes baseados em modelos, testes a interfaces gráficas e testes de carga, permita com o menor esforço possível gerar e executar testes de carga a partir da interface gráfica.",
      "with the evolution of technology, web applications have a crucial role in the community. conse quently, it is essential that they follow the technological development and become more and more reliable and available. the graphical user interface (gui), which in this case is accessed through web browsers, is one of the most important components of the success of an interactive system. with the increase of browser capabilities, many web applications make use of these capabilities and allow that a logic compo nent can run in the browser. thus, it is essential to analyze the computational impact resulting from the execution of the logic component in the browser. one way to do this is through load tests that are exe cuted from the graphical interface, allowing the identification of possible errors, such as implementation problems, high response times or bottlenecks. however, applications are increasingly complex and the testing process becomes more difficult and time consuming, creating a need to automate it. the model based testing (mbt) process supports the automatic generation and execution of tests from a system model. when mbt is applied to guis it allows a more exhaustive evaluation of the application because it automates the simulation of the user’s interaction with the system. building on the concepts of model-based testing, gui testing and load testing, this dissertation aims to develop a solution that allows, with the least possible effort, to generate and execute load tests from the graphical interface."
    ],
    [
      "due to global climate change, the temperatures of streams and rivers are increasing, negatively affecting aquatic life, including bivalve species. freshwater mussels are vital components of rivers, streams, and lake ecosystems, participating in essential ecological roles such as nutrient cycling, and increasing water quality. furthermore, they serve as essential ecosystem engineers, providing habitat to other organisms and supporting intricate food webs. besides their biological importance, freshwater mussels are poorly studied in terms of genomics. in the present work, the iberian dolphin freshwater mussel unio delphinus spengler, 1793 (bivalvia: unionoida) was used as a model species to investigate the effects of climate change in freshwater mussels. the primary objective of this thesis was to determine the gene expression patterns in a model species of freshwater mussels under the effects of thermal stress exacerbated by climate change, with an overall goal of understanding the potential consequences for freshwater mussel populations. two different ecological experiments were performed: chronic and acute. the chronic experiments where temperatures were gradually increased to simulate a scenario of progressive increasing temperatures. the acute experiments where temperatures were rapidly increased to replicate the effects of a briefer extreme climatic event. to achieve this main goal, a comprehensive bioinformatic pipeline focused on transcriptomics analysis was developed using the r bioconductor package to generate the differential gene expression profiles of these individuals under thermal stress. the bioinformatic methodology of this work differs from the past studies, by developing an r code compilation of three methods, edger, limma, and deseq2 for differential gene expression analysis in these organisms. the output of the present work provides a comprehensive overview of gene expression profile responses of u. delphinus under climate change scenarios. additionally, the results revealed a wide range of pathways and the corresponding genes that are impacted by thermal stress, with a particular emphasis on the up-regulation of the genes atp6v1a, atp6v0a1, atp6v0a, and atp6v1. in the chronic experiments, and high temperatures, mussels expressed these genes and, interestingly, all the pathways that these genes included appeared up-regulated. the discovered genes and pathways provide vital insights into these organisms’ adaptation tactics and identify prospective targets for monitoring and conservation efforts.",
      "devido às alterações climáticas globais, a temperatura dos rios e ribeiros está a aumentar, afectando a vida aquática, incluindo as espécies de bivalves. os mexilhões de água doce são componentes dos rios, ribeiros e lagos, participando em funções vitais dos ecossistemas, como a reciclagem de nutrientes e melhorando a qualidade da água. além disso, atuam como engenheiros de ecossistemas, providenciando habitat a outros organismos e fazendo parte de teias alimentares complexas. apesar da sua importância biológica, os mexilhões de água doce são pouco estudados em termos genómicos. no presente trabalho, o mexilhão de água doce unio delphinus spengler, 1793 (bivalvia: unionoida) é uti-lizado como espécie modelo para investigar os efeitos das mudanças climáticas em mexilhões de água doce. o objetivo principal desta dissertação foi determinar os padrões de expressão genética numa es-pécie modelo de mexilhões de água doce sob os efeitos de stress térmico exacerbado pelas mudanças climáticas, com o objetivo geral de comprender as consequências para populações de mexilhões de água doce. duas experiências foram realizadas: crónica e aguda. as experiências crónicas onde a temperatura foi gradualmente aumentada para simular um cenário de aumento progres-sivo da temperatura. as experiências agudas onde a temperatura foi aumentada rapidamente de modo a reproduzir os efeitos de um extremo climático breve. de modo a atingir esse objetivo, uma pipeline de bioinformática focada na análise transcriptómica foi desenvolvido usando o pacote r bioconductor para gerar os perfis diferenciais de expressão genética desses indivíduos sob stress térmico. a metodologia bioinformática deste trabalho difere dos estudos anteriores, por desenvolver uma compilação de código em r de três métodos, edger, limma e deseq2 para análise diferencial de expressão genética nesses organismos. o resultado do presente trabalho fornece uma visão abrangente das respostas do perfil de expressão genética de u. delphinus num cenário de mudanças climáticas. os resultados revelaram uma ampla gama de vias e os genes correspondentes relacionados com o stress térmico, com ênfase particular na regulação positiva dos genes atp6v1a, atp6v0a1, atp6v0a e atp6v1. nas experiências crónicas, a uma temperatura elevada, os mexilhões expressaram estes genes e, curiosamente, todas as vias que estes genes incluíam pareciam reguladas positivamente. os genes e vias descobertos fornecem informações vitais sobre adaptação destes organismos e identificam alvos potenciais para esforços de conservação."
    ],
    0.06666666666666667
  ],
  [
    [
      "quantum computing has noticeably grown over the last two decades, making it a revolu tionary field of investigation in the current era of technological research. such a growth has been leading to an increasing demand in research by several big enter prises such as ibm, google and microsoft, paving the way for a richer ecosystem and untold benefits among the quantum computing community. verification is a crucial aspect of software development, as it ensures that a program per forms as intended and reduces the risk of introducing errors. this is especially important in the field of quantum computing, where the complexity of programs is high and the behavior of quantum systems is often counterintuitive. verification of quantum programs can help detect errors that may lead to incorrect results, which is of utmost importance when dealing with quantum algorithms and quantum simulations. as a result, having a formal verification framework for quantum programs can greatly benefit the development of reliable and accurate quantum software. qbricks is a verification framework for building quantum programs, and corresponds to the framework on which this project has been integrated. during the course of this thesis, iqbricks – an intuitive and user-friendly language to build and formally verify quantum programs – was developed, along with a framework to translate and generate verifiable qbricks programs from iqbricks. this project’s main achievements were: (1) the design and implementation of a high-level programming language for describing quantum circuits in an intuitive and user-friendly way and (2) the implementation of a translator, embedded in qbricks’ framework, that converts iqbricks programs to qbricks ones. the developed framework was evaluated against two different quantum algorithms: the quantum fourier transform and grover’s algorithm. this project was accompanied by an internship at the commissariat à l’énergie atomique et aux énergies alternatives (cea) - lsl, where this implementation was developed in direct involvement with qbricks’ team of investigators.",
      "a computação quântica cresceu de forma notável nas últimas duas décadas, tornando-se um campo revolucionário de investigação na atual era da investigação tecnológica. tal crescimento tem levado a uma crescente procura na investigação por parte de várias grandes empresas como a ibm, google e microsoft, abrindo caminho para um ecossistema mais rico e benefícios inéditos entre a comunidade da computação quântica. a verificação é um aspecto crucial no desenvolvimento de software, pois garante que um programa execute conforme o previsto e reduz o risco de introduzir erros. isso é especialmente importante no campo da computação quântica, onde a complexidade dos programas é alta e o comportamento dos sistemas quânticos é frequentemente contra-intuitivo. a verificação de programas quânticos pode ajudar a detectar erros que possam levar a resultados incorretos, o que é de extrema importância ao lidar com algoritmos quânticos e simulações quânticas. como resultado, ter um framework de verificação formal para programas quânticos pode beneficiar grandemente o desenvolvimento de software quântico confiável e preciso. qbricks é uma estrutura de verificação para a construção de programas quânticos, e corresponde à estrutura sobre a qual este projecto foi integrado. durante o curso desta tese, foi desenvolvida iqbricks - uma linguagem intuitiva e de fácil utilização para construir e verificar formalmente programas quânticos - juntamente com uma ferramenta para traduzir e gerar programas qbricks verificáveis a partir de iqbricks. as principais concretizações deste projecto foram: (1) a concepção e implementação de uma linguagem de programação de alto nível para descrição de circuitos quânticos de uma forma intuitiva e de fácil utilização e (2) a implementação de um tradutor, embutido em qbricks, que converte programas iqbricks para qbricks. a ferramenta desenvolvida foi testada utilizando dois algoritmos quânticos diferentes: o transformada de fourier quântica (qft) e algoritmo de grover. este projecto foi acompanhado por um estágio no commissariat à l’énergie atomique et aux énergies alternatives (cea) - lsl, onde esta implementação foi desenvolvida em colaboração direta com a equipa de investigadores de qbricks."
    ],
    [
      "o desenvolvimento dos serviços cloud gerou uma migração massiva de dados para os mesmos. o facto da cloud fornecer um serviço acessível a partir de qualquer dispositivo (computador pessoal, telemóvel, tablet, etc.) através do acesso à internet fez com que a sua utilização aumentasse. estes serviços trazem bastantes vantagens para os utilizadores. a facilidade em aceder e partilhar os dados pelos diferentes utilizadores combinada com a maleabilidade e escalabilidade oferecidas pela cloud, faz com que este serviço se torne numa plataforma bastante pretendida pelas empresas para armazenar os seus projetos remotamente e, desta forma reduzir custos associados aos servidores locais. no entanto, armazenar dados remotamente provoca o aparecimento de questões acerca de segurança e privacidade. de forma a garantir confiabilidade no serviço, o fornecedor cloud tem de garantir que os dados estejam seguros em termos de privacidade, confidencialidade e integridade. ataques recentes a fornecedores de serviços cloud (greene, 2015) levantaram questões em termos de segurança da informação, provocando um decréscimo na confiança depositada nestes serviços. esta dissertação aborda um desafio de segurança que a cloud está a enfrentar, a computação sobre dados. esta computação tem de ser feita sobre dados cifrados para que o fornecedor de cloud não tenha acesso ao valor real da informação nem ao resultado da computação.",
      "the development of cloud services produced a massive migration of data to such services. the fact that the cloud provides a mean for users to access their data in every device (personal computer, mobile phone, tablet, etc.) just by having an internet connection made it widely used. using a cloud service brings several advantages to users. for example, the ease with which data is accessed and shared among different users, combined with the malleability and scalability provided by cloud storage, made cloud a target for enterprises to maintain their projects remotely and thus save money storing them in local servers. nevertheless, having data stored and managed remotely rises several security issues. in order to be trustable, a cloud provider must ensure that data is properly secure in terms of privacy, confidentiality and integrity. recent attacks on largely used cloud providers (greene, 2015) have made cloud services questionable in terms of information security, reducing the confidence placed on them. this master thesis addresses one security challenge that cloud is facing, namely the computation over data. computation should be made over encrypted data so that the cloud provider does not learn anything about the original data nor the result of the computation."
    ],
    0.06666666666666667
  ],
  [
    [
      "hypatiamat, namely through its hypatiamat association (ahm) has, among others, the objective of contributing to awakening among students of various educational levels an interest for mathematics and a better understanding of its nature; of promoting the development of mathematics teaching at all levels; of promoting the quality of teaching/learning of mathematics through the use and integration of new technologies in the classroom, through the resources available on the hypatiamat platform, thus capitalizing on the familiarity and enthusiasm of students for more technological environments. considering that the first years of schooling are essential for the construction of knowledge in mathematics and for the development of transversal skills such as logical reasoning and problem solving, and taking into account that the evolution of mathematical learning takes place in a continuous and spiral dynamic, it is important to establish solid foundations so as not to compromise the learning of the following years. note that failure in this subject is often rooted in incomplete or dysfunctional learning, resulting from a poor construction of the mathematics foundations. the intervention of the hypatiamat project, being directly aimed at the students, takes advantage of their natural taste for technological environments. with this in mind, it proposes to provide teachers with tools that will allow them to incorporate, in their daily practice, methodologies that use this type of environment, duly articulated with other methodologies. it should be noted that learning is richer and more effective if the experiences and contexts provided to students are more diversed, and it is in this perspective that the hypatiamat project offers a great diversity of materials and approaches, allowing teachers to work them not only in a technological context, but also in a concrete one. in this dissertation the objective is to update and improve the component corresponding to the management of championships of hypatiamat, thus allowing users of the platform an easier and more intuitive interaction with it.",
      "o hypatiamat, nomeadamente através da sua associação hypatiamat (ahm) tem, entre outros, o objetivo de contribuir para despertar junto dos alunos dos vários graus de ensino o interesse pela matemática e uma melhor compreensão da sua natureza; de promover o desenvolvimento do ensino da matemática a todos os níveis; de promover a qualidade do ensino/aprendizagem da matemática mediante a utilização e integração das novas tecnologias em sala de aula, através dos recursos disponíveis na plataforma hypatiamat, capitalizando-se, assim, a familiaridade e o entusiasmo dos alunos por ambientes mais tecnológicos. considerando que os primeiros anos de escolaridade são essenciais para a construção do conhecimento em matemática e para o desenvolvimento de capacidades transversais como o raciocínio lógico e resolução de problemas, e tendo em conta que a evolução da aprendizagem matemática se processa numa dinâmica contínua e em espiral, é importante o estabelecimento de bases sólidas para não serem comprometidas as aprendizagens dos anos seguintes. note-se que o insucesso nesta disciplina tem, frequentemente, as suas raízes em aprendizagens incompletas ou disfuncionais, resultantes de uma construção pobre do edifício da matemática. a intervenção do projeto hypatiamat, orientando-se de uma forma direta para os alunos, aproveita o gosto natural que estes têm por ambientes tecnológicos. com esse objetivo, propõe-se munir os professores de ferramentas que lhes possibilitem incorporar, nas suas práticas diárias, metodologias que utilizem esse tipo de ambientes, devidamente articuladas com outras metodologias. note-se que as aprendizagens são tanto mais ricas e eficazes quanto mais diversificadas as experiências e contextos que são facultados aos alunos, e é nessa perspetiva que o projeto hypatiamat oferece uma grande diversidade de materiais e abordagens, permitindo aos professores trabalhá-los não só em contexto tecnológico, mas também concreto. na presente dissertação o objetivo é atualizar e melhorar a componente correspondente à gestão de campeonatos do hypatiamat, permitindo assim aos utilizadores da plataforma uma interação mais facilitada e acessível com a mesma."
    ],
    [
      "the trend of increasing size of datasets in storage-based applications has promoted the research of new methods and technologies for efficiently storing, processing, and analyzing large amounts of data. as a result, log structured merge (lsm) key-value stores (kvss) have been highly adopted since their design allows high write throughput and enforces sequential disk access patterns. additionally, with the advent of non-volatile main memory (nvmm), new storage technologies have emerged that offer faster access times compared to traditional block-based storage devices, thus accelerating kvss. however, while nvmm devices offer faster access to data, they are typically limited in capacity and are often more expensive. to address this trade-off, contemporary storage solutions harness the capabilities of heterogeneous storage devices in two fundamental manners: caching and tiering. in this dissertation, we show that, on one hand, read-dominated workloads benefit from a caching approach, but their performance degrades under tiering. on the other hand, for write-dominated workloads, the tiering approach presents better performance, while storing the entire dataset on nvmm actually degrades performance. to overcome these challenges, this dissertation proposes keigo, a novel storage middleware that al lows lsm-based kvs to efficiently use storage hierarchies composed of nvmm and block-based devices. keigo is aware of the different i/o operations done by the kvs (e.g., foreground requests, and background flushes and compactions) and the characteristics of the underlying devices (e.g., concurrency, read/write asymmetry). this knowledge serves as a pivotal factor in optimizing keigo’s performance in the face of dynamic and mixed production workloads such as those observed in nutanix and meta. moreover, keigo requires minimal code modifications to integrate into production-ready lsm kvss. conducted experiments show that keigo significantly enhances the throughput of lsm kvs solu tions, including rocksdb, speedb, and leveldb, by as much as 12.4×. furthermore, it substantially reduces tail latency by up to 21.3× over both general-purpose storage solutions and lsm kvss built from the ground up for hierarchical storage.",
      "o aumento do volume de dados gerido por aplicações orientadas ao armazenamento tem promovido a in vestigação de novos métodos e tecnologias para os armazenar, processar e analisar eficientemente. como resultado, os sistemas de armazenamento chave-valor (kvs) baseados em log structured merge (lsm) têm sido amplamente adotados, já que o seu desenho permite um elevado débito de escritas e impõe padrões de acesso sequenciais. além disso, com o desenvolvimento de dispositivos de memória não volátil (nvmm), surgiram novas técnicas de armazenamento que oferecem tempos de acesso mais rápidos comparativamente aos discos tradicionais baseados em blocos, acelerando assim o desempenho das kvss. contudo, embora os dispositivos nvmm ofereçam melhor desempenho, são normalmente limitados em termos de capacidade e mais caros. para resolver este problema, as soluções de armazenamento atuais tiram partido das capacidades de armazenamento heterogéneo de duas formas: caching e tiering. nesta dissertação demonstramos que, por um lado, as cargas de trabalho dominadas por leituras benefi ciam de caching, mas o seu desempenho degrada com as soluções de tiering. para as cargas de trabalho dominadas por escritas, a abordagem de tiering é mais benéfica, tendo em conta que o armazenamento de todo o conjunto de dados em nvmm degrada o desempenho. de forma a resolver estes problemas, esta dissertação propõe um novo middleware de armazena mento, keigo, que ajuda kvss a utilizar eficientemente armazenamento hierárquico composto por nvmm e outros dispositivos orientados ao bloco. o keigo tem conhecimento das diferentes operações de i/o efectuadas pela kvs (p.ex., pedidos de leitura e escrita, flushes e compactações) e das características dos dispositivos subjacentes (p.ex., concorrência, assimetria de leitura/escrita), permitindo otimizar o desempenho face a cargas de trabalho de produção dinâmicas e mistas, como as da nutanix e da meta. através de uma avaliação compreensiva, mostramos que o keigo melhora o débito de lsm kvss, incluíndo rocksdb, speedb e leveldb, em até 12.4×. reduz ainda a latência em até 21.3× em relação a soluções genéricas e a kvss criadas especificamente para armazenamento hierárquico."
    ],
    0.3
  ],
  [
    [
      "tools for programming languages processing, like static analysers (for instance, a static application security testing (sast) tool, one of checkmarx’s main products), must be adapted to cope with a given input when the source programming language changes. complexity of the programming language is one of the key factors that deeply impact the time of giving support to it. this master’s project aims at proposing an approach for assessing language complexity, measuring, at a first stage, the complexity of its underlying context-free grammar (cfg). from the analysis of concrete case studies, factors have been identified that make the support process more time-consuming, in particular in the stages of language recognition and in the transformation to an abstract syntax tree (ast). in this sense, at a second stage, a set of language features is analysed in order to take into account the referred factors that also impact on the language processing. the main objective of the master’s work here reported is to help development teams to improve the estimation of time and effort needed to adapt the sast tool in order to cope with a new programming language. in this dissertation a tool is proposed, that allows for the evaluation of the complexity of a language based on a set of metrics to classify the complexity of its grammar, along with a set of language properties. the tool compares the new language complexity so far determined with previously supported languages, to predict the effort to process the new language.",
      "ferramentas para processamento de linguagens de programação, como os analisadores estáticos (por exemplo, uma ferramenta de testes estáticos para análise da segurança de aplicações, um dos principais produtos da checkmarx), devem ser adaptadas para lidar com uma dada entrada quando a linguagem de programação de origem muda. a complexidade da linguagem de programação é um dos fatores-chave que influencia profundamente o tempo de suporte à mesma. este projeto de mestrado visa propor uma abordagem para avaliar a complexidade de uma linguagem de programação, medindo, numa primeira fase, a complexidade da gramática independente de contexto (gic) subjacente. a partir da análise de casos concretos, foram identificados fatores (relacionados como facilidades específicas oferecidas pela linguagem) que tornam o processo de suporte mais demorado, em particular nas fases de reconhecimento da linguagem e na transformação para uma árvore de sintaxe abstrata (ast). neste sentido, numa segunda fase, foi identificado um conjunto de características linguísticas de modo a ter em conta os referidos fatores que também têm impacto no processamento da linguagem. o principal objetivo do trabalho de mestrado aqui relatado é auxiliar as equipas de desenvolvimento a melhorar a estimativa do tempo e esforço necessários para adaptar a ferramenta sast de modo a lidar com uma nova linguagem de programação. como resultado deste projeto, tal como se descreve na dissertação, é proposta uma ferramenta, que permite a avaliação da complexidade de uma linguagem com base num conjunto de métricas para classificar a complexidade da sua gramática, e em um conjunto de propriedades linguísticas. a ferramenta compara a complexidade da nova linguagem, avaliada por aplicação do processo referido, com as linguagens anteriormente suportadas, para prever o esforço para processar a nova linguagem."
    ],
    [
      "many people believe that information technology has the potential to change the way the healthcare industry approaches its current challenges by improving healthcare quality, safety, and efficiency by bringing decision support to the point of care and enabling routine quality measurement. in the medical field, healthcare information technology refers to any information technology tool or software that is intended to increase hospital and administrative productivity, provide new information about medications and treatments, or improve overall quality of care. infrastructures in hospitals must manage both information technology and specialized healthcare systems and protocols. it is important in this type of structure to ensure that all operations and information technology run smoothly and one of the ways to achieve this is by continuously and automatically monitor the hospital environment’s systems. the right monitoring and reporting tools can help keep medical staff efficient without worrying about failing systems, provide visibility into usage trends, equipment performance, downtime, and much more, saving time and resources. a monitoring application of this type is regarded as an important source of information. the main goal of this dissertation within the scope of this project is to develop a monitoring web platform for healthcare information technology administrators that is based on a multi-site and multi-organization scheme. the proposed solution will monitor various hospital services and is expected to provide the current state of the system in a timely manner through a set of graphs and reports, allowing for appropriate operational decisions and ensuring that the system functions as expected. a web application for monitoring hospital services was developed, implemented, and evaluated during the dissertation work period. using questionnaires, the platform was evaluated and validated in order to understand if this approach may improve information technology availability and, in the long run, alleviate some of the healthcare industry’s pains. a formal evaluation of the solution was also performed, which comprised a strengths, weaknesses, opportunities and threats analysis and a risk assessment report, both of which gave helpful insights into the system’s strengths and shortcomings, as well as potential improvement areas.",
      "muitas pessoas acreditam que a tecnologia da informação tem o potencial de mudar a forma como a indústria da saúde aborda os seus desafios atuais, melhorando a qualidade, segurança e eficiência dos cuidados de saúde, levando o apoio à decisão ao ponto dos cuidados e permitindo a medição da qualidade de forma rotineira. no campo médico, a tecnologia da informação sobre cuidados de saúde refere-se a qualquer ferramenta ou software informático que se destine a aumentar a produtividade hospitalar e administrativa, fornecer novas informações sobre medicamentos e tratamentos, ou melhorar a qualidade global dos cuidados de saúde. as infraestruturas dos hospitais devem gerir tanto as tecnologias da informação como os sistemas e protocolos especializados de cuidados de saúde. é importante, neste tipo de estrutura, assegurar que todas as operações e tecnologias de informação funcionem sem problemas e uma das formas de o conseguir é monitorizando-a de forma contínua e automática. a escolha de ferramentas de monitorização e relatórios adequadas podem ajudar a manter o pessoal médico eficiente sem se preocupar com falhas nos sistemas, dar visibilidade às tendências de utilização, desempenho do equipamento, tempo de inactividade e muito mais, poupando tempo e recursos. uma aplicação de monitorização deste tipo é considerada como uma importante fonte de informação. o principal objectivo desta dissertação no âmbito deste projecto é desenvolver uma plataforma web de monitorização para administradores de tecnologias de informação na área da saúde que se baseie num esquema multi-site e multi-organização. a solução proposta irá monitorizar vários serviços hospitalares e espera-se que forneça o estado actual do sistema em tempo útil através de um conjunto de gráficos e relatórios, permitindo decisões operacionais adequadas e assegurando que o sistema funciona como esperado. foi desenvolvida, implementada e avaliada uma aplicação web para monitorizar os serviços hospitalares durante o período de trabalho de dissertação. utilizando questionários, a plataforma foi avaliada e validada de modo a compreender se esta abordagem pode melhorar a disponibilidade das tecnologias da informação e, a longo prazo, aliviar algumas das dores da indústria da saúde. foi também realizada uma avaliação formal da solução, que incluiu uma análise de pontos fortes, pontos fracos, oportunidades e ameaças e um relatório de avaliação de risco, ambos dando uma visão útil dos pontos fortes e fracos do sistema, bem como de potenciais áreas de melhoria."
    ],
    0.3
  ],
  [
    [
      "in recent years, our research group on language processing, gepl, has been collaborating with centro neurosensorial de braga, led by dr. ana paula azevedo. in this context, some serious games were developed and installed for recognizing shapes, emotions and training central and peripheral vision. they are used in memory therapy, deconcentration, dyslexia, and other problems that affect the acquisition of knowledge in learning processes.the ideas that rose up along the literature review done on those areas, will be exposed along the state-of-the-art chapter in this report. this thesis proposes a system that will implement an error detection algorithm based on speech-to-text analysis to check whether the spoken sequence contains errors or not. as the system is intended to be installed in the neurosensory center, the results will be presented visually to help the therapist in their day-to-day work and monitor the actual use of the system.",
      "nos últimos anos, o nosso grupo de investigação em processamento de linguagens, gepl, tem vindo a colaborar com o centro neurossensorial de braga, liderado pela drª ana paula azevedo. neste contexto, foram desenvolvidos e instalados alguns jogos sérios para reconhecimento de formas e de emoções e para treino da visão central e periférica. estes são usados na terapia da memória, desconcentração, dislexia e outras perturbaçoes que afetam a aquisição de conhecimentos em processos de aprendizagem. as ideias que surgiram, no decorrer da revisão, de literatura feita sobre essas áreas serão expostas ao longo do capítulo de estado da arte deste relatório. esta tese propõe um sistema que implementará um algoritmo de deteção de erros baseado na análise de fala para texto de modo verificar se a sequência falada contém erros ou não. como o sistema se destina a ser instalado no centro neurossensorial, os resultados serão apresentados visualmente para auxiliar o terapeuta no seu dia-a-dia e monitorar o real uso do sistema."
    ],
    [
      "nesta dissertação é explorado o tema da monitorização de múltiplos objetos no contexto de um ”smart campus”, com foco no contexto específico num campus universitário, sendo este o tema principal do projeto de investigação lab4uspace. a monitorização de múltiplos objetos, especialmente de pessoas, é relevante para diversas aplicações, incluindo aplicações de vigilância, mobilidade e inteligência ambiental. no entanto, torna-se particularmente desafiante no contexto de espaços abertos, às quais exigem soluções com múltiplas câmaras com problemas inerentes, tais como a reidentificação. o objetivo desta dissertação é desenvolver um framework capaz de fornecer informações sobre o percurso de várias pessoas ao longo do campus universitário usando um cenário com múltiplas câmaras. a solução visa não só a monitorização de uma pessoa num único cenário, mas também em todo o campus, coberto por diversas câmaras com ou sem sobreposição. esta dissertação discute os diversos desafios enfrentados durante o desenvolvimento deste projeto, incluindo preocupações com a privacidade e segurança dos utilizadores do campus. com isso, optou-se por não enviar imagens para nenhuma aplicação, tratando apenas das informações estritamente retiradas da monitorização das pessoas. um dos principais desafios foi desenvolver um framework que rastreie vários objetos num ambiente de um ”smart campus”, abordando desafios de espaços abertos e problemas de reidentificação. além disso, devido aos recursos computacionais limitados, foi usado um computador de bordo para lidar com processamento de imagens e operações relacionadas às técnicas de visão computacional de maneira mais eficaz. o framework proposto utiliza modelos de deteção de objetos e algoritmos de monitorização em tempo real que foram comparados neste contexto específico. depois de pesquisar outras alternativas, a estrutura usa o modelo yolov7-tiny para deteção de objetos, bot-sort para a monitorização dos vários objetos e deep person reid para a reidentificação. o programa foi desenvolvido em python e juntamente a ele foi também criado um website para alterar as configurações do sistema de monitorização utilizando o framework flask. um message broker também foi utilizado para a comunicação entre os diversos componentes do sistema. os testes de validação demonstram a eficácia da framework proposta na monitorização das várias pessoas em todo o campus. o sistema proposto contribui significativamente para o desenvolvimento de soluções de múltiplas câmaras mais eficientes e eficazes para aplicações de ”smart campus”, com benefícios potenciais para a segurança, proteção e gestão do campus. no geral, esta dissertação apresenta uma estrutura que rastreia de maneira eficaz várias pessoas num ambiente de ”smart campus”. a framework é uma contribuição importante para o desenvolvimento na área do ”smart campus” e tem potencial para desenvolvimento futuro e aplicações para além do campus universitário.",
      "this dissertation explores the topic of object multi-tracking in the context of a smart campus, focusing on the specific context of a university campus, being the main topic of the lab4uspace research project. multi-tracking of objects, especially people, is relevant for different applications, including surveillance, mobility, and ambient intelligence. however, it becomes particularly challenging in open spaces, which require multi-camera solutions with inherent issues like re-identification. the objective of this dissertation is to develop a framework capable of providing information about the path of multiple people throughout the university campus using a multi-camera scenario. the solution aims not only to track a person in a single scenario but also over the entire campus, covered by various cameras with or without overlapping. this dissertation discusses the challenges faced during the development of this project, including concerns about the privacy and security of campus users. as a result, the decision was made not to send images for any application, dealing only with the information strictly retrieved from the tracking. one main challenge was developing a framework that tracks multiple objects in a smart campus environment, addressing the challenges of open spaces and re-identification issues. additionally, due to limited computational resources, an edge computer was used to handle image processing and computer vision-related operations more effectively. the proposed framework uses different object detection models and real-time tracking algorithms that were compared in this specific context. after researching other alternatives, the framework uses the yolov7 tiny model for object detection, bot-sort for multiple object tracking, and deep person reid for re-identification. the program was developed in python and alongside it was also created a website to change the configurations of the tracking system using the flask framework. a message broker was also used for communication between the various components of the system. validation tests demonstrate the effectiveness of the proposed framework in tracking multiple people across the campus. the proposed framework significantly contributes to developing efficient and effective multi-camera solutions for smart campus applications, with potential benefits for campus safety, security, and management. overall, this dissertation presents a framework that effectively tracks multiple people in a smart campus environment. the framework is an important contribution to the smart campus context and has the potential for future development and applications beyond the university campus."
    ],
    0.3
  ],
  [
    [
      "real-time monitoring has become one of the most important and clinically relevant tasks in medical settings, yet one of the most repetitive and tiresome tasks is the analysis of 24-hour ecg records. one way to automate this long task is to convert this process into a real-time process with the automatic classification of the heart rate and with this, the classification of arrhythmias. thus, this master thesis focuses on the study of deep learning models for the classification of arrhythmias and data processing tools for the streaming and processing of data in real time. consequently, this master’s thesis comprises several phases. in the first place, a more theoretical part is presented which is the ground truth of the use of the tools later used for the development of the system. the development of the system includes a more practical part of data streaming composed by an iot middleware, apache kafka as an intermediate agent between this middleware and apache spark, and elasticsearch for real-time data storage for visualization. on the other hand, in the main scope of this thesis, two models of deep learning were created, one for the classification of arrhythmias and another one for their forecast. the results obtained are promising with the arrhythmia classification yielding 98% accuracy in the classification of each beat in one of the four classes used. when the model was tested in data obtained directly from the hospital of braga, it was not possible to obtain such good results, however, the model after new training was able to obtain accuracy values of 81% for the testing dataset. this deep learning model was also tested with the integration of apache spark, in order to create data parallelism and increase the speed of the deep learning process, which tends to be very time consuming, without neglecting its performance. the development of the model for the prediction of arrhythmias was done based on long- short term memory layers, in order to create a neural network with memory, the results obtained in the records with 30 minutes were not high. despite the less good results in the first dataset, when the model was tested in the 24-hour records, the results obtained were quite high which demonstrated that the model can predict if it is based on a longer record. nevertheless, these results were obtained individually because the electrocardiograms can be an object of human identification. based on the results obtained it was possible to conclude that more tests should be done increasing the spectrum of arrhythmias to be classified so that this process becomes fully automatic, without neglecting the precision of the results since human lives may depend on it.",
      "a monitorização em tempo-real tem vindo a tornar-se numa das mais importantes e clinicamente relevantes tarefas em ambientes médicos, todavia, uma das tarefas mais repetitivas e cansativas à análise dos registos de eletrocardiogramas de 24 horas. uma das formas de automatizar este longo processo é convertendo este processo num processo em tempo-real com a classificação automática do ritmo cardíaco e com isto a classificação de arritmias. desta forma, o objetivo primordial desta tese de mestrado passa pelo estudo de modelos de deep learning para a classificação de arritmias assim como de ferramentas de processamento de dados para a transferência e processamento dos dados em tempo real. consequentemente, esta tese de mestrado é composta por várias fases. em primeiro lugar é apresentada uma parte mais teórica que fundamenta a utilização das ferramentas posteriormente usadas para o desenvolvimento do sistema. o desenvolvimento do sistema engloba uma parte mais prática de transmissão de dados composta por um middleware iot, apache kafka como agente intermediário entre este middleware e o apache spark e ainda o elasticsearch para armazenamento em tempo-real dos dados para visualização dos mesmos. por outro lado, no âmbito principal desta tese, dois modelos de deep learning foram criados, um para a classificação de arritmias e outro para a previsão destas. os resultados conseguidos mostraram-se promissores, com a classificação de arritmias a obter resultados com precisão de 98% na classificação de cada batimento numa das quatro classes usadas. quando o modelo foi testado em dados obtidos diretamente do hospital de braga não foi possível obter diretamente bons resultados contudo após novo treino conseguiu-se obter valores de precisão de 81% para o dataset de teste. este modelo de deep learning foi também testado com a integração do apache spark por forma a criar o paralelismo de dados e aumentar a rapidez do processo de deep learning, que tendencialmente é muito moroso, sem descurar a performance do mesmo, ponto que foi conseguido. por outro lado, o desenvolvimento do modelo para a previsão de arritmias foi feito tendo por base camadas long-short term memory, de forma a criar uma rede neuronal com memória, os resultados obtidos nos registos com 30 minutos não foram muito altos ainda assim foi possível concluir que os eletrocardiogramas podem ser a base de estudos mais pormenorizados no sentido de serem considerados biomarcadores. apesar dos resultados menos bons no primeiro dataset, quando o modelo foi testado nos registos de 24 horas, os resultados obtidos foram bastante altos o que demonstrou que o modelo consegue prever caso tenha por base um registo mais longo. não obstante, estes resultados foram obtidos individualmente assentando no facto de que os eletrocardiogramas são um objeto de identificação humano. com base nos resultados obtidos foi possível concluir que mais testes devem ser feitos tendo em conta o aumento do espetro de arritmias a serem classificadas de forma a que este processo se torne totalmente automático, sem desprezar a precisão dos resultados, uma vez que vidas humanas poderão depender disso."
    ],
    [
      "a saúde constitui um direito inegável dos cidadãos. como tal, é necessário que o acesso aos cuidados de saúde seja o melhor possível. o acesso aos cuidados de saúde pode ser definido como a possibilidade dos utentes receberem assistência de forma adequada às necessidades por si apresentadas, sejam estas de ordem geográfica, financeira ou temporal, obtendo, assim, melhor qualidade de vida. nesta dissertação, o acesso aos cuidados de saúde é analisado na sua componente geográfica. sendo vários os factores que influenciam a acessibilidade geográfica, a distância será aquele sobre o qual incidirá este projeto. cada vez mais, os sistemas de informação geográfica (sig) têm sido utilizados para mapear e explorar a variação geográfica das necessidades de cuidados de saúde e, ainda, desenvolver indicadores inovadores, nesse sentido. no presente trabalho, os sig serão aplicados à acessibilidade aos serviços de saúde, sendo o mesmo dividido em duas partes principais. em primeiro lugar, pretendeu-se desenvolver uma plataforma de suporte que permitisse aos cidadãos decidir quais as formas mais vantajosas de acesso aos diversos serviços de saúde. foi criada uma plataforma de routing, na qual o utilizador poderá indicar qual o equipamento de saúde a que pretende chegar e, através da referência do ponto onde se encontra, ser-lhe-á apresentado o melhor trajeto. em segundo lugar, criou-se um mapa de isolinhas do município de braga, referente às distâncias de acesso ao hospital de braga. esse mapa torna evidente a distância, quer em quilómetros quer em minutos, da população de braga ao hospital referido. para alcançar os objetivos propostos, foi necessário recolher dados relativos às redes de equipamentos de saúde e de corporações de bombeiros, provenientes de diversas fontes. foi, no entanto, notada a falta de alguma informação correta e atualizada. recorreu-se, também, aos dados da população retirados do ine.",
      "health is an undeniable right for all citizens. as such, access to health care services must be the best possible. access to health care can be defined as the ability of population to obtain adequate assistance according to financial, temporal and geographic location needs, and so improving quality of life. this dissertation approaches the geographic component of access to health care. even though there are several factors related to geographic accessibility, distance is the one this project will focus on. progressively more, geographic information systems are used to map and to explore geographic variation in health care and also to develop innovative indicators on that theme. this project is divided into two major parts, applying geographic information systems to health care access. firstly, it was developed a supporting platform to help citizens on the decisionmaking process of the best paths to choose when in need of health assistance. it was created a routing platform where the user can choose the health facility he intends to get to and, by pointing the spot he’s in, the finest path will be shown. secondly, it was created an isochrone map of braga with access distances to hospital de braga. this map reveals the distance between braga’s population and the hospital, in kilometers and in minutes. in order to achieve the proposed goals, data collection from health facilities, fire brigades’ networks and population was indispensable. however, it was noticed the lack of some correct and updated information."
    ],
    0.0
  ],
  [
    [
      "os estudos de conectividade cerebral têm-se tornado bastante relevantes no meio científico e clinico. no entanto, estes estudos não são fáceis de realizar, uma vez que as aplicações existentes foram desenvolvidas por investigadores para as suas necessidades, não sendo apropriadas para um profissional de saúde. assim, foi criada a aplicação braincat que permite que os estudos de conectividade cerebral de ressonância magnética funcional e tensor de difusão sejam realizados de forma intuitiva e que seja facilmente manuseada por um utilizador com poucos conhecimentos na área. assim, o objetivo desta dissertação centra-se na aplicação braincat. assim, esta dissertação pode ser dividida em cinco parte: teste da aplicação com um número elevado de casos, levantamento de falhas e de formas de melhorar a aplicação, implementação das melhorias através da programação em objective-c, testar novamente a aplicação para os mesmos casos e, por fim, comparar os resultados das duas análises. as melhorias implementadas baseiam-se em formas de tornar a aplicação mais intuitiva e de melhorar os resultados. a nível de melhorias de resultados centrou-se na etapa da normalização, uma vez que esta era a que apresentava mais problemas. assim, entre outras implementações, foram adicionadas etapas como a remoção do pescoço, a verificação da extração do crânio na imagem estrutural e a normalização não linear. relativamente à comparação entre resultados, verificou-se que com a extração do pescoço da imagem estrutural, a etapa seguinte de extração do crânio melhorou. a verificação do resultado desta imagem, facilita o utilizador e previne que casos não sejam vistos. a normalização linear apresentou resultados melhores, contudo apresenta uma maior sensibilidade a artefactos ou erros de pré-processamento.",
      "brain connectivity studies have become relatively relevant in the scientific and clinical fields. however, these studies are complex, since existing applications have been developed by researchers for their needs and they are not appropriate for a healthcare professional. thus was created the application braincat that allows studying brain connectivity in functional mri and diffusion tensor imaging, intuitively and easily handled by a user with little knowledge in the area. the objective of this dissertation is to test the application braincat. this dissertation can be divided into five parts: testing the application with a large number of cases, review of failures and ways to improve the application, implementation of improvements by programming in objective-c, test the application again for the same cases and, ultimately, comparing the results of the two analyzes. the improvements were made based on how to make the application more intuitive and improve results. at the level of improvement, results focused on the stage of normalization, since it showed that have more problems. thus, among other implementations were added steps as neck removal, check the extraction in the structural image of the skull and non-linear normalization. concerning the comparison results, it was found that with the extraction of the neck in the structural image, the step of extracting the skull was improved. the verification of the skull stripping facilitates the user and prevents cases of not beeing analyzed. the linear normalization showed better results, but is more sensitive to artifacts and errors in preprocessing."
    ],
    [
      "for the past few years, we have begun to witness an exponential growth in the inform ation and communication technologies (ict) sector. while undoubtedly a milestone, all of this occurs at the expense of high energy costs needed to supply servers, data centers, and any use of computers. associated with these high energy costs is the emission of greenhouse gases. these two issues have become major problems in society. the ict sector contributes to 7% of the overall energy consumption, with 50% of the energy costs of an organization being attributed to the information technology (it) departments. most of the measures taken to address the high level of energy consumption have been on the hardware side. although is the hardware that does consume energy, it is the software that operates that hardware. as a consequence, the software is the main responsible for the energy consumed by the hardware, very much like a driver that drives/operates a car inﬂuences drastically the fuel consumed by the car. this dissertation proposes and implements a methodology to analyze the software energy consumption. this methodology relates energy consumption to the source code of a soft ware application, so that software developers are aware of the energy footprint that he/she is creating with his/her application. the proposed technique interprets abnormal energy consumption as software faults, and adapts a well-known technique for locating faults on programs’s source code, to locate “energy faults”, that we name as “energy leaks”. this methodology has been fully implemented in a software framework that monitors the energy consumed by a software program and identiﬁes its energy leaks, given its source code. moreover, a list of problematic parts of the code is produced, thus, helping software developers identifying energy faults on their source code. we validate our ﬁndings by showing that our methodology can automatically ﬁnd energy leaks in programs for which such leaks are known. with this results, one intends to provide help to the development phase and to gener ate more energy eﬃcient programs that will have less energy costs associated with, while supporting practices that promote and contribute to sustainability.",
      "nos últimos anos, temos vindo a assistir a um crescimento exponencial no sector das tecnologias de comunicação e informação (tic). contudo, e apesar de, inquestionavelmente, se tratar um marco importante, tudo isto ocorre a` custa de altos gastos de energia necessários para alimentar servidores, centros de dados e qualquer uso de computadores. paralelamente, associado aos altos custos de energia estão as emissões dos gases de efeito de estufa. estas duas questões têm-se tornado grandes problemas da sociedade. o sector das tic contribuí para 7% do consumo global de energia, o que representa, para o departamento de tecnologias de informação de uma organização, 50% de custos, associados, à energia. a maioria das medidas adotadas para resolver o nível elevado do consumo de energia, têm sido feitas do lado do hardware. apesar de ser o hardware que consume energia efetivamente, é o software que opera esse hardware. como consequência deste facto, o software é o maior responsável pela energia consumida pelo hardware, tal como um condutor que dirige/opera um carro influencia drasticamente o consumo de combustível de um carro. esta dissertação propõe e implementa uma metodologia para analisar o consumo de energia por parte do software. esta metodologia relaciona o consumo de energia com o código fonte de uma aplicação, permitindo que os desenvolvedores das aplicações estejam conscientes da pegada de energia que a sua aplicação está a ter. a técnica proposta interpreta um consumo de energia anormal como falhas no software, e adapta uma técnica de localização de falhas em código fonte bem conhecida, para localizar falhas de energia denominadas energy leaks. a metodologia foi implementada numa framework que monitoriza a energia consumida por uma aplicação e dado o seu código fonte, identifica as suas falhas energéticas. como adição, uma lista das partes problemáticas do código é produzida, ajudando assim os desenvolvedores a identificar as falhas de energia no seu código. validamos os nossos resultados mostrando que a nossa metodologia consegue automaticamente encontrar falhas de energia em programas para os quais essas falhas são conhecidas."
    ],
    0.0
  ],
  [
    [
      "webrtc is a standard technology which allows real-time communications between browsers, without installing additional plugins. in this way, for each device (computers, smartphones, etc.) with an installed browser, it is possible to perform peer-to-peer real-time communications natively, for instance, video and voice calls, chatting or instant messaging, file sharing and screen sharing. this recent technology has grown exponentially both in implemented solutions and in browsers compatibility. webrtc is therefore an evolutionary technology with a strong growth, where more solutions over-the-top (ott) could appear and where the telecommunications operators could invest creating their own service solutions. facing the lack of standards regarding the communication between webrtc endpoints, this project studies in depth thewebrtc technology in order to identify its potentiality and to assess in which way it could impact on the telecommunications world. this project also aims to create a framework that helps developing webrtc applications and services at a higher level. as proof-of-concept awebrtc client is developed to allow testing the services implemented in the framework. the evaluation results address functionality tests, attesting that the implemented features of the framework work properly, and measure the cpu and memory consumption of webrtc technology.",
      "webrtc é uma tecnologia normalizada que permite a comunicação em tempo real entre browsers, sem a necessidade de instalar plugins adicionais. desta forma, é possível a qualquer dispositivo (computadores, smartphones, etc.), que tenha instalado um browser, realizar comunicações em tempo real peer-to-peer, de uma forma nativa. exemplo disso são as comunicações de voz, vídeo e também a possibilidade de falar por chat, partilhar ficheiros e partilhar ecrã. sendo uma tecnologia relativamente recente, o seu uso tem vindo a crescer exponencialmente, tanto a nível de soluções implementadas, como também a nível de compatibilidade de web browsers. assim, a webrtc torna-se uma tecnologia em forte crescimento e evolutiva, onde poderão surgir cada vez mais soluções de serviços over-the-top e os operadores de telecomunicações poderão investir, criando as suas próprias soluções e provocando um forte impacto ao nível de oferta de serviços. atendento a que ainda não está definida uma implementação normalizada para a comunicação entre endpoints webrtc, nesta dissertação apresenta-se o resultado do estudo efetuado à tecnologiawebrtc, no sentido de identificar as suas potencialidades e o impacto que esta poderá ter no mundo das telecomunicações.apresenta-se tambem a framework desenvolvida com o objetivo de tornar mais fácil a criação e implementação de serviços webrtc, que servirá como uma solução de comunicação entre vários clientes. como prova de conceito, foi desenvolvida uma aplicação cliente, com a implementação de alguns serviços alvo. para além dos testes de funcionamento dos serviços, foram realizadas análises de desempenho à utilização de cpu e de memória, no que diz respeito à tecnologia webrtc."
    ],
    [
      "metabolic engineering targets the microorganism's cellular metabolism to design new strains with an industrial purpose. applications of these metabolic manipulations in biotechnological derive from the need of enhanced production of valuable compounds. the development of in silico metabolic models proposes a quantifiable approach for the manipulation these microorganisms. in this context, constraint based modelling is one of the major approaches to predict cellular behaviour. it allows to prune the feasible space of possibilities describing possible phenotype outcomes in terms of metabolic fluxes. under these conditions, cellular metabolism can be represented as an algebraic system constrained by the laws of mass balance and thermodynamics. these systems are prone to be represented as networks, taking advantage of different graph-based paradigms, including bipartite graphs, hypergraphs and process graphs. this thesis explores these representations and underlying algorithms for metabolic network topological analysis. the main aim will be to identify potential pathways towards the optimized biochemical production of selected compounds. related to this task, algorithms will also be designed aiming to complement networks of specific organisms, taking as input larger metabolic databases, inserting new reactions making them able to produce a new compound of interest. to address these problems, and also related tasks of data pre-processing and evaluation of the solutions, a complete computational framework was developed. it integrates a number of previously proposed algorithms from distinct authors, together with a number of improvements that were necessary to cope with large-scale metabolic networks. these are the result of problems identi ed in the previous algorithms regarding their scalability. a case study in synthetic metabolic engineering was selected from the literature to validate the algorithms and test the capabilities of the implemented framework. it allowed to compare the performance of the implemented algorithms and validate the proposed improvements.",
      "a engenharia metabólica visa a alteração do metabolismo celular dos microorganismos com vista ao desenho de novas estirpes com fins industriais. as aplicações destas modificações genéticas na biotecnologia derivam da necessidade de produzir de forma otimizada compostos de alto valor. o desenvolvimento de modelos computacionais propõe uma abordagem quantitativa para a manipulação destes organismos. neste contexto, a modelação baseada em restrições constitui uma das abordagens mais usadas para a previsão do comportamento celular. esta permite reduzir o espaço de soluções viáveis descrevendo o fenótipo celular a partir dos fluxos metabólicos. nestas condições, o metabolismo celular pode ser representado como um sistema algébrico restringido pelas leis da conservação de massa e termodinâmica. estes sistemas podem ser representados como redes, tirando partido de diferentes paradigmas baseados em grafos, incluindo os grafos bipartidos, os hipergrafos e os grafos de processos. esta tese explora estas representações e os respetivos algoritmos para a análise topológica de redes metabólicas. o objetivo principal será o de identificar potenciais vias metabólicas para a optimização da produção de compostos selecionados. relacionado com esta tarefa, serão desenhados algoritmos com o objetivo de complementar redes de organismos específicos, tomando como entradas bases de dados metabólicas de maior dimensão, inserindo novas reações de forma a torná-los capazes da produção de novos compostos de interesse. para abordar estes problemas, bem como tarefas relacionadas ao nível do pré-processamento e avaliação das soluções, foi desenvolvida uma plataforma computacional completa. esta integra um conjunto de algoritmos previamente propostos por diversos autores, em conjunto com melhorias significativas que foram necessárias para que estes pudessem lidar com redes metabólicas de grande escala. estas melhorias resultam da identificação de problemas nos algoritmos no que diz respeito à sua escalabilidade. um caso de estudo na engenharia metabólica sintética foi selecionado da literatura para validar os algoritmos e testar as capacidades da plataforma implementada. este permitiu comparar o desempenho dos algoritmos implementados e validar as melhorias propostas."
    ],
    0.0
  ],
  [
    [
      "a alocação adequada de órgãos para transplantação é crítica e crucial. no entanto, o número de órgãos a ser doados não é suficiente dada a quantidade de pacientes em lista de espera. assim, a determinação do maior número possível de potenciais dadores, de forma eficiente e eficaz torna-se essencial e pode contribuir para melhorar a taxa de sucesso de transplantação de órgãos. ao longo dos últimos anos, a utilização de tecnologias de informação (tis) e de ferramentas computacionais em vários setores económicos, incluindo o setor da saúde, cresceu exponencialmente, já que têm potencial para transformar e melhorar a prestação de cuidados de saúde. assim, e aliando a necessidade da eficiência na descoberta de potenciais dadores com a emergência das tis na saúde, surge a necessidade de uma plataforma web de apoio à decisão clínica. o objetivo desta plataforma é automatizar o processo de descoberta de informaçãoútil e acionável, através da utilização de tecnologias como business intelligence (bi) e data mining (dm), ajudando na tomada de decisão clínica diária. assim, esta é responsável pela recolha, gestão, armazenamento e sinalização de potenciais dadores. no âmbito deste projeto de dissertação, foi redesenhada e otimizada a plataforma web organite, atualmente implementada no centro hospitalar do porto (chp). envolveu transformações tanto no design da interface do utilizador, como no modo como a informação está organizada na plataforma, de forma a melhorar a experiência do utilizador e a interação com os dados clínicos. foi ainda desenvolvida uma metodologia, com base em técnicas de data mining, para construir um modelo preditivo que avalia quais os pacientes que dão entrada no hospital que têm maior probabilidade em ser potenciais dadores de órgãos. o objetivo é tornar mais simples e eficaz o processo de identificação de potenciais dadores, contribuindo positivamente na tomada de decisão do gabinete de coordenação de colheita e transplantação (gcct), e impactando na redução da lista de doentes que aguarda um transplante.",
      "proper allocation of organs for transplantation is critical and crucial. however, the number of organs to be donated is not sufficient given the number of patients on the waiting list. thus, the efficient determination of as many potential donors as possible becomes essential and can contribute to improved organ transplantation success rate. over the last few years, the use of information technology (it) and computing tools in different economic sectors, including the health sector, has grown exponentially since they have the potential to transform and improve health care delivery. accordingly, and combining the need for efficiency in potential donors discovery with the emergence of it on healthcare, this dissertation relies on the development of a platform to support clinical decision-making. the goal of this platform is to automate the process of discovering useful and actionable information using technologies such as business intelligence (bi) and data mining (dm), with the ultimate goal of improving daily clinical decisions. in this way, this platform is responsible for the collection, management, storage and signaling of potential donors. in the scope of this dissertation project, the web platform, organite, currently implemented at centro hospitalar do porto (chp), was redesigned and optimized. it involved transformations both on user interface and on backend tasks, to improve user experience and interaction with the clinical data. furthermore, a methodology based on data mining techniques was developed, with the aim to build a predictive model that evaluates which hospital admitted patients are most likely to be potential organ donors. the goal is to make the process of identifying potential donors easier and more effective, contributing positively to clinical decision-making, and consequently reducing the list of patients awaiting for an organ transplant."
    ],
    [
      "o desenvolvimento de processos etl é uma tarefa dispendiosa e complexa. não admira, pois, o cuidado que os seus implementadores têm, em particular, durante as suas fases de planeamento e análise. muito trabalho tem sido desenvolvido em prol do estabelecimento de novos e melhores métodos e técnicas de modelação conceptual e lógica destes processos. todavia, ainda ocorrem inúmeros problemas durante as primeiras fases de execução dos processos de etl, muitos deles provocados por erros de análise, de desenvolvimento, ou de simples esquecimento. como tal, é vital que antes da entrada destes processos em produção, eles sejam submetidos a algum tipo de mecanismo que permita validá-los e comprovar a sua correção, relativamente àquilo que se espera que eles realizem. a utilização da linguagem alloy na especificação e validação de processos etl oferece esse tipo de validação. neste trabalho de dissertação, suportado por um caso de estudo específico, alloy é estudada, utilizada e avaliada quanto à sua aplicação na especificação formal e validação de processos etl.",
      "the development of etl processes is an expensive and complex task, hence the attention and care given by its developers, especially during the planning and analysis stages. a lot of effort has been put into establishing new and improved methods and techniques for etl processes logical and conceptual modelling. however, even with the given attention, several problems occur during the first stages of the execution of etl processes, a lot of them caused by analysis errors, development errors, or simply due to forgetfulness. thus, it is vital that, before these processes are deployed into production, they are submitted to some mechanism which enables their validation and offers proofs about their correctness. the use of alloy language for the specification and validation of etl processes provides this kind of validation. in this dissertation work, supported by a specific study case, the alloy language is studied, applied and evaluated regarding its application in the formal specification and validation of etl processes."
    ],
    0.3
  ],
  [
    [
      "a estimação de pose procura de permitir aos computadores calcular a pose do corpo humano através da utilização de sensores unidades de medição inercial (imu). por esta razão, apresenta diversas utilizações na indústria, onde pode ser usado na melhoria da colaboração entre humanos e robôs, na qual permite aos robôs monitorizar os movimentos humanos. a estimação de pose também tem aplicações no setor médico, mais precisamente na avaliação de riscos ergonómicos, reabilitação e desportos. esta dissertação visa o desenvolvimento de um modelo de estimação de pose utilizando deep le arning, através da fusão de dados provenientes de sensores acelerómetro, giroscópio e magnetómetro, existentes nos imu. para alcançar este objetivo foi implementada uma framework em pytorch, onde estão elaborados os modelos para a estimativa de pose, na qual estão incluídos modelos que usam redes neuronais e modelos que usam redes híbridas, que juntam redes neuronais e filtros. nesta framework, também foi implementado um modelo de calibração de dados (calibnet), que usa redes neuronais e procura a diminuição do erro de calibração dos dados provenientes dos sensores, para posteriormente serem usados na estimativa de pose. na avaliação dos métodos implementados foram usados dois datasets o ergowear e o mtw awinda, onde se obtiveram como melhores resultados de 18,78º e de 7,556º, respetivamente, na estimativa de pose.",
      "pose estimation seeks to allow computers to calculate the pose of the human body through the use of unidades de medição inercial (imu) sensors. for this reason, it has several uses in industry, where it can be used to improve collaboration between humans and robots, allowing robots to monitor human movements. pose estimation also has applications in the medical sector, more precisely in the assessment of ergonomic risks, rehabilitation and sports. this dissertation aims to develop a pose estimation model using deep learning, through the fusion of data from accelerometer, gyroscope and magnetometer sensors, existing in imu. to achieve this objective, a framework was implemented in pytorch, where models for pose estimation are created, which includes models that use neural networks and models that use hybrid networks, which combine neural networks and filters. in this framework, a data calibration model (calibnet) was also implemented, which uses neural networks and seeks to reduce the calibration error of data from sensors, to later be used in pose estimation . in evaluating the implemented methods, two datasets were used: ergowear and mtw awinda, where the best results of 18.78º and 7.556º, respectively, were obtained in pose estimation."
    ],
    [
      "e-commerce is constantly expanding, leading to greater market competitiveness. the number of online platforms offering products or services is increasing; so there is a growing need for companies to stand out from the competition, which leads to the application of various marketing strategies. however, not all are adequate and mismanagement, as well as a bad investment of these strategies, may prejudice companies. hence the implementation of recommendation systems in e-commerce platforms, as a safe and economical strategy. by investing in a good recommendation mechanism, one can provide better user experience, taking his interests into account. as a result, more traffic on the platforms is ensured, which may result in a higher sales rate and, consequently, a higher number of revenues. however, to develop a recommendation system, the first step must consist in obtaining information about the sales platform, where data about its users and products/services form the basis of recom mendations. but not all information is useful, which can influence the accuracy of the forecasting models used by the system to produce results. following this perspective, a data analysis methodology is proposed, as well as an architecture of a recommendation system, which allows to extract and treat relevant data, in order to integrate a recommendation engine for most e-commerce platforms.",
      "o comércio eletrónico (e-commerce) está em constante expansão, o que leva a uma maior competitividade no mercado. existem cada vez mais plataformas de venda online e, consequentemente, há uma crescente necessidade das empresas se destacarem da concorrência, o que leva à aplicação das mais variadas estratégias de marketing. porém, nem todas são adequadas e uma má gestão e investimento destas estratégias pode causar prejuízo às empresas. daí surge a implementação de sistemas de recomendação nas plataformas de venda, como uma estratégia segura e económica. ao investir num bom mecanismo de recomendação, é possível proporcionar uma melhor experiência para o utilizador, tendo em conta os seus interesses. desta forma, assegura-se um maior tráfego nas plataformas, o que poderá resultar numa maior taxa de vendas e, consequentemente, num maior número de receitas. no entanto, para desenvolver um sistema de recomendação é necessário, em primeiro lugar, obter informação sobre a plataforma de vendas, onde os dados sobre os seus utilizadores e produtos/serviços constituem a base das recomendações. mas nem toda a informação é útil, o que pode influenciar a acurácia do modelos de previsões utilizado pelo sistema. seguindo esta perspetiva, propõe-se uma metodologia de análise de dados, assim como uma arquitetura de um sistema de recomendação, que permitam extrair e tratar dados relevantes de modo a integrar um motor de recomendação para a generalidade das plataformas de e-commerce."
    ],
    0.3
  ],
  [
    [
      "provisioning commodity hardware used for scientific research while making it customizable and available for a large group of researchers is a process that requires automation. this dissertation describes the infrastructure, design and implementation of mocas and bootler, an approach to management, allocation and provisioning of physical and virtual resources focused on enabling the users to remotely manage their nodes. mocas provides the necessary infrastructure and tools along with an appropriate web interface so researchers may lease bare metal resources and customize the full provisioning process, from installation to configuration without the need of specialized human-resources. bootler, on the other hand, simplifies virtual machine (vm) life cycle management by providing a streamlined user interface and delegating vm scheduling to openstack. in this context, high-assurance software laboratory (haslab) researchers are now able to seemingly operate a 104 nodes (416 cores) commodity hardware cluster by leveraging the automation and abstractions these platforms provide.",
      "o aprovisionamento de hardware comum para ser utilizado por um vasto grupo investigadores no âmbito de investigação cientifica e ao mesmo tempo permitir personalização ao nível do sistema, é algo difícil de alcançar sem algum tipo de automação. nesta dissertação descreve-se a infraestrutura, desenho e implementação de duas plataformas, mocas e bootler, como proposta para gestão, alocação e aprovisionamento de sistemas físicos e virtuais, cujo foco principal é permitir que os utilizadores sejam capazes de gerir os seus próprios recursos remotamente. o mocas fornece toda a infraestrutura, bem como, um conjunto de ferramentas que se fazem acompanhar de uma interface web através da qual os investigadores podem, não só reservar recursos físicos, mas também, personalizar todo processo de aprovisionamento, desde a instalação atá à configuração, sem necessidade de recorrer a recursos humanos especializados. por outro lado, o bootler, dinamiza a gestão do ciclo de vida de máquinas virtuais, para isso, recorre a uma interface web simplificada, através da qual se delega a instanciação dos recursos virtuais à plataforma openstack. com recurso a estes processos de abstração e automação proporcionados por ambas plataformas, atualmente, os investigadores do haslab têm a capacidade de operar de forma simplificada um cluster com 104 máquinas (416 cores) baseadas em hardware comum."
    ],
    [
      "emergency vehicles are bound to lose unnecessary time on their response. furthermore, research shows that emergency vehicles are prone to fatal crashes while on an emergency call. emergency vehicle’s response time is commonly affected by factors unbeknownst to it, such as traffic, traffic lights, road conditions, and accidents. the overall goal of this dissertation project was to improve the response time of an emergency vehicle, while improving its safety along the route. thus, v2x communications was used to obtain knowledge from the environment, so that the emergency vehicle can choose the best decisions to reduce the response time. the emergency vehicle issued alerts to all of the entities of the road system so that these can adapt their behaviour collaboratively. in this context, v2x communications may occur between vehicles and every other entity of the system, such as infrastructures, pedestrians, vehicles, and the network. the present document describes the research and development work that aimed to establish a comparison between two scenarios: with and without support on a vehicular network simulator, namely veins, incorporating the american standard wave. the v2x scenario benefited from use cases as emergency avoidance, dynamic routing, intersection and traffic light’s logic. alternatively, the non-v2x communications setup did not have access to communications, simulating a current context for urban road environments. the evaluation of the v2x benefits was made by analysing the most relevant criteria like response time, stop time, distance covered by the emergency vehicle. the performance of the system prototype based on these metrics showed a clear improvement when using v2x communications. in particular, the response time was reduced by 41% using v2x, guaranteeing that the use of v2x is a step forward in this context. since the results from the tests using a full simulated setup were positive, a pair of additional experiments integrating real bosch v2x communication boards were planned: one with the integration of these boards into the already simulated environment and another with the integration of these boards on real bosch prototype vehicles. while this later experiment was not possible to realize during this dissertation work time, the former was conducted with successes proving that the it is already possible to develop rd projects that use real v2x communications boards, which is a step closer to test these new technologies on real environments or, at least, on controlled environments that better emulate real environments.",
      "os veículos de emergência tendem a perder tempo desnecessário na sua resposta de emergência. além disso, estudos revelam que os veículos de emergência estão propensos a ter acidentes fatais e, considerando a natureza dos mesmos, que é suposto ajudarem os pacientes, não devem estar envolvidos do outro lado da ajuda médica. além disso, o tempo de resposta é sempre afetado por fatores que o veículo de emergência não consegue controlar, tal como trânsito, semáforos, stops, acidentes. este projeto tem como objetivo principal melhorar o tempo de resposta de um veículo de emergência, enquanto aumenta a segurança do sistema. assim, as comunicações v2x foram utilizadas para obter conhecimento sobre o ambiente, para que o veículo de emergência con-siga escolher as melhores decisões para minimizar o tempo de resposta, enquanto alerta todas as restantes entidades, para que se possam adaptar e não ser um obstáculo para a emergência. neste contexto, comunicações v2x são comunicações entre veículos e todos os outros constituintes do sistema, tal como infraestruturas, pedestres, veículos, e a rede. este documento descreve o trabalho de investigação e desenvolvimento através de uma comparação entre um cenário com e outro sem v2x, cenários feitos num simulador rodoviário e de comunicações veiculares, o veins, utilizando o protocolo americano wave. o cenário com v2x irá usar funcionalidades como as rotas dinâmicas, veículos desviarem-se do veículo de emergência, e lógica dos semáforos e dos stops. entretanto, o cenário sem v2x não terá acesso a quaisquer comunicações, simulando um cenário atual rodoviário. a avaliação dos benefícios do v2x irá resultar da comparação entre o tempo de resposta de um veículo de emergência, o tempo que esteve parado, a distância percorrida, e a emissão de co2. a avaliação do protótipo desenvolvido recai na análise das métricas consideradas, que in-dica uma clara melhoria utilizando comunicações v2x. analisando o tempo de resposta, este foi reduzido em 41% utilizando v2x, garantindo que o uso do vax é uma mais valia neste contexto. considerando que os resultados dos testes utilizando um sistema totalmente simulado foram positivos, um par adicional de experiências integrando placas de comunicações bosch vax reais foi planeado: um com a integração destas placas num sistema de simulação já desenvolvido e outro com a integração destas placas nos veículos protótipo da bosch. enquanto que esta última experiência não foi possível realizar durante o tempo de desenvolvimento desta dissertação, a primeira foi realizada com sucesso provando que é já é possível desenvolver projetos rd que utilizem placas reais de comunicações v2x. assim, os testes destas novas tecnologias em cenários reais mais fáceis, ou, pelo menos, em sistemas controlados que representem melhor ambientes reais."
    ],
    0.0
  ],
  [
    [
      "the time spent analysing a software with the goal of comprehending it is huge and expensive. reduce the time necessary to a professional understand a program is essential for the advance of technology. therefore, the program comprehension has always been an area of interest as realizing how a programmer thinks can help facilitate many of their daily activities, making the developer a more productive worker. as the world begins to reshape itself thanks to the advances of technology, this area of research gains more and more relevance. this project aim to study the tools developed within the comprehension of programs that usually are associated to software maintenance and analysing the animation web tool python-tutor. after this study, it’s required to explore python-tutor to understand how it can be improved with the addition of important features to program comprehension as control flow graph (cfg), data flow graph (dfg), function call graph (fcg) and system control graph (scg). the idea behind this is to allow new programmers to view their programs and create a visual image of them in order to understand them and improving their skills to understand someone else’s programs.",
      "o tempo despendido a analisar um programa de forma a compreendê-lo é enorme e dispendioso. reduzir o tempo necessário para um profissional compreender um programa é fulcral para o avanço da tecnologia. assim, a compreensão de programas sempre foi uma área de interesse pois perceber como um programador pensa pode ajudar a facilitar muitas atividades diárias deste, tornando o programador num trabalhador mais produtivo. à medida que o mundo se vai moldando à informática, esta área de pesquisa tem ganho cada vez mais relevância. neste projecto iremos estudar as ferramentas desenvolvidas no âmbito da compreensão de programas associadas à manutenção de software e analisar a ferramenta de animaçãoweb python-tutor. iremos explorar esta ferramenta de modo a perceber como a podemos melhorar através da inclusão de novos recursos importantes para a compreensão de programas, tais como: o grafo de controlo de fluxo, grafo de fluxo de dados e o grafo de chamadas de funções. a ideia base passa então, por permitir aos novos programadores visualizar os seus programas e criar uma imagem visual destes de modo a os compreenderem e a melhorarem as suas competências para compreenderem programas de outrem."
    ],
    [
      "ad-hoc networks can be useful in many contexts because they can be spontaneously created and do not require any sort of infrastructure. they can be useful for small groups when no other network is accessible. they can also be used in wider areas as a low cost replacement for wireless infrastructure networks with multiple dedicated access points. despite this, ad-hoc networks are not a very popular option for most users. unfortunately, ad-hoc networks are not as user friendly as infrastructure networks. the latter ones usually provide standardized mechanisms that perform the essential configurations for the correct functioning of the network. ad-hoc networks do not have standardized mechanisms adapted to them. each wireless network manager supports a different set of configuration mechanisms. there is usually no problem when every machine uses the same operating system but when different ones are used, users may need to manually perform the required configurations. another cause for this low popularity is the lack of useful and easy to use applications. these applications are usually hosted on the internet, as it provides a larger variety of business models. to tackle these problems, new forms of automatically configuring machines and providing services should be explored. these services must be easy to develop, in order to attract the developers that would develop them. the designed solutions must also be adapted to ad-hoc environments. another important aspect that must be addressed is security. in some contexts, such as public and corporate environments, security can be essential to provide authentication and even to allow the correct functioning of the network.",
      "as redes ad-hoc podem ser uteis em muitos contextos visto poderem ser criadas espontaneamente e não necessitarem de qualquer tipo de infraestrutura. elas podem ser uteis para grupos pequenos quando nenhuma outra rede pode ser acedida. tamb em podem ser usadas em areas amplas como uma alternativa de baixo custo para redes sem os infraestruturadas com vários pontos de acesso dedicados. ainda assim, as redes ad-hoc não são uma opção muito popular para a maioria dos utilizadores. infelizmente, as redes ad-hoc não são tão simples de utilizar como as redes de infraestrutura. estas ultimas normalmente utilizam mecanismos standardizados para efectuar as configurações que são essenciais para o correcto funcionamento da rede. as redes ad-hoc não têm mecanismos standardizados adaptados a elas. cada aplicação gestora de redes sem os suporta um conjunto diferente de mecanismos de confi guração. normalmente não há problemas quando todas as máquinas usam o mesmo sistema operativo mas quando são usados diversos, os utilizadores podem ter que con- figurar manualmente as máquinas. outra causa para esta baixa popularidade e a falta de aplicações uteis e de fácil utilização. estas aplicações são normalmente utilizadas na internet, visto que esta fornece uma maior variedade de modelos de negócio. para abordar estes problemas, novas formas de configurar máquinas automaticamente e de fornecer serviços devem ser exploradas. estes serviços devem ser fáceis de desenvolver, de forma a atrair os developers que os irão desenvolver. as solu ções desenhadas também devem estar adaptadas a ambientes ad-hoc. outro aspecto importante que tem de ser focado e a segurança. em alguns contextos, tais como ambientes públicos e empresariais, a segurança pode ser essencial para fornecer autenticação e mesmo permitir o correcto funcionamento da rede. neste trabalho, uma framework funcional que trata estes problemas e desenhada e implementada. a framework e capaz de efectuar automaticamente as configurações necessárias a criação de um ambiente ad-hoc funcional e seguro. e também capaz de fornecer serviços especializados e de fácil desenvolvimento, sobre o ambiente seguro criado de forma espontânea. a framework desenvolvida e fácil de utilizar e foi exaustivamente testada em android e linux, embora possa ser facilmente estendida de forma a funcionar em muitos outros sistemas operativos."
    ],
    0.0
  ],
  [
    [
      "no âmbito da dissertação, incluída no plano curricular do mestrado em engenharia de redes e serviços telemáticos, foi proposta a realização de um período de estudo a um prestador de serviços de um isp (internet service prqv/del) que fornece tecnologias de acesso de voz, internet e televisão (tv) em cobre e fibra. este prestador de serviços é responsável por toda a construção e implementação das infraestruturas de redes, tanto de cliente como de acesso público. todo este trabalho em backgroundé o que suporta e dá vida às redes de nova geração. sem ele, não seria possível construir esta gigante rede a que se chama \"internet\", que permite o acesso facilitado à comunicação e troca de informações. com este documento pretende apresentar-se a metodologia para a criação de infraestruturas de rede cio operador, de modo a fazer chegar os serviços 3play (voz, tv e internet) à casa do cliente. o caso de estudo apresentado visa analisar a implementação de uma rede de fibra ótica na ribeira grande, ilha de são miguel, açores, tendo em consideração as especificidades insulares, como é o caso da distância ao continente e os perigos geológicos associados. são exemplos destes perigos os sismos e os movimentos de vertentes.",
      "within the scope of dissertation, included in the curriculum of the master course in engineering of computer networks and teiematic services, it was proposed to carry out a period of study in a service provider, related to an isp (internet service provider), which supplies access to voice, internet and television technologies, over copper and fiber. this service provider is responsible for ali the construction and implementation of network infrastructures, related with both client and public access. ali this background work is what supports and gives life to the next generation networks. without it, it wouid not be possible to build this giant network caiied \"interner, that aliows easy access to communication and information exchange. this document presents the methodoio' used to design the network operator's infrastructure, 50 that the 3play service (voice, tv and internet) can be delivered to the consumer's house. the presented case study aims to analyze the implementation of a fiber optic network in ribeira grande, sao miguel istand, azores, taking into account the specificities of the island, such as the distance to the mainland and geological hazards. examples of these hazards are earthquakes and landslides."
    ],
    [
      "o estudo da conectividade, estrutura, e integração das funções cerebrais é actualmente uma das ferramentas de maior importância na compreensão do cérebro humano. a realização destes estudos via aquisições de ressonância magnética exige no entanto acessibilidade e disponibilidade constante de informação. a quantidade de procedimentos e técnicas de análise, associado à produção de grandes volumes de dados e multitude de soluções de software são alguns dos principais entraves à organização, manutenção e partilha de estudos neuroimagiológicos. desta forma, o objectivo principal deste trabalho consiste na concepção de um fluxo de processamento, que possa servir de padrão à conjugação de resultados de análises multimodais, através de uma estrutura de estudos definida e nomenclatura de ficheiros própria. tendo por base este fluxo foi desenvolvida uma aplicação, designada brainarchive, para automatização do processo de organização e partilha de estudos neuroimagiológicos. esta permite por sua vez a disponibilização de grandes volumes de informação sem necessidade de caracterização manual de cada ficheiro, ao mesmo tempo que se revela uma ferramenta simples e intuitiva na aquisição de dados. o protótipo desenvolvido responde às necessidades do seu contexto de utilização e, por conseguinte, é espectável que potencie o processo de investigação, através da simplificação e redução do tempo despendido na organização e partilha de informação.",
      "the study of the connectivity, structure and integration of brain functions is nowadays one of the most relevant tools in the comprehension of the human brain. the realization of these mri studies requires constant accessibility and availability of information. the amount of procedures and analytical techniques, associated with the production of large volumes of data and multitude of software solutions are among the main obstacles to organization, maintenance and sharing of neuroimagiology studies. thus, the main objective of this project consists on the conception of a processing flow, which can serve as a standard to the combination of results from multimodal analysis, through a defined study structure and file nomenclature. based on this flow was developed an application, designated brainarchive, for automating the process of organization and sharing of neuroimagilogy studies. this in turn allows the delivery of large volumes of data, without the need for manual characterization of each file, while reveals a simple and intuitive tool for data acquisition. the developed prototype meets the needs of the context of use and therefore it is expected to enhance the research process, through the simplification and reducing of the time spent organizing and sharing information."
    ],
    0.021428571428571425
  ],
  [
    [
      "the edge computing paradigm aims at leveraging the computational and storage capabilities of internet of things (iot) devices, while resorting to cloud computing services for more demanding processing tasks that cannot be done at commodity devices. however, deploying distributed services across edge and cloud nodes raises new challenges that must be addressed. namely, the choice of what nodes run each service component may be critical for ensuring an efficient service for users. for example, if two critical components, that must frequently exchange data, are placed in different geographic locations, the whole performance of the service will be affected. therefore, these geographically dispersed environments demand new orchestration and distribution systems for hybrid cloud and edge environments, based on geographic location, service demand, business objectives, laws, and regulations. this thesis proposes geolocate, a generic scheduler for workload orchestration and distribution across heterogeneous and geographically distant nodes. in more detail, it provides the design and implementation of a scheduling and placement algorithm based on nodes’ geographic location and resource availability and a fully functional prototype, integrating geolocale with kubeedge, an edge computing orchestration platform based on kubernetes. the experimental results show that as the network latency and amount of data being transmitted between nodes increases, so does the response time for applications resorting to these distributed deployments. our evaluation of an e-commerce application shows that the use of geolocate can reduce, relative to kubeedge’s default-scheduler, the average response time for requests by about 85%.",
      "o paradigma da computação na borda visa alavancar as capacidades computacionais e de armaze namento dos dispositivos internet of things (iot), ao mesmo tempo que recorre aos serviços de computa ção em nuvem para tarefas de processamento mais exigentes que não podem ser feitas em dispositivos comuns. no entanto, a implementação de serviços distribuídos através de nós na nuvem e na borda levanta novos desafios que devem ser resolvidos. nomeadamente, a escolha dos nós que executam cada componente do sistema pode ser fundamental para assegurar um serviço eficiente para os utilizadores. por exemplo, se dois componentes críticos, que devem frequentemente trocar dados, forem colocados em localizações geográficas diferentes, todo o desempenho do serviço será afectado. assim sendo, es tes ambientes geograficamente dispersos necessitam de novos sistemas de orquestração e distribuição para ambientes híbridos de cloud e edge, com base na localização geográfica, utilização dos serviços, objectivos empresariais, leis, e regulamentos. esta tese propõe o sistema geolocate, um scheduler genérico para orquestração e distribuição de cargas de trabalho em nós heterogéneos e geograficamente distantes. em detalhe, esta tese fornece o design e implementação de um algoritmo de scheduling baseado na localização geográfica dos nós e na disponibilidade de recursos, e ainda um protótipo totalmente funcional, integrando geolocale com kubeedge, uma plataforma de orquestração computacional de borda baseada em kubernetes. os resultados experimentais mostram que à medida que a latência da rede e a quantidade de dados transmitidos entre nós aumenta, aumenta também o tempo de resposta das aplicações que recorrem a estas implantações distribuídas. a nossa avaliação de uma aplicação de e-commerce mostra que a utilização de geolocate pode reduzir, relativamente ao scheduler por defeito de kubeedge, o tempo médio de resposta aos pedidos em geral em cerca de 85%."
    ],
    [
      "a dissertação insere-se no projeto zeroskin, um projeto que pretende fornecer uma solução a baixo custo para renovação da fachada de edifícios que não vão de encontro com os objetivos europeus para 2050 em termos de eficiência energética. a dissertação acrescenta inteligência à fachada com a capacidade de sensorizar o meio, recolhendo dados para a integração da fachada numa casa inteligente. a dissertação resulta em duas versões de protótipos de sensores com comunicação sem fios. a comunicação será assegurada pelos microprocessadores xbee que comunicam através do protocolo de comunicação z-wave, evitando interferências com os restantes equipamentos wireless presentes na habitação, garantido um alcance até 100 metros em ambiente urbano. a primeira das versões possui capacidade de medir a temperatura ambiente, a humidade relativa e a luminosidade. esta versão consegue medir temperatura num intervalo de -50 até 150 graus celsius, medir humidade relativa entre 30 e 90 % e medir iluminância entre 0 e, aproximadamente, 1000 lux. a segunda versão foi desenvolvida para garantir o correto funcionamento da fachada, detetando falhas através da intrusão de água, possuindo sensores de vazamento instalados nos pontos críticos da fachada renovada. as medições destes sensores foram dividas em três intervalos, ausência de água, presença de alguma humidade e presença de água. o protótipo final resultou num protótipo de baixo custo, baixa manutenção, dimensões reduzidas e baixa pegada na fachada, baixo consumo energético e autónomo energeticamente. a autonomia do módulo de sensores é assegurada através de uma bateria de lítio, capaz de alimentar o módulo durante 9 dias, ligada a um gestor de carga, alimentada por um painel solar. resultando num módulo compacto e capaz de permanecer em funcionamento durante longos períodos, desde que exposto esporadicamente a luz solar. a suportar as funcionalidades que o hardware desenvolvido possui, um programa em python controla as comunicações no recetor central, enviando comandos, processando os dados recebidos, terminando a apresentar ao utilizador e gravando em ficheiro os dados processados.",
      "the dissertation is part of the project zeroskin, a project that want give a rentable solution for the renovation of the building’s facade, on building where the energy efficient do not meet the 2050 european objectives. the dissertation add intelligence to the façade with the capability of sensing the environmental, collecting data for the integration of the façade in a smart home. the dissertation results in two versions of the prototypes with wireless communication. the communication will be assured by the xbee microprocessors which communicate thought the z wave communication protocol, avoiding interference with others wireless equipment present in the habitation, guaranteed a range of 100 meters in urban environment. the first version of the prototype has the capacity to measure ambient temperature, relative humidity and luminosity. this version is capable of measure temperature in a range of 50 to 150 celsius, measure relative humidity between 30 and 90 % and measure illuminance between 0 and, approximately, 1000 lux. the second version has developed to guarantee the correct façade perform, detecting faults through the intrusion of water, having four leak sensors which will be instigated in the critical points of the renovated façade. the sensor’s measurement was divided in three gaps, lack of water, some condensation and water presence. the final prototype result in a prototype with low cost, low maintenance, small dimensions and low footprint in the façade, low energy consumption and energetic autonomous. the sensor module’s autonomy is assured thought a lition battery, capable of feed the module during 9 days in function, connected at a charge gestor feed by a solar cell. resulting in a compact module and capable of maintaining functions during long periods, meanwhile sporadic exposure to sun light. supporting the hardware functionalities developed, a program in python controls the communications in the central receptor, sending commands, processing the data received, finishing showing the user and writing in a document the data processed."
    ],
    0.3
  ],
  [
    [
      "presentemente, plataformas cooperativas para edição de partituras musicais, como a wiki::score que utiliza a notação abc, não têm à sua disposição utilitários de avaliação e deteção de erros, nem ferramentas que auxiliem a musicologia. esta carência impede os utilizadores de tirarem o melhor partido dessas plataformas e proporciona um sentimento de limitação na composição e transcrição de partituras. para colmatar estas falhas, e adotando a filosofia utilizada pelo sistema operativo unix, criar-se-á um toolkit, em que cada ferramenta trata um problema individualmente, como a deteção e correção de erros sintáticos, léxicos, entre outros. para que estas ferramentas tenham uma componente musicológica como a análise tonal e deteção de padrões, é necessária a construção de corpora de obras musicais, onde, após análise, é possível extrair conhecimento que será integrado nas ferramentas criadas ou exibido ao utilizador num formato específico.",
      "abc [58] é uma notação musical simples mas poderosa que permite a produção de partituras completas e profissionais. atualmente, existe uma escassez de ferramentas genéricas para processamento de notação musical, particularmente para abc. esta dissertação apresenta o abc::dt, uma linguagem de domínio específico [39, 38] baseada em regras (embutida em perl), projetada para simplificar a criação de ferramentas para processamento de abc. inpiradas na filosofia unix, essas ferramentas pretendem ser simples e composicionais à semelhança dos filtros unix. a partir das regras do abc::dt obtém-se uma ferramenta para processamento de abc cujo algoritmo principal segue a arquitetura de um compilador tradicional, dessa forma consistindo em três fases: 1) parsing de abc (baseado no parser do abcm2ps [46]), 2) transformação semântica de abc (associada a atributos abc) e 3) geração de output (um gerador definido pelo utilizador or fornecido pelo sistema). umconjunto de ferramentas para processamento de abc foi desenvolvido utilizando o abc::dt. cada uma delas tem uma finalidade única, desde detetar erros, a auxiliar no estudo de música e até imitar o comportamento de algumas ferramentas unix. estas têm o objetivo de serem provas de conceito e ainda podem ser melhoradas, no entanto demonstram quão facilmente ferramentas compactas para processamento de abc podem ser criadas. umteste e avaliação foram realizados a uma das ferramentas criadas (canon_abc) com uma partitura abc real, o canon de pachelbel.",
      "abc [58] is a simple, yet powerful, textual musical notation which allows to produce professional and complete music scores. presently, there is a lack of music notation general processing tools, particularly for abc. this dissertation presents abc::dt, a rule-based domain-specific language (dsl) [39, 38] (perl embedded), designed to simplify the creation of abc processing tools. inspired by the unix philosophy, those tools intend to be simple and compositional in a unix filters’ way. from abc::dt’s rules an abc processing tool whose main algorithm follows a traditional compiler architecture is obtained, therefore consisting of three stages: 1) abc parsing (based on abcm2ps’ [46] parser), 2) abc semantic transformation (associated with abc attributes) and 3) output generation (either a user defined or system provided abc generator). a set of abc processing tools was developed using abc::dt. every one of them has its single purpose, from error detection, to aiding in music studying and even imitating some unix tools behavior. they are intended to be proof of concept and can still be improved, yet they demonstrate how easily compact abc processing tools can be created. a test and evaluation were done to one of the created abc processing tools (canon_abc) with a real abc score, pachelbel's canon."
    ],
    [
      "a internet está em constante evolução, bem como as aplicações e serviços disponibilizados aos utiliza dores. com o crescimento na adoção de plataformas como youtube, netflix, disney + , hbo e amazon prime video, é possível ter acesso a um amplo conteúdo em formato de vídeo. estas mudanças na internet obrigam a que se criem formas de melhorar a experiência de utilização nestes novos cenários. o quic é um novo protocolo de transporte considerado atualmente uma peça chave no suporte à nova norma http/3. este protocolo opera sobre user datagram protocol (udp) e visa oferecer um serviço de transporte multistream, rápido, robusto e seguro, que permite contornar limitações conhecidas do protocolo de transporte transmission control protocol (tcp), a base para os atuais protocolos http/1 e http/2. este trabalho, propõe avaliar o protocolo de transporte quic usado como base no recém-imple mentado http/3, em alternativa ao http/2, assente em tcp. como já existem alguns estudos que comparam estes protocolos no acesso a páginas web, neste trabalho é analisado o protocolo http/3 no contexto de streaming de vídeo, em cenários que possam existir condições adversas de rede. pretende se, neste trabalho, desenvolver uma plataforma experimental que consiga transmitir vídeo recorrendo a estes protocolos e analisar esta transmissão para perceber se o protocolo quic realmente será útil para a web em constante evolução. é desenvolvido um software, com recurso à framework hls.js, com o objetivo de receber e reproduzir o vídeo, recolhendo determinadas métricas relativas à reprodução. é, ainda, criada uma ferramenta para analisar e gerar gráficos, para os dados adquiridos. são apresentados alguns resultados, ainda que preliminares, que permitem observar diferenças entre os protocolos http/2 e http/3. estas diferenças são mais notórias em condições adversas de rede, em que o http/3 proporcionou uma transmissão de vídeo mais fluída, com menor latência, menor drift e maior carga de buffer, na generalidade dos cenários escolhidos.",
      "the internet as well as the user applications and services supported are constantly evolving. with the growing adoption of platforms like youtube, netflix, disney + , hbo, and amazon prime video, access to a wide range of video content is now possible. these changes on the internet require the creation of ways to enhance the user experience in these new scenarios. quic is a new transport protocol currently considered a key component in supporting the new http/3 standard. this protocol operates over user datagram protocol (udp) and aims to provide a fast, robust, and secure multistream transport service that overcomes known limitations of the transmission control protocol (tcp) transport protocol, which is the basis for current http/1 and http/2 protocols. this study proposes to evaluate the quic transport protocol as the basis for the newly implemented http/3, as an alternative to http/2 based on tcp. since there are already some studies comparing these protocols when accessing web pages, this study analyses them in the context of video streaming, in scenarios where adverse network conditions may exist. this work aims to develop an experimental platform capable of transmitting video using these protocols and analyse this transmission to understand if the quic protocol will indeed be useful for the constantly evolving web. a software is developed utilizing the hls.js framework with the aim of receiving and playing back the video while collecting specific playback metrics. additionally, a tool is created to analyze and generate graphs based on the acquired data. presented preliminary results allow for the observation of differences between the http/2 and http/3 protocols. these differences become more pronounced under adverse network conditions, where http/3 facilitated smoother video transmission, with lower latency, reduced drift, and increased buffer load, across the majority of the chosen scenarios."
    ],
    0.0
  ],
  [
    [
      "a mudança contínua do mercado de software, acompanhado pelo constante aparecimento de novas tecnologias, pressiona as equipas de tecnologia a entregarem os seus projetos mais rapidamente sem comprometer a qualidade do produto. assim, o surgimento de plataformas low-code (lcp) tornou-se inevitável e rapidamente se alastrou pelo mercado, sendo estimado que em 2023, 50% das médias e grandes empresas recorram a este tipo de plataformas. o desenvolvimento low-code de software é um paradigma emergente, que combina a utilização mínima de código com interfaces gráficas interativas, que possibilita o desenvolvimento rápido de aplicações. assim, esta dissertação tem como objetivo o estudo de plataformas low-code, passando para um estudo pormenorizado das plataformas mendix, ms powerapps e outsystems. este estudo consiste na análise de várias características destas plataformas, sendo feitas também algumas comparações entre estas. para além disso, é ainda feito um estudo empírico usando vários participantes de várias universidades. deste modo, este estudo pode também ser usado para avaliar a usabilidade das plataformas.",
      "the continuous change of the market, together with the recurrent evolution of new technologies, pres sures technology teams to deliver their projects faster without compromising product quality. thus, the emergence of low-code platforms (lcp) became inevitable and quickly spread through the market. it is estimated that by 2023, 50% of medium and large companies will use this type of platform. low-code software development is an emerging paradigm that combines the minimal use of code with interactive graphical interfaces, which allows for faster software applications’ development. therefore, this dissertation aims to study low-code platforms, going through a detailed study of mendix, ms powerapps, and outsystems platforms. this study consists of the analysis of several features of these platforms, being made also some comparisons between them. additionally, this study can also be used to evaluate the usability of the platforms."
    ],
    [
      "ao longo dos últimos anos, a aplicação de modelos de machine learning (ml) tem desempenhado um papel fundamental na nossa sociedade, ajudando a obter conhecimentos sobre grandes quantidades de dados. no entanto, existe uma preocupação quanto à clareza da forma como estes modelos chegam às suas conclusões. por outras palavras, existe uma ”blackbox”sobre a forma como estes modelos chegam às suas conclusões, quer sejam problemas de classificação ou de regressão. atualmente, a explicabilidade tornou-se uma peça fundamental no desenvolvimento e aplicação de modelos de previsão para séries temporais. em contextos onde a tomada de decisões é sensível, requerem fundamentação sólida e os dados são maioritariamente séries temporais, modelos com explicabilidade oferecem vantagens significativas. a transparência nas previsões não só ajuda a identificar possíveis falhas, mas também facilita a adoção desses modelos em ambientes críticos, promovendo uma integração mais eficaz da inteligência artificial (ia) no dia a dia. deste modo, esta dissertação visa a implementação de técnicas de explicabilidade num modelo longshort term memory (lstm), com o objetivo de aumentar a interpretabilidade deste modelo de blackbox. os dados utilizados para treinar o modelo, e efetuar o estudo da explicabilidade, são relativos a emissões de vários poluentes na cidade do porto. desta forma, cada feature é uma série temporal que contribui para prever as próximas 24 horas do dióxido de nitrogénio (no2). o modelo foi treinado inicialmente sozinho e depois foi inserida uma camada de attention no início. desta forma, foi possível extrair explicabilidade ante-hoc da camada de attention e ainda aplicar estratégias post-hoc, como o shapley additive explanations (shap) e feature permutation, ao modelo com e sem attention. através dos resultados obtidos, é possível perceber que o modelo foi capaz de identificar dependências temporais e relações não lineares nos dados que foram cruciais para o ”raciocínio”e tomada de decisão destes. estas relações apenas foram possíveis de observar graças aos métodos de explicabilidade implementados, que, pela sua diversidade e convergência nos resultados, transmitem segurança e confiança na sua veracidade.",
      "over the last few years, the application of ml models has played a fundamental role in our society, helping to gain insight into large amounts of data. however, there is concern about the clarity of how these models reach their conclusions. in other words, there is a ”blackbox”about how these models reach their conclusions, whether they are classification or regression problems. nowadays, explainability has become a key element in the development and application of forecasting models for time series. in contexts where decision-making is sensitive, requires solid reasoning and the data is mostly time series, models with explainability offer significant advantages. transparency in predictions not only helps to identify possible biases or failures, but also facilitates the adoption of these models in critical environments, promoting a more effective integration of artificial intelligence into everyday life. therefore, this dissertation aims to implement explainability techniques in a lstm model, in order to increase the interpretability of this blackbox model. the data used to train the model and carry out the explainability study refers to emissions of various polutants in the city of porto. thus, each feature is a time series of one of the gases measured and is used to predict the the next 24 hours of no2 concentrations. the model was initially trained alone and then a attention layer was inserted at the beginning. in this way, it was possible to extract ante-hoc explainability from the attention layer and also apply post-hoc strategies, such as shap and feature permutation, to the models with and without attention. from the results obtained, it can be seen that the model was able to identify temporal dependencies and non-linear relationships in the data that were crucial for their ”reasoning”and decision-making. these relationships were only possible to observe thanks to the explainability methods implemented, which, due to their diversity and convergence in the results, convey security and confidence in their veracity."
    ],
    0.3
  ],
  [
    [
      "nowadays, cybernetic attacks are a real threat that can compromise any individual, organization or company’s integrity. every day new cases are reported, that show the real damage cyber criminals can cause. sensitive data exposure, identity theft, service malfunctioning or shutdown are just a few of the most common threats, which in many cases might impact companies either with financial loss or by damaging their reputation. population, in general, is becoming each time more aware of the risks of using electronic devices connected to the web and so are companies. with the rise of this awareness, over the last years, cyber security has become a major concern for software companies. this threat also led to the birth of the software vulnerability detection market. companies started commercializing software and advisory to other companies, in order to keep them less exposed to cybernetic risks. there are many mechanisms and technologies used by these companies to identify vulnerabilities in applications. the most popular technology used to detect vulnerabilities is sast (static application security testing) as it focus on the detection of vulnerabilities at the early stages of software development. however, this requires the analysis of the source code, which in many cases, is huge and thus such analysis is too time consuming. being that the context and motivation for this dissertation, the goal is to investigate the possibility of performing source code analysis in a faster way, relying on machine learning approaches. code embeddings, classification algorithms and clustering algorithms were the main approaches explored in this work. along the project, it was realized that some approaches performed better than others, in the task of detecting software vulnerabilities. clustering algorithms, according to the performed experiments, are not suitable for the problem. classification algorithms produced results that can be considered worthy of further investigation, but did not meet the established goals. after some failed attempts, this project demonstrated that it is possible to train a prediction model, based on code2seq approach, capable of detecting vulnerabilities in source code, with better performance and accuracy than classic sast solutions (according to a specific set of experiments). moreover, the used approach allows to easily extend the developed work to find vulnerabilities in any programming language.",
      "hoje em dia, os ataques cibernéticos são uma ameaça real que podem comprometer a integridade de qualquer indivíduo, organização ou empresa. novos casos são reportados todos os dias, demonstrando os estragos que um atacante cibernético pode causar. exposição de dados sensíveis, roubo de identidade, mau funcionamento ou corte de fornecimento de serviços são apenas algumas das ameaças mais comuns, que em muitos casos podem afetar as empresas com perdas financeiras ou prejudicando a sua reputação. a internet das coisas cresce rapidamente todos os dias. com a criação de dispositivos controlados por computador, desde relógios inteligentes até máquinas de lavar programáveis à distáncia, cada vez mais eletrodomésticos e outros acessórios eletrónicos estão diariamente ligados à internet. estes dispositivos tornam as nossas vidas mais fáceis e mais confortáveis, contudo podem expôr uma porta a redes corporativas para atacantes cibernéticos dotados e criminosos cibernéticos. a população em geral está a ficar cada vez mais consciente dos riscos de usar dispositivos ligados a rede, e as empresas também. com o crescimento desta consciência, ao longo dos últimos anos, a segurança cibernética tornou-se uma maior preocupação para as empresas de software. esta ameaça também deu origem ao nascimento do mercado de deteção de vulnerabilidades em software. as empresas começaram a comercializar software e aconselhamento a outras empresas, com o objetivo de as manter menos expostas aos riscos cibernéticos. há muitos mecanismos e tecnologias usadas por estas empresas para detetar vulnerabilidades em aplicações. a tecnologia mais popular para identificar vulnerabilidades é o sast (static application security testing), que se foca na deteção de vulnerabilidades logo nas primeiras fases do desenvolvimento de software. contudo, exige a análise de código-fonte, o que nos casos de programas de grandes dimensões pode ser muito demorada. sendo esta a motivação para esta dissertação, o objetivo é investigar a possibilidade de analisar código-fonte mais rapidamente, recorrendo a técnicas de machine learning. este trabalho demonstra que e possível treinar um modelo capaz de detetar vulnerabilidades em código-fonte, com bom desempenho e acerto aceitável. para além disso, as tecnologias usadas permitem estender facilmente o trabalho desenvolvido para detetar vulnerabilidades em qualquer linguagem de programação."
    ],
    [
      "atualmente os chatbots são usados por diversas organizações para automatizar tarefas. os chatbots são desenvolvidos para diversos casos de uso, desde ajudar os utilizadores a navegar nas aplicações até resolver problemas que os utilizadores encontram. no entanto, a criação de um chatbot exige recursos monetários e de conhecimento. assim, a motivação deste projeto passa é permitir a democratização de criação de chatbots com o desenvolvimento da ferramenta chatbotwizard, permitindo que um utiliza dor possa criar um chatbot sem grande conhecimento tecnológico, seja o chatbot de elevado grau de complexidade ou não. o chatbotwizard usa o rasa como sistema de diálogo, permitindo integrar vários módulos para a criação de um chatbot. os módulos disponíveis no chatbotwizard são: módulo para a extração de entida des, módulo realizar pedidos a api, módulo de template para construir texto a partir de json e módulo de question answering (qa) baseado em transformers (bert). estes módulos podem ser conectados para criar o fluxo do chatbot desejado. do chatbotwizard fazem parte dois componentes: o backend e o chatbotwizard web. o chatbotwizard web permite a utilizadores criarem os seus chatbots integrando e configurando os diversos módulos. o backend tem a responsabilidade de receber o fluxo do chatbot e criar um chatbot baseado no rasa. com o desenvolvimento do chatbotwizard conseguiu-se uma aplicação que permite o utilizador criar chatbots e integrar os mesmos no telegram. e por fim, foi criado um caso de estudo baseado numa api pública.",
      "in this age, chatbots are used by several organizations to automate tasks. the chatbots are developed for a variety of use cases, from helping users navigate applications to solve issues that the users find. nonetheless, the development of a chatbot require monetary and knowledge resources. so, the motivation of this project is to allow the democratization of creation of chatbots with the development of our tool named chatbotwizard, allowing a user to create a chatbot without great technological knowledge, whether the chatbot is of a high degree of complexity or not. the chatbotwizard uses rasa as dialog system, allowing to integrate several modules for a criation of a chatbot. the modules available in chatbotwizard are: a module to extract entities, a module to do api requests, a template module to create text from json and a module for question and answering based in transformers (bert). this modules can be connected to create a chatbot flow.. chatbotwizard have two components: the backend and the chatbotwizard web. the chatbotwizard web allows the users to create chatbots, integrating the various modules. the backend is responsible to receive a chatbot flow and create the chatbot using rasa as template. with the development of the chatbotwizard, the application allows a user to create chatbots and integrate them into telegram. finally, a case study based on a public api was created to show how to use the chatbotwizard"
    ],
    0.3
  ],
  [
    [
      "ataxia is a neurological sign indicative of dysfunction in the cerebellum, a part of the brain that coordinates movement. the typical symptoms include lack of balance when walking or standing, loss of limb coordination, change in speech, and difficulty with fine motor tasks, strongly affecting the person’s daily activities. cerebellar ataxia can be inherited or caused by other disorders such as stroke, multiple sclerosis, or cerebral palsy. while there is no known cure for inherited ataxia, the symptoms can be managed through intensive and personalised rehabilitation, including physical, speech, and occupational therapy. physical therapy, also known as conventional therapy, is a standard practice in the rehabilitation of patients with ataxia. it focuses on performing different physical exercises repeatedly, where a therapist monitors the session and regulates the intensity. although effective, the repetitiveness of conventional therapy can become monotonous, besides being a very time-consuming process, which can be cumbersome for the therapist. without any additional stimulation and decreasing motivation, patients may experience stagnation or even drop out of therapy. robot-assisted gait training (ragt) is being introduced in the rehabilitation of persons with motor disabilities. this technology allows personalised and intensity-adapted training, which could benefit the patient’s recovery. however, this therapy can also become monotonous, so there is a need for more interactive and appealing strategies. a possible solution to this problem is the development of exergames (serious games) and their inclusion in robotic-assisted therapy. this can become advantageous since the robotic devices integrate several sensors that read the patient’s movement and can be used as controllers in the game, allowing a more immersive experience. considering this, this dissertation proposes two serious videogames, which were integrated into a robotic walker, the walkit smart walker, intended for gait ataxia rehabilitation. the first game is cognitive, whose goal is to react to a certain shape or sound as quickly as possible. with this game, it is intended to study the influence of dual tasking on motor rehabilitation. the second game is a dynamic one whose goal is to incite the patient’s balance control. this game uses an algorithm for torso orientation estimation to control an avatar holding two buckets full of water. the main goal is to give patients biofeedback regarding their postural balance and to exercise their balance with specific events, or minigames, that encourage them to lean in a way that causes weight transfer. both games were validated with healthy volunteers, in terms of functionality and usability. the results allowed to conclude that both were fun to play and showed potential to aid rehabilitation. moreover, the results emphasised the relevance of customisation, replayability, and aesthetics in serious games. future work includes the thorough validation of both serious games with patients with cerebellar ataxia, to assess their effectiveness in rehabilitation along with walkit smart walker.",
      "a ataxia é um sinal neurológico indicativo de disfunção no cerebelo, uma parte do cérebro que coordena o movimento. os sintomas típicos incluem a falta de equilíbrio ao caminhar ou ficar de pé, perda de coordenação dos membros, alteração na fala e dificuldade em tarefas motoras finas, gravemente afetando as atividades diárias da pessoa. a ataxia cerebelar pode ser herdada ou causada por outros distúrbios, como acidentes vasculares cerebrais, esclerose múltipla ou paralisia cerebral. embora não haja cura conhecida para a ataxia hereditária, os sintomas podem ser controlados através de reabilitação intensiva e personalizada, incluindo fisioterapia, terapia da fala e terapia ocupacional. a fisioterapia, também conhecida como terapia convencional, é a norma na reabilitação de pacientes atáxicos. foca-se em fazer diferentes exercícios físicos de forma repetida, juntamente com um terapeuta que regula a intensidade. embora eficaz, a repetitividade da terapia convencional pode tornar-se monótona. sem estímulo adicional e motivação decrescente, os pacientes podem sentir estagnação ou até mesmo abandar a terapia. o treino de marcha assistida por robô está a ser introduzido na reabilitação de pessoas com deficiência motora. esta tecnologia permite treino repetitivo e adaptado, o que pode beneficiar a recuperação do paciente. no entanto, esta terapia também se pode tornar monótona, pelo que são necessárias estratégias mais interativas e apelativas. uma possível solução é o desenvolvimento de exergames (jogos sérios) e sua inclusão na terapia assistida por robôs. pode ser vantajoso uma vez que os dispositivos robóticos integram vários sensores que leem o movimento do paciente e podem assim ser usados como controladores no jogo, permitindo uma experiência mais imersiva. esta dissertação propõe dois videojogos sérios, que foram integrados num andarilho robótico, o walkit smart walker, criado para a reabilitação da ataxia de marcha. o primeiro jogo sério é um jogo cognitivo, cujo objetivo é reagir a uma determinada imagem ou som o mais rápido possível. este jogo pretende estudar a influência do dual tasking na reabilitação motora. o segundo jogo é um jogo dinâmico, cujo objetivo é estimular o controlo do equilíbrio do paciente. este jogo usa um algoritmo para estimar a orientação do tronco, para controlar um avatar que segura dois baldes cheios de água. o principal objetivo é caminhar sem entornar os baldes e, desta forma, dar aos pacientes biofeedback sobre seu equilíbrio postural e exercitar seu equilíbrio com eventos específicos, ou minijogos, que os estimulem a se inclinar de forma a causar transferência de peso. ambos os jogos foram validados com participantes saudáveis, em termos de funcionalidade e usabilidade. os resultados permitiram concluir que ambos foram divertidos de jogar e mostraram potencial para ajudar na reabilitação. os resultados enfatizaram a importância da personalização, replay value e estética em jogos sérios. o trabalho futuro inclui a validação exaustiva dos dois jogos sérios com pacientes com ataxia, de forma a avaliar a sua eficácia na reabilitação juntamente com o andarilho walkit smart walker."
    ],
    [
      "existe uma preocupação cada vez maior em relação a quantidade de informação gerada e recebida por diversas instituições não só no que concerne ao consumo excessivo de papel, como também à gestão de grandes quantidades de informação. com o intuito de simplificar esta gestão documental, foi criado o “projeto m51-clav-arquivo digital: plataforma modular de classificação e avaliação da informação pública” que visa a classificação e a avaliação de toda a documentação que circula na administração publica portuguesa, utilizando um referencial comum, a lista consolidada (lc), que permite o desenvolvimento de instrumentos de natureza transversal a aplicar em contexto organizacional. estando o projeto já numa fase avançada, a lc foi modelada como uma ontologia owl e encontra-se disponível para consulta na plataforma do clav, sendo que estão em desenvolvimento outras funcionalidades com o fim de tornar esta plataforma capaz da gestão documental referida. uma dessas funcionalidades pretendidas e a criação de instrumentos de classificação e avaliação, chamados de tabelas de seleção (ts), que é o objetivo principal desta dissertação. pretende-se então tornar a plataforma capaz do processo de criação de uma ts de uma forma simples, rápida e intuitiva, e que se prende com o preenchimento de um formulário com vários passos. após este preenchimento, é gerado um pedido de submissão de uma nova ts, pedido esse que terá de passar por uma tramitação para que seja validado e, caso tal aconteça, a ts passara a ser um instrumento de extrema importância para avaliar e classificar os documentos produzidos pela(s) entidade(s) em causa, ou seja, permite uma melhor gestão documental.",
      "there is a growing concern about the quantity of generated and received information by multiple institutions. regarding not only the excessive consumption of paper, but also the managment of such huge quantity of information. with the purpose of simplifying the documental managment, the clav project was created. this project aims at the classification and the evaluation of all the documentation that exists in the portuguese public administration, using a common referential, the lista consolidada, that allows the development of broad nature instruments to apply in organizational context. the lc was modelled as an owl ontology and it is available for consultation in the clav platform. other features are under development in order to make this platform capable of the mentioned documental managment. one of these features is the creation of classification and evaluation instruments, known as tabelas de seleção, which is the main focus of this dissertation. the main purpose of this disseration is to make the platform capable of the tabela de seleção creation in a simple, fast and intuitive way, by filling a form with different steps. once the form is complete, it is submited one request to create a new ts, that request then goes into different stages until it is validated. once validated, the ts becomes one very important instrument to evaluate and classify the documents produced by one or more entities, allowing a better documental managment."
    ],
    0.3
  ],
  [
    [
      "com o amadurecimento e ampla utilização das bases de dados nosql tem havido um interesse crescente na adição de transações multi-linha, que proporcionem as propriedades acid sem comprometer o desempenho e capacidade de escala destes sistemas. apesar das propostas nesta área assentarem em técnicas bem conhecidas de bases de dados, como a multi-versão e a recuperação, a sua aplicação está agora enquadrada em pressupostos diferentes, não sendo claro que os compromissos tradicionais se mantenham. neste contexto, este trabalho sistematiza os compromissos relacionados com a escolha de um mecanismo de recuperação, que garante que as alterações efetuadas por uma transação confirmada persistem atomicamente. além de analisar qual o impacto na arquitetura do sistema da escolha do mecanismo de recuperação, comparamos experimentalmente as alternativas mais interessantes com diferentes cargas de trabalho.",
      "with the maturing and extensive use of nosql databases there has been a growing interest in adding multi-line transactions that provide acid properties without compromising the performance and scalability of these systems. although proposals in this area are based on traditional database techniques such as multi-version and recovery, their application is now framed in different assumptions, and it is not clear that traditional compromises remain. in this context, this work systematizes the compromises related to the choice of a recovery mechanism, which guarantees that the changes made by a commited transaction persist atomically. in addition to analyzing the impact on the system architecture of the choice of the recovery mechanism, we compared experimentally the most interesting alternatives with different workloads."
    ],
    "we have an increasingly sedentary population without the concern about a healthy diet. therefore, it becomes necessary to give the population the opportunity, despite living a very busy and tiring life, to have control over important aspects of their health. this work aims to analyze and evaluate the impact of an ambient intelligence system on weight control and physical activity in active individuals. to accomplish this objective we have developed a mobile application that allows users to monitor their weight over a period of time, identify the amount of food they consume and the amount of exercise they practice. university students will be invited and selected, in a first stage, to participate in this study. all of the students must be considered “active students”, according to our selection criteria. students with physical disabilities will be excluded from the study. this mobile application gives information to the users about dietary and physical activity guidelines in order to improve their lifestyles. it is expected that students improve their lifestyles.",
    0.0
  ],
  [
    [
      "atualmente, a inteligência artificial, e em especial a subárea da aprendizagem automática e profunda, é alvo de enorme interesse por parte das comunidades académica e empresarial. das áreas aplicacionais da aprendizagem automática, a par com o processamento de língua natural, a visão por computador é aquela que tem suscitado mais interesse e gerado mais resultados científicos. a geração de imagens é um dos problemas que se enquadra na visão por computador. com esta dissertação pretende-se estudar os modelos geradores de imagens, e de entre as alternativas para atacar este problema, o foco do trabalho são os modelos variational autoencoders. na fase inicial da dissertação é feito um levantamento bibliográfico do estado da arte do tema do trabalho, visando adquirir os conhecimentos necessários para concretizar a parte experimental. os conhecimentos adquiridos na fase de levantamento bibliográfico foram aplicados na fase seguinte, onde se desenvolveu, treinou e avaliou modelos capazes de gerar imagens novas com faces humanas. o foco foi a arquitetura vector quantized variational autoencoder (vq-vae), auxiliada por um modelo autorregressivo pixelcnn. no entanto, foram também explorados outros modelos geradores, tendo em mente complementar o estudo em causa, e consequentemente, poder tirar conclusões mais abrangentes. após a implementação dos modelos, foi possível concluir que dentro de todos os modelos testados o vq-vae apresentou o melhor desempenho, quer seja a nível qualitativo através da inspeção visual das faces geradas, quer seja a nível quantitativo com a aplicação da métrica frechet inception distance. além do vq-vae, o outro modelo que se destacou foi o vector quantized generative adversarial network, comprovando assim o potencial da aplicação da quantização de vetores nos modelos geradores.",
      "nowadays, the artificial intelligence, particularly the subarea of machine and deep learning, is the subject of enormous interest from the academy and the companies. among the application areas of machine learning, along with natural language processing, computer vision is the one that has aroused the most interest and generated the most scientific results. image generation is one of the problems that fall within computer vision. with this dissertation, it is intended to study image generation models, and among the alternatives to address this problem, the main focus of the present work are the variational autoencoders models. in the initial phase of the dissertation, a bibliographic review of the state of the art of the subject of the work was carried out, aiming to acquire the necessary knowledge to carry out the experimental phase. the knowledge acquired with the bibliographic review was applied in the next phase, where models capable of generating new images of human faces were developed, trained and evaluated. the main focus was the vector quantized variational autoencoder (vq-vae) architecture, aided by an autoregressive pixelcnn model. however, other types of generative models were also explored, in order to complement the study at hand and consequently reach more comprehensive conclusions. after implementing the models, it was possible to conclude that among all the evaluated models, vq-vae was the one that presented the best performance, both qualitatively, visually inspecting the generated faces, and quantitatively, applying the frechet inception distance metric. in addition to vq-vae, the other model to stand out was vq-gan, thus justifying the potential of applying the concept of vector quantization to generative models. keywords: image generation model, variational autoencoder, vq-vae, pixelcnn, vq-gan"
    ],
    [
      "this document is the report for a master thesis, included in the second year of the master’s degree in informatics engineering at universidade do minho in braga, portugal. the project consists on the design and development of a web ide for xml to support teaching annotation languages to students that deal with digital documents but are not related to computer programming. the developed tool must be easy to install and to use, considering it is made to help users that are not experienced with programming or annotation languages nor ides. the goal of this project is to get an user-friendly, easy to learn, web-based platform to assist in: creating well-formed xml documents, dtds, xml schemas and xml stylesheets; converting dtds to xml schemas; validating xml documents according to given dtds or schemas; running xpath expressions on xml documents; and automatically generating xml documents from a dtd or a schema and some parameters. the state of the art on xml ides was analyzed in order to prove the necessity for the proposed application. the different ides were analyzed regarding their features, ui/ux, ease-of-use, price and availability. this document also contemplates the proposed approach for meeting the project’s ob jectives, the plan for the development stage of this project, as well as the definition for the desired features, and the report for the development process itself, describing how the different features were implemented. the developed application was compared to the analyzed state of the art ides and proved to meet the proposed objectives for the project. the webxmlide application is publicly available for free in the following link: https: //webxml.epl.di.uminho.pt/.",
      "este documento serve como relatório de uma tese de mestrado, incluída no segundo ano do mestrado em engenharia informática na universidade do minho em braga, portugal. este projeto consiste no design e desenvolvimento de um web ide de xml para suportar o ensino de linguagens de anotação a estudantes que lidam com documentos digitais mas não são de áreas relacionadas com programação. a ferramenta proposta deve ser fácil de instalar e usar, considerando que é feita para ajudar utilizador que não são experientes em programação, linguagens de anotação ou ides. o objetivo deste projeto é obter uma plataforma baseada na web, fácil de utilizar e aprender para ajudar: na criação de documentos xml, dtds, xml schemas e xml stylesheets bem formados; na conversão de dtds para schemas; na validação de documentos xml de acordo com dtds ou schemas; na testagem de expressões xpath em documentos xpath; e na geração automática de documentos xml a partir de dtds ou schemas e alguns parâmetros. o estado da arte no que toca a ides de xml foi analisado a fim de comprovar a necessidade da aplicação proposta. os diferentes ides foram analisados quanto às suas funcionalidades, ui/ux, facilidade de uso, preço e disponibilidade. este documento contempla ainda a abordagem proposta para o cumprimento dos objetivos do projeto, o plano para a fase de desenvolvimento deste projeto, bem como a definição das funcionalidades pretendidas, e o relatório do próprio processo de desenvolvimento, descrevendo como foram implementadas as diferentes funcionalidades. a aplicação desenvolvida foi comparada com os ides do estado da arte analisados e provou corresponder aos objetivos definidos para o projeto. a aplicação webxmlide está publicamente disponível de forma gratuita no seguinte link: https://webxml.epl.di.uminho.pt/."
    ],
    0.3
  ],
  [
    [
      "the exponential growth of digital information that has been witnessed in recent years requires a continuous evolution and optimization of data management systems, such as databases and storage solutions. in order to provide efficient processing and storage capabilities for large amounts of data, data man agement systems must adopt different optimizations (e.g., caching, replication, data reduction) that in crease their complexity. as a result, developing, configuring and maintaining a data management system becomes increasingly difficult and costly. tracing and analyzing the interactions and exchanges between components of these systems is funda mental to uncover performance, correctness and dependability issues almost unavoidable in any complex solution. on the other hand, this presents several challenges, such as minimizing the impact on applica tions’ performance and storage space, improving tracing accuracy and achieving real-time analysis, that must be explored. with this thesis, we present a tracing and analysis pipeline capable of capturing and analyzing the i/o patterns of these data-centric systems in order to better understand their behavior, using lttng as tracing tool. in particular, the proposed solution includes a tracing component that efficiently collects disk and network i/o metrics originated by the target application. this component is the major focus of this thesis and allows for the capture of system calls that the application executes, as well as their arguments, in a non-intrusive and almost real-time way. the rest of the pipeline facilitates the analysis and visualization of captured events through search queries and diagrams, allowing the user to find potential performance and optimization problems. in the end, we demonstrate that the proposed solution allows for the identification of inefficient and redundant i/o patterns in production applications without causing significant impacts on the runtime performance of the application and allowing for near real-time analysis.",
      "o crescimento exponencial de dados digitais a que se tem assistido nos últimos anos exige uma evolução e otimização dos sistemas de gestão de dados, como bases de dados e sistemas de armazenamento. de modo a fornecer processamento e armazenamento eficaz de grandes quantidades de dados, os sistemas de gestão de dados devem adotar diferentes otimizações (e.g., caching, replicação, redução de dados) que aumenta a sua complexidade. desta forma, desenvolver, configurar e manter um sistema de gestão de dados tornam-se cada vez mais tarefas difíceis e dispendiosas. analisar as interações entre componentes destes sistemas é fundamental para descobrir problemas de desempenho, correção e confiabilidade quase inevitáveis em qualquer solução complexa. por outro lado, isto apresenta vários desafios, como minimizar o impacto no desempenho das aplicações e no espaço de armazenamento, melhorar a precisão do tracing e alcançar análise em tempo real, que devem ser explorados. com esta tese, apresentamos uma pipeline de tracing e análise capaz de capturar e analisar padrões i/o destes sistemas com o objetivo de melhor compreender o seu comportamento, utilizando o lttng como ferramenta de tracing. em particular, a solução proposta contempla uma componente que coleciona de forma eficaz os pedidos de disco e métricas de e/s de rede originados pela aplicação alvo. esta componente é o foco principal da tese e permite a captura de todas as chamadas ao sistema operativo que a aplicação execute, bem como os seus argumentos, de forma não intrusiva e quase em tempo real. o resto da pipeline facilita a análise e visualização dos eventos capturados através de interrogações de pesquisa e de diagramas, permitindo ao utilizar encontrar potenciais problemas de desempenho e otimização. no final, demonstramos que a solução proposta permite identificar padrões e/s ineficientes e redundantes em aplicações de produção sem causar impactos significativos na execução normal da aplicação e possibilitando análise quase em tempo real."
    ],
    [
      "o crescente uso de aplicações que geram altos volumes de tráfego motivou o desenvolvimento de novas abordagens de engenharia de tráfego que pudessem melhorar o desempenho e eficiência das infraestruturas de comunicação, e.g. redes dos isps (internet service providers), data centers, etc. neste contexto, a área denominada por software defined networking (sdn) poderá ser útil para a definição de alguns mecanismos inovadores nestes cenários. este paradigma, que tem sido recentemente explorado, oferece novas tecnologias e protocolos proporcionando novas oportunidades para uma gestão mais expedita e eficiente das infraestruturas de rede. este trabalho propõe-se contribuir para o desenvolvimento de mecanismos de engenharia de tráfego na área das sdn. os mecanismos a estudar estarão orientados para tarefas de balanceamento de carga em redes de data centers e implementados com a ferramenta de emulação mininet. para tal, será feito inicialmente um estudo das diversas arquiteturas de redes de data centers, dos conceitos que englobam o paradigma sdn e uma análise das estratégias de balanceamento de carga já existentes. de seguida será desenvolvida uma bancada de testes e implementados alguns mecanismos de balanceamento de carga. posteriormente, serão efetuados testes de desempenho aos mecanismos desenvolvidos",
      "the increasing use of applications that generate high traffic volumes prompted the development of new approaches to traffic engineering field that could improve the performance and efficiency of communication infrastructures, e.g. isps (internet service providers) networks, data centers, etc. in this context, arises the area of software defined networking (sdn) that may be helpful to define some innovative mechanisms in such scenarios. this paradigm, which has recently been explored, offers new technologies and protocols providing new opportunities for a more expeditious and efficient traffic management strategies in network infrastructures. this work aims to contribute to the development of traffic engineering mechanisms in the area of sdn. the mechanisms to develop will be oriented to load balancing tasks in data centers networks and implemented with the mininet emulation tool. it will be made initially a study of the various data center networks architectures, concepts that encompass the sdn paradigm and an analysis of existing load balancing strategies. then it will developed a test bench and implemented some load balancing mechanisms. subsequently, performance tests will be made to the developed mechanisms."
    ],
    0.3
  ],
  [
    [
      "this master’s project presents a comprehensive exploration of a novel virtual reality (vr) application designed to evaluate and enhance user performance within the context of constraints experienced by individuals, including those confined to an intensive care unit (icu). the work unfolds through a detailed examination of the proposal, development, and assessment phases. the proposal lays the foundation for the project, emphasizing the need for an immersive technology-based solution to assess icu patients’ abilities. it includes a well-structured system architecture, deployment architecture, and data architecture. this framework guides the subsequent phases, offering insights into the development and assessment processes. in the development phase, the practical realization of the vr application is explored. it highlights adjustments tailored to the specific needs of icu patients, offering valuable insights into user progress, reducing dependency on external assistance. eight distinct tasks are detailed, categorized based on complexity and fundamental functionalities. the assessment phase evaluates the real-world impact of the vr application through three interventions. while limited to non-icu environments, these interventions capture data from users who share critical constraints with icu patients. the assessment involves correlation analysis of numerous variables, including age, cognitive function (assessed through the mini-mental status examination), and prior vr experience. the results unearth significant correlations, shedding light on age-related differences, the influence of cognitive ability, and the impact of prior vr exposure on task performance. this comprehensive exploration represents an essential contribution to the burgeoning field of vr applications for healthcare, specifically targeting constrained user groups like icu patients. the findings underscore the significance of considering user characteristics, prior experience, and cognitive function when designing vr interventions. further research is warranted to refine assessment methodologies and expand the scope of real icu patient testing, ultimately paving the way for improved patient care and enhanced rehabilitation practices.",
      "este projeto de mestrado apresenta uma exploração abrangente de uma nova aplicação de realidade virtual (rv) concebida para avaliar e melhorar o desempenho do utilizador no contexto dos constrangimentos vividos pelos indivíduos, incluindo os que estão confinados a uma unidade de cuidados intensivos (uci). o trabalho desenrola-se através de uma análise pormenorizada das fases de proposta, desenvolvimento e avaliação. a proposta estabelece as bases do projeto, salientando a necessidade de uma solução imersiva baseada na tecnologia para avaliar as capacidades dos doentes da uci. inclui uma arquitetura de sistema bem estruturada, uma arquitetura de implementação e uma arquitetura de dados. esta estrutura orienta as fases subsequentes, oferecendo informações sobre os processos de desenvolvimento e avaliação. na fase de desenvolvimento, é explorada a realização prática da aplicação de rv. destaca os ajustes adaptados às necessidades específicas dos pacientes da uci, oferecendo infor mações valiosas sobre o progresso do utilizador, reduzindo a dependência da assistência externa. são detalhadas oito tarefas distintas, categorizadas com base na complexidade e nas funcionalidades fundamentais. a fase de avaliação avalia o impacto da aplicação de rv no mundo real através de três intervenções. embora limitadas a ambientes não uci, estas intervenções captam dados de utilizadores que partilham restrições críticas com doentes da uci. a avaliação envolve a análise de correlação de numerosas variáveis, incluindo a idade, a função cognitiva (avaliada através do mini-mental status examination) e a experiência anterior em rv. os resultados revelam correlações significativas, lançando luz sobre as diferenças relacionadas com a idade, a influência da capacidade cognitiva e o impacto da exposição prévia à rv no desempenho da tarefa. esta exploração exaustiva representa um contributo essencial para o crescente campo das aplicações de rv nos cuidados de saúde, visando especificamente grupos de utilizadores limitados, como os doentes de uci. os resultados sublinham a importância de considerar as características do utilizador, a experiência anterior e a função cognitiva ao conceber inter venções de rv. justifica-se a realização de mais investigação para aperfeiçoar as metodologias de avaliação e alargar o âmbito dos testes em doentes reais de uci, abrindo caminho a melhores cuidados para os doentes e a melhores práticas de reabilitação."
    ],
    [
      "this document reports a master’s work, the final project of the 5th year of the integrated master’s in informatics engineering, that was accomplished at universidade do minho in braga, portugal. on february 24, 2022, a conflict between two countries, ukraine and russia, began. the war between two countries is devastating and affects many people, both residents of the countries directly involved and neighboring countries. as a highly significant event, it gathers coverage from many sources globally, including traditional print newspapers, online news platforms, social networks, blogs, television programs, and more. however, all of this information is scattered across different websites and social networks. if researchers (in the areas of linguistics, history, humanities, etc.) and curious people want to analyze this data, their work will be very difficult. therefore, it is essential to gather the information on a single platform. this work aims to create an online corpus in the portuguese language regarding the ukraine war, based on portuguese online newspapers’ news as well as comments on social media. to fulfill the goal of this work, initially, a variety of news sources were considered, and the portuguese online newspapers “público” and “jornal de negócios” were selected, as well as the platform “reddit”. to extract the required information, the technique of web scraping was used. therefore, for each source, an extractor was developed that extracted the necessary information and saved it in a json file. following that, natural language processing techniques were used to process the gathered information. afterward, the extracted information was stored in a non-relational database, mongodb. finally, a website called guco was designed and implemented, providing users with the capability to navigate and explore the created corpus. the guco website is available at the address: https://guco.epl.di.uminho.pt/.",
      "este documento relata um trabalho de mestrado, o projeto final do 5º ano do mestrado integrado em engenharia informática, que será realizado na universidade do minho em braga, portugal. em 24 de fevereiro de 2022, o conflicto entre dois países, ucrânia e rússia, começou. a guerra entre dois países é devastadora e afecta muitas pessoas, tanto residentes nos países directamente envolvidos como nos países vizinhos. sendo um evento altamente importante, este reúne cobertura de muitas fontes por todo o mundo, incluindo jornais tradicionais impressos, plataformas de notícias online, redes sociais, blogs, programas de televisão e muito mais. no entanto, toda esta informação está espalhada por diferentes websites e redes sociais. se pesquisadores (das áreas de linguística, história, ciências humanas, etc.) e curiosos quiserem analisar esses dados, o seu trabalho será muito difícil. portanto, é fundamental reunir as informações numa única plataforma. este trabalho tem como objetivo a criação de um corpus online na língua portuguesa sobre a guerra da ucrânia baseado em notícias de jornais online portugueses, bem como em comentários nas redes sociais. para cumprir o objetivo deste trabalho, inicialmente, foram consideradas diversas fontes de notícias, e os jornais online portugueses “público” e “jornal de negócios” foram se lecionados como fontes de informação para o corpus, bem como posts e comentários da plataforma “reddit ”. para extrair a informação necessária, foi utilizada a técnica de web scraping. portanto, para cada fonte foi desenvolvido um extrator que extraiu as informações necessárias e guardou-as num ficheiro json. de seguida, foram utilizadas técnicas de pro cessamento de linguagem natural para processar a informação recolhida. posteriormente, os dados foram armazenadas numa base de dados não relacional, mongodb. por fim, foi desenvolvido um website para que o utilizador possa explorar o corpus criado. o website guco pode ser acedido em: https://guco.epl.di.uminho.pt/."
    ],
    0.3
  ],
  [
    [
      "deep learning (dl) is a widely used technique often applied to many domains, from computer vision to natural language processing. to avoid overfitting, dl applications have to access large amounts of data, which affects the training performance. although significant hardware advances have already been made, current storage systems cannot keep up with the needs required by dl techniques. considering this, multiple storage solutions have already been developed to improve the input/output (i/o) performance of dl training. nevertheless, they are either specific to certain dl frameworks or present drawbacks, such as loss of accuracy. most dl frameworks also contain internal i/o optimizations, however they cannot be easily decoupled and applied to other frameworks. furthermore, most of these optimizations have to be manually configured or comprise greedy provisioning algorithms that waste computational resources. to address these issues, we propose prisma, a novel storage middleware that employs data prefetching and parallel i/o to improve dl training performance. prisma provides an autotuning mechanism to automatically select the optimal configuration. this mechanism was designed to achieve a good trade-off between performance and resource usage. prisma is framework-agnostic, meaning that it can be applied to any dl framework, and does not impact the accuracy of the training model. in addition to prisma, we provide a thorough study and evaluation of the tensorflow dataset application programming interface (api), demonstrating that local dl can benefit from i/o optimization. prisma was integrated and evaluated with two popular dl frameworks, namely tensor flow and pytorch, proving that it is successful under different i/o workloads. experimental results demonstrate that prisma is the most efficient solution for the majority of the scenar ios that were studied, while for the other scenarios exhibits similar performance to built-in optimizations of tensorflow and pytorch.",
      "aprendizagem profunda (ap) é uma área bastante abrangente que é atualmente utilizada em diversos domínios, como é o caso da visão por computador e do processamento de linguagem natural. a aplicação de técnicas de ap implica o acesso a grandes quantidades de dados, o que afeta o desempenho de treino. embora já tenham sido alcançados avanços significativos em termos de hardware, os sistemas de armazenamento atuais não conseguem acompanhar os requisitos de desempenho que os mecanismos de ap impõem. considerando isto, foram desenvolvidas várias soluções de armazenamento com o objetivo de melhorar o desempenho de entrada/saída (e/s) do treino de ap. no entanto, as soluções existentes possuem certas desvantagens, nomeadamente perda de precisão do modelo de treino e o facto de serem específicas a determinadas plataformas de ap. a maioria das plataformas de ap também possuem otimizações de e/s, contudo essas otimizações não podem ser facilmente desacopladas e aplicadas a outras plataformas. para além disto, a maioria destas otimizações tem que ser configurada manualmente ou contém algoritmos de provisionamento gananciosos, que desperdiçam recursos computacionais. para resolver os problemas anteriormente mencionados, esta dissertação propõe o prisma, um middleware de armazenamento que executa pré-busca de dados e paralelismo de e/s, de forma a melhorar o desempenho de treino de ap. o prisma providencia um mecanismo de configuração automática para determinar uma combinação de parâmetros ótima. este mecanismo foi desenvolvido com o objetivo de obter um bom equilíbrio entre desempenho e utilização de recursos. o prisma é independente da plataforma de ap e não afeta a precisão do modelo de treino. além do prisma, esta dissertação providencia um estudo e uma avaliação detalhados da interface de programação de aplicações (api) dataset do tensorflow, provando que ap local pode beneficiar de otimizações de e/s. o prisma foi integrado e avaliado com duas plataformas de ap amplamente utilizadas, o tensorflow e o pytorch, demonstrando que este middleware tem sucesso sob diferentes cargas de trabalho de e/s. os resultados experimentais demonstram que o prisma é a solução mais eficiente na maioria dos cenários estudados, e possui um desempenho semelhante às otimizações internas do tensorflow e do pytorch."
    ],
    [
      "a toolchain is a fundamental element of software development and provides developers with the resources necessary for the development, testing, and maintenance of applications. unreliable test environments can cause major problems for the successful completion of tests. resulting in a decrease in performance and quality of the application. the disparity be tween tests executed locally and remotely by different development teams can exacerbate this issue, leading to inconsistency in results. in order to guarantee reliable and comparable results it is essential that all developers use the same environment and configuration when conducting tests. this consistency is particularly important to obtain an accurate depiction of the application’s capabilities, revealing any potential issues that may not be prevalent in one environment but exist in another. this research aims to create a streamlined and efficient testing methodology for toolchain developers, developed in collaboration with synopsys, a leader in the semiconductor indus try, provides a range of arc risc processors supported by various commercial and open source operating systems and middleware. this work proposes a tool designed to simplify the testing process for different tools in the toolchain, allowing developers to easily execute, compare, profile, explore, and report tests with confidence. the tool will enable direct local comparisons, reducing the time and effort required for manual testing and reporting. by us ing this tool, developers can save valuable time while ensuring their tests are accurate and reliable. an improved testing methodology will be beneficial to all toolchain developers, as it will allow them to focus less on manual testing processes and more on developing innovative solutions with confidence. tests are an essential component of any toolchain development process, so having an efficient testing methodology in place is key to ensuring the highest standards of quality code. with the proposed tool, developers can rest assured that their tests are accurate, reliable and replicable.",
      "a toolchain é um elemento fundamental no desenvolvimento de software que fornece aos desenvolvedores os recursos necessários para o desenvolvimento, teste e manutenção de aplicações. ambientes de teste pouco confiáveis podem causar grandes problemas para a conclusão bem-sucedida dos testes, resultando numa diminuição de desempenho e qualidade da aplicação. a discrepância entre testes executados localmente e remotamente por diferentes equipas de desenvolvimento pode agravar esse problema, levando a inconsistências nos resultados. para garantir resultados confiáveis e comparáveis, é essencial que todos os desenvolvedores usem o mesmo ambiente e configuração ao realizar testes. essa consistência é especialmente importante para obter uma representação precisa das capacidades da aplicação, revelando quaisquer problemas potenciais que possam não ser evidentes em um ambiente, mas que existam em outro. esta pesquisa tem como objetivo criar uma metodologia de teste simplificada e eficiente para os desenvolvedores da toolchain, desenvolvida em colaboração com a synopsys, líder na indústria de semicondutores, que fornece uma variedade de processadores risc arc suportados por vários sistemas operativos comerciais e de código aberto, além de middleware. este trabalho propõe uma ferramenta projetada para simplificar o processo de teste para diferentes ferramentas na toolchain, permitindo que os desenvolvedores executem, com parem, explorem e reportem testes com confiança. a ferramenta possibilitará comparações locais diretas, reduzindo o tempo e esforço necessários para testes manuais e relatórios. ao usar essa ferramenta, os desenvolvedores podem economizar tempo valioso, garantindo que seus testes sejam precisos e confiáveis. uma metodologia de teste aprimorada será benéfica para todos os desenvolvedores da toolchain, permitindo que se concentrem menos nos processos manuais de teste e mais no desenvolvimento de soluções inovadoras com confiança. testes são um componente essencial de qualquer processo de desenvolvimento de toolchain, portanto, ter uma metodologia de teste eficiente é fundamental para garantir os mais altos padrões de código de qualidade. com a ferramenta proposta, os desenvolvedores podem ter certeza de que seus testes são precisos, confiáveis e replicáveis."
    ],
    0.3
  ],
  [
    [
      "terrain surveys play an important role in the field of telecommunications. in order to expand a fiber optics network, it is fundamental to have knowledge of the area being considered for expansion, especially knowledge about existing structures, and possible limitations of that area. a terrain survey provides a way to collect this information, which can then be examined and acted upon. survey results can often have issues depending on what tools and methods were used for the collection of data. the main goal of this work is to develop a tool capable of efficiently, and correctly collecting georeferenced data, and outputing that data in a way that is usable in the following stages of expansion of the fiber optics network.",
      "os estudos de terreno desempenham um papel importante na área das telecomunicações. para expandir uma rede de fibra ótica, é fundamental ter conhecimento da área que está a ser considerada para expansão, especialmente conhecimento sobre as estruturas existentes e possíveis limitações dessa área. um estudo de terreno é uma forma de recolher estas informações, que podem depois ser analisadas e utilizadas. os resultados do estudo podem apresentar problemas, dependendo das ferramentas e métodos utilizados para a recolha de dados. o principal objetivo deste trabalho é desenvolver uma ferramenta capaz de recolher dados georreferenciados de forma eficiente e correcta, e de produzir resultados de uma forma que possa ser utilizada nas fases seguintes de expansão da rede de fibra ótica."
    ],
    [
      "os dispositivos móveis, em particular os tablets e smartphones, alcançaram uma enorme popularidade ao longo dos últimos anos devido à sua grande versatilidade e multifuncionalidade, conquistando deste modo, meritoriamente um espaço de destaque no nosso dia-a-dia, tanto a nível pessoal como profissional. neste contexto, os utentes das bibliotecas da universidade do minho não são uma exceção, e os sdum (serviços de documentação da universidade do minho) no cumprimento da sua missão, definiu como uma das linhas gerais proporcionar aos utentes uma melhor qualidade de assistência, com o desenvolvimento de uma aplicação móvel de gestão de empréstimos e reservas de publicações em posse. contudo, existe um ainda um grande entrave no mercado do desenvolvimento de aplicações para dispositivos móveis, devido à sua fragmentação em termos de plataformas móveis utilizadas (ios, android, windows phone, etc.). esta diversificação exige um maior esforço no desenvolvimento das aplicações, de modo que obriga o desenvolvimento das mesmas para cada plataforma móvel em particular. é neste sentido que as abordagens de desenvolvimento multiplataforma ganharam relevância, permitindo o desenvolvimento de aplicações para várias plataformas a partir de um único código fonte. o principal objetivo desta dissertação é desenvolver uma aplicação de apoio a gestão de empréstimos e reservas nas bibliotecas da universidade do minho. os objetivos intercalados são: realização de estudos sobre as abordagens e ferramentas de desenvolvimento multiplataforma, adotar métodos de engenharia de requisitos e conceção, implementação e teste da solução final (fundamental no processo de correção de falhas, de modo que a permitir uma solução final com maior qualidade). a primeira fase do modelo de processo de engenharia de requisitos consiste no levantamento/definição e priorização de requisitos, que tem como objetivo conhecer as técnicas de levantamento, assim como identificar e aplicar as que melhor se adequam a este projeto. após a execução da fase de analise e negociação, efetuou-se a documentação dos requisitos a um nível de detalhe apropriado, como consta no anexo b – documento de especificação de requisitos.",
      "mobile devices, especially smartphones and tablets, achieved an enormous popularity over the past few years, due to their great versatility and multi-functionality, so gaining a prominent space is in our daily lives, both personally and professionally. in this context, the users of the libraries of university of minho are not an exception, and the sdum (documentation services of the university of minho) in the fulfilment of its mission, defined as one of the general lines provide users a better quality, assistance with the development of a mobile application loan management and booking of publications in possession. however, there is still a big obstacle in developing applications for mobile devices, because of its fragmentation in terms of mobile platforms used (ios, android, windows phone, etc.). this diversification requires a greater effort in the development of applications, since it requires the development of same for each mobile platform in particular. it is in this sense that the crossplatform development approaches have gained importance, enabling the development of application for multiple platforms from a single source code. the main objective of this dissertation aims at the development of a mobile application to support for the management of loans and reserves of publications in libraries of the university of minho. the secondary objectives are: make a study of development tools and cross-platform approaches, adopt methods of requirements engineering, and implementation, conception and test of the final solution (fundamental in fault correction process, thereby to permit a higher quality of final solution). the first phase of this model of process of requirements engineering consists of the survey/definition and prioritization of requirements, which aims to meet the lifting techniques, as well as identify and apply the best suited to this project. after the execution of analysis and negotiation phase, made-if the documentation of the requirements to a level of detail appropriate, as stated in the anexo b – documento de especificação de requisitos."
    ],
    0.3
  ],
  [
    [
      "a procura incessante por água potável devido à sua escassez levou ao interesse crescente na recuperação de recursos em estações de tratamento de águas residuais, o que ao desenvolvimento de estações de tratamento de águas residuais (etar) como instalações de reciclagem de recursos hídricos. a modelação matemática tem vindo a ser extremamente importante na implementação, operação e otimização desses processos. a introdução de modelos padrão, incluindo várias subunidades, permitiu uma avaliação objetiva do desempenho das estratégias de controle por simulação de maneira a combater problemas operacionais que geram perdas económicas e ambientais devido ao desequilíbrio nas populações microbianas. ao longo do tempo foram propostas muitas estratégias de controlo sendo que têm sido usados softwares e benchmarks simulation models para testar e avaliar estratégias, melhorando a operação das etar. o modelo de simulação de referência número 2 inclui sedimentadores primários, espessadores de lamas, desaguamento e tratamento de lamas com digestão anaeróbia, ampliando o alcance em comparação com os anteriores, que se focavam principalmente nas lamas ativadas e no sedimentador secundário. esses avanços são fundamentais para otimizar o tratamento de águas residuais e superar desafios microbiológicos. este projeto teve como objetivo principal criar um problema de otimização podendo mais tarde ser formulado também como um problema multiobjectivo. neste estudo, a abordagem híbrida do algoritmo hgpsal mostrou-se altamente eficaz ao lidar com um modelo mais complexo, o bsm2, sem ultrapassar limites legais nas variáveis críticas. a inclusão de novas variáveis, como fósforo, oxigénio dissolvido, sódio, snh (composto de enxofre azotado), ph e sno (composto de azoto nitrado), representou um avanço significativo no tratamento de águas residuais. em suma, este estudo contribui significativamente para a aplicação de algoritmos de otimização em estações de tratamento de águas residuais, oferecendo soluções adaptáveis e eficazes. os resultados obtidos representam um avanço importante em direção a soluções mais económicas e sustentáveis no tratamento de águas residuais, com potencial para impactar positivamente a gestão hídrica e ambiental, destacando a importância da recuperação de recursos, a complexidade microbiológica e a necessidade de estratégias inovadoras de controle, bem como o uso de modelos de benchmark para avaliação de desempenho.",
      "the relentless demand for potable water due to its scarcity has led to a growing interest in resource recovery in wastewater treatment plants (wwtps), resulting in the development of wwtps as facilities for water resource recycling. mathematical modeling has become increasingly crucial in the implementation, operation, and optimization of these processes. the introduction of standardized models, including various subunits, has allowed for an objective assessment of the performance of control strategies through simulation, aiming to combat operational issues that result in economic and environmental losses due to imbalances in microbial populations. over time, numerous control strategies have been proposed, and software and benchmark simulation models have been used to test and evaluate strategies, enhancing the operation of wwtps. the reference simulation model number 2 includes primary settlers, sludge thickeners, dewatering, and anaerobic sludge treatment, expanding its scope compared to previous models that primarily focused on activated sludge and secondary settling. these advances are essential for optimizing wastewater treatment and addressing microbiological challenges. this project's main objective was to create an optimization problem that could later also be formulated as a multi-objective problem. in this study, the hybrid approach of the hgpsal algorithm has proven to be highly effective when dealing with a more complex model, the bsm2, without exceeding legal limits in critical variables. the inclusion of new variables such as phosphorus, dissolved oxygen, sodium, snh (nitrogen sulfur compound), ph, and sno (nitrogen nitrate compound) represented a significant advancement in wastewater treatment. in summary, this study significantly contributes to the application of optimization algorithms in wastewater treatment plants, providing adaptable and effective solutions. the results obtained represent a significant step toward more economical and sustainable solutions for wastewater treatment, with the potential to positively impact water and environmental management. it underscores the importance of resource recovery, the microbiological complexity, and the need for innovative control strategies, as well as the use of benchmark models for performance evaluation."
    ],
    [
      "o aumento da quantidade de dados a processar, em diversos domínios, levou à necessidade de escalar os sistemas de armazenamento. além dos sistemas de bases de dados tradicionais, que têm suporte a transações com propriedades acid (atomicidade, coerência, isolamento, durabilidade), surgiram sistemas com base em outros paradigmas, que oferecem operações mais simples, baseadas no modelo chave-valor. nestes sistemas, abdicou-se do suporte a transações com propriedades acid para atingir a escalabilidade necessária. por outro lado, apareceram os serviços de armazenamento na nuvem, seguindo o modelo chave-valor, em que o tarifário de utilização é baseado no número de operações aprovisionadas e o nível de coerência a que estas são executadas. no entanto, continuam a haver aplicações que necessitam de aceder a dados com garantias de coerência. para tal, surgiram camadas transacionais de interface com sistemas de armazenamento chave-valor que medeiam todos os acessos das aplicações ao serviço de armazenamento. esta dissertação analisa os compromissos dos modelos de coerência, oferecidos por serviços de armazenamento na nuvem, e propõe uma arquitetura que tira partido da mediação dos acessos à nuvem para otimizar o custo e o desempenho. esta proposta é avaliada com um modelo de simulação, que permite demonstrar a sua validade.",
      "the increasing amount of data to process, in several domains, led to the need to scale storage systems. in addition to traditional database systems, that support acid (atomicity, consistency, isolation, durability) transactions, systems based on other paradigms emerged, which have simpler operations, based on the key-value model. in these systems, support for acid transactions was dropped in favor of scalability. on the other hand, there are cloud storage services offering the key-value model in which the billing plan is based on the number of operations provisioned and the consistency level at which these are executed. however, some applications still require data access with consistency guarantees. for this purpose, transactional layers that interface with key-value storage systems by intercepting all applications requests have emerged. this dissertation contributes an analysis of trafe-offs in consistency models offered by cloud storage services and proposes an architecture which takes advantage of intercepting accesses to the cloud service to optimize performance and cost. this proposal is evaluated with a simulation model, that shows its validity."
    ],
    0.06666666666666667
  ],
  [
    [
      "computer music generation is an active research field encompassing a wide range of approaches. as we involve more and more technology in our creative endeavors, it becomes vital that we provide our systems with the capability to understand and represent art concepts internally, as well as understand and predict artistic intent and emotion. in respect to music, being able to work with this information opens up fantastic possibilities for artistmachine synergetic collaborations as well as machine creativity endeavors. in this spirit, this dissertation explores a system capable of analyzing in real-time a performance piece played on a midi (musical instrument digital interface) capable instrument and produce a musically coherent piece of accompaniment music. this system comprises of two major subsystems: one responsible for analyzing and extracting features from the live performance and one that takes these features and generates midi tracks to be played in conjunction with the live audio, in a way that blends in with the performance.",
      "a geração algorítmica de música é um campo de investigação vasto e com múltiplas abordagens. à medida que incorporamos mais tecnologia nos nossos processos criativos, torna-se fundamental equipar os nossos sistemas com as capacidades para entender e representar arte, bem como analisar e prever intenção artística e emoções. no que diz respeito a música, ter este tipo de informação disponível abre possibilidades para colaborações entre artista e máquina, bem como sistemas capazes de exibir capacidades criativas. esta dissertação propõe um sistema capaz de analisar uma performance musical num dispositivo controlador midi (musical instrument digital interface) em tempo real e produzir acompanhamento adequado e coerente. este sistema é composto por dois subsistemas: um responsável por analisar e extrair características musicais de uma performance, e outro que a partir destas características seja capaz de gerar trechos midi de forma a se complementarem musicalmente."
    ],
    [
      "clinical records are a fundamental component of the correct treatment and follow-up of patients. the management of patient flow is complex and in the current pandemic times, the work organization must be increasingly focused and oriented towards simple and effective records. communication among the different departments is fundamental and must ensure that data and information flows without breaks. the provision of health care to patients is a vital importance in a society and the more advanced the medicine is the better is the treatment. the impact of technologies drastically benefits the treatment chosen by the doctor and the respective diagnoses, so the more information the doctor has regarding of the state of health and all kinds of interventions of his patients, the more prepared he will be to face and the more appropriate the methods will be used for each one. the main objective of this dissertation is to study the applicability of tools to develop a web platform to be used in the management of information to health professionals. an organic platform that can be molded to various situations and that are capable to supporting clinical records where each health professional is able to visualize the tasks according to their specialty, such as physiotherapy and rehabilitation professionals. a web platform that allows any professional in the area, a simple registration and access, offering a practical history of patients with all the necessary information and all the interventions they have done.",
      "os registos clínicos são uma componente fundamental da correta evolução e tratamento dos pacientes. a gestão do fluxo de utentes é complexa e nos tempos atuais da pandemia, a organização do trabalho deve ser cada vez mais focada e orientada a registos simples e eficazes. a comunicação entre os diferentes setores é fundamental e deve garantir que a mesma flui sem quebras. a prestação de cuidados de saúde aos utentes é de vital importância numa sociedade e quanto mais evoluída estiver a medicina, melhor será o tratamento. o impacto das tecnologias drasticamente beneficia o tratamento escolhido pelo médico e os respetivos diagnósticos, desta forma, quanta mais informação o médico tiver relativamente ao estado de saúde e todo o tipo de intervenções dos seus utentes, mais preparado estará para enfrentar e mais adequados serão os métodos utilizados para cada um. o principal objetivo desta de dissertação é estudar o benefício de ferramentas que permitam o desenvolvimento de uma plataforma web, a ser utilizada no auxílio de gestão de informação a profissionais de saúde. uma plataforma orgânica que seja moldável a diversas situações e que seja capaz de suportar registos clínicos, onde cada profissional de saúde seja capaz de visualizar as tarefas que possui de acordo com a sua especialidade. concretamente, profissionais da área da fisioterapia e reabilitação. trata-se de uma plataforma web que possibilita a qualquer profissional da área, um registo e acesso simples, oferecendo um histórico prático dos utentes com toda a informação relevante de todas as intervenções que efetuou."
    ],
    0.3
  ],
  [
    [
      "atualmente existem várias soluções comerciais que utilizam a tecnologia rfid para diversos fins aplicacionais. na literatura estão também documentados várias abordagens e algoritmos para localização de dispositivos rfid. partindo deste contexto, o objetivo principal da presente dissertação era modelar e conceber uma solução global de monitorização/localização baseada na tecnologia rfid (pontos de acesso e etiquetas), que fosse adequada para vários tipos de organismos/empresas. ou seja, nunca foi intenção desta dissertação propor novos algoritmos de localização por rfid. seguindo uma metodologia de desenvolvimento de software orientada aos modelos, e apoiada na tecnologia uml, o desenvolvimento do sistema percorreu as seguintes fases: (i) levantamento e documentação de requisitos, baseados na identificação das necessidades de três tipos de organismos/empresas e utilizando o modelo de volere, (ii) identificação dos utilizadores do sistema, (iii) conceção do sistema, tarefa que incluiu a elaboração de casos de uso detalhados, diagramas de sequência ao nível do sistema, o modelo de dados persistentes do sistema, diagrama de classes e diagramas de sequência ao nível da implementação, (iv) implementação de um protótipo em java e (iv) realização de testes com o protótipo. muito mais do que um algoritmo, ou conjunto de algoritmos, de localização, o sistema desenvolvido é uma solução integrada de localização baseada na tecnologia rfid. a estratégia seguida no desenvolvimento do sistema assenta em 3 princípios: (1) uma dada empresa ou organização deseja monitorizar pessoas e bens, de forma impedir/permitir o acesso a determinadas zonas do espaço físico do edifício dessa empresa, (2) uma das tecnologias mais versátil para este atingir este fim é a rfid, (3) combinando a potência do sinal rf, enviado pela mesma etiqueta rfid, e recebido em pelo menos três pontos de acesso, é possível obter uma estimativa da localização do emissor desse sinal rf. deste modo, o sistema desenvolvido pressupõe que existem três pontos de acesso (conjunto de aps) em cada local de acesso a zonas críticas do edifício, geometricamente localizados de forma a facilitar a trilateração de medições do sinal rf. entre as funcionalidades do sistema de localização desenvolvido incluem-se: acesso ao sistema controlado, gestão de utilizadores do sistema, gestão de etiquetas rfid (atribuição, recolha, programação), definição da geometria de cada piso do edifício (usando os conceitos de ponto, segmento e zona), definição das zonas permitidas e interditas a cada tipo de utilizador, gestão dos pontos de acesso e dos conjuntos de pontos de acesso, localização de etiquetas rfid por trilateração e emissão de alertas, para uma ou mais entidades de segurança, nos casos em que ocorrer a “invasão” de uma zona interdita. em termos de resultados, pode dizer-se que se concebeu um protótipo funcional que cumpre a maioria dos requisitos propostos, possui uma interface fácil de utilizar e é suficientemente genérico para poder ser usado por empresas de tipos diferentes. dado que o código e os modelos estão perfeitamente sincronizados, alterar ou adicionar funcionalidades ao sistema é relativamente seguro e com um custo moderado. em termos da qualidade das estimativas da localização, os resultados não são muito positivos. duas alternativas para melhorar este aspeto passariam por (i) melhorar o algoritmo de localização e (ii) usar um tipo de hardware rfid mais sofisticado e vocacionado para ser aplicado num sistema de localização.",
      "currently there are several commercial solutions that use the rfid technology for different applications. in the literature we can find several approaches and algorithms for the localization of rfid devices. in this context, the main objective of this dissertation is to model and design a global monitoring/locating solution based on the rfid technology (access points and tags), that would be adequate for several kinds of organizations/companies. in other words, it was never the intention of this dissertation to propose new rfid localization algorithms. following a model-based software development methodology, supported by the uml technology, the system development has taken the following steps: (i) requirements analysis and documentation, based on the identification of the needs of three types of organizations/companies, and using the volere template, (ii) identification of the system users, (iii) system design, a task that includes the development of a detailed specification for each use case, system-level sequence diagrams, the persistent data model of the system, class diagram and implementation-level sequence diagrams, (iv) implementation of a java prototype and (v) testing the prototype. much more than an algorithm, or a set of localization algorithms, the developed system is an rfid-based integrated solution for localization. the strategy followed on the system development is based in three principles: (1) a company or organization wants to monitor people and needs, so they could forbid/allow the access to certain zones of the physical space of the building, (2) one of the most versatile technologies to achieve this goal it is the rfid, (3) combining the strength of the rf signal, sent by the same rfid tag, and received at least in three access points, it’s possible to achieve an estimate for the localization of the rf signal emitter. in this way, the developed system requires that there are three access points (an ap set) in the access to each critical zone of the building, geometrically located in such a way to facilitate the trilateration of the rf signal measurements. the localization system includes the following features: controlled access to the system, management of the system users, management of rfid tags (assignment, collecting, programming), definition of the geometry of each floor of the building (using the point, segment and zone concepts), defining the allowed and forbidden zones to each type of user, management of access points and sets of access points, localization of rfid tags by trilateration, and emission of alerts for one or more security entities, in the cases that occur the “invasion” of one forbidden zone. in terms of results, we may say that it was conceived a functional prototype that meets most of the proposed requirements, it has a user friendly interface and it is generic enough to be used by different types of companies. since the code and the models are perfectly synchronized, alter or add functionalities to the system is relatively safe and with moderate cost. in terms of the quality of the localization estimates, the results are not very positive. two alternatives for improving this aspect would be (i) improve the localization algorithm, and (ii) use a more sophisticated rfid hardware, geared to be applied in a localization system."
    ],
    "nowadays many mathematical applications allow the user to introduce its own equations in the system and also observe through different possibilities the desired results. regarding physics, an extended range of virtual laboratories allow the user to accomplish virtual physics experiments. these virtual laboratories consist in predefined scenarios where the user can change the value of the physics variables and then visualise the changes accomplished. other virtual laboratories uses a physics engine allowing the user to create its own scenarios. however, the physical behaviour of the objects is hardcoded since it results strictly on the physics equations used internally by the physics engine. this dissertation pretends to investigate how far and with what degree of scientific rigor it is possible to associate the idea of the user introducing its own equations with the idea of accomplishing virtual experiments of physics. as a proof of concept, this dissertation focus on a specific area of mechanics: the dynamic of rigid bodies. the result of this research is a virtual laboratory completely different relatively the others. our system has no knowledge about physics. even the most general laws of physics such as the newton’s second law are not known by the system. to the system, any equation introduced is considered just as one more equation without any particular meaning associated to it. the same happens for any physics entity. for example, if the gravitational acceleration is introduced by the user, to the system it is just another attribute of the world. taking into account the dynamics of rigid bodies, an object can be identified as being, at any time, in one of three different states. these are: when a object is not in contact with any other, when an object collides with another object and they immediately separate, and when two objects remain in contact over time. the user must specify all the equations that drive each of these three states. using its geometrical knowledge, the engine determines at any time in which state an object is. also, the system provides all the relevant geometrical information. for instance, in a collision between two objects, the point and the two normals vectors of the collision are provided. the graphical simulations reflects strictly on the equations introduced. therefore, if the equations to solve a collision between two objects does not reflect the real underlying physics of the situation, it is possible that the objects simply ends-up penetrating each other. all the relevant numerical information about an experience can be processed through different forms. in fact, the user can request plots of variables, the graphical application of vectors on objects, and even the tracing of the variables at a specific event.",
    0.0
  ],
  [
    [
      "o clav é um projeto nacional financiado pelo simplex. o objetivo deste projeto é classificar e avaliar toda a documentação circulante na administração pública portuguesa. desta forma, as entidades públicas disporão de uma ferramenta que possibilita a identificação da documentação que deve ser eliminada ou arquivada. no entanto, como em todas as plataformas, podem ocorrer imprevistos que resultem em perda de informação. nesta dissertação foi criada uma ferramenta web externa ao clav, que permita a execução de backups e importação de informação na plataforma, a fim de ser possível o armazena mento da informação em volumes externos ao clav de modo a esta informação poder voltar a ser recolocada no clav mais tarde. para a criação dos pacotes de backup, recorreu-se a formatos standard de armazenamento de informação.",
      "clav is a national project funded by simplex. the objective of this project is to classify and evaluate all documents circulating in the portuguese public administration. in this way, public entities will have a tool that makes it possible to identify the documentation that must be eliminated or archived. however, as with all platforms, unforeseen events may occur and result in loss of information. in this dissertation, a web application external to clav was created, which allows to execute backups and import information on the platform, in order to be able to store the information in volumes external to clav so that this information can be replaced in clav later. to create the backup packages, it was used standard information storage formats."
    ],
    [
      "a georreferenciação é o processo de localização geográfica de um determinado objeto espacial através da atribuição de coordenadas. os sistemas de georreferenciação utilizam um processamento espacial automático executado por computador, por exemplo para colocar uma entidade num mapa ou fornecer um recurso espacial. quando este processo é aplicado a coleções de documentos textuais, é descrito como uma combinação de reconhecimento de entidades nomeadas. o livro das propriedades, também designado como o tombo da mitra, contém informação relativa aos tipos de terras, acidentes de terreno, nomes de ruas, proprietários e apontamentos biográficos e genealógicos das várias propriedades que a mesa arcebispal de braga possuía no século xvii. este trabalho de dissertação teve como objetivo conceber e implementar um sistema de georreferenciação textual para o conteúdo existente no livro das propriedades, com particular enfoque nos lugares que nele estão referidos, de forma a permitir aos estudiosos destes conteúdos possuírem informação acerca da localização geográfica desses elementos.",
      "georeferencing is the process of geographically locating a given spatial object by assigning coordinates. georeferencing systems use automatic spatial processing performed by a com puter, for example to place an entity on a map or provide a spatial resource. when this pro cess is applied to collections of textual documents, it is described as a combination of named entity recognition. the livro das propriedades, also designated as ”tombo da mitra”, contains information regarding land types, landforms, street names, owners, and biographical and genealogical notes of the several properties that the archbishop’s table of ”braga”owned in the 17th century. this dissertation work aimed to design and implement a textual georeferencing system for the existing contents of the ”livro das propriedades”, with particular focus on the places mentioned in it, in order to allow scholars of these contents to have infor mation about the geographical location of those elements."
    ],
    0.0
  ],
  [
    [
      "using any kind of dolls as a tangible interface, has the potential to provide a friendly and easy to learn interface that allows children to control virtual characters in a more intuitive way. the research effort in this domain has been motivated by the shortcomings of conventional interfaces, typically mouse and keyboard, which in this context are neither compelling nor do promote immersion. this dissertation focuses on the design and evaluation of a system which can interpret the behaviors that children give to a doll in order to provide this behavioral information to the virtual characters. with this system, the user (children) gets the role of movie director, directing virtual characters through this natural form of interaction. this dissertation aims to evaluate the hypothesis that dolls behaviors recognition based on the context of a well-known story, may enhance the ability of children in the creation of an animated film (virtual characters animations). unlike many approaches that use a direct mapping of the doll movements to the virtual character, it is intended to test the mapping based on the crosses between, user behavioral intention, and the context where the doll it is inserted (the role and the location of the character in the story). the results show that the concept of interaction proposed to empower the children with a way to create animated films is actually very intuitive and easy to use. however, due to the technology used, it was not possible to assess to what extent this concept really empowers children to easily and joyfully create animated films.",
      "a utilização de qualquer tipo de boneco como sendo uma interface tangível, tem o potencial de oferecer uma interface fácil de usar e mais amigável. isto permite as crianças o controlo de personagens virtuais de uma forma mais intuitiva. o esforço de pesquisa nesta área tem sido motivado pelo facto de os utilizadores não possuírem numa interface convencional uma forma imersiva e intuitiva para interagir. esta dissertação foca-se na construção e avaliação de um sistema que consegue interpretar os comportamentos que uma criança dá a um boneco no sentido de animar um personagem virtual. com este sistema o utilizador (criança) assume o papel de um realizador de cinema, dirigindo e dando ordens aos personagens virtuais através de uma forma de interação especifica. nesta dissertação pretende-se avaliar a hipótese de que o reconhecimento dos comportamentos de um boneco baseado no contexto de uma historia conhecida, pode potenciar a capacidade das crianças na interação e criação de filmes 3d (animação de personagens virtuais). contrariamente a várias aproximações que usam o mapeamento direto dos movimentos do boneco para o personagem virtual, pretende-se neste trabalho testar o mapeamento baseado no cruzamento entre, a deteção da intenção do utilizador, e o contexto onde o boneco está inserido (o papel e localização do personagem tendo em conta a história). os resultados demonstram que o conceito de interação proposto para capacitar as crianças com uma forma de criar filmes de animação é realmente muito intuitivo e fácil de utilizar, no entanto devido as tecnologias utilizadas não foi realmente possível avaliar até que ponto este conceito realmente potencia a capacidade das crianças para que de uma forma divertida consigam criar filmes de animação."
    ],
    [
      "multicast is a group communication paradigm created in order to reduce, as much as possible, the amount of data generated to the network. however, limited deployment of ip multicast protocols has motivated an interest in alternative approaches which implement a similar process of multicast at an application-level (using solely end-systems and not the routers). in this context, different methodologies are presented, entitled application-layer multicast or overlay multicast, which may vary in the way they operate. this dissertation’s objective is to develop and experiment a prototype of an overlay multicast system. this system should be easily configurable and adaptable in order to assume different strategies when establishing the multicast distribution tree. it is also expected to explore and integrate collaborative mechanisms between the overlay network and the internet service providers (isp). with the presented context, the first step to take is an investigation on the state of the art, where technologies relevant to this work will be presented. after this initial step, the developed system’s architecture will be described, one which enables different ways of building and maintaining the multicast distribution tree. the envisioned system can operate independently, integrating mechanisms where the distribution tree relies solely on peer decisions, which will be firstly addressed. then, this work will move on to collaborative mechanisms between the overlay’s management (the central node) and the internet service providers. based on the proposed system architecture, several mechanisms are explored, not only focusing on alternative ways to build distribution trees, but also mechanisms allowing for some traffic engineering objectives involving the internet service providers. using the core network emulator, all the proposed mechanisms are tested, and results are analyzed to corroborate the system’s correct operation.",
      "o multicast é um paradigma de comunicação em grupo que tem como objetivo reduzir, tanto quanto possível, a quantidade de tráfego gerada para a rede. no entanto, a implantação limitada de protocolos ip multicast tem motivado o interesse em abordagens alternativas que implementam processos de distribuição multicast na camada aplicacional (ou seja, usando apenas os sistemas/aplicações finais e não os routers). neste contexto, surgem as soluções denominadas por application-layer multicast ou overlay multicast, podendo estas apresentar algumas variantes na sua operação. nesta dissertação, tem-se como objetivo o desenvolvimento e experimentação de um protótipo de um sistema de overlay multicast. este sistema deverá ser capaz de ser facilmente (re)configurado para assumir diferentes estratégias no estabelecimento da árvore de distribuição multicast, e integrar mecanismos de colaboração entre a rede overlay e os internet service providers. no contexto apresentado, o primeiro passo consiste na investigação do estado da arte, onde tecnologias relevantes ao atual trabalho serão apresentadas. após este passo inicial, a arquitectura do sistema será apresentada, uma arquitectura que considera diferentes maneiras de construir e manter a árvore de distribuição multicast. o sistema proposto pode operar de forma independente, contemplando mecanismos onde a árvore de distribuição depende apenas das decisões dos vários peers, sendo que estes serão os primeiros mecanismos a serem apresentados. de seguida, o sistema direcciona-se para mecanismos colaborativos entre a gestão da rede overlay e o isp, de maneira a incluir conhecimento acerca da topologia da rede que nenhuma outra entidade seria capaz de providenciar. com base na arquitectura do sistema proposto, vários mecanismos são explorados, não só mecanismos que se concentram em formas alternativas de construir a árvore de distribuição, mas também mecanismos que permitem cumprir os objetivos de engenharia de tráfico dos isps. por fim, utilizando o emulador de redes core, todas as soluções serão testadas, e os seus resultados analisados por forma a validar a correta operação de todo o sistema."
    ],
    0.0
  ],
  [
    [
      "na área de ciência de dados, o machine learning está-se a revelar uma ferramenta essencial para resolver problemas complexos. as empresas estão a investir em equipas de ciência de dados e machine learning para desenvolver modelos que apresentem valor para os clientes. no entanto, estes modelos são uma pequena percentagem de uma pipeline de projetos de machine learning (ml) e, para entregar um produto de ml completo, é necessário um número maior de componentes. devops é uma mentalidade de engenharia e um conjunto de práticas que visa unificar o processo de desenvolvimento e o processo de operações em um software, mlops é um conceito similar a devops mas aplicado ao desenvolvimento e entrega de soluções de ml. o nível de automatização das etapas em uma pipeline de ml define a maturidade do processo de ml, que reflete a velocidade de treino de novos modelos com novos dados ou de treino de novos modelos com diferentes implementações. um sistema de ml é um sistema de software, desenvolvimento e atualizações contínuas são necessárias para garantir um sistema que escale conforme as necessidades. o principal objetivo desta tese é apoiar a criação de um sistema integrado de ml com uma arquitetura que proporcione a capacidade de ser continuamente operada em um ambiente de produção. um conceito para avaliação de desempenho de algoritmos deve ser elaborado e implementado. o principal obetivo e melhorar e ace'erar o cicio de desenvolvimento de modelos de ml na empresa. para atingir este objetivo surge a necessidade de definir uma arquitetura com especificações e a implementação de processos automatizadas num pipeline de ml existente, este processo têm como objetivo alcançar uma ferramenta de benchmark de modelos, com capacidade de analisar o desempenho do modelo, um motor de inferência e um banco de dados para armazenar todas as métricas computadas. um sistema baseado em ia em desenvolvimento fornece o caso de estudo para desenvolver e validar a arquitetura. os avanços atuais na área da condução semiautomática introduz a necessidade de sistemas de monitoramento que podem localizar e detectar eventos especificas no veículo. os conjuntos de sensores são instalados dentro da cabine para alimentar sistemas inteligentes que visam analisar e sinalizar certos comportamentos que podem impactar a segurança e o conforto dos passageiros..",
      "in the field of data science, ml is proving to be a core feature to solve complex real-world problems. businesses are investing in data science and ml teams to develop ai based models that can deliver business value to their users. however, these models are only a small fraction of an ml project pipeline, and to deliver an end to end ml product, a greater number of components are needed. devops is an engineering mindset and a set of practices that aims to unify the development process and the operation process on software. mlops is a similar concept to devops but applicable to the development and delivery of ml based solutions. the automation of the steps in a ml pipeline defines the maturity of the ml process, reflecting the velocity of training new models given new data or training new models given new implementations. an ml system is a software system that can support development, provide continuous integration and continuous delivery apply to help guarantee that one can reliably build and operate ml systems at scale. the main objective of this thesis are to support the creation of an integrated ml system with an archi tecture that provides the ability to be continuously operated in a production-like environment. furthermore, a concept to evaluate the performance of algorithms shall be devised and implemented. the end goal is to improve and accelerate the ml development lifecycle. to achieve this goal surges the need to define an architecture alongside specifications and the implementation of several automated steps into an existing ml pipeline. to improve and accelerate model development an model engine benchmark tool is devised capable of several features, including the ability to have dashboards for model performance evaluation, an automatic inference engine, performance metrics for the model and a database to store all the computed metrics and metadata. an ai-based system under development provides the case study to develop and validate this architec ture. the current advances of semi-automated driving introduce the need for monitoring systems to scan and detect specific events in the vehicle. sensor clusters are installed inside the vehicle cabin to feed data to intelligent systems that aim to analyze and red flag certain behaviours that can potentially impact passengers safety and comfort while using the vehicle."
    ],
    [
      "a few years ago, data was not shared and kept isolated, preventing communication between datasets. currently, we have more significant data volumes, and in a world where everything is connected, our data is now also following this trend. data model focus changed from a square structure like the relational model to a model centered on the relations. knowledge graphs are the new paradigm to represent and manage this new kind of information structure. along with the new paradigm, graph databases emerged to support the new requirements. despite the increasing interest in the field, only a few native solutions are available. most are under a commercial license, and the open-source options have very basic or outdated interfaces, and because of that, they are a little distant for most end-users. in this thesis, we introduce the open web ontobud and discuss its design and develop ment. ontobud is a web application aimed at improving the interface for one of the most fascinating and influential frameworks in this area: rdf4j. rdf4j is a java framework to deal with rdf triple storage, management, and query. open web ontobud is an open-source rdf4j web frontend created to reduce the gap between end-users and the rdf4j backend. we created a web interface that enables users with a basic knowledge of owl and sparql to explore ontologies via resource tables or graphs and extract information from them with sparql queries. the interface aims to remain intuitive, providing tooltips and help when needed, as well as some statistical data in a readily available form. despite the frontend being the main focus, a backend and two databases are also used for a total of four components in the framework. for the best deployment experience, docker was used for its simplicity, allowing deployment in just a few commands. each component has a dedicated image, following a modular design and allowing them to be executed on separate machines if desired.",
      "no passado, dados não era partilhada e permanecia isolada, impedindo comunicação entre datasets. atualmente, temos maiores volumes de dados e num mundo onde tudo está interligado, os nossos dados também seguem essa tendência. o foco de modelo de dados alterou de uma estrutura quadrada, como o modelo relacional, para um modelo centrado em relações. grafos de conhecimento são o novo paradigma para a representação e manipulação desta nova estrutura de dados. com o novo paradigma, bases de dados de grafos emergiram para suportar as novas necessidades. apesar do aumento de interesse neste campo, apenas algumas soluções nativas estão disponíveis. a maioria requere uma licença comercial, e as opções open-source são interfaces básicas ou desatualizadas, e por consequência, distantes a muitos utilizadores. nesta tese introduzimos o open web ontobud e discutimos o seu design e desenvolvi mento. o ontobud é uma aplicação web direcionada ao melhoramento da interface de uma das mais fascinantes e influentes frameworks nesta área: o rdf4j. o rdf4j é uma framework em java para guardar, manipular e inquirir grafos rdf. open web ontobud é um open-source web frontend para o rdf4j criado para diminuir a separação entre os utilizadores e o rdf4j backend. nós criamos uma interface web que permite utilizadores com conhecimento básico de owl e sparql explorar ontologias através de tabelas de recursos ou grafos, e inquirir informação com queries sparql. o objetivo da interface é ser intuitiva, com tooltips e ajuda quando necessário, bem como alguma informação de estatísticas numa forma facilmente acessível. apesar do frontend ser o foco principal, o backend e duas bases de dados também são utilizadas, para um total de quatro componentes nesta framework. para a melhor experiência de inicialização utilizamos docker pela sua simplicidade, permitindo inicialização em poucos comandos. cada componente tem uma imagem dedicada, seguindo um design modular e permitindo cada componente ser executada em máquinas separadas se necessário."
    ],
    0.3
  ],
  [
    [
      "machine learning is trending in computer science, especially deep learning. training algorithms that follow this approach to machine learning routinely deal with vast amounts of data. processing these enormous quantities of data requires complex computation tasks that can take a long time to produce results. distributing computation efforts across multiple machines makes sense in this context, as it allows conclusive results to be available in a shorter time frame. distributing the training of a deep neural network is not a trivial procedure. various architectures have been proposed, following two different paradigms. the most common one follows a centralized approach, where a centralized entity, broadly named parameter server, synchronizes and coordinates the updates generated by a number of workers. the alternative discards the centralized unit, assuming a decentralized architecture. the synchronization between the multiple workers is assured by communication techniques that average gradients between a node and its peers. high-end clusters are the ideal environment to deploy deep learning systems. low latency between nodes assures low idle times for workers, increasing the overall system performance. these setups, however, are expensive and are only available to a limited number of entities. on the other end, there is a continuous growth of edge devices with potentially vast amounts of available computational resources. in this dissertation, we aim to implement a fault tolerant decentralized deep neural net work training framework, capable of handling the high latency and unreliability characteristic of edge networks. to manage communication between nodes, we employ decentralized algorithms capable of estimating parameters globally",
      "machine learning, mais especificamente deep learning, é um campo emergente nas ciências da computação. algoritmos de treino aplicados em deep learning lidam muito frequentemente com vastas quantidades de dados. processar estas enormes quantidades de dados requer operações computacionais complexas que demoram demasiado tempo para produzir resultados. distribuir o esforço computacional por múltiplas máquinas faz todo o sentido neste contexto e permite um aumento significativo de desempenho. distribuir o método de treino de uma rede neuronal não é um processo trivial. várias arquiteturas têm sido propostas, seguindo dois diferentes paradigmas. o mais comum segue uma abordagem centralizada, onde uma entidade central, normalmente denominada de parameter server, sincroniza e coordena todas as atualizações produzidas pelos workers. a alternativa passa por descartar a entidade centralizada, assumindo uma arquitetura descentralizada. a sincronização entre workers é assegurada através de estratégias de comunicação descentralizadas. clusters de alta performance são o ambiente ideal para a implementação de sistemas de deep learning. a baixa latência entre nodos assegura baixos períodos de inatividade nos workers, aumentando assim o rendimento do sistema. estas instalações, contudo, são muito custosas, estando apenas disponíveis para um pequeno número de entidades. por outro lado, o número de equipamentos nas extremidades da rede, com baixo aproveitamento de poder computacional, continua a crescer, o que torna o seu uso desejável. nesta dissertação, visamos implementar um ambiente de treino de redes neuronais descentralizado e tolerante a faltas, apto a lidar com alta latência nas comunicações e baixa estabilidade nos nodos, caraterística de redes na extremidade. para coordenar a comunicação entre os nodos, empregamos algoritmos de agregação, capazes de criar uma visão geral de parâmetros numa topologia."
    ],
    [
      "the system that detects and identifies human activities are named human action recognition. on the video approach, human activity is classified into four different categories, depending on the complexity of the steps and the number of body parts involved in the action, namely gestures, actions, interactions, and activities, which is challenging for video human action recognition to capture valuable and discriminative features because of the human body’s variations. so, deep learning techniques have provided practical applications in multiple fields of signal processing, usually surpassing traditional signal processing on a large scale. recently, several applications, namely surveillance, human-computer interaction, and video recovery based on its content, have studied violence’s detection and recognition. in recent years there has been a rapid growth in the production and consumption of a wide variety of video data due to the popularization of high quality and relatively low-price video devices. smartphones and digital cameras contributed a lot to this factor. at the same time, there are about 300 hours of video data updates every minute on youtube. along with the growing production of video data, new technologies such as video captioning, answering video surveys, and video-based activity/event detection are emerging every day. from the video input data, the detection of human activity indicates which activity is contained in the video and locates the regions in the video where the activity occurs. this dissertation has conducted an experiment to identify and detect violence with spatial action localization, adapting a public dataset for effect. the idea was used an annotated dataset of general action recognition and adapted only for violence detection.",
      "o sistema que deteta e identifica as atividades humanas é denominado reconhecimento da ação humana. na abordagem por vídeo, a atividade humana é classificada em quatro categorias diferentes, dependendo da complexidade das etapas e do número de partes do corpo envolvidas na ação, a saber, gestos, ações, interações e atividades, o que é desafiador para o reconhecimento da ação humana do vídeo para capturar características valiosas e discriminativas devido às variações do corpo humano. portanto, as técnicas de deep learning forneceram aplicações práticas em vários campos de processamento de sinal, geralmente superando o processamento de sinal tradicional em grande escala. recentemente, várias aplicações, nomeadamente na vigilância, interação humano computador e recuperação de vídeo com base no seu conteúdo, estudaram a deteção e o reconhecimento da violência. nos últimos anos, tem havido um rápido crescimento na produção e consumo de uma ampla variedade de dados de vídeo devido à popularização de dispositivos de vídeo de alta qualidade e preços relativamente baixos. smartphones e cameras digitais contribuíram muito para esse fator. ao mesmo tempo, há cerca de 300 horas de atualizações de dados de vídeo a cada minuto no youtube. junto com a produção crescente de dados de vídeo, novas tecnologias, como legendagem de vídeo, respostas a pesquisas de vídeo e deteção de eventos / atividades baseadas em vídeo estão surgindo todos os dias. a partir dos dados de entrada de vídeo, a deteção de atividade humana indica qual atividade está contida no vídeo e localiza as regiões no vídeo onde a atividade ocorre. esta dissertação conduziu uma experiência para identificar e detetar violência com localização espacial, adaptando um dataset público para efeito. a ideia foi usada um conjunto de dados anotado de reconhecimento de ações gerais e adaptá-la apenas para deteção de violência."
    ],
    0.3
  ],
  [
    [
      "perante os recorrentes problemas de qualidade na utilização da rede eduroam no departamento de informática (di) da universidade do minho, foi efetuada uma reformulação e reestruturação da rede sem fios. a sua monitorização permite tomar conhecimento de eventuais situações que possam causar o mau funcionamento de serviços, como outras redes (i.e., ad-hoc, não autorizadas, etc.) no mesmo espaço sem fios que afetam negativamente o desempenho da rede eduroam ou má utilização dessas redes com intenções maliciosas. portanto, monitorizar a operação da eduroam assume um papel importante em garantir uma prestação adequada dos serviços de rede. para que tal seja possível, irá ser necessário estudar a rede sem fios do di, fazer uma recolha e análise de dados sem fios e disponibilizar os resultados da análise em tempo real. neste contexto, foi desenvolvida uma aplicação web com interface amigável de forma a monitorizar o ambiente sem fios em tempo real, identificando comportamentos incorretos e/ou degradação de desempenho. foram utilizadas algumas linguagens de programação como php e javascript para processamento de dados e interação com o utilizador e ainda rrdtool como forma de armazenamento de dados. após a análise dos resultados obtidos foi possível concluir que os 33 pontos de acesso do di estão sobre uma taxa de utilização inferior a 30% sendo suficientes para a quantidade de dispositivos atual.",
      "facing the perceived low service quality of the eduroam network covering the department of informatics (di) of the university of minho (um), a redesign and restructuring of the wireless network was carried out. monitoring the wireless network allows to be aware of possible situations that may cause service malfunction, such as other networks (e.g., ad-hoc, unauthorised, etc.) in the same wireless space affecting negatively eduroam performance or the use of other networks for malicious purposes. therefore, monitoring eduroam operation assumes an important role to guarantee a proper network service delivery. for making this possible, it will be necessary to study the department wireless network, collect and analyse wireless data and provide the results of the analysis in real time. in this context, an integrated solution with a user-friendly interface was developed in order to monitor the use of the wireless space in a real-time, identifying incorrect behaviours and/or performance degradation. some programming languages were used like php and javascript in order to make the data proccessing and handle user interaction. it was also used rrdtool as a way of data storage. after the analysis of the obtained results it was concluded that the 33 access points of di are under less than 30% usage rate being enough for the actual ammount of devices."
    ],
    [
      "predicting chemical reaction yields is a widely investigated problem in drug discovery due to the natu ral appearance of diseases and viruses worldwide. with the evolution and further study of machine learn ing, this area of computer science has provided alternatives that help chemists search for more effective molecule combinations. this dissertation presents two research hypotheses with different bases that seek to improve this prediction problem. the first research hypothesis is related to the support vector regression algorithm, which uses graph kernels to measure similarity between molecules and then perform the pre diction. we propose the application of non-linearity in the weisfeiler-lehman graph kernel to improve the measure of comparison between molecules and thus enhance the complexity of the support vector regres sion models. the second research hypothesis is related to the class of neural networks. we propose a deep learning base to solve this problem through graph neural networks, which use graph convolutional layers and global read-out operations to extract molecular features from graph-structure data. the main focus is to ensure that all models generalise well to obtain good results in experiments with unknown molecules. we performed tests on chemical data for both methods and achieved improvements. the non-linearity in graph kernels proved to be the most advantageous, having surpassed the state-of-the-art methods in one of the two global tests performed. the graph neural networks were not as effective, although they showed competitive results. concerning neural networks, we highlight the creation of the deep learning base and the in-depth analysis of the hyperparameters to enhance further research on the reaction yield prediction problem, as this area shows immense potential in drug discovery.",
      "a previsão do rendimento de reações químicas é um problema amplamente investigado na descoberta de medicamentos devido ao aparecimento natural de doenças e vírus em todo o mundo. com a evolução e o estudo do machine learning, esta área da ciência da computação tem oferecido alternativas que ajudam os químicos a encontrar combinações de moléculas mais eficazes. esta dissertação apresenta duas hipóteses de pesquisa com bases distintas que procuram melhorar este problema de previsão. a primeira hipótese de pesquisa está relacionada com o algoritmo support vector regression, que usa graph kernels para medir a similaridade entre as moléculas e, em seguida, realizar a previsão. propomos a aplicação da não-linearidade no weisfeiler-lehman graph kernel para melhorar a medida de comparação entre moléculas e, assim, aumentar a complexidade dos modelos support vector regression. a segunda hipótese de pesquisa está relacionada com as redes neuronais. propomos uma base de deep learning para resolver este problema através de redes neuronais, que usam camadas convolucionais adaptadas a grafos e operações de aglomeração de nodos que extraem características moleculares dos grafos. o objetivo principal é garantir que todos os modelos generalizam bem para obter bons resultados em novas experiências com moléculas desconhecidas. realizámos testes em dados químicos para ambos os métodos e alcançamos melhorias. a não-linearidade nos graph kernels mostrou-se a mais vantajosa, tendo superado os métodos do estado-da-arte em um dos dois testes globais realizados. as redes neuronais de grafos não foram tão eficazes, embora tenham apresentado resultados competitivos. ainda no que diz respeito às redes neuronais, destacamos a criação da base de deep learning e a análise aprofundada dos hiperparâmetros como fundamentação para as pesquisas futuras no problema de previsão de rendimento de reações químicas, uma vez que esta área apresenta muito potencial na descoberta de medicamentos."
    ],
    0.3
  ],
  [
    [
      "as technology continues to advance, many companies are seeking ways to integrate artificial intel ligence (ai) into their operations in order to optimize their workflow and improve efficiency. one area that could greatly benefit from ai solutions is the human resources (hr) department. this disserta tion explores the use of ai and natural language processing (nlp) to extract emotions from text-based communications, such as emails and instant messages between hr representatives and employees. by providing insight into the states of mind being expressed in text, this technology has the potential to im prove communication and understanding between the two parties, which in turn may lead to better conflict resolution, increased employee engagement, and improved productivity. the study will examine the use of various machine learning algorithms ranging from convolutional neural network (cnn) and recurrent neural network (rnn) to current state-of-the-art architectures such as transformers like bert and xlnet to extract better insights from text and to identify the emotions contained in messages. the results of this thesis contribute to the development of an ai solution capable of identifying one out of seven emotions in both general and specialized conversations with an accuracy of 75.38%, having the capacity of enhancing the efficiency of hr departments by facilitating communication and understanding with employees.",
      "à medida que a tecnologia avança, muitas empresas procuram formas de integrar a inteligência artificial (ia) nas suas operações, a fim de optimizar o seu fluxo de trabalho e melhorar a eficiência. uma área que pode beneficiar da inteligência artificial é o departamento de recursos humanos (rh). este artigo explora o uso de ia e processamento de linguagem natural (pnl) para extrair emoções em comunicações baseadas em texto, tais como e-mails e mensagens entre representantes de rh e empregados. ao fornecer informações sobre o estado de espírito expresso no texto, esta tecnologia tem o potencial de melhorar a comunicação e a compreensão entre as duas partes, o que por sua vez pode levar a uma melhor resolução de conflitos, a um maior envolvimento dos funcionários e a uma maior produtividade. o estudo examinará a utilização de vários algoritmos de aprendizagem de máquinas, tais como convolutional neural network (cnn) e recurrent neural network (rnn) para extrair melhores percepções do texto e para identificar as emoções contidas nas mensagens. os resultados desta tese contribuem para o desenvolvimento de uma solução de ia capaz de identificar uma de entre sete possíveis emoções em conversas de contexto profissional tal como conversas gerais com uma precisão de 75.38%, tendo a capacidade de melhorar o fluxo de trabalho dos departamentos de rh, facilitando a comunicação e a compreensão com os funcionários."
    ],
    [
      "nos dias que correm a utilização de sistemas multidimensionais de dados faz parte do quotidiano de qualquer organização de média ou de grande dimensão. este tipo de sistema, cuja principal finalidade consiste em auxiliar os seus utilizadores nas suas atividades de tomada de decisão, tem por principais características a flexibilidade na exploração de dados e a celeridade na disponibilização de informação. apesar de todos os mecanismos já existentes, que contribuem para este objetivo, é, por vezes, muito difícil manter os níveis de desempenho desejados pelos utilizadores. como consequência disto, foram estudadas outras formas para reduzir a carga imposta ao servidor multidimensional de dados – o servidor olap. um destes mecanismos é a criação de caches que armazenam informação já consultada e que, aquando de um pedido, o satisfazem sem terem a necessidade de consultar a sua fonte. devido à natureza dos utilizadores dos sistemas olap, é possível determinar com bastante precisão os seus padrões de acesso e de exploração, isto é, quais as consultas efetuadas por um determinado utilizador ao longo de um dado período de tempo. levando esta análise para um pouco mais à frente é, ainda, possível prever com antecedência qual a sequência exata de consultas que será efetuada por um determinado utilizador quando este iniciar uma qualquer sessão de exploração de dados. após esta fase de previsão, consegue-se decidir quais as queries que deverão ser pré-materializadas, armazenando-as numa cache, de forma a servir o maior número possível de pedidos do utilizador a partir desta. a técnica proposta nesta dissertação centra-se na problemática que gira em torno da simplificação do número de queries que deverão ser pré-materializadas. o objetivo final desta técnica consiste em manter um balanço positivo entre o tempo dispendido a realizar esta pré-materialização e o que seria gasto caso tudo fosse calculado apenas quando requisitado.",
      "nowadays, the use of multidimensional data systems has become a part of everyday actions in medium and large companies. this type of system, which concerns mainly in aiding it‟s users in the process of decision making, has a very large flexibility in data exploration and speed of response to queries. despite all the existing techniques, it is sometimes, very hard to maintain such high levels of performance users demand. with the purpose of tackling these performance losses, other techniques where developed, wich try to reduce central data servers load. one of such mechanisms is the creation of olap caches that maintain previous queries and serve them upon subsequent requests without having to ask the central server. due to olap systems organization, it is possible to identify the characteristics of its users and its exploration patterns – what queries will a user submit during a session. it is, however, possible to go one step further, and to predict exactly wich data will be requested by a specific user and, more important, the sequence of those requests. this is called the prediction phase and is followed by the pre-materialization of views that correspond to the user‟s requests in the future. these views are then stored in the cache and served to the user in the appropriate time. the proposed technique‟s main goal consists in maintaining a positive ration between the time spent to predict and materialize the views, and the time that would be spent if no prediction had been done."
    ],
    0.0
  ],
  [
    [
      "n the current spectrum of the world, there’s been a significant increase in cybersecurity threats, one of each is ransomware threats from whom linux environments have become a more recent target compared to windows environments and the area of study of ransomware in linuxsystems have been relatively under studied compared to their windows counterparts, despite the growing diversity of linux operating systems and their significance in various infrastructures. to address this research gap, this dissertation conducts a comprehensive investigation into the behaviour of ransomware in a compromised linux system. the study employs both static and dynamic analyses to gain insights into the behaviours and characteristics of three distinct ransomware payload families, avos, ransomexx, and revil. to test the analysis, a customized sandbox environment has been developed using virtualbox (oracle, 2023) and the lubuntu operating system. a set of automated tests have been created with shell and python scripts to carry out dynamic analysis tests. the static analysis uncovered that these payload families contained visible strings associated with malicious activities, and some of the executables presented obfuscation to challenge reverse engineering efforts. this suggests a strong likelihood that the payload’s primary objective is malicious. in the dynamic analysis, examining ransomware payloads in action yielded valuable insights. notably, the analysis uncovered that the overall behaviour is significantly similar between all three families studied. the research also identified the influence of the number of files and file size on the ransomware behaviour, particularly in terms of execution time and the number of executions by the system calls. this effect highlights the importance of specific system calls, such as write, read, lseek, stat, and newfstat. this system calls plays a crucial role in the behaviour of ransomware payloads across the families studied, and they also highlight the similarities with file-intensive programs with no maliciousness. intricate relationships are also observed between lseek,read, and write system calls, indicating efficient file manipulation by the ransomware. some patterns found that might help distinguish the payloads from non-malicious behaviour are the intriguing pattern discovered between error behaviour and the system call futex. in the payloads where the system call futex is present, the majority of the errors that occurred during the payloads execution are related to this system call this pattern although not as evident is also replicated in the clock_nanosleep system call behaviour. a distinguishable pattern found in the rename and chmod system callsis that the number of executions is the same as the number of files present in the directory encrypted. this research also allowed to detect in the studied families, a process behaviour that showed specific malicious intentions targeting vmware systems, confirming the findings from (vmware, 2022). in the realm of future research, this study paves the way for further exploration of the significance of specific system calls in ransomware payloads and their responses to various environmental factors. this understanding can significantly enhance ransomware detection methods in linux systems. moreover, it sets the stage for future investigations into linux ransomware across diverse scenarios, including different linux operating systems, containerized environments, and a wide array of ransomware payloads, such as locker ransomware.",
      "no mundo atual, tem-se observado um aumento significativo de ameaças a nível de cibersegurança, uma das quais são as ameaças de ransomware, das quais os ambientes linux se tornaram um alvo recente comparativamente aos ambientes windows. esta área de estudo de ransomware em sistemas linux está relativamente pouco estudada comparativamente aos sistemas windows, apesar da crescente diversidade de sistemas operativos linux e da sua importância em várias infraestruturas. para colmatar esta carência ao nível da investigação, esta dissertação conduz uma investigação compreensiva sobre o comportamento do ransomware num sistema linux afetado. este estudo utiliza análises estáticas e dinâmicas para obter informações sobre os comportamentos e as características de três famílias distintas de cargas úteis de ransomware avos, ransomexx e revil. para testar a análise, foi desenvolvido um ambiente sandbox personalizado utilizando o virtualbox (oracle, 2023) e o sistema operativo lubuntu. foi criado um conjunto de testes automatizados com o auxilio de scripts shell e python para efetuar testes análise dinâmica. o análise estática revelou que os binários destas famílias de ransomware continham cadeias de caracteres visíveis associadas a actividades maliciosas e alguns dos binários apresentavam ofuscação para desafiar os esforços de engenharia reversa. isto sugere uma forte probabilidade de o objetivo principal do payload ser malicioso. na analise dinâmica, que permite examinar as executáveis em ação, permitiu obter informação valiosa. a análise revelou que o comportamento geral é significativamente semelhante entre as três famílias estudadas. além disso, a investigação identificou a influência do número de ficheiros e do tamanho dos ficheiros no comportamento do ransomware, particularmente em termos de tempo de execução e do número de execuções de chaamdas ao sistema. este efeito realça a importância de algumas chamadas ao sistema específicas, tais como write, read, lseek, stat e newfstat. estas chamadas ao sistema desempenham um papel crucial no comportamento dos payloads de ransomware nas famílias estudadas, embora também realce as semelhanças de comportamento com programas de ficheiros intensivos sem intenções maliciosas. são também observadas relações intrincadas entre as chamadas ao sistema lseek,read e write, indicando uma manipulação eficiente dos ficheiros por parte do ransomware. alguns padrões encontrados que podem distinguir os payloads de programas não-maliciosos são, o padrão descoberto entre o comportamento de erros e a chamada ao sistema futex. nos casos em que o futex está presente na execução, a maioria dos erros ocorreu dentro desta chamada ao sistema, este mesmo padrão, embora não tão evidente, também é encontrado na chamada ao sistema clock_nanosleep. outro padrão interessatne encontrado é o padrão presente nas chamadas ao sistema rename e chmod estas têm sempre o mesmo número de execuções que o número de ficheiros presentes na diretoria que foi afetada. esta investigação permitiu ainda detetar, que nas famílias estudadas, um comportamento nos processes que revelam intenções maliciosas específicas dirigidas a sistemas vmware, confirmando as conclusões do estudo (vmware, 2022). no âmbito de futuras investigações, este estudo abre caminho para uma maior exploração da im portância de chamadas ao sistema específicas no comportamento do ransomware e das suas reacções a vários elementos do ambiente. esta compreensão pode melhorar significativamente os métodos de deteção de ransomware em sistemas linux. além disso, prepara o terreno para futuras investigações sobre o ransomware em linux nos diversos cenários, incluindo diferentes sistemas operativos linux, ambientes em contêineres e a diferentes tipos de ransomware, como o locker."
    ],
    [
      "within the past decade, not only societies in general but also medicine and healthcare, in particular, have changed tremendously. in large part because of the rapid dissemination of computers and digital communications which lead to the appearance of new medical disciplines, such as medical informatics. nowadays, one of the most prominent field in medical informatics is medical imaging, as it is implied, it is a collection of methodologies and techniques used in order to visually and spatially represent parts of the brain for diagnostic and research purposes. in the research ecosystem, neuroimaging is an increasing popular field, with applications in neurology and psychiatry. however, due to the difficulties to handle neuroimaging data, since data has its own specificities, researchers have encountered problems to correctly handling this data. this can be a crucial issue specially with large volumes of neuroimaging data and all the research materials associated. this work aims to architect and build a research platform to correctly archive medical imaging data and all the associated research materials, where researchers can exchange imaging data and collaborate in neuroimaging research projects. the platform offers a correct way to collect and store all imaging data, archiving all of patient exams with the correspondent information, making available the correspondent information to researchers in a confidential, secure and efficient way. the two main outcomes of this work are an architecture of a platform that manages all imaging data and associated research materials, plus an open-source python package to easily interact with that platform.",
      "na última década as sociedades em geral, mas a medicina e os cuidados de saúde em particular mudaram significativamente. em grande parte, devido à rápida disseminação dos computadores e das comunicações digitais, o que permitiu o aparecimento de novas disciplinas médicas como é o caso da informática médica. hoje em dia, um dos campos mais proeminentes na informática médica é a imagem médica, a coleção de metodologias e técnicas usadas para visualmente e espacialmente representar partes do cérebro para diagnóstico e investigação. em investigação, a neuroimagem é cada vez mais popular, com aplicações que vão desde a neurologia até à psiquiatria. contudo, os investigadores têm encontrado um problema em como gerir estes dados já que estes têm as suas próprias especificidades e são, normalmente, armazenados em grandes quantidades. este trabalho pretende arquitetar e construir uma plataforma para armazenar e gerir dados de imagem médica e todos os materiais de investigação associados onde os investigadores possam arquivar corretamente e partilhar com outros interessados. assim como, construir ferramentas para interagir programaticamente com a plataforma, tendo como objetivo a sua integração no fluxo de trabalho atual."
    ],
    0.3
  ],
  [
    [
      "cloud computing environments, particularly cloud databases, are rapidly increasing in importance, acceptance and usage in major (web) applications, that need the partition-tolerance and availability for scalability purposes, thus sacrificing the consistency side (cap theorem). with this approach, use of paradigms such as eventual consistency became more widespread. in these environments, a large number of users access data stored in highly available storage systems. to provide good performance to geographically disperse users and allow operation even in the presence of failures or network partitions, these systems often rely on optimistic replication solutions that guarantee only eventual consistency. in this scenario, it is important to be able to accurately and e ciently identify updates executed concurrently. in this dissertation we review, and expose problems with current approaches to causality tracking in optimistic replication: these either lose information about causality or do not scale, as they require replicas to maintain information that grows linearly with the number of clients or updates. then, we propose dotted version vectors (dvv), a novel mechanism for dealing with data versioning in eventual consistent systems, that allows both accurate causality tracking and scalability both in the number of clients and servers, while limiting vector size to replication degree. we conclude with the challenges faced when implementing dvv in riak (a distributed key-value store), the evaluation of its behavior and performance, and discuss the advantages and disadvantages of it.",
      "ambientes de computação na nuvem, em especial sistemas de base de dados na nuvem, estão rapidamente a aumentar em importância, aceitação e utilização entre as grandes aplicações (web), que precisam de alta disponibilidade e tolerância a partições por razões de escalabilidade, para isso sacrificando o lado da coerência (teorema de cap). com esta abordagem, o uso de paradigmas como a coerência inevitável tornou-se generalizado. nestes sistemas, um grande número de utilizadores têm acesso aos dados presentes em sistemas de dados de alta disponibilidade. para fornecer bom desempenho para utilizadores geograficamente dispersos e permitir a realização de operações mesmo em presença de partições ou falhas de nós, estes sistemas usam técnicas de replicação optimista que garantem apenas uma coerência inevitável. nestes cenários, é importante que a identificação de escritas concorrentes de dados, seja o mais exata e eficiente possível. nesta dissertação, revemos os problemas com as abordagens atuais para o registo da causalidade na replicação optimista: estes ou perdem informação sobre a causalidade ou não escalam, já que obrigam as réplicas a manter informação que cresce linearmente com o número de clientes ou escritas. propomos então, os dotted version vectors (dvv), um novo mecanismo para lidar com o versionamento de dados em ambientes com coerência inevitável, que permite tanto um registo exato e correto da causalidade, bem como escalabilidade em relação ao número de clientes e número de servidores, limitando o seu tamanho ao factor de replicação. concluímos com os desafios surgidos na implementação dos dvv no riak (uma base de dados distribuída de chave/valor), a sua avaliação de comportamento e de desempenho, acabando com uma análise das vantagens e desvantagens da mesma."
    ],
    [
      "as técnicas de teste baseados em modelos (do inglês, model based testing (mbt)) comparam o comportamento do sistema sob teste com o comportamento do modelo do sistema (o oráculo). a aplicação de mbt às interfaces gráficas do utilizador (do inglês, graphical user interface (gui)) permite uma avaliação mais exaustiva e contínua do sistema, através da simulação de ações do utilizador com a interface gráfica. desta forma, é possível reduzir significativamente o custo de avaliação do sistema, e identificar, eventualmente, erros de implementação através da gui, sem o envolvimento de utilizadores externos. este processo decorre através da execução dos casos de teste, gerados a partir do modelo do sistema, na aplicação sobre teste. são estes casos de teste que verificam se a implementação está de acordo com o modelo, assegurando assim uma melhoria da qualidade do sistema desenvolvido. esta dissertação descreve uma ferramenta de mbt para aplicações web, a tom framework. parte da framework (tom generator) aproveita trabalho anteriormente desenvolvido, a outra (tom editor) é aqui apresentada. os objetivos principais da framework passam por automatizar e facilitar a criação de modelos do sistema que, posteriormente, são utilizados para gerar automaticamente casos de teste executáveis na interface gráfica sobre teste. a captura e interpretação da interação do utilizador com a aplicação web sobre teste foi um dos desafios ultrapassados no desenvolvimento desta dissertação. no final da mesma, encontra-se uma aplicação da framework a um caso de estudo.",
      "the techniques of model based testing (mbt) compare the behaviours of the system under test with the system model (the oracle). the application of mbt to the graphical user interface allows a more exaustive and continuous evaluation of the system, through the simulation of user actions on the graphical interface. as such, it is possible to meaning substantially reduce the cost of system evaluation and, eventually, identify implementation errors through the gui, without the involvement of external users. this process occurs through the execution of test cases, generated from the system model, on the application under test. these are the tests case that verify if the implementation is following the model, ensuring the improvement of the developed system’s quality. this dissertation describes a mbt tool for web applications, the tom framework. part of the framework (tom generator) takes advantage of previously developed work, the other (tom editor) is presented here. the main goals of the framework are to automate and facilitate the creation of models of the system that will be used to automatically generate executable test cases in the graphical interface under test. the capture and interpretation of user interaction with the web application under test was one of the challenges that was overcome during the development of this dissertation. at the end of it, one can find an application of the framework in a case study."
    ],
    0.0
  ],
  [
    [
      "the dairy food industry is constantly changing as novel biotechnological techniques improve the manufacturing process of dairy products. widely used over the years in the yogurt and cheese manufacturing, streptococcus thermophilus is now considered as an extremely valuable lactic acid bacterium for the annual market of the dairy industry. a specific, but of easy-access knowledge regarding the thermophilic bacteria metabolism would be a plus for the continuous growth of such industry. in this work, we present the genome-scale metabolic (gsm) model for the lmd- 9 strain of s. thermophilus together with the detailed description of the species metabolic capabilities at the cellular level. the reconstruction of the genome-scale metabolic model, was performed using metabolic models reconstruction using genome-scale information (merlin) together with cobrapy tool and optflux platform. s. thermophilus lmd-9 genome was functionally annotated and the encoded metabolic information was afterwards used to assemble a draft network. after extensive manual curation, the metabolic network was converted to a comprehensive metabolic model. the assembled gsm model was then validated against experimental data. the metabolism of this important stater for the dairy industry has been accessed in detail through the reconstruction. the organism possesses a simple machinery for central carbon metabolism and shows a narrow spectrum of carbohydrate utilization. the genome-scale metabolic model additionally suggests the existence of several pyruvate dissipating pathways which end in the synthesis of various compounds of interest. in silico simulations demonstrated the production of lactate and residual amounts of formate, acetolactate and acetaldehyde. regarding the amino acid metabolism, the organism possesses complete pathways for the biosynthesis of all amino acids, except for lysine, methionine and cysteine. furthermore, the gsm model can be used to simulate other relevant features of the s. thermophilus metabolism, such as the aroma compounds and exopolysaccharides (eps) synthesis, oxygen tolerance, absence of complete citrate cycle and pentose phosphate pathway, urea metabolism or amino acid catabolism.",
      "a indústria dos lacticínios encontra-se em constante mudança devido ao aparecimento de novas técnicas biotecnológicas que permitem o melhoramento da produção dos laticínios. amplamente utilizado ao longo dos anos na produção de iogurte e queijo, streptococcus thermophilus é agora considerado extremamente valioso para o mercado anual desta indústria. portanto, conhecimento especifico, mas facilmente acessível e compreensivo sobre o metabolismo da bactéria seria uma vantagem para o crescimento continuo desta industria. nesta tese, apresentamos o modelo metabólico à escala genómica para a estirpe lmd-9 de s. thermophilus, juntamente com um estudo aprofundado das suas capacidades metabólicas. para obter a reconstrução do modelo metabólico à escala genómica, foi usada principalmente a ferramenta merlin com o apoio da ferramenta cobrapy e a plataforma optflux. o genoma de s. thermophilus lmd-9 foi anotado e as informações metabólicas codificadas foram usadas para construir uma rede rascunho. após curação manual, a rede metabólica foi convertida num modelo metabólico à escala genómica. posteriormente, o modelo de s. thermophilus foi validado contra dados experimentais. o metabolismo desta bactéria acido láctica foi estudado em detalhe através da reconstrução. o organism dispõe de um metabolismo de carbono muito simples e um espectro de utlização de hidratos de carbono bastante reduzido. além disso, o modelo desenvolvido sugere a existência de várias vias metabólicas que se iniciam no piruvato e terminam na síntese de vários compostos de interesse, embora as simulações in silico tenham demonstrado apenas a produção de lactato e quantidades residuais de formato, acetolactato e acetaldeído. no que diz respeito ao metabolismo dos aminoácidos, o organismo possui as vias completas para a biossíntese de todos aminoácidos, à exceção da lisina, metionina e cisteína. o modelo pode ser usado para simular outras características relevantes de s. thermophilus, tais como a síntese de eps e compostos aromáticos, tolerância ao oxigénio, ausência de um ciclo completo do ácido cítrico ou da via das pentoses fosfato, metabolismo da ureia ou catabolismo de aminoácidos."
    ],
    [
      "a análise de sentimentos é uma das áreas mais importantes na ciência da computação, nomeadamente no processamento da linguagem natural. as suas aplicações vão desde a análise de produtos até à contenção do cyberbullying. a importância da análise dos sentimentos é inigualável, mas quando se trata de línguas menos faladas, o campo parece ficar para trás. neste contexto, omnium ai propôs uma dissertação onde exploramos a análise de sentimentos para a língua portuguesa, com a intenção de criar uma nova ferramenta computacional. esta dissertação vai examinar o campo da análise de sentimentos e o desenvolvimento do package omnia. este package é composto por ferramentas para a leitura de dados, o seu processamento e a criação de modelos machine learning (ml) e deep learning (dl) a partir dos dados lidos. em específico, vamos concentrarnos no desenvolvimento do package omnia text mining, com objectivo de criar ferramentas de pré-processamento e modelos de ml e dl para a análise de sentimentos para a língua portuguesa. esta dissertação vai criar uma abordagem para lidar com problemas de análise de sentimentos composta por um processo de recolha de dados, seguido de um passo de pré-processamento e acabando com o desenvolvimento de modelos de ml e dl. esta abordagem será aplicada ao tópico do covid-19. após serem criados os modelos para os datasets relativos ao covid, avaliamos os resultados para as diferentes combinações de métodos de pré-processamento e modelos onde apuramos que as long short term memory (lstm)s e o hfautomodel com o embedding bert foram os melhores modelos. no geral, os modelos de dl e autogluon obtiveram melhores resultados que os modelos de ml. nos métodos de pré-processamento visualizamos que não existe uma pipeline geral que possa ser utilizada para todos os casos. no final, iremos discutir as conclusões que podemos retirar desta dissertação juntamente com uma secção de trabalho futuro, onde exploraremos os próximos passos possíveis para este projecto.",
      "sentiment analysis is one of the most important areas in computer science, namely in natural language processing. its applications range from product reviews to cyberbullying containment. the importance of sentiment analysis is unprecedented, but when it comes to lesser-used languages, the field seems to be lagging behind. in this context, omnium ai proposed a dissertation where we explore sentiment analysis for the portuguese language with the aim of creating a new computational tool. this dissertation is going to delve into the sentiment analysis field and the development of the omnia package. this package is composed of tools for reading datasets, processing them and creating ml and dl models from the data read. specifically, we will focus on developing the omnia text mining package, with aim of creating pre-processing tools and models for sentiment analysis (sa) in the portuguese language. this dissertation creates an approach to tackle sa problems that involve a data gathering step followed by a pre-processing step and finishing with a model step where we develop different ml and dl models. this approach will be applied to a covid-19 topic. from this approach, we obtained two datasets, from which we created ml, dl and autogluon models. after creating the models we evaluated the results from the different combinations of pre-processing methods (pipelines) and ml and dl models where we ascertained that lstms and hfautomodel with a bert embedding were the best models for the datasets we used. in general, dl and autogluon models gave us better results than ml. for the pre-processing pipelines, we were able to visualise that there is no one pipeline fits all solution, each model had different pipelines working better. lastly, we will discuss the conclusions we can take from this work along with a future work section, where we explore the possible next steps for this project."
    ],
    0.0
  ],
  [
    [
      "software engineering has been contributing, over the years, to a better and more efficient production of software. through its methodologies and processes, it has been used to increase the assurance that the software produced is robust, of quality, easy to update, and above all that, conforms to the requirements identified by the stakeholders. with the growth of data sharing, collection and storage in the utilities sector, there is also an urgent need to maintain and use the available information in a useful way. this requires the adoption of strategies and infrastructures that can help leveraging this form of treatment to make it possible to improve the quality of certain utility sectors (gas, electricity, water, internet, communications). however, much of this process is still nowadays controlled solely by the production and distribution companies, preventing the all other users to participate in the process. to try to undo the supremacy of the production and distribution companies have on the utilities panorama, and in support of the european commission vision, the energy sector is trying to move towards a liberalized market and, with this, it aims to enable all entities such as consumers, retailers, producers, distributors, to contribute to the management of the network and for new business models to emerge. to sustain this ecosystem, where these entities can communicate and share data, it would be advantageous to have a platform that would allow the communication of such data between all users. in this project, and through the application of se techniques, we will develop, step by step, a model for a modular, scalable and integrated environment to enable demand response, data exploration, storage and fulfill all the european union-wide data protection regulation (gdpr).",
      "a engenharia de software tem contribuído, ao longo dos anos, para uma melhor e eficiente produção de software. através de metodologias e processos, é possível produzir software produzido robusto, com qualidade, confiável, que possa ser atualizado e, acima de tudo, que respeite os requisitos identificados pelas partes interessadas. com o crescimento da partilha, coleta e armazenamento de dados no setor de serviços públicos, existe uma necessidade urgente de manter e usar as informações disponíveis de maneira útil, com que se consiga extrair conhecimento. isso requer a adoção de estratégias e infraestruturas que possam apoiar essa forma de tratamento para que, a partir da coleta de informações, seja possível melhorar a qualidade de certos setores de serviços públicos (gás, eletricidade, água, internet, comunicações). no entanto, e nos tempos que correm, grande parte desses processos é controlada exclusivamente pelas empresas de produção e distribuição, impedindo que os demais utilizadores participem do processo. de forma desfazer a supremacia que as empresas de produção e distribuição têm, e com apoio e motivação da comissão europeia, o setor energético tem vindo a tentar implementar um mercado de energia liberalizado e, com isso, permitir que todas as entidades como, consumidores, retalhistas, produtores, distribuidores, possam contribuir para a gestão da rede e na criação de novos modelos de negócio. para sustentar este ecossistema, onde todas as entidades podem comunicar e compartilhar dados, seria útil e vantajoso ter uma plataforma que permitisse a comunicação de tais dados entre todos os utilizadores. neste projeto, e através da aplicação de técnicas de se, pretenderemos mostrar passo a passo um método para construir um ambiente modular, escalável e integrado para permitir resposta a procura, exploração dos dados, armazenamento e que cumpra a nova lei da união europeia respeitante à proteção de dados (gdpr)."
    ],
    [
      "nas empresas de telecomunicações, as centrais de chamadas são os elementos que têm maior interação com os clientes, e o desempenho dos operadores é vital porque um excelente serviço satisfaz o cliente e ajuda a um melhor funcionamento. deste modo, tenta-se utilizar dados de clientes, dados de operadores de chamadas e dados históricos de serviço de forma a melhorar o suporte. o emparelhamento de um cliente com um operador que se sinta confortável com o problema a resolver ajuda as empresas a reduzir custos, melhora o atendimento ao cliente e aumenta a produtividade dos colaboradores. nesta dissertação, propõe-se um modelo de previsão, baseado em machine learning e em otimização, que antevê o problema pelo qual o cliente está a ligar e encaminha a chamada e o cliente para o operador mais apropriado. após a análise de um dataset não balanceado, com aproximadamente 2.9 milhões de entradas, e recorrendo à metodologia crisp-dm para modelação, alguns algoritmos inovadores como o lightgbm, permitiram obter um micro-f1 de 0.41 e auc-roc de 0.84. deste modo, pode-se aferir alguma capacidade na previsão da razão para o cliente estar a contactar a empresa. a alocação e otimização num cenário simulado, pós-previsão, indicou uma melhoria na ordem dos 50%, relativamente ao ganho de eficiência, eficácia e rapidez do call-center. os resultados mostram que a utilização de grandes quantidades de dados comerciais para a previsão pode melhorar o desempenho do suporte ao cliente. além disso, as conclusões e as estratégias exploradas nesta tese podem levar à utilização deste sistema, e com isso ajudar a empresa a obter maiores lucros e podem ajudar futuros investigadores a tomar melhores decisões no estudo e no desenvolvimento de soluções parecidas.",
      "at telecommunications companies, call-centers have the highest interaction with customers, and the operators’ performance is vital because an excellent service satisfies the customer and helps a better operation. therefore, attempts are made to use customer data, call operator data, and historical service data to improve support. pairing a customer with an operator who is comfortable with the problem to solve helps companies reducing costs, improves customer service, and increases employee productivity. in this thesis, we propose an approach based on machine learning and optimization, which predicts the problem for which the customer is calling and routes the call and the customer to the most appropriate call operator. after the analysis of an unbalanced dataset, with approximately 2.9 million entries, and using the crisp-dm methodology for modeling, some innovative algorithms such as lightgbm, allowed obtaining a micro-f1 of 0.41 and auc-roc of 0.84. these results allow us to gauge some capacity in predicting why the client is contacting the company. the optimization in a simulated scenario, post-prediction, indicated an improvement in the order of 50%, concerning the efficiency gain, effectiveness, and speed of the call-center. the results show that using large amounts of business data for the prediction can improve customer support performance. also, the findings and strategies explored in this thesis can lead to the use of this system, thus helping the company obtain higher profits and help future researchers make better decisions in the study and development of similar solutions."
    ],
    0.3
  ],
  [
    [
      "microbial communities participate in many biological processes, directly affecting its surrounding environment. thus, the study of a community’s behaviour and interactions among its members can be very useful in the biotechnology, environmental and human health fields. nevertheless, decoding the metabolic exchanges between microorganisms and community dynamics remains a challenge. computational modelling methods have gained interest as a way to unravel the interactions and behaviour. gsm models allow the prediction of an organism’s response to changes in genetic and environmental conditions. thus, the extension of such method to a community level can help decode a community’s phenotype. in this work, different gsm models and current bioinformatics tools were used to model the metabolism of different microbial communities. the different tools’ performances were compared to assess which is currently the best method to perform an analysis on a community level. distinct case studies regarding microbial communities for which its interactions were already known, were selected. to assess the tools’ performances, each tools output was compared to what was expected in theory. cobra toolbox's methods proved to be useful to build a community structure from individual gsm models, while pfba and steadycom’s simulation methods can predict exchange between the organisms and the environment. additionally, dynamic flux balance analysis (dfba) approaches, such as dfbalab and dymmm, can successfully simulate metabolite and biomass variation over time. nevertheless, these methods are more limited as they require specific organism information, which is not always available. several gsm models are available for use. nonetheless, their quality control has to gain attention as the simulations’ results are directly affected by the individual models accuracy to represent an organism’s metabolism. thus, community model builders should carefully chose a gsm model, or combination of models before performing simulations.",
      "comunidades microbianas participam em inúmeros processos biológicos, afetando diretamente o ambiente que as engloba. assim, o estudo do comportamento de uma comunidade e interações entre os seus membros pode ser muito útil nas áreas da biotecnologia, ambiente e saúde. no entanto, descodificar as trocas entre microrganismos e a dinâmica de comunidades continua um desafio. métodos de modelação computacional têm ganho interesse como forma de desvendar tais interações e comportamento de comunidades. modelos metabólicos à escala genómica permitem prever a resposta de um certo organismo a mudanças genéticas e ambientais. assim, a extensão de tal método ao nível de comunidade pode ajudar a prever o fenótipo de uma certa comunidade. no presente trabalho, diferentes modelos metabólicos à escala genómica e ferramentas bioinformáticas foram utilizados para modelar o metabolismo de diferentes comunidades microbianas, comparando o desempenho destas ferramentas para avaliar qual o melhor método para análise ao nível da comunidade. casos de estudo distintos, relativos a comunidades para as quais se conhecem as interações, foram selecionados. por fim, para aferir o desempenho das ferramentas, os respetivos resultados foram comparados ao teoricamente esperado. os métodos da ferramenta cobra toolbox provaram ser úteis para construir a estrutura da comunidade, usando modelos metabólicos à escala genómica dos organismos individuais. quanto a métodos de simulação, pfba e steadycom são úteis para prever trocas entre os organismos e o ambiente que os envolve. para além disso, abordagens dfba, como dfbalab e dymmm, podem simular a variação da concentração de metabolitos e biomassa ao longo do tempo. no entanto, estes métodos apresentam limitações por requererem informação específica ao organismo, que nem sempre se encontra disponível. vários modelos metabólicos à escala genómica estão disponibilizados. no entanto, o controlo na qualidade destes tem que ganhar atenção, visto que os resultados das simulações são diretamente afetados pela sua precisão na representação do metabolismo de um organismo e consequentemente, da comunidade. assim, para construir um modelo de comunidades, é necessária uma seleção cuidadosa dos modelos individuais a usar, antes de serem feitas simulações."
    ],
    [
      "a curiosidade do ser humano é algo que estará sempre presente no caminho da humanidade, sendo este aspeto que nos faz querer perceber como nós e o mundo que nos rodeia funciona. com os diferentes estudos e ensaios realizados ao longo dos tempos, foi possível observar que os seres humanos não são tão diferentes de outros animais em diversos aspetos, desta maneira, ao estudar os comportamentos e anatomia destes animais foi possível descobrir e estudar de forma análoga novos aspetos dos seres humanos. nesta dissertação o foco principal é o cérebro de rato da estripe wistar e tem como objetivo auxiliar o estudo desta estrutura. mais especificamente foram usadas dois tipos de imagens: uma de microscopia de secções do cérebro do rato e outra de um atlas que representam esquematicamente as primeiras. de modo a facilitar o processo de análise das imagens de microscopia, esta dissertação explica o desenvolvimento de uma linha de processos capaz de auxiliar esta etapa essencial na realização de qualquer tipo de estudo com as estruturas. o resultado obtido ao executar o método, publicamente disponível num repositório, consiste na sobreposição da representação esquemática de uma parte do cérebro do rato com a imagem de microscopia do tecido que lhe corresponde, o que permite a visualização mais clara dos componentes presentes na imagem original da secção coronal do cérebro de rato, auxiliando no seu estudo.",
      "the human curiosity is something that had always been present in mankind, being this on of the aspects that makes us want to understand ourselves and the world surrounding us. over the years studies and essays have been written, in which is possible to realize that we are not that difference from other animals in so many levels, thus, through the studies of the anatomy and behavior of these animals, it was possible to find and study new aspects related to the human being. in this thesis the main focus is the rat brain of the wistar strain and has as the main purpose assist in the study of this. specifically, 2 different types of images will be used: microscopy from a section of the rat brain and its schematical representation from an atlas. in order to ease the process of microscopy image analysis, throughout this thesis is explained the development of a pipeline of processes capable of assisting in this crucial step of any kind of study with these structures. the outcome accomplished through this pipeline processes, publicly available in a repository, consist of the overlapping of the schematical representation of the section of the rat brain and the related microscopy image of the tissue, allowing a clearer way to analyze the different components of the source image of a rat brain coronary section, supporting its study."
    ],
    0.06666666666666667
  ],
  [
    [
      "biotechnology plays an essential role in the modern industry and in guaranteeing sustainable future for humankind. advances of metabolic engineering and systems biology allow the adaption of complex cellular networks for the production or uptake of certain molecules, with great economical interest, enabling the creation of cell factories. among the potential microorganisms that fit this role is the well-known group, due to their role in food fermentation and, in particular, their use in dairy industry, known as lactic acid bacteria (lab). their metabolism is known for its relative simplicity and lack of biosynthesis capacity, creating a potential application as a cell factory in transformation processes. the purpose of this work is to develop through evolutionary engineering a strain of lab capable of utilizing mannitol as the sole carbon source and identify mutations in the evolved strain, with the objective of associate these mutations with the mannitol consuming phenotype. through the usage of adaptive laboratory evolution (ale), several strains of lab were evolved and a selected evolved strain of lactococcus lactis subsp cremoris, capable of consuming mannitol as the sole carbon source successfully, was sequenced using next-generation sequencing. from the analysis of this genomic data using several bioinformatics tools available, 3 mutations affecting the genes pta, adha and mtlf were identified as likely having an impact in the new phenotype presented by the evolved strain. this work provides an initial inquiry into a potential application of brown algae, which accumulate mannitol, as a new feedstock for biofuel production using lab as cell factories.",
      "a biotecnologia tem assumido um papel preponderante nos processos industriais da atualidade, tendo em vista a conjugação destes com a questão da sustentabilidade da espécie humana. os avanços na engenharia metabólica e na biologia de sistemas tem permitido a adaptação das complexas redes celulares, com o intuito de produzir ou consumir certos compostos, de forma a aumentar o seu valor económico, criando ‘fábricas celulares’. entre os potenciais organismos para este tipo de aplicação encontra-se um grupo bastante conhecido devido à sua função na fermentação de certos alimentos, especialmente lacticínios, denominadas bactérias ácido-lácticas. estas possuem um metabolismo relativamente simples e não apresentam várias capacidades biossintécticas, tornando-as em potenciais candidatas a serem usadas como ‘fábricas celulares’ em processos de transformação. neste trabalho pretende-se adaptar através de engenharia evolutiva várias espécies de bactérias ácido-lácticas à utilização de manitol como fonte de carbono e proceder à identificação de mutações no genoma das estirpes evoluídas através de tecnologias de sequenciação de adn, com o propósito de relacionar estas mutações com o fenótipo capaz de consumir manitol. com recurso à engenharia evolutiva, várias estirpes de bactérias ácido-lácticas foram evoluídas e uma dessas estirpes, lactococcus lactis subsp cremoris, capaz de consumir manitol como a única fonte de carbono, foi selecionada para ser sequenciada com recurso tecnologias de sequenciação de adn. através da análise destes dados genómicos usando várias ferramentas bioinformáticas, foi possível determinar 3 mutações que afectam os genes pta, adha e mtlf que possivelmente estarão relacionadas com o fenótipo exibido pelas espécies evoluídas. este trabalho serve como uma avaliação ao potencial da utilização de algas castanhas, que acumulam manitol, como um novo recurso para a produção de biocombustíveis, usando bactérias ácido-lácticas como ‘fábricas celulares’ para a sua transformação."
    ],
    [
      "nos dias de hoje, existe cada vez mais a necessidade de interligação/integração entre múltiplos sistemas heterogéneos no que respeita a arquiteturas e tecnologias. para que seja possível a comunicação entre os diversos sistemas recorre-se a serviços de middleware. estes providenciam programming interfaces (apis) com protocolos e padrões que todos entendem e através dos quais conseguem comunicar. assim, também é possível fornecer soluções de interoperabilidade com sistemas já existentes e que à partida não estarão preparados para a heterogeneidade de sistemas e arquiteturas que entretanto surgiram. desta forma, este tema de dissertação pretende conceber uma solução de middleware que possibilite a interoperabilidade com o sistema existente da caravela seguros, através da análise e conceção de uma arquitetura e posteriormente elaborando alguns serviços que provem a validade da solução encontrada. assim, pretende-se obter um sistema que facilite a utilização dos sistemas da caravela seguros por parte das entidades que com ela se relacionem.",
      "nowadays, there is increasingly a need for interconnection / integration of multiple heterogeneous systems in technologies and architectures. so that communication is possible between the various systems, we resort to middleware services. these provide programming interfaces (apis) with protocols and standards that everyone understands and through which they are able to communicate. thus, it is also possible to provide interoperability solutions with existing systems and that from the outset are not prepared for the heterogeneity of systems and architectures that have emerged in the meantime. as such, this dissertation topic intends to design a middleware solution that allows interoperability with the existing of caravela seguros systems’, through the analysis and design of an architecture and subsequent development of some services to prove the validity of the solution. so, it is intended to get a system that facilitates the use of caravela seguros systems by entities that are connected to it."
    ],
    0.06666666666666667
  ],
  [
    [
      "this document reports a master’s project that fits in the 5th year of the integrated master’s in informatics engineering from the universidade do minho. this master work, online-soba (online social behavior analysis), aims at developing a semi-automatic system capable of analyzing short texts that correspond to comments written in reaction to ’posts’ in portuguese social media (social networks or online newspapers), in order to identify behaviors that characterize social opinions about a given topic along a given period of time. to meet the challenge posed, these short texts are analyzed and the information is extracted according to schemes that describe the phenomena to be studied, that is, sentence structures in natural language that characterize the referred behaviors. in order to achieve this goal, a pipeline was implemented to process the texts contained in the netlang corpus, and later a web platform was developed to present the generated results to the end user, as well as to allow a fluid navigation over them. the report introduces the system architecture designed to attain the objectives, discusses the implementation decisions and the development steps, and shows the interface created for a fruitful knowledge extraction. before closing the document with the conclusions, an experiment conducted is described and the results are analyzed to assess the soba from the end user perspective.",
      "a tese de mestrado aqui relatada enquadra-se no quinto ano do mestrado integrado em engenharia informática, na universidade do minho. este projecto, online-soba (online social behavior analysis), tem como objectivo desenvolver um sistema semi-automático capaz de analisar textos curtos correspondentes a comentários escritos em português como reacção a ’posts’ em plataformas de comunicação social (nomeadamente redes sociais ou jornais online), de modo a que seja possível identificar comportamentos que caracterizam a opinião da sociedade sobre um determinado tema ao longo de um dado período de tempo. para enfrentar o desafio proposto, esses textos curtos (os comentários) são analisados e a informação extraída segundo esquemas que descrevem o fenómeno em causa, ou seja, estruturas de frases em linguagem natural que caracterizam os referidos comportamentos. para que este objetivo pudesse ser atingido, foi implementada uma pipeline para fazer o processamento dos textos contidos no corpus netlang e posteriormente foi desenvolvida uma plataforma web para apresentar os resultados gerados ao utilizador final, assim como permitir uma navegação fluida sobre os mesmos. o relatório introduz a arquitetura do sistema concebida para atingir os objetivos, discute as decisões de implementação e as etapas de desenvolvimento, e mostra a interface criada para uma extração proveitosa do conhecimento. antes de encerrar o documento com as conclusões, é descrita uma experiência conduzida e os resultados são analisados para avaliar o soba a partir da perspetiva do utilizador final."
    ],
    [
      "infrastructure as code (iac) is an innovative devops approach to infrastructure configuration and management. instead of using traditional interactive tools — such as command line — or cloud provider web interfaces, it automates several tasks through extensive use of scripting languages and tools. being a relatively new field, with a fast-paced developing set of tools, it is of crucial importance to assist its users and its developers to tackle security concerns that might affect the environments these tools are meant to manage. some of those security concerns must always be handled within an actual live, running environment. this is the case, for example, of checking for service availability. issues like this are already being addressed by existing dynamic analysis tools. others should be handled using a static analysis approach, which, in turn, should prevent those security concerns from ever becoming a live security issue. in this dissertation, we focus on trying to bridge the gap between the set of security checks currently being addressed by tools that follow these approaches. we identify 150 security checks currently being performed only by dynamic analysis tools, and we implement 23% of them in kics, a checkmarx-backed, open source, static code analysis tool for iac solutions. the new checks we contribute to kics address misconfiguration and non-compliance problems that can be prevented using static analysis, mainly focusing on access control, but also on network security. overall, this dissertation addresses 34 security checks, effectively bridging the gap between static and dynamic analysis for iac in the kics context. although not always possible, we strive to make available each security check to ansible, cloudformation, and terraform. these new security checks and the necessary changes to kics were submitted to the github project’s repository, were approved by the kics team, and are now into its master branch. this means that new kics releases will make available these security checks to its current users and to a broader audience, and, hopefully, will foster the development of community-based extensions and enhancements, such as support for other iac platforms and security domains that we were unable to tackle due to time constraints.",
      "infrastructure as code (iac) é uma prática inovadora de devops para configuração e gestão de infraestrutura. em alternativa ao uso tradicional de ferramentas interativas — como a linha de comandos — ou interfaces web de cloud providers, iac automatiza várias tarefas, através do uso de linguagens e ferramentas de script. por ser um campo novo, com um conjunto de ferramentas em desenvolvimento acelerado, é fulcral ajudar os seus utilizadores e developers a lidar com problemas de segurança, que possam afetar os ambientes que essas ferramentas devem gerir. algumas dessas preocupações devem sempre ser tratadas num ambiente em execução. é o caso, p. ex., da verificação de service availability. questões como esta já são abordadas por ferramentas de análise dinâmica. outras devem ser tratadas através de uma abordagem de análise estática, que deve impedir que essas preocupações se tornem um problema de segurança ativo. nesta dissertação, focámo-nos em tentar preencher a lacuna entre o conjunto de security checks das ferramentas que seguem estas abordagens, atualmente. identificámos 150 security checks atualmente realizadas apenas por ferramentas de análise dinâmica e implementámos 23% delas no kics, uma ferramenta open source de análise estática de código, apoiada pela checkmarx, para soluções de iac. as novas security checks, que contribuímos para o kics, abordam problemas de misconfiguration e non-compliance, que podem ser evitados através de análise estática, com foco principal em access control, mas também em network security. no geral, esta dissertação aborda 34 security checks, preenchendo efetivamente a lacuna entre a análise estática e dinâmica para iac, no contexto do kics. embora nem sempre seja possível, esforçámo-nos para disponibilizar cada security check para ansible, cloudformation e terraform. as novas security checks e alterações no kics foram submetidas no repositório github do kics, foram aprovadas pela equipa do kics e estão no master branch. tal significa que as novas versões do kics terão essas security checks para os seus utilizadores atuais e para um público mais amplo e, esperançosamente, promoverão contribuições da comunidade, como o suporte para outras plataformas de iac e domínios de segurança que não conseguimos resolver devido a limitações de tempo."
    ],
    0.0
  ],
  [
    [
      "estudos de interfaces celulares são fundamentais para a compreensão do comportamento e das interações celulares em sistemas biológicos complexos, particularmente em neurociência, onde as interfaces entre células neuronais e seu microambiente são essenciais para a investigação da função cerebral, desenvolvimento e transtornos neurológicos. avanços recentes em nanotecnologia facilitaram o desenvolvimento de biomateriais ideais para interagir com células neuronais, com o grafeno emergindo como um candidato promissor. para ser considerado amigável às células neuronais, uma interface ideal deve possuir alta wettability e condutividade elétrica. embora o grafeno pristino tenha alta condutividade intrínseca, as aplicações biológicas são limitadas pelo comportamento de baixa hidrofilia. funcionalização com grupos químicos específicos permite ajustar a wettability do grafeno e ampliar suas aplicações. este trabalho tem como objetivo desenvolver interfaces inovadoras de grafeno para células neuronais. para aprimorar as suas propriedades de superfície (i.e., wettability), filmes de grafeno foram funcionalizados com grupos químicos de flúor após serem transferidos para substratos de politereftalato de etileno (pet) flexíveis, eletricamente isolantes e biocompatíveis. os efeitos de fluorinação na wettability do grafeno ainda são enigmáticos devido a resultados inconsistentes na literatura. portanto, a fluorinação e a caracterização do grafeno em diferentes substratos (si e si/sio2) foram realizadas para uma melhor compreensão desses efeitos. além disso, métodos para a geração seletiva de áreas de grafeno funcionalizado foram explorados para aplicação de controle celular aprimorado. para obter a amostra de substrato ideal, foi necessário testar vários métodos e parâmetros de preparação e identificá-los como promissores em relação às propriedades finais da amostra. o ângulo de contato com a água (wca) e o método de quatro pontas foram usados para monitorar a wettability e a resistência de folha das amostras, respetivamente. além disso, a composição química e a estrutura cristalina foram caracterizadas por espectroscopia raman e fotoeletrônica de raios-x, e os efeitos na topografia foram analisados com perfilômetro de contato e microscopia de força atômica. testes de células cancerígenas foram realizados por nossos colaboradores no inl para resultados preliminares em adesão celular.",
      "cell interface studies are crucial for understanding cellular behavior and interactions in complex biological systems, particularly in neuroscience where the interfaces between neuronal cells and their microenvironment are essential for investigating brain function, development, and neurological disorders. recent advances in nanotechnology have facilitated the development of ideal biomaterials for interacting with neuronal cells, with graphene emerging as a promising candidate. to be considered neuron-friendly, an ideal interface should possess high wettability and electrical conductivity. while pristine graphene has high intrinsic conductivity, some biological applications might be limited by its poor wetting behavior. functionalization with specific chemical groups allows to tune graphene’s wettability and broaden its applications. this work aims to develop innovative graphene-based interfaces for neuronal cells. to enhance its surface properties (i.e., wettability), the graphene films were functionalized with fluorine chemical groups after being transferred to flexible, electrically insulating, and biocompatible polyethylene terephthalate (pet) substrates. the effects of fluorine functionalization on the wettability graphene remain enigmatic from inconsistent results in the literature. hence, fluorination and characterization of graphene on different substrates (si and si/sio2) were carried out to better understand these effects. furthermore, methods to selectively generate areas of functionalized graphene were explored for enhanced cellular control applications. to obtain the optimal substrate sample, it was necessary to test several preparation methods and parameters and identify them as promising in relation to the final properties of the substrate. the water contact angle (wca) and four-point probe method were used to monitor the wettability and sheet resistance of the samples, respectively. in addition, the chemical composition and crystalline structure were characterized via raman and x-ray photoelectron spectroscopy (xps), and the effects on topography were analyzed with contact profilometer and atomic force microscopy (afm). cancer cell assays were conducted by our collaborators at inl for preliminary results related to cell adhesion."
    ],
    [
      "com o emergir da era da informação foram muitas as empresas que recorreram a data warehouses para armazenar a crescente quantidade de dados que dispõem sobre os seus negócios. com essa evolução dos volumes de dados surge também a necessidade da sua melhor exploração para que sejam úteis de alguma forma nas avaliações e decisões sobre o negócio. os sistemas de processamento analítico (ou olap – on-line analytical processing) vêm dar resposta a essas necessidades de auxiliar o analista de negócio na exploração e avaliação dos dados, dotando-o de autonomia de exploração, disponibilizando-lhe uma estrutura multiperspetiva e de rápida resposta. contudo para que o acesso a essa informação seja rápido existe a necessidade de fazer a materialização de estruturas multidimensionais com esses dados já pré-calculados, reduzindo o tempo de interrogação ao tempo de leitura da resposta e evitando o tempo de processamento de cada query. a materialização completa dos dados necessários torna-se na prática impraticável dada a volumetria de dados a que os sistemas estão sujeitos e ao tempo de processamento necessário para calcular todas as combinações possíveis. dado que o analista do negócio é o elemento diferenciador na utilização efetiva das estruturas, ou pelo menos aquele que seleciona os dados que são consultados nessas estruturas, este trabalho propõe um conjunto de técnicas que estudam o comportamento do utilizador, de forma a perceber o seu comportamento sazonal e as vistas alvo das suas explorações, para que seja possível fazer a definição de novas estruturas contendo as vistas mais apropriadas à materialização e assim melhor satisfaçam as necessidades de exploração dos seus utilizadores. nesta dissertação são definidas estruturas que acolhem os registos de consultas dos utilizadores e com esses dados são aplicadas técnicas de identificação de perfis de utilização e padrões de utilização, nomeadamente a definição de sessões olap, a aplicação de cadeias de markov e a determinação de classes de equivalência de atributos consultados. no final deste estudo propomos a definição de uma assinatura olap capaz de definir o comportamento olap do utilizador com os elementos identificados nas técnicas estudadas e, assim, possibilitar ao administrador de sistema uma definição de reestruturação das estruturas multidimensionais “à medida” da utilização feita pelos analistas.",
      "with the emergence of the information era many companies resorted to data warehouses to store an increasing amount of their business data. with this evolution of data volume the need to better explore this data arises in order to be somewhat useful in evaluating and making business decisions. olap (on-line analytical processing) systems respond to the need of helping the business analyst in exploring the data by giving him the autonomy of exploration, providing him with a multi-perspective and quick answer structure. however, in order to provide quick access to this information the materialization of multi-dimensional structures with this data already calculated is required, reducing the query time to the answer reading time and avoiding the processing time of each query. the complete materialization of the required data is practically impossible due to the volume of data that the systems are subjected to and due to the processing time needed to calculate all combinations possible. since the business analyst is the differentiating element in the effective use of these structures, this work proposes a set of techniques that study the user‟s behaviour in order to understand his seasonal behaviour and the target views of his explorations, so that it becomes possible to define new structures containing the most appropriate views for materialization and in this way better satisfying the exploration needs of its users. in this dissertation, structures that collect the query records of the users will be defined and with this data techniques of identification of user profiles and utilization patterns are applied, namely the definition of olap sessions, the application of markov chains and the determination of equivalence classes of queried attributes. in the end of this study, the definition of an olap signature capable of defining the olap behaviour of the user with the elements identified in the studied techniques will be proposed and this way allowing the system administrator a definition for restructuring of the multi-dimensional structures in “size” with the use done by the analysts."
    ],
    0.0
  ],
  [
    [
      "com o crescente número de componentes para gestão de sistemas de queues e o crescente número de aplicações cliente a fazer uso desses mesmos componentes é necessário a criação de um middleware para de sacoplar as aplicações cliente dos sistemas de queues. os sistemas de queues são também conhecidos com message-oriented middleware (mom). o acoplamento das aplicações cliente a esses componentes torna-as muito dependentes destes, pelo que a introdução de um middleware faz com que a aplicação cliente fique isolada das particularidades e das tarefas de manutenção. posto isto, fica o middleware dependente dessas particularidades e das tarefas de manutenção. o rabbitmq, o activemq e o kafka são exemplos de sistemas de queues onde existe um sistema intermediário externo entre as aplicações que estão a comunicar, e o zeromq que é um sistema de queues onde a própria aplicação fica como um nodo do sistema de queues, isto é, o zeromq é um sistema intermediário interno. todos estes são implementados de diferentes formas, pelo que a troca de um sistema para outro leva a uma reestruturação das aplicações que o estejam a usar, por isso estes serão estudados durante esta dissertação de forma a avaliar as suas caraterísticas, vantagens e desvantagens para realizar a sua integração no middleware a desenvolver. o middleware desenvolvido desacopla as aplicações dos sistemas de queues, permitindo assim a troca de um sistema para outro sem ser necessária uma reestruturação da aplicação. este integrou o rabbitmq, o activemq e o kafka por forma a ser possível realizar as operações básicas de envio e leitura de mensagens. além destas operações é também possível reler mensagens quando seja necessário. por forma a demonstrar e testar o middleware ir-se-á recorrer a um caso de estudo.",
      "with the growing number of components for managing queue systems and the growing number of client appli cations making use of those same components, it is necessary to create middleware to decouple client applications from queue systems. queue systems are also known as message-oriented middleware (mom). the coupling of client applications to these components makes them very dependent on them, so the intro duction of middleware makes the client application isolated from particularities and maintenance tasks. that said, middleware is dependent on these particularities and maintenance tasks. rabbitmq, activemq and kafka are examples of queue systems where there is an external intermediary service between the applications that are communicating, and zeromq is a queue system where the application itself is a node of the queue system, that is, zeromq is an internal intermediation service. all of these are implemented in different ways, so switching from one system to another leads to a restructuring of the applications that are using it, so these will be studied during this dissertation in order to evaluate its characteristics, advantages and disadvantages to perform its integration in the middleware to be developed. the developed middleware decouples the applications from the queue systems, thus allowing the exchange from one system to another without needing to restructure the application. this integrated rabbitmq, activemq and kafka in order to be able to carry out the basic operations of sending and reading messages. in addition to these operations, it is also possible to reread messages when necessary. in order to test the middleware, a case study will be used."
    ],
    [
      "a análise dos dados tem sido, tradicionalmente, realizada em servidores na nuvem, onde a capacidade de armazenamento e de processamento são quase ilimitadas. em contrapartida, os dispositivos periféricos têm severas limitações tanto de armazenamento como de processamento. no entanto, estes dispositivos encontram-se mais próximos do local onde os dados são gerados. por causa disso, estes são, usualmente, utilizados para cargas de trabalho transacionais onde a confiabilidade e interatividade são fulcrais. devido às limitações dos dispositivos periféricos, os dados são, geralmente, extraídos periodicamente para a nuvem onde são depois armazenados e processados. de modo a permitir a análise exploratória de dados heterogéneos, é comum utilizar uma infraestrutura data lake que permite gerir dados em formato bruto de múltiplas fontes. no entanto, transferir todos os dados coletados para a nuvem é inviável devido à limitada capacidade da rede que não tem conseguido acompanhar o crescimento do volume de dados coletados. esta dissertação ultrapassa estes desafios ao implementar um componente middleware capaz de armazenar os dados previamente transmitidos na nuvem e propaga partes da interrogação para a periferia. deste modo, consegue-se reduzir o volume de dados transferido ao enviar, idealmente, apenas uma vez os dados necessários para responder aos pedidos. além disso, esta solução equilibra o impacto na rede e o custo computacional na periferia de modo a minimizar o tempo de execução.",
      "data analysis has traditionally been performed on dedicated servers in the cloud, where storage and processing capabilities are almost unlimited, in contrast to edge devices. nonetheless, these devices are closer to where data is generated. because of this, they have, usually, a transactional workload, where reliability and interactivity are essential. due to the limitations of edge devices, generally, data is extracted periodically to the cloud to be stored and processed. in order to allow exploratory data analysis, the heterogeneous data is stored in a data lake infrastructure that manages data in raw format from multiple data sources. nonetheless, transferring all collected data to the cloud is unfeasible because the increase in the volume of collected data has surpassed the network capabilities. this thesis overcomes these challenges by employing a middleware component capable of storing previously transmitted data in the cloud and pushing down query fragments to the edge. consequently, the volume of data transmitted to the cloud is reduced by uploading, ideally, only once the required data. furthermore, the solution balances the impact on the network and the computational effort in the edge in order to minimize execution time."
    ],
    0.3
  ],
  [
    [
      "esta dissertação aborda o acordo distribuído aproximado, no qual é pretendido que um grupo de processos decida um valor dentro de um intervalo com uma amplitude limitada. na primeira fase procede-se ao levantamento dos algoritmos existentes na literatura, onde se incluem algoritmos que propõem a resolução no modelo assíncrono, no qual são consideradas faltas bizantinas. através de uma análise comparativa, são evidenciadas as principais diferenças entre os algoritmos. posteriormente, após a seleção criteriosa de um dos algoritmos, e feita a sua análise detalhada sobre os fatores de escalabilidade em resultado da sua implementação. com base nos resultados, são propostas alterações que promovem a performance do algoritmo face ao aumento do número de processos no sistema.",
      "this dissertation addresses the approximate distributed agreement, in which it is intended that a group of processes decides a value within a range with a limited amplitude. in the first phase, a study of existing algorithms in the literature is carried out, including algorithms that propose a resolution in the asynchronous model, where byzantine faults are considered. then, through a comparative analysis, the main differences between the algorithms are highlighted. in the next phase, after a careful selection of one of the algorithms, a detailed analysis of the scalability factors resulting from its implementation is carried out. based on the results, changes are proposed to promote the performance of the algorithm given the increase in the number of processes in the system."
    ],
    [
      "the information that a company possesses is one of its most valuable assets. this information is nowadays digitally managed, which is the reason for the exponential increase in security breaches, where information is defiled or even stolen. seeking to solve this problem, watchful software developed a product, rightswatch, that allows for an organization to protect and watch over its information. by monitoring what happens to information, rightswatch provides, in case of an incident, the means to undertake a very complete post-mortem analysis. nevertheless, by the time this analysis is complete, it might have been hours (or days) since the incident occurred. to make matters worse, nowadays most threats actually come from the inside of the company. that being said, this dissertation defines as its main objective the need to understand if it is possible to detect data leaks in an intelligent way, through a real time analysis of the user’s behaviour while he handles the classified information. this possibility was indeed confirmed through an investigation comprising experiences with real world use cases and a variety of data preparation and data analysis techniques.",
      "a informação gerada por uma organização é um dos seus bens mais valiosos. actualmente, essa informação é normalmente gerida de forma digital, razão pela qual se tem verificado um aumento exponencial de incidentes de segurança, onde esta informação ´e adulterada, ou até mesmo roubada. com vista ao combate a este problema, a watchful software desenvolveu um produto, o rightswatch, que permite a uma organização proteger e monitorizar a sua informação. ao monitorizar o que acontece à informação, o rightswatch permite, no caso de incidente, que seja efectuada uma análise post-mortem bastante completa. não obstante, quando essa análise é feita, podem já ter passado horas (ou dias) desde a ocorrência do incidente. para piorar a situação, a maior parte das ameaças de hoje em dia são internas. assim sendo, esta dissertação tem como questão central perceber se existe a possibilidade de detectar, de forma inteligente e em tempo real, fugas de informação através da análise do comportamento dos utilizadores aquando do manuseamento da informação protegida. esta possibilidade foi de facto confirmada através de um trabalho de investigação que envolveu experiências com casos de uso reais e a aplicação de várias técnicas de preparação e análise de dados."
    ],
    0.3
  ],
  [
    [
      "as técnicas de aprendizagem automática são amplamente empregadas em smart homes, em combinação com dispositivos iot. a combinação entre ambas as tecnologias permite a coleta e análise de dados de sensores e outros dispositivos para a aprendizagem e automatização de tarefas domésticas, como regulação de temperatura, controle de iluminação e segurança. algumas técnicas comuns de ml aplicadas incluem aprendizagem por reforço, aprendizagem supervisionada e aprendizagem não supervisionada. essas tecnologias permitem que os dispositivos iot aprendam com o comportamento do utilizador e melhorem as suas capacidades de automatizar tarefas, tornando a vida dos utilizadores mais conveniente e eficiente. a presente dissertação tem como objetivo principal a análise das principais técnicas de ml utilizadas no âmbito das smart homes. neste sentido, é feito um levantamento sobre a automação residencial antes da popularização das técnicas de ml. analisam-se as técnicas do período anterior ao atual, quando não havia um poder computacional que permitisse a implementação de técnicas de ml em ambientes residenciais. além disso, é abordada a relação entre as técnicas de ml e a criação de ambientes domésticos mais sustentáveis, onde a racionalização de recursos se torna uma realidade e a sua implementação não causa alterações significativas na qualidade de vida dos utilizadores deste tipo de residência. por fim, é apresentada a implementação de uma prova de conceito relacionada com o reconhecimento de atividades humanas em ambiente doméstico.",
      "machine learning techniques are widely used in smart homes using iot. these technologies allow iot devices to collect and analyze data from sensors and other devices to learn and automate household tasks such as temperature regulation, lighting control, and security. some common ml techniques applied include reinforcement learning, supervised learning, and unsupervised learning. these technologies allow iot devices to learn from user behavior and improve their capabilities to automate tasks, making users’ lives more convenient and efficient. the aim of this dissertation is to analyse the main ml techniques used in smart homes. an approach was taken to home automation before the popularisation of ml techniques. techniques from the period before the current one were analysed, when there was no computing power to enable the implementation of ml techniques in residential environments. in addition, the relationship between ml techniques and the creation of more sustainable domestic environments was addressed, where the rationalisation of resources becomes a reality and their implementation does not cause significant changes in the quality of life of the users of this type of residence. finally, a proof of concept was implemented related to recognising human activities in a domestic environment."
    ],
    [
      "from a simple entertainment activity to a learning tool, it is undeniable that video games are one of the most active and relevant areas in today’s society. since their inception, video games experienced an evolution unmatched by almost any other area and today’s video games go beyond being simple software. they are authentic pieces of art that programmers, designers and artists work together to build. as such, it presents no surprise that building a state of the art (aaa) video game is a very expensive process. if one breaks down the budget it takes to develop a aaa a video game, one finds that a sizable amount of money goes to developing content for that video game. procedural content generation is based on a strict set of rules and it offers an alternative to manual design of video game content, optimizing the process of development and thus reducing its cost. because procedural content generation relies on a set of rules, it is possible to take into consideration the specific needs of a given player and dynamically adjust the game content to better suit its play style and increase the value s/he takes out of the game. this work focuses on the development of a preliminary version of a new procedural content generation (pcg) methodology that generates customized game content that is dependent on the player’s context. the main goal of this methodology is to close the gap between players and developers, offering an easier way for the game developers to access information about each specific player, allowing them to craft generation algorithms that make use of individual player information. this work also contemplates the development of two games that make use of said methodology and thus generate adaptive content.",
      "desde o simples entretenimento a uma ferramenta de aprendizagem, é inegável que os videojogos são uma das áreas mais activas e relevantes da sociedade atual. desde o seu aparecimento, os videojogos têm sofrido uma evolução quase incomparável às restantes áreas, sendo que os jogos actuais são mais do que um pedaço de software. são autenticas obras de arte que programadores, designers e músicos (entre outros) trabalham em conjunto para construir. como tal, não é surpresa que desenvolver um videojogo topo de gama (aaa) seja um processo muito dispendioso. ao analisar o orçamento de desenvolvimento de um jogo deste tipo, constata-se que uma grande percentagem do orçamento está atribuída ao desenvolvimento do conteúdo para o jogo. a geração procedimental de conteúdo é baseada num conjunto rígido de regras e oferece uma alternativa ao desenvolvimento manual de conteúdos, optimizando assim o processo de desenvolvimento e consequentemente baixar o seu custo. uma vez que a geração procedimental de conteúdos é baseada num conjunto de regras, é possível que estas tenham em consideração as necessidades de um jogador específico. deste modo é possível ajustar dinamicamente o conteúdo à medida do jogador, fazendo com que seja tirado o máximo de proveito do jogo. este trabalho foca-se no desenvolvimento de uma versão preliminar de uma nova metodologia de geração procedimental de conteúdos para jogos, dependente do contexto do jogador. o objectivo principal desta metodologia é o de aproximar os jogadores e os criadores de jogos, de modo a que os criadores possam ter um acesso mais fácil à informação de cada jogador específico, permitindo que os criadores desenvolvam algoritmos de geração de conteúdo utilizando a informação individual de cada jogador. assim, este trabalho também contempla o desenvolvimento de dois videojogos que fazem uso da metodologia desenvolvida, gerando assim conteúdo adaptativo."
    ],
    0.3
  ],
  [
    [
      "either to improve the drivers knowledge about the road, or focusing on the current development of autonomous vehicles, most car manufacturers began offering driving assistance systems in their vehicles. a crucial part of the monitoring task performed by those systems is the detection and reaction over found traffic signs. since the decision flow of those solutions is reliant on the information gathered from the found signs, these systems deeply rely on their recognition stage. to achieve high-accuracy classification rates at nearly real-time, recognition is usually implemented using machine learning techniques, such as state of the art convolution neural networks (cnns). however, these methods demand a large amount of data for their learning process. due to the lack of large traffic signs repositories, these systems are restricted to one of the few available datasets. a significant decrease in accuracy was observed when using a recognition model trained with samples from a given country and latter used to classify signs from another country, thus supporting the need for country-specific repositories and trained models. traffic signs reveal several differences when compared to the same functional sign over different countries. those changes, although similar for the human perception, cause a significant disturbance in the classification abilities learned by a given machine learning model. aiming to overcome this issue, this dissertation proposes a semi autonomous tool to create traffic sign repositories, for almost any given country. the created dataset is intended to later be applied to train country-specific models, with traffic sign images from the country where the recognition model is going to be used. to achieve this, a pipeline based on an ensemble architecture joining several computer vision techniques is proposed. the combination of various methods allows to improve the recognition rates and, more importantly, decrease the number of false positives gathered in the produced repositories. finally, the pipeline was used to create the first portuguese traffic sign repository, having currently around 33000 labelled signs.",
      "focando o atual desenvolvimento de carros autónomos ou, procurando aumentar o conhecimento do condutor face ao ambiente rodoviário, a maioria dos fabricantes automóveis tem introduzido sistemas auxiliares de condução nos seus veículos. uma parte crucial da monitorização feita por estes sistemas encontra-se na deteção e reação perante os sinais de trânsito encontrados. para alcançar taxas de classificação elevadas, esta tarefa é normalmente feita utilizando modelos de reconhecimento como redes neuronais convolucionais. contudo, estes algoritmos requerem uma elevada quantidade de imagens para o seu processo de aprendizagem. devido à falta de repositórios de sinais de trânsito, estes sistemas são habitualmente treinados utilizando imagens sintéticas ou um dos datasets publicamente disponibilizados. um decréscimo acentuando na precisão é verificado quando se utliza um modelo treinado com imagens de um determinado pais e posteriormente usado para reconhecer sinais de outro país distinto. este impacto suporta a necessidade de utilizar imagens e modelos de reconhecimento específicos para cada país. os sinais revelam várias diferenças visuais quando comparados a sinais equivalentes mas pertencentes a outro país. estas variações causam um impacto significativo nas capacidades de classificação de um algoritmo pré-treinado. esta dissertação propõe assim um pipeline semi automático para a criação de repositórios de imagens de sinais de trânsitos, disponível para praticamente qualquer país. os repositórios criados são destinados ao treino de métodos de machine learning, específicos para o país sobre o qual as imagens foram recolhidas e onde o modelo de classificação será usado. a ferramenta baseia-se num conjunto de estruturas ensemble, integrando técnicas de visão por computador. a combinação de vários métodos aumenta as taxas de reconhecimento e permite garantir um rácio reduzido de imagens incorretamente classificadas. a ferramenta desenvolvida é, por fim, utilizada para criar o primeiro repositório de sinais de trânsito para portugal, tendo atualmente cerca de 33000 imagens classificadas."
    ],
    [
      "os mapas de código, que podem ser representados como grafos, são usados para descrever completamente um sistema de software. quando inspecionamos um mapa de código, somos capazes de observar o software de uma nova perspetiva e, portanto, entendê-lo melhor. por exemplo, podemos analisar o seu comportamento bem como as dependências que possam existir entre os vários elementos do sistema. nesta dissertação, estudámos um mapa de código, em forma de grafo, contendo dados de controlo de versão, provenientes de projetos que estavam guardados em repositórios git. o grafo referido contém vários tipos de informação sobre o sistema, incluindo métricas de código, como complexidade, e dados sobre os desenvolvimentos realizados. uma vez que a estrutura organizacional definida pelos autores que desenvolvem o sistema pode originar problemas de qualidade no código, o nosso estudo concentrou-se nos problemas relacionados com os autores dos programas desenvolvidos, usando principalmente alguns dos seus dados de desenvolvimento. após explorados os problemas relacionados com os autores, agrupámos os seus dados consoante as suas equipas e analisámos os problemas detetados, nomeadamente fragmentação e perda de conhecimento, tendo como perspetiva de análise a própria equipa de desenvolvimento. nesse sentido, desenvolvemos um programa que é capaz de detectar os referidos problemas e de os revelar ao utilizador de forma que a sua identificação seja feita quase que instantaneamente, o que, como se sabe, facilita muito a gestão de um projeto de software.",
      "code maps, which can be represented as graphs, are used to describe an entire software system. when we inspect a code map, we are able to observe the software in a new form, and therefore understand it better. for example, we can analyse the software behaviour and dependencies between several elements of the system. in this thesis, we study a code mapping graph that contains version control data of a set of projects located in git repositories. the referred graph contains various information about the system, not only about code metrics like complexity, but also developers’ development data. since the organizational structure of the developers that build the system can lead to quality problems in the software, our study focused on developers’ related problems using mainly developers’ development data. after the developers’ related problems are explored, we group the developers’ data into their correspondent teams, and analyse the problems (knowledge loss and fragmentation) once again but now in a team view. in this sense, we developed a program that detects these problems and display them to the user in a way that the identification of the problems is almost done instantly, facilitating the management of a software project."
    ],
    0.06666666666666667
  ],
  [
    [
      "as organizações de saúde têm como principal objectivo a prestação de serviços de qualidade à população, e a tomada de decisões de forma rápida e e caz é essencial para que tais objectivos sejam atingidos. deste modo, neste sector, a adopção de ferramentas tecnológicas automatizadas que facilitam este processo tem vindo a aumentar ao longo dos anos. neste contexto, surge o conceito de business intelligence (bi) que auxilia a tomada de decisão por parte dos pro ssionais de saúde, uma vez que estes sistemas se baseiam na extracção de conhecimento (ec) gerado pelos sistemas de informação transaccionais, sendo capazes de integrar uma enorme quantidade de dados provenientes de diversas fontes, normalmente de bases de dados que se encontram em diferentes tecnologias, plataformas e totalmente desintegradas. assim, ultrapassando-se a heterogeneidade das bases de dados, através da estruturação dos dados, extrai-se informação que permitirá atingir conhecimento importante para as decisões clínicas. especi camente, a unidade de cuidados intensivos (uci) de um hospital é a unidade mais cara e que mais recursos exige, de tal forma que os sistemas de bi podem desempenhar um papel preponderante não só na racionalização dos custos, mas também na melhoria da qualidade dos cuidados prestados, através da monitorização dos dados clínicos dos pacientes. deste modo, este projecto pioneiro incidiu na análise da aplicação do pentaho, um software open-source (os) de bi, nos processos de ec a estas unidades hospitalares, tendo como fonte os dados dos pacientes de um hospital localizado no norte de portugal, avaliando o conhecimento obtido e o seu impacto na tomada de decisão. este software disponibiliza ferramentas que analisam, sintetizam, assimilam e dão sentido às enormes quantidades de informação, sendo capaz de estabelecer ligações so sticadas e discernir padrões, dando oportunidade para tirar conclusões e agir de forma preventiva.",
      "health organizations have as their main objective the provision of quality services to the population, so making decisions in a fast and e ective way is essential to achieve these objectives. thus, in this sector, the adoption of technological and automated tools that facilitate this process has been increasing over the past few years. in this context, it arises the concept of bi which assists decision making by physicians, since these systems are based on the knowledge extraction generated by transactional information systems, being able to integrate a huge amount of data from various sources - tipically databases which are in di erent technologies, platforms, and completely disintegrated. thus, passing up the databases' heterogeneity, by structuring the data, it is possible to extract important and useful information for clinical decisions. speci cally, the intensive care unit (icu) of a hospital is the most expensive unit and the one that requires more resources, such that bi systems can play an important role not only in streamlining costs, but also improving the quality of care provided, through constant patients monitoring. thus, this pioneering project focused on reviewing the implementation of pentaho, an os bi tool, in the knowledge extract processes at the icus, having as data source clinical data of patients from an hospital located in the north of portugal, assessing the knowledge gained and its impact on decision making. this software provides tools that analyze, synthesize, assimilate and make sense of massive amounts of information, being able to discern and to liaise sophisticated standards, giving an opportunity to draw conclusions and act preventively."
    ],
    [
      "nos últimos anos, a grande utilização de dispositivos móveis tem levado a um considerável aumento de interesse da comunidade de investigação e da indústria na área das redes oportunistas. nestas redes, os dispositivos móveis aproveitam a sua mobilidade para trocarem dados entre si. uma vez que estes dispositivos são principalmente transportados por humanos, surgem oportunidades de contacto aquando da proximidade física entre eles, permitindo essa troca de dados. é por esta razão que os investigadores têm vindo a explorar a mobilidade humana, aplicando-a ao estudo destas redes. neste contexto surge o objetivo principal deste trabalho que visa explorar a disponibilidade de dados reais sobre o movimento de pessoas para avaliar o desempenho de redes oportunistas, considerando que a análise de traces wi-fi pode fornecer informações importantes sobre o assunto, em particular, para estimar o atraso e a largura de banda. entre outros, um dos grandes desafios a abordar é a grande quantidade de dados a analisar (big data), onde serão consideradas técnicas para analisar esses grandes volumes de dados de forma eficiente. nesse sentido foram desenvolvidos algoritmos que permitissem avaliar o desempenho destas redes. um primeiro capaz de gerar logs radius sintéticos num ambiente controlado, onde se pode configurar a simulação com os valores que se pretendam. a sua validação possibilitou constatar a eficiência do mesmo, permitindo gerar corretamente uma grande quantidade de registos sintéticos e num curto espaço de tempo. um segundo algoritmo que possibilita extrair encontros de uma lista de registos preexistente. a validação deste algoritmo também permitiu verificar que é possível extrair uma grande quantidade de encontros de forma simples e rápida. por fim, um terceiro algoritmo que visa o encaminhamento epidémico de mensagens a partir de um conjunto de encontros preexistente e através da utilização de duas diferentes estratégias de transmissão de mensagens. a partir dos resultados alcançados nas experiências realizadas com o terceiro algoritmo, foi possível constatar-se que de facto as dtns caracterizam-se pelas suas baixas taxas de entrega. apesar disso, face ao protocolo aplicado nas experiências realizadas, esperavam-se taxas mais elevadas. foi possível igualmente constatar-se que a taxa de entrega a alcançar é influenciada pela estratégia de transmissão de mensagens que se adote, uma vez que foi sempre superior quando se utilizou a segunda estratégia. por outro lado, através dos resultados obtidos foi também possível concluir que a variação do tamanho da mensagem não se reflete na taxa de entrega alcançada. os resultados também confirmaram o facto do protocolo epidémico requerer uma enorme quantidade de recursos, dada à excessiva quantidade de transmissões que efetua e ao considerável espaço de armazenamento que as stations necessitam ter. assim, dado ao facto das stations serem nós móveis e possuírem baterias relativamente limitadas, é de prever que tendam a ficar sem energia passado algum tempo. ao nível do atraso, os resultados mostraram que os valores médios e máximos dependeram sempre da estratégia de transmissão de mensagens que se utilizou. por outro lado, não se verificou uma dependência em relação ao tamanho das mensagens geradas na rede, uma vez que não existiu uma tendência visível nos resultados ao variar o tamanho das mesmas.",
      "nowadays with the dissemination of mobile devices, the interest from the research community and the industry in opportunistic networks has increased significantly. in these networks, mobile devices take advantage of their mobility to exchange data with each other. since these devices are mainly carried by humans, contact opportunities arises during physical proximity among them, enabling data exchange. it is for this reason that researchers have been exploring human mobility applied to the study of these networks. in this context arises the main objective of this study which aims to explore the availability of real data about the movement of people to assess the performance of these networks, considering that the analysis of wi-fi data traces can provide significant information about this, in particular can be use to estimate delay and bandwidth. among others, one of the major challenges to address is the large amount of data to be analyzed (big data), where will be considered techniques to analyze these large volumes of data efficiently. in this sense, algorithms were developed to allow to assess the performance of these networks. a first able to generate synthetic radius records in a controlled environment, where one can configure the simulation with values desired. their validation allowed to verify the efficiency of the same, generating a large amount of synthetic records correctly and in a short time. a second algorithm that allows extracting encounters from a pre-existing list of records. the validation of this algorithm also allowed to verify that it is possible extract a large amount of encounters simply and quickly. finally, a third algorithm that aims the epidemic routing of messages from a set of pre-existing encounters using two different message transmission strategies. from the results achieved in experiments with the third algorithm, it was possible to verify that in fact dtns are characterized by their low delivery rates. despite this, given the protocol applied in the experiments, were expected higher rates. it was also possible to verify that the delivery rate achieved is influenced by the message transmission strategy adopted, since it was always higher when the second strategy was used. on other hand, based on the obtained results it was also possible concluded that the variation of the size of the message is not reflected in the delivery rate achieved. the results also confirmed that the epidemic protocol require a huge amount of resources due to the excessive amount of transmissions that performs and significant storage space that stations need to have. thus, given the fact that the stations are mobile nodes and having relatively small batteries, it is expected that they will tend to run out of energy spent some time. at the delay level, the results showed that the average and maximum values always depended on the message transmission strategy that was used. on the other hand, not there was a dependence on the size of messages generated in the network, because there has been no noticeable tendency in the results when varying their size."
    ],
    0.0
  ],
  [
    [
      "heart diseases can often manifest themselves by irregularities in the movement of the heart muscle. to assess the function of the myocardium, a method based in the optic flow constrain equation (ofce) is applied in tagging mr images. the sequence of tagging mr images allows us to detect deviations in deformation and strain through time. however, the application of the ofce implies the assumption of spatial phase conservation. therefore, harmonic filters in the fourier domain were used in each frame of the sequence to remove the variation of intensity trough time. in order to achieve a model capable of distinguishing a malfunction from normal function of the cardiac wall it is necessary to acknowledge what is the ground truth and which factors can affect the results. this study explores several scenarios using synthetic data that mimic tagged mr images in order to discover which variables can optimize the ofce. this work allows us to analyze up to what extension the ofce can be applied to a cardiac motion simulator (cms) based on waks et al. [1], capable of reproducing the normal function of the heart. after a series of tests with simulated data and the respective comparison with real volunteers data, it is possible to assess quantitatively the method used.",
      "grande parte das doenças cardíacas estão associadas a um consequente mau funcionamento dos músculos cardíacos. sendo que o músculo cardíaco maior e do qual depende o funcionamento do coração é o miocárdio, torna-se relevante quantificar a deformação do mesmo. esta quantificação permite calcular o volume do sangue que é bombeado por ciclo cardiaco e a fracção de ejecção cardíaca. neste trabalho propoem-se a aplicação de um método que utiliza a \"optic flow constrain equation\"(ofce), a imagens de ressonância magnética (rm) marcadas. a sequencia de imagens the rm marcadas permite-nos detectar desvios na deformação e tensão no tempo. no entanto, o uso da ofce implica que se assuma a existência de conservação de fase. para tal, para remover a variação de intensidade no tempo, foram aplicados filtros no domínio de fourier a cada uma das imagens da sequência. para atingir um modelo capaz de distinguir um funcionamento anormal do miocárdio é necessário saber o que significa numéricamente o comportamento normal de um músculo saudável e quais os factores que podem afectar a sua quantificação. este estudo baseia-se na simulação de cenários para determinar que variáveis podem ajudar a optimização do método com ofce. este trabalho permite analizar a aplicabilidade da ofce a um simulador de movimento cardíaco (smc), baseado no trabalho dewaks et al. [1], capaz de reproduzir o movimento normal do coração. depois do estudo intensivo das várias simulações e respectiva comparação com dados reais, é possível avaliar quantitativamente o método utilizado."
    ],
    [
      "falls represent one of the biggest causes of deaths related to unintentional injuries. the increasing number of occurrences is associated a continuously expanding elderly population, along with its detrimental effects on the survival and well-being of those aged 65 and above, has turned the issue of falls into a global public health concern. it is estimated that 684,000 people worldwide lose their lives due to falls, which happen approximately 37.3 million times annually. as a result, the financial expenses associated with hospitalisations are significant and present a complex challenge. the electroencephalogram (eeg) technique is widely utilised to assess brain electrical activity and detect indicators of balance disruptions, such as perturbation evoked potentials (peps), in brain signals. this is possible because eeg data provides insights into motor planning and intention, making it a valuable tool for monitoring both falls and activities of daily living (adls). accordingly, this dissertation will establish two experimental protocols: one for simulating slip-like incidents and another for adls, with the aim of collecting eeg data. the primary goal of this dissertation is to leverage artificial intelligence (ai)-based systems to identify slip-like perturbations and various adls using the data from both protocols. the ultimate objective is to integrate these algorithms into assistive robotic devices, e.g. exoskeletons. in the context of the methods employed, the pep components were identified within a time frame of 75–137 ms after the external perturbation onset. to analyse the pre-processed eeg data, four distinct artificial neural networks were evaluated, each with varying network architecture parameters. among these architectures, the convolutional neural network (cnn)-long short-term memory (lstm) model, trained to predict eeg perturbations, exhibited superior classification performance, achieving an accuracy rate of 86% when using a short time window of 100 ms. in contrast, for classifying adl, the best result obtained was 53% accuracy, and this was also achieved using the cnn-lstm architecture.",
      "as quedas representam uma das maiores causas de morte relacionadas com lesões não intencionais. o número crescente de ocorrências está associado a uma população idosa em constante expansão, juntamente com os seus efeitos prejudiciais na sobrevivência e no bem-estar das pessoas com 65 anos ou mais, o que transformou a questão das quedas num problema de saúde pública global. estima-se que 684.000 pessoas em todo o mundo perdem a vida devido a quedas. consequentemente, as despesas financeiras são significativas e representam um desafio complexo. a técnica eletroencefalograma (eeg) é amplamente utilizada para avaliar a atividade eléctrica cerebral e detetar potenciais evocados por perturbação (peps) e atividades diárias (adls). assim, esta dissertação irá estabelecer dois protocolos experimentais: um para simular incidentes do tipo escorregar e outro para adls, com o objetivo de recolher dados eeg. o objetivo principal desta dissertação é utilizar sistemas baseados em inteligência artificial (ia) para identificar perturbações do tipo escorregar e várias adls utilizando os dados de ambos os protocolos. o objetivo final é integrar estes algoritmos em dispositivos robóticos de assistência, por exemplo, exoesqueletos. no contexto dos métodos utilizados, os componentes pep foram identificados num período de tempo de 75-137 ms após o início da perturbação externa. para analisar os dados eeg, foram avaliadas quatro redes neurais artificiais distintas, cada uma com parâmetros de arquitetura de rede variáveis. entre estas arquitecturas, o modelo de rede neuronal convolucional (cnn)-long short-term memory (lstm) apre sentou um desempenho de classificação superior, alcançando uma taxa de precisão de 86% quando se utilizou uma janela de tempo curta de 100 ms. em contraste, para a classificação de adl, o melhor resul tado obtido foi uma precisão de 53%, o que também foi conseguido utilizando a arquitetura cnn-lstm."
    ],
    0.0
  ],
  [
    [
      "a extração e o processamento de dados nas redes sociais são complementares e, por vezes, fundamentais no apoio à tomada de decisões em diversos negócios. os eventos desportivos não são exceção, podendo dessa forma beneficiar dessas técnicas. nesse contexto, os clubes desportivos podem aproveitar as informações obtidas nas redes sociais durante determinados eventos para dar suporte nas decisões, podendo dessa forma melhorar a experiência do espectador e, assim, atrair mais audiência. foi necessário realizar uma análise ao tipo de dados a serem recolhidos das redes sociais, assim como a melhor forma para os exibir ao utilizador, dando ao mesmo uma boa experiência. foi elaborado um protótipo como solução para o desafio, apresentando assim uma plataforma web, alinhada às melhores práticas de user experiente e user interface, que auxilia clubes desportivos e produtores de conteúdo televisivo a visualizar facilmente dados sociais, durante ou após um evento desportivo em específico. foram apresentados os resultados obtidos com esta proposta, demonstrando que é possível obter informações valiosas e relevantes para os clubes desportivos, conseguindo captar e reunir certos conhecimentos em tempo real.",
      "data extraction and processing of social media are complementary and sometimes fundamental to support decision making in several businesses. sports events are no exception and can benefit from these techniques. in this context, sports clubs can take advantage of the information obtained from social media during events to support decisions that can improve the spectator experience and thus attract more audiences. it was necessary to carry out an analysis of the type of data to be collected from social networks, as well as the best way to display them to the user, giving them a good experience. a prototype was developed as a solution to the challenge, thus presenting a web platform, aligned with the best practices of user experience and user interface, which helps sports clubs and tv content producers to easily visualize social data, during or after a specific sporting event. the results obtained with this proposal were presented, demonstrating that it is possible to obtain valuable and relevant information for sports clubs, managing to capture and gather certain knowledge in real time."
    ],
    [
      "climate change and a growing human population necessitate improved crop adaptability and yield. improving photosynthesis is one promising route to boosting plant productivity. photosynthesis is hampered by the dual activity of its main co2-fixing enzyme ribulose-1,5-bisphosphate carboxylase/oxygenase (rubisco). the enzyme side-reacts with o2, leading to the production of a toxic byproduct, which must be expensively recycled through the photorespiratory pathway. rubisco’s oxygenation rate depends on the co2 : o2 ratio and increases under high temperatures. in c3 plants, which make up 90% of the known plant species, this phenomenon can decrease photosynthetic efficiency by an estimated fourth. c4 plants have evolved a carbon-concentration mechanism that suppresses photorespiration by spatially separating initial carbon fixation and re-fixation by rubisco. initial carbon fixation occurs in the mesophyll cells, while decarboxylation and carbon fixation by rubisco occurs in the bundle sheath cell and releases pyruvate or phosphoenolpyruvate which then moves back to the mesophyll cells for the next cycle. to successfully engineer c4 metabolism in c3 plants, it is important to obtain a quantitative understanding of both the energetics and distribution of metabolic fluxes of this metabolic cycle. here, we tackle this question by analysing a large-scale metabolic model, consisting of mesophyll and bundle sheath cells connected through the exchange of cytosolic metabolites. we parameterized the model for the main c4 crop maize (zea mays) by using biochemical and anatomical constraints derived from the literature. these constraints also enable the model to correctly predict the appearance of the c4 cycle, different c4 subtypes and decarboxylation enzyme co-activity. accounting for the volumetric ratio between the two cell types leads to more accurate predictions of c2 photosynthesis, a triose phosphate-3-phosphoglycerate shuttle between the cell types, mesophyll-specific nitrate reduction, choice of decarboxylation enzyme, the ratio of atp production between the cell types, cell type-specific cyclic or linear electron transport activity and biomass production. thus, our modelling approach can guide biological engineering strategies to implement c4 photosynthesis into other plant systems to ultimately improve crop productivity.",
      "as alterações climáticas e o crescimento da população humana tornam necessário uma melhor adaptabilidade e rendimento agrícola. melhorar a fotossíntese é uma via promissora para aumentar a produtividade vegetal. a fotossíntese é dificultada pela dupla actividade da principal enzima fixadora de co2, a ribulose-1,5-bisfosfato carboxilase/oxigenase (rubisco). a enzima reage paralelamente com o2, levando à produção de um subproduto tóxico, que é reciclado através da via fotorrespiratória. a taxa de oxigenação da rubisco depende do rácio co2 : o2 e aumenta com temperaturas elevadas. nas plantas c3, que constituem 90% das espécies vegetais conhecidas, este fenómeno pode diminuir a eficiência fotossintética em um quarto. as plantas c4 desenvolveram um mecanismo de concentração de carbono que suprime a fotorrespiração separando espacialmente a fixação inicial de carbono e a refixação pela rubisco. a fixação inicial de carbono ocorre nas células do mesofilo, enquanto a descarboxilação e a fixação de carbono pela rubisco ocorre nas células da bainha do feixe e liberta piruvato ou fosfoenolpiruvato, que depois se desloca de volta para o mesofilo para o ciclo seguinte. para incorporar o metabolismo c4 em plantas c3, é importante obter uma compreensão quantitativa da produção de energia e da distribuição dos fluxos metabólicos deste ciclo. neste trabalho abordamos esta questão através da análise de um modelo metabólico em grande escala, constituído por uma célula do mesofilo e da bainha de feixe conectadas pela troca de metabolitos citosólicos. parametrizámos o modelo com restrições bioquímicas e anatómicas de milho (zea mays) derivadas da literatura. estas restrições permitem ao modelo prever o ciclo e diferentes subtipos de fotossíntese c4 e co-actividade de enzimas de descarboxilação. a inclusão do volume celular leva a previsões mais precisas de componentes metabólicos tais como a fotossíntese c2, o transportador triose fosfato-3-fosfoglicerato, a redução de nitrato no mesofilo, a escolha da enzima de descarboxilação, o rácio entre a produção de atp nas células, a actividade cíclica ou linear de transporte de electrões e a produção de biomassa. assim, a nossa abordagem de modelação pode guiar estratégias de engenharia biológica para implementar a fotossíntese c4 noutros sistemas vegetais e melhorar a produtividade agrícola."
    ],
    0.06666666666666667
  ],
  [
    [
      "risk assessment is an important topic for financial institution nowadays, especially in the context of loan applications or loan requests and credit scoring. some of these institutions have already implemented their own custom credit scoring systems to evaluate their clients’ risk supporting the loan application decision with this indicator. in fact, the information gathered by financial institutions constitutes a valuable source of data for the creation of information assets from which credit scoring mechanisms may be developed. historically, most financial institutions support their decision mechanisms on regression algorithms, however, these algorithms are no longer considered the state of the art on decision algorithms. this fact has led to the interest on the research of new types of learning algorithms from machine learning able to deal with the credit scoring problem. the work presented in this dissertation has as an objective the evaluation of state of the art algorithms for credit decision proposing new optimization to improve their performance. in parallel, a suggestion system on credit scoring is also proposed in order to allow the perception of how algorithm produce decisions on clients’ loan applications, provide clients with a source of research on how to improve their chances of being granted with a loan and also develop client profiles that suit specific credit conditions and credit purposes. at last, all the components studied and developed are combined on a platform able to deal with the problem of credit scoring through an experts system implemented upon a multi-agent system. the use of multi-agent systems to solve complex problems in today’s world is not a new approach. nevertheless, there has been a growing interest in using its properties in conjunction with machine learning and data mining techniques in order to build efficient systems. the work presented aims to demonstrate the viability and utility of this type of systems for the credit scoring problem.",
      "hoje em dia, a análise de risco é um tópico importante para as instituições financeiras, especialmente no contexto de pedidos de empréstimo e de classificação de crédito. algumas destas instituições têm já implementados sistemas de classificação de crédito personalizados para avaliar o risco dos seus clientes baseando a decisão do pedido de empréstimo neste indicador. de facto, a informação recolhida pelas instituições financeiras constitui uma valiosa fonte de dados para a criação de ativos de informação, de onde mecanismos de classificação de crédito podem ser desenvolvidos. historicamente, a maioria das instituições financeiras baseia os seus mecanismos de decisão sobre algoritmos de regressão. no entanto, estes algoritmos já não são considerados o estado da arte em algoritmos de decisão. este facto levou ao interesse na pesquisa de diferentes algoritmos de aprendizagem baseados em algoritmos de aprendizagem máquina, capaz de lidar com o problema de classificação de crédito. o trabalho apresentado nesta dissertação tem como objetivo avaliar o estado da arte em algoritmos de decisão de crédito, propondo novos conceitos de optimização que melhorem o seu desempenho. paralelamente, um sistema de sugestão é proposto no âmbito do tema de decisão de crédito, de forma a possibilitar a perceção de como os algoritmos tomam decisões relativas a pedidos de crédito por parte de clientes, dotando-os de uma fonte de pesquisa sobre como melhorar as possibilidades de concessão de crédito e, ainda, elaborar perfis de clientes que se adequam a determinadas condições e propósitos de crédito. por último, todos os componentes estudados e desenvolvidos são combinados numa plataforma capaz de lidar com o problema da classificação de crédito através de um sistema de especialistas, implementado como um sistema multi-agente. o uso de sistemas multi-agente para resolver problemas complexos no mundo de hoje não é uma nova abordagem. no entanto, tem havido um interesse crescente no uso das suas propriedades, em conjunto com técnicas de aprendizagem máquina e data mining para construir sistemas mais eficazes. o trabalho desenvolvido e aqui apresentado pretende demonstrar a viabilidade e utilidade do uso deste tipo de sistemas no problema de decisão de crédito."
    ],
    [
      "como em todas as áreas tecnológicas, também os utilizadores de sistemas áudio procuram soluções o mais práticas, convenientes e eficazes possível. as tecnologias sem ﬁos têm vindo, gradualmente, a substituir as soluções com ﬁos (ou cabladas) nos sistemas de distribuição de áudio, tanto, digital como analógico. o mercado está repleto de soluções que cumprem este propósito em variados graus de usabilidade e capacidades técnicas, sendo que o maior desaﬁo é colocado pelo problema da sincronização na distribuição individual e independente de canais áudio no formato digital. as soluções atuais são quase exclusivamente versões comerciais, utilizando algoritmos e tecnologias proprietárias relativamente complexas e, como tal, não gratuitas. este aspeto tende a atrasar o desenvolvimento de novos sistemas. esta dissertação documenta o projeto que teve como principal objetivo provar que é possível construir um sistema de distribuição independente e simultânea de canais áudio digitais, alcançando níveis ótimos de sincronização usando algoritmos e mecanismos alternativos sobre tecnologias bem conhecidas de rede local sem ﬁos. assim, este relatório apresenta a investigação elaborada sobre tecnologias universais e gratuitas, que facilitem o desenvolvimento de um novo mecanismo de livre acesso para a distribuição digital de áudio sobre redes locais ieee 802.11. esse mecanismo e os algoritmos associados são discutidos em detalhe, sendo depois apresentados os passos da implementação e teste desse mecanismo num sistema protótipo. o protótipo desenvolvido oferece um nível excelente de sincronização dos canais áudio em todos os dispositivos reprodutores do sistema, mesmo quando sujeito a condições de stress numa rede sobrecarregada. a eﬁcácia do sistema foi medida analiticamente e não foi detetada nenhuma deterioração signiﬁcativa na sincronização dos canais áudio nos sistemas reprodutores ﬁnais. também nos testes empíricos de audição os utilizadores não conseguiram detetar qualquer nível de dessincronização. espera-se que a tecnologia desenvolvida, implementada e testada, permita o aparecimento futuro de novas soluções gratuitas, ou comercialmente mais competitivas, mas que poderão rivalizar com as soluções atuais em termos das capacidades técnicas de distribuição de áudio digital, sobre redes locais de utilização genérica e comum.",
      "nowadays, more and more, consumers look for practical and comfortable solutions for their everyday lives problems. wireless technologies are, gradually, surpassing and replacing wired ones as they become faster, more efficient and reliable. the market, however, is full of solutions that, despite fulfilling most of users’ needs, are proprietary versions and, therefore, need licensing, which holds back the evolution and development of these kind of systems. another problem is that the majority of these solutions only address distribution of non-synchronized audio channels. this document reports the investigation done on open-source, universal technologies that can ease the development of an alternative approach to audio distribution and synchronization. it discusses the application mechanisms developed for distribution and synchronization of digital audio channels on top of common wireless local area networks. it also presents the results of the tests made with the implemented prototype system. thus, the project’s main goal was to prove that it is possible to achieve an optimal synchronization of independent distribution of individual audio channels over widely used ieee 802.11 wireless local area networks. the developed prototype achieved very good audio channel synchronization across all the system’s speaker devices, even under stress tests on overloaded networks. this results in no perceptible deterioration (to human ear) of the audible experience for the end user as compared with a full cabled, digital or traditionally analog, audio system. this means the project’s goal have been achieved and the algorithms could allow the development of new simpler and cheaper commercial solutions, with equal or improved quality when compared with the majority of paid solutions present on today’s market."
    ],
    0.0
  ],
  [
    [
      "nowadays, xml is one of the most used markup languages for storing and sharing data. as so, special care must be taken when validating the content of these files, ensuring that they comply with the standards defined for the data they represent. in order to ensure that these standards are met, xml validation languages such as dtd, xsd, or schematron can be used. these languages allow the definition of structural or semantic rules, that need to be followed in order to guarantee that the xml documents contain valid data. in terms of xml visualizing and editing, although there are several xml editors available, many of which have a wide variety of features that make them easier to use, there is a problem that no editor addresses: as these documents grow in size, it becomes more difficult to identify the boundaries of each element, which makes editing these files way harder than it should be. this paper proposes and documents the development of a tool with two main components. the first component is an xml validator that aims to offer greater freedom in defining validation constraints for these files, by allowing users to use a predefined programming language to write functions that will analyze the graph in order to determine if it is both syntactically and semantically correct. the second component, an xml graphic editor, will allow an easier creation, editing, and visualization of these files, by representing them in the form of 2d graphs.",
      "nos tempos correntes, xml é uma das linguagens de marcação mais utilizadas para armazenamento e partilha de dados. assim sendo, deve haver um cuidado especial na validação do conteúdo destes ficheiros, garantindo que cumprem os padrões definidos para os dados que representam. de forma a garantir que estes padrões se cumprem, podem ser utilizadas linguagens de validação de xml, como dtd, xsd ou schematron. estas linguagens permitem a definição de um conjunto de regras, estruturais ou semânticas, que necessitam ser cumpridas de forma a garantir que os documentos xml apresentam o formato de dados correto. a nivel da visualização e edição de documentos xml, embora existam vários editores de xml disponíveis, muitos dos quais apresentam uma grande variedade de funcionalidades que facilitam a sua utilização, existe um problema que nenhum editor corrige. consoante estes documentos crescem em dimensões, começa a tornar-se mais dificil identificar os limites de cada elemento, o que torna a edição destes docu mentos mais complicada do que necessita ser. neste documento é proposta e documentada a implementação de uma ferramenta que encorpore duas componentes. a primeira é um validador de xml que tem como objetivo dar uma maior liberdade na definição de restrições de validação destes ficheiros, recorrendo à utilização de uma linguagem de programação predefinida para a escrita de funções de que irão correr sob o grafo de forma a determinar se este se encontra sintatica e semanticamente correto. a segunda componente, um editor gráfico de xml, permitirá uma mais fácil criação, edição e visualização destes ficheiros, representando-os sob a forma de grafos 2d."
    ],
    [
      "the present document identifies and details the research and development held under the scope of a msc thesis pertaining to the scientific area of pedagogic tools for teaching support, ontolo gies and learning resources. this masters thesis in informatics engineering was developed in the university of minho, braga. the purpose of the project is to study the learning process of adults and how it connects to learn ing resources (lrs) in order to understand if a learning resource used to teach computational thinking (ct) to children, is suitable for adult learners. this approach ought to take into account adult learning theory to set its requirements, as well as ct principles and learning resources classification. to this end, an approach to the adequacy of learning resources in adult education was created which comprises the ontology ontoal that describes in detail the domain of adult learning (al) including the theory of al and a classification of both the adult learner and the learning resources. this ontology was developed in ontodl and prolog. in addition, we analyze the experiment conducted as part of the validation of this approach and the ontoal ontology. therefore, in this document, it is presented the state of the art pertaining to this field, exploring the concepts of learning resources, computational thinking, ontologies and adult learning and education. furthermore, it is rendered an introduction of the subject and the project, detailing the context of the problem, the objectives to be accomplished and the research hypothesis of said thesis. next, it is presented the state of the art regarding computational thinking, adult learning and education, ontologies and learning resources. thereafter, it is put forward the work proposal. then it is introduced the ontoal ontology in both ontodl and prolog (detailing the process of its development and the choices made), the questionnaires that were created as well as the analysis of the responses that we obtained. lastly, there are listed the conclusions and the future work.",
      "o presente documento identifica e detalha a investigação e desenvolvimento realizados no âmbito de uma tese de mestrado relativa à área científica de ferramentas pedagógicas de apoio ao ensino, ontologias e recursos educativos. esta tese de mestrado em engenharia informática foi desenvolvida na universidade do minho, braga. o objectivo do projecto é estudar o processo de aprendizagem dos adultos e como este se relaciona com os recursos educativos (re), de modo a compreender se um recurso educativo utilizado para ensinar pensamento computacional (ct) a crianças, é adequado para alunos adultos. esta abordagem deve ter em conta a teoria de ensino de adultos para estabelecer os seus requisitos, bem como os princípios de ct e a classificação dos recursos educativos. neste sentido foi criada uma abordagem à adequação de recursos educativos na educação de adultos que contém a ontologia ontoal que descreve em detalhe o domínio do ensino de adultos (ea) incluindo a teoria de ea e uma classificação tanto do aluno adulto como dos recursos educativos. esta ontologia foi desenvolvida em ontodl e prolog. para além disso, analisamos a experiência levada a cabo como parte da validação desta abordagem e da ontologia ontoal. por conseguinte, neste documento, é apresentado o estado da arte neste campo, explorando os conceitos de recursos educativos, pensamento computacional, ontologias e educação de adultos. além disso, é feita uma introdução ao tema e ao projecto, detalhando o contexto do problema, os objectivos a alcançar e a hipótese de investigação da referida tese. a seguir, é apresentado o estado da arte em matéria de pensamento computacional, ensino e educação de adultos, ontologias e recursos educativos. em seguida, é descrita a proposta de trabalho, a ontologia ontoal desenvolvida em ontodl e em prolog (detalhando o processo do seu desenvolvimento e as escolhas feitas), os questionários que foram criados, e a análise das respostas obtidas. finalmente, são listadas as conclusões bem como o trabalho futuro."
    ],
    0.3
  ],
  [
    [
      "vehicular ad hoc networks (vanets) is a term used to describe networks of moving vehicles equipped with devices that allow spontaneous communication with other vehicles and infrastructures. developing collaborative driving applications for vanets is currently a hot topic and has an increasing popularity in the intelligent transportation systems (its) domain. the goal of this thesis is to study the development and testing of advanced its applications, using platooning as a use case. it presents a state of the art on typical its applications, its evaluation and corresponding implementation and testing methods. the platooning management protocol (pmp) was then implemented and tested by means of simulation, resorting to the v2x simulation runtime infrastructure (vsimrti) framework, which couples simulation of urban mobility (sumo) and network simulator 3 (ns-3). results show that it is able to work in a smooth and efficient manner: the maneuvers happen during an acceptable interval, the proposed communication requirements are met and the lane capacity is increased.",
      "vehicular ad hoc networks (vanets) é um termo usado para descrever redes de veículos em movimento, equipados com dispositivos que permitem uma comunicação espontânea com outros veículos e infraestruturas. desenvolver aplicações de condução colaborativa para vanets é atualmente um tópico muito estudado e cuja popularidade tem crescido no domínio dos intelligent transportation systems (its) - sistemas de transporte inteligentes. o objetivo desta tese é o estudo, desenvolvimento e teste de aplicações de its avançadas, utilizando platooning como caso de uso. neste documento é apresentado o estado da arte relativo às aplicações its tipicamente avaliadas e os respetivos métodos de implementação e teste. o platooning management protocol (pmp) foi implementado e testado através de simulação, utilizando a ferramenta v2x simulation runtime infrastructure (vsimrti), que acopla as ferramentas simulation of urban mobility (sumo) e network simulator 3 (ns-3). os resultados mostram que o protocolo funciona de forma leve e eficiente: as manobras decorrem num intervalo de tempo aceitável, os requisitos de comunicações são cumpridos e a capacidade das faixas é aumentada."
    ],
    [
      "bacterial resistance to antibiotics is nowadays becoming a major concern. several reports indicate that bacteria are developing resistance mechanisms to various antibiotics. moreover, the processes involved in the development of new antibiotics are lengthy and expensive. therefore, an alternative to antibiotics is needed. one promising alternative are bacteriophages, viruses that specifically infect bacteria, causing their lysis. hence, it would be interesting to discover which bacteria a specific phage recognizes. the bacterial receptors determine phage specificity, using tail spikes/fibres as receptor binding proteins to detect carbohydrates or proteins, in bacterial surface. studying interactions between phage tail spikes/- fibres and bacterial receptors can allow the identification of interaction pairs. machine learning algorithms can be used to find patterns in these interactions and build models to make predictions. in this work, phagehost, a tool that predicts hosts at a strain level, for three species, e. coli, k. pneumoniae and a. baumannii was developed. several data was extracted from genbank, retrieving general, protein and coding information, for both phages and bacteria. the protein data was used to build an important phage protein function database, that allowed the classification of protein functions, namely, phage tail spikes/fibres. in the end, several machine learning models with relevant protein features were created to predict phage-host strain interactions. compared with previously performed works, these models show better predictive power and the ability to perform strain-level predictions. for the best model, a matthews correlation coefficient (mcc) of 96.6% and an f-score of 98.3% were obtained. these best predictive models were implemented online, in a server under the name phagehost (https://galaxy.bio.di. uminho.pt).",
      "resistência bacteriana a antibióticos está a tornar-se uma preocupação hoje em dia. várias bactérias foram descritas desenvolvendo mecanismos de resistência a diversos antibióticos. aliado a isto, estão os longos e dispendiosos processos envolvidos no desenvolvimento de antibióticos. por isso, há a necessidade de procurar uma alternativa aos antibióticos. uma alternativa promissora são os bacteriófagos, vírus que infetam especificamente bactérias e levam à sua lise. posto isto, seria interessante descobrir qual a bactéria que um certo fago reconhece. a especificidade de fagos é dada pelos recetores da superfícies das bactérias que conseguem reconhecer. eles usam proteínas das spikes/fibras para reconhecer recetires proteicos ou hidratos de carbono nas bactérias. estudar as interações entre spikes/fibras das caudas de fagos e recetores bacterianos pode permitir a identificação de pares de interação. algoritmos de aprendizagem máquina podem ser utilizados para descobrir padrões nestas interações e construir modelos para realizar previsões. neste trabalho, a ferramenta phagehost foi desenvolvida. permite a previsão de hospedeiros ao nível da estirpe, para três espécies, e. coli, k. pneumoniae e a. baumannii. vários dados foram extraídos do genbank, nomeadamente informações gerais, de proteína e codificante, para fagos e bactérias. com todos os dados proteicos, uma base de dados importante foi construída, que permitiu a classificação de funções proteicas, nomeadamente, spikes/fibras das caudas dos fagos. finalmente, vários modelos de aprendizagem máquina, com características proteicas relevantes, capazes de prever interações fago-hospedeiro, a nível da estirpe. em comparação com outros trabalhos semelhantes, estes modelos demonstraram melhor poder preditivo, assim como capacidade de prever interações a nível da estirpe. para o melhor modelo foram obtidos um coeficiente de correlação de matthews de 96.6% e um f-score de 98.3%. os melhores modelos foram implementados online, num servidor com o nome phagehost (https://galaxy.bio.di.uminho.pt)."
    ],
    0.06666666666666667
  ],
  [
    [
      "a análise da expressão genética é essencial para uma identificação da função dos genes e para a identificação destes quando relacionados com doenças. para a realização de um estudo em larga escala de mudanças na expressão genética é necessário encontrar um método que o faça com precisão e exatidão. desta forma, foi aqui incluída, uma análise pela tecnologia de microarrays, uma ferramenta importante no diagnóstico de doenças. a execução de um método que identificasse genes com regulação negativa e positiva e genes diferencialmente expressos simultaneamente, tornou-se, a principal motivação deste trabalho. de entre as diferentes técnicas estatísticas, a metodologia roc (receiver operating characteristic) foi a escolhida para o efeito. quando se associa a metodologia roc com a análise de dados de microarrays é possível ver que uma das principais aplicações é a identificação de grupos de genes associados ao desenvolvimento de qualquer patologia cancerígena. para a análise deste último parâmetro é utilizado o arrow plot com a representação do ovl (overlapping coefficient) e da auc (area under the curve) para cada gene, numa experiência de microarays e comparar a sua eficácia com outros métodos existentes para o mesmo propósito. através da análise de um conjunto de dados de pacientes afetados pelo adenocarcinoma do pâncreas foi possível identificar os genes diferencialmente expressos, sendo este o principal objetivo do trabalho em questão.",
      "genetic expression analysis is essential for the identification of gene function and when they are related with diseases. to perform a large-scale study of changes in gene expression it is necessary to find a method to do it with precision and accuracy. thus, it was included here an analysis by microarray technology, an important tool in the diagnosis of diseases. the execution of a method to identify genes with negative and positive regulation and differentially expressed genes simultaneously has become the main motivation of this work. among different statistical techniques, the receiver operating characteristic (roc) was the chosen one. when combining the roc methodology with microarray data analysis it is possible to see that one of the main applications is the identification of gene groups associated with the development of any kind of cancer. for the analysis of this last parameter is used the arrow plot with the overlapping coefficient (ovl) and the area under the curve (auc) representation for each gene of a microarray experience and compare its effectiveness with other existing methods for the same purpose. through the analysis of a set of affected patient data of pancreatic adenocarcinoma it was possible to identify differentially expressed genes, which is the main goal of this work."
    ],
    [
      "a predominância de um mundo orientado pelas tecnologias de informação (ti) é uma noção que está bem presente no quotidiano, desde a mais ínfima tarefa rotineira ao dia-adia num posto de trabalho. de todas as atividades onde as ti incidem, destaca-se neste trabalho a aplicação das mesmas numa área cujo foco é de extrema importância – a saúde. o setor da saúde, tal como outras áreas organizacionais, não é uma exceção à influência das tecnologias e tornou-se uma área que integra a informação como um bem fundamental para seu o bom funcionamento. o crescente aumento do volume de dados de registos eletrónicos e de fontes diversas com que os profissionais de saúde lidam todos os dias originou uma nova necessidade – a transformação desses dados em informação para extração de conhecimento. consequentemente, a dificuldade do processamento de tamanho volume de informação potenciou o aparecimento dos sistemas de business intelligence (bi), capazes de lidar com a quantidade de dados armazenada e cujo objetivo passa por apresentar informação sob a forma de conhecimento para suportar o processo de tomada de decisão. a grande motivação para a implementação de sistemas de bi surgiu da possibilidade de conceção de uma forma de disponibilizar a informação de forma rápida, eficaz e visualmente apelativa cuja interpretação seja algo intuitiva. a informação relevante pode ser disponibilizada em diversos formatos, como por exemplo num dashboard – técnica de visualização interativa crucial na análise da informação e no suporte à decisão. o pressuposto principal desta dissertação é evidenciar que na modalidade cirúrgica de uma unidade hospitalar é também possível transmitir informação que permita auxiliar os profissionais de saúde na gestão de um bloco operatório. deste modo, foram construídos indicadores de diversas categorias que poderão ser relevantes face às possíveis necessidades hospitalares. na seleção da tecnologia a utilizar para o desenvolvimento da plataforma de bi optou-se pelo power bi, ferramenta de business intelligence bastante intuitiva e que permite a partilha dos elementos visuais que vão influenciar a leitura do profissional de saúde responsável pela gestão da unidade cirúrgica. após o desenvolvimento dos dashboards, pode-se afirmar que o resultado foi satisfatório, uma vez que foram criados indicadores de desempenho que permitem perceber a importância de um sistema de bi para a gestão mais eficiente de uma unidade cirúrgica.",
      "years have gone by within a world ruled by information technology in everyday life, since the slightest daily routine task to the day-to-day work at job post. from all the activities operating within the it environment, it is highlighted throughout this project its application on one of the utmost important areas – health. such as further organizational areas, the healthcare sector does not escape from the influence of technologies and it has become an area in which information is set as a fundamental asset to ensure maximum accomplishment on things. the growing volume of data from electronic records and other sources that a health professional has to handle gave rise to a new need – the transformation from collected data to relevant information for knowledge extraction. as a consequence, a difficulty in processing large volumes of information emerged, leading to the outbreak of business intelligence (bi) systems, which are prepared to manage massive amounts of stored data and can convert information into knowledge to complement the decision support process. the main contribution of this project, as far as the bi systems is concerned, has emerged from the possibility of creating a fast, effective and visually appealing way of presenting user-friendly straightforward information. the information can be made available in several formats like a dashboard, for instance, which is an interactive visualization technique of the essence bearing in mind information analysis and decision making. this project aims to demonstrate that it is possible to gather information for practitioners and healthcare managers of surgical units to help improving control operations. for that reason, several performance indicators were designed in order to supply this kind of managing needs. as for the tool selected for developing the bi platform, power bi was preferred for being an intuitive means able to share visual elements which will influence the healthcare professional assigned for managing the surgical unit. after developing the dashboards it is reasonable to say that the result was satisfactory, since the created performance indicators can translate the importance of a bi system for a proper and more efficient management of a surgical unit."
    ],
    0.06666666666666667
  ],
  [
    [
      "neste milénio, a sociedade global tem não só criado tecnologias que revolucionaram o mundo e a perceção do mesmo como tem existido uma facilidade muito maior de obter dispositivos que contém uma imensidão de funcionalidades e sensores a um preço cada vez mais baixo. dessa forma, a monitorização e recolha de indicadores de saúde e médicos em tempo real é algo possível e a sua recolha através dum smartwatch, smartphone ou dispositivo semelhante para posterior informação sobre o estado de saúde atual do utilizador e dessa forma calcular um índice de saúde de bem-estar geral. como consequência natural, também é importante funcionar como uma ajuda pessoal para alertar em tempo real irregularidades no estado de saúde do utilizador. toda esta informação utilizada de forma inteligente pode criar uma comunidade de saúde e bem-estar essenciais para os atuais utilizadores e também futuros. neste trabalho de dissertação idealizámos e implementámos um sistema de índices de bem-estar e saúde capaz de fornecer informação em tempo real sobre uma variedade de métricas de bem-estar sobre o utilizador, em que essa informação é extraída dos sensores do smartwatch do mesmo, fornecendo assim um sistema de notificações para alertar uma variação elevada no bem-estar do indíviduo mas também um histórico do seu índice de bem-estar ao longo do tempo.",
      "in this millennium, the global society not only has created technologies that revolutionized the world and the perception of it, but also has been creating an easiness to obtain those devices that contain a vast array of functionalities and sensors at a low price. in that way, monitoring and collecting of health, medical and wellness indicators in real time is now possible to collect through a smartwatch, smartphone, or similar device to then give insightful information about the current health of the user and therefore create a health index of well-being. as a natural consequence, this is important also to serve as a personal help to alert in real time and intelligently irregularities found in the health state of the user. in that way, all this information and data can create a community of health and well-being essential to the current and future users. in this masters’ dissertation we idealized and implemented a health and wellness index system capable of providing real-time information about a variety of wellness metrics about the user, in which that information is extracted of the smartwatch’s sensors, giving therefore a notification system to alert a high variation of those metrics and the wellness of the individual, but also providing a timeline of that health and wellness index over time."
    ],
    [
      "no mundo de hoje, a cibersegurança, ou segurança digital, é cada vez mais relevante à medida que a tecnologia avança, o que é constatável por regulares notícias de ataques informáticos a infraestruturas de organizações que afetam muitas vezes os serviços das empresas e podem até resultar em consequências gravíssimas a nível económico e financeiro. mesmo em portugal, nota-se que o número de ataques informáticos a empresas ou grandes corporações têm aumentado e deixado um rasto de destruição em muitas delas, o que pode levar até à falência da empresa, fruto da má reputação adquirida por esta, o que deixa por vezes a empresa para trás relativamente à sua concorrência que apresenta melhores garantias em segurança. deste modo, esta dissertação de mestrado tem o principal intuito de mostrar a importância das empresas definirem e implementarem um plano de cibersegurança, pelo que é desenvolvida uma estratégia de cibersegurança capaz de mitigar ou eliminar potenciais consequências graves à infraestrutura de uma organização oriundas de incidentes de cibersegurança. numa primeira fase, é analisado um conjunto de documentos relacionados ao tema em questão fundamentais para uma segunda fase onde são descritos um conjunto de 7 passos para ajudar as empresas a criarem um plano que reflita as medidas que estão e as que serão implementadas no âmbito da cibersegurança. para isso, é seguida a nist cybersecurity framework v1.1 (nist csf) como a base da estratégia, o cybersecurity capability maturity model (c2m2) v2.0 como ferramenta de autoavaliação e os cis controls v8.0 como controlos adicionais para reforço da cibersegurança, além de outras fontes relevantes na área, tais como os standards desenvolvidos pelo international organization for standardization (iso). numa terceira fase, a estratégia de cibersegurança é aplicada a uma empresa portuguesa que atua na área dos serviços de confiança, o que constitui uma evidência de que a estratégia definida pode ser aplicada em qualquer organização em portugal. assim, seguindo todos os princípios-base da cibersegurança, através da análise documental de boas práticas, legislação, frameworks e standards de cibersegurança, é desenvolvida uma estratégia de cibersegurança dedicada às organizações, que teve aplicação prática num caso concreto e pode ser seguida por qualquer organização que tenha a intenção de reforçar a sua infraestrutura digital em cibersegurança. o objetivo final é que a cibersegurança fique formalizada na organização com planos/políticas que contenham o nível desejado em cibersegurança (“perfil-alvo”) e se consiga gerir os riscos através da implementação de ações para combater as lacunas identificadas na empresa.",
      "in today’s world, cybersecurity, or digital security, is increasingly relevant as technology advances, which can be seen in regular news reports of cyberattacks on organizations’ infrastructures that often affect the companies’ services and may even have very serious economic and financial consequences. even in portugal, the number of cyberattacks on companies or large corporations has increased and left a trail of destruction in many of them, which can even lead to the bankruptcy of the company, due to the bad reputation acquired by it, which sometimes leaves the company behind its competitors that have better guarantees in security. thus, this master’s thesis has the main purpose of showing the importance of companies defining and implemen ting a cybersecurity plan, which is why a cybersecurity strategy capable of mitigating or eliminating potential serious consequences to the infrastructure of an organization arising from incidents of cybersecurity. in a first phase, a set of documents related to the topic in question is analyzed, which are fundamental for a second phase, where a set of 7 steps are described to help companies create a plan that reflects the measures that are and will be implemented within the scope of the cybersecurity. for this, the nist cybersecurity framework v1.1 (nist csf) is followed as the basis of the strategy, the cybersecurity capability maturity model (c2m2) v2.0 as a self-assessment tool and the cis controls v8.0 as additional controls to reinforce cybersecurity, in addition to other relevant sources in the area, such as the standards developed by the international organization for standardization (iso). in a third stage, the cybersecurity strategy is applied to a portuguese company operating in the area of trust services, which is evidence that the defined strategy can be applied to any organization in portugal. therefore, following all the basic principles of cybersecurity, through document analysis of best practices, legislation, frameworks and cybersecurity standards, a cybersecurity strategy dedicated to organizations is developed, which had practical application in a specific case and can be followed by any organization that intends to strengthen its digital infrastructure in cybersecurity. the ultimate goal is that cybersecurity is formalized in the organization with plans/policies that contain the desired level of cybersecurity (“target profile”) and that risks are managed through the implementation of actions to combat the gaps identified in the company."
    ],
    0.06666666666666667
  ],
  [
    [
      "as máquinas têm demonstrado várias vantagens em comparação com os humanos, nomeadamente a reproduzir e escalar tarefas, apresentando velocidade e precisão elevadas. todavia, nem sempre é possível compreender o funcionamento dos seus algoritmos. assim, a necessidade de explicar os resultados destes tem vindo a crescer, levando ao aumento da relevância de ferramentas de explicabilidade, já que estas possibilitam a redução das divergências entre a interpretação do modelo e o nível de raciocínio humano. o principal objetivo desta dissertação passou pelo desenvolvimento de uma técnica drill-down para avaliar modelos de regressão caixa negra, considerando interações multivariável no âmbito dos preditores. assim, propomos edrs, uma combinação entre drs e edps. de modo a facilitar a sua análise, foram implementadas múltiplas formas de visualização: boxplots, histogramas e gráficos de densidade, exibindo distribuições completas, uma visualização em grafo para explorar interações entre preditores e tabelas de desempenho, comparando os quartis de cada distribuição com uma referência. com base em pontos de corte e uma distribuição de referência, foi ainda efetuada uma extrapolação de contra-factos para regressão. aplicaram-se quatro algoritmos distintos a uma gama heterogénia de conjuntos de dados com o intuito de eliminar qualquer potencial enviesamento de modelo. estas experiências mostraram que as edrs apresentam vantagens em comparação com os edps. o número de gráficos a analisar foi reduzido, já que apenas os subgrupos interessantes são apresentados. além disso, podem ser detetadas interações compostas por mais de três condições. foi, também, considerado um caso de estudo, retratando um problema de seleção de modelo. as edrs mostraram-se cruciais para compreender como os modelos se comportam em relação a combinações específicas de dados e provar que o melhor modelo geral nem sempre é o melhor para certos subgrupos. deste modo, as edrs podem ser usadas para escolher um modelo ou para gerar ensembles, usando os modelos com melhor desempenho para cada subgrupo. apesar das vantagens comparativamente às ferramentas existentes, o uso das regras não esgota o domínio das variáveis, pois não se exibem todas as combinações possíveis, com até três condições. no futuro, pode ser proveitoso estudar uma discretização dos preditores numéricos guiada pelas regras, já que esta etapa depende de técnicas externas. meta-modelos também devem ser definidos para produzir ensembles baseados no desempenho de cada subgrupo.",
      "machines have shown several advantages compared to humans, namely to reproduce and scale tasks, presenting high speed and precision. however, it is not always possible to understand how the algorithms used work. consequently, the need to explain the results of these models has been increasing, leading to a boost in the relevance of explainability tools, as these enable the reduction of divergences between the interpretation of the model and the human level of reasoning. the main goal of this dissertation consisted of developing a drill-down technique to evaluate black box regression models, that considered multivariate interactions within the scope of the predictors. thus, we propose edrs, a combination between drs and edps. in order to ease the examination of these, multiple visualization forms were implemented. namely, boxplots, histograms and density plots to display complete distributions of values, a network visualization to rapidly check interactions of every feature condition and performance tables, comparing the quartiles of every distribution with a reference. based on the cutting point values and a reference distribution, an extrapolation of counter-factual examples to regression was also implemented. four distinct algorithms were applied to a heterogeneous range of datasets in order to eliminate any potential model bias. these experiments showed that edrs present some advantages in comparison to edps. first, the number of plots to analyze is reduced, as only subgroups that differ significantly from the reference and similar subgroups are presented. also, interactions composed by more than three conditions of feature values can be detected. a case study was considered, applying the developed tools to a model selection problem. edrs showed to be crucial in helping users to understand how the models behave regarding specific combinations of data. moreover, it was shown that the best model overall is not always the best for every subgroup. hence, edrs can be used to select a model or to generate ensembles, using the best performing models for each subgroup. despite the advantages compared to the existing tools, the usage of rules does not exhaust the domain of variables, as not every possible combination of values, with up to three conditions, is displayed. in the future, a rule based discretization of numerical features might be proven fruitful, as this step relies on external techniques. meta-models are also to be defined to produce ensembles based on performance for each subgroup."
    ],
    [
      "over the years, metagenomics has demonstrated to play an essential role on the study of the microorganisms that live in microbial communities, particularly those who inhabit the human body. several bioinformatic tools and pipelines have been developed, but usually they only address one question: \"who is there?\" or \"what are they doing?\". this work aimed to develop a computational framework to answer the two questions simultaneously, that is, perform a taxonomic and functional analysis of microbial communities. merlin, a previously developed software designed for the construction of genome-scale metabolic models for single organisms, was extended to deal with metagenomics data. it has an userfriendly and intuitive interface, not requiring command-line knowledge and further libraries dependencies or installation, as many other tools. the extended version of merlin can predict the taxonomic composition of an environmental sample based on the results of homology searches, where the proportions of phyla and genera present are discriminated. regarding the metabolic analysis, it allows to identify which enzymes are present and calculate their abundance, as well as to nd out which metabolic pathways are e ectively present. the performance of the tool was evaluated with samples from the human microbiome project, particularly from the saliva. the taxonomic membership predicted in merlin was in agreement with other tools, despite some di erences in the proportions. the functional characterization showed a conserved pool of pathways through di erent samples, although merlin sometimes presented less pathways than expected because the routine is highly dependent on the enzymes annotation. overall, the results showed the same pattern as reported before: while the pathways needed for microbial life remain relatively stable, the community composition varies extensively among individuals. in the end, merlin demonstrated to be a reliable standalone alternative to web services for those scientists that have concerns about sharing data.",
      "ao longo dos anos, a metagenómica demonstrou ter um papel essencial no estudo dos microorganismos que vivem em comunidades bacterianas, particularmente aqueles que habitam o corpo humano. várias ferramentas e pipelines bioinformáticas foram desenvolvidas, mas normalmente estas apenas abordam uma destas questões: \"quem está lá?\" ou \"o que é que estão a fazer?\" este trabalho teve como objectivo o desenvolvimento duma ferramenta computacional para responder aos dois problemas em simultâneo, isto é, realizar tanto uma análise taxonómica como funcional de comunidades microbianas. o merlin, um software anteriormente desenvolvido para construir modelos metabólicos à escala genómica para um organismo, foi estendido para tratar dados de metagenómica. o programa possui uma interface intuitiva e amiga do utilizador, não necessitando de conhecimentos de linha de comandos nem de dependências de bibliotecas ou instalação de aplicações adicionais. esta versão estendida do merlin prevê a composição taxonómica global dum metagenoma baseado nos resultados de procuras de sequências homólogas, onde as proporções dos fila e géneros são apresentadas. no que diz respeito à análise metabólica, o merlin permite identificar quais as enzimas presentes e calcular a sua abundância, bem como identificar quais as vias metabólicas que estão efectivamente presentes. o desempenho da ferramenta foi avaliado com amostras do projecto do microbioma humano, particularmente com amostras da saliva. a composição taxonómica prevista no merlin esteve de acordo com outras ferramentas, apesar de algumas diferenças observadas nas proporções. a caracterização funcional mostrou um conjunto conservado de vias metabólicas nas diferentes amostras, mesmo que o merlin tenha identificado menos enzimas que o esperado, pois o método é bastante dependente do processo anotação. globalmente, os resultados revelaram o mesmo padrão reportado anteriormente: enquanto as vias metabólicas necessárias para a vida microbiana se mantêm estáveis, a composição taxonómica varia bastante entre indivíduos. no final, o merlin demonstrou ser uma alternativa fidedigna a serviços web para aqueles cientistas que têm restrições em divulgar os seus dados não publicados num website."
    ],
    0.0
  ],
  [
    [
      "this dissertation explores a novel approach for the development of a virtual entity or artificial intelligence capable of simulating user behavior within the immersive and expansive virtual realm of the world of warcraft video game. classified as a three-dimensional massively multiplayer online role-playing game, world of warcraft serves as an exemplary context for studying and refining techniques that can be readily adapted to other applications. the research methodology employed in this study involves a systematic analysis of the application’s process memory space, with a focus on identifying crucial memory data locations. furthermore, the investigation entails the identification and preservation of pathways leading to the aforementioned memory data points, ensuring their efficient and viable accessibility. to enable the creation of the virtual entity, the neuroevolution of augmenting topologies technique is employed, which facilitates the generation and intricate development of an artificial neural network—the entity’s simulated brain. by utilizing the previously acquired memory data points as sensory inputs, and emulating the entity’s responses as inputs within the running process, a comprehensive framework for emulating user behavior is established. the findings presented in this dissertation contribute to the advancement of knowledge in the field of virtual entity creation and artificial intelligence, offering practical implications for a range of applications beyond world of warcraft.",
      "esta dissertação explora uma abordagem inovadora para o desenvolvimento de uma entidade virtual ou inteligência artificial capaz de simular o comportamento do usuário no ambiente imersivo e expansivo do jogo de vídeo world of warcraft. classificado como mmorpg, este serve como um contexto exemplar para estudar e aprimorar técnicas que podem ser facilmente adaptadas a outras aplicações. a metodologia de pesquisa empregada neste estudo envolve uma análise sistemática do espaço de memória do processo do aplicativo, com foco na identificação de locais cruciais de dados na memória. além disso, a investi gação envolve a identificação e preservação de caminhos que levam aos pontos de dados de memória mencionados anteriormente, garantindo uma acessibilidade eficiente e viável. para permitir a criação da entidade virtual, é empregada a técnica de neuroevolução de topologias ampliáveis, que facilita a geração e o desenvolvimento intrincado de uma rede neural artificial - o cérebro simulado da entidade. ao utilizar os pontos de dados de memória adquiridos anteriormente como entradas sensoriais e emular as respostas da entidade como entradas no processo em execução, é estabelecido um framework abrangente para simular o comportamento do usuário. as descobertas apresentadas nesta dissertação contribuem para o avanço do conhecimento no campo da criação de entidades virtuais e inteligência artificial, oferecendo implicações práticas para uma variedade de aplicações além do world of warcraft."
    ],
    [
      "databases are a critical point for medical institutions since, without them, information systems can not work. in such way, the usage of a secure, easily scalable and highly available database is crucial when dealing with medical data. nowadays, medical institutions use rdbmss – or sql databases – in their medical images’ information systems. however, this type of databases has drawbacks at various levels. for instance, they show scalability issues in distributed environments or when dealing with high numbers of users. besides, their data schema is complex and difficult to create. in terms of functionalities, this kind of databases show a huge set of features, which add extra unnecessary complexity when dealing with large complex datasets. an alternative to rdbmss are nosql databases. these are databases design to deal with large amounts of heterogeneous data without giving up high performance rates. however, both sql and nosql databases still present major concerns regarding security. in the medical field, since there is sensitive and personal data being exchanged, their protection is a crucial point when implementing a storage solution for medical purposes. having this said, this master thesis focuses on the development of a medical imaging database with increased availability by the employment of a nosql hbase backend. in order to protect the data being stored, a secure version of hbase protected with symmetric cipher was also implemented. both protected and unprotected versions were implemented, tested, and compared with an open source toolkit. from the results obtained by executing performance tests, it is possible to conclude that the hbase backend shows a better overall performance in terms of latency and throughput in comparison to the open source toolkit. however, the appliance of the encryption service implies higher latency and lower throughput.",
      "as bases de dados são um ponto crítico das instituições médicas, uma vez que sem elas os sistemas de informação não podem funcionar. desta forma, no que respeita a dados médicos, a utilização de uma base de dados segura, facilmente escalável e com alta disponibilidade é crucial. hoje em dia, as instituições médicas utilizam bases de dados relacionais – ou sql – nos sistemas de informação que lidam com imagens médicas. no entanto, este tipo de bases de dados apresentam desvantagens a vários níveis. por exemplo, estas demonstram problemas a nível de escalabilidade em ambiente distribuído, bem como no que respeita à sua utilização por parte de um número elevado de utilizadores. para além disto, o seu modelo de dados é complexo e difícil de criar. este tipo de bases de dados tem também um elevado número de funcionalidades que, quando o volume de dados é elevado, adicionam complexidade desnecessária ao sistema. uma alternativa a este tipo de bases de dados são as bases de dados nosql. estas foram desenhadas para lidar com grandes volumes de dados heterogéneos, mantendo uma elevada performance. no entanto, tanto as bases de dados sql como nosql, apresentam problemas a nível de segurança. na área médica, uma vez que são manuseados dados sensíveis e de cariz pessoal, a implementação de uma base de dados segura é fundamental. desta forma, a presente dissertação de mestrado foca-se no desenvolvimento de uma base de dados para imagens médicas com disbonibilidade aumentada através da implementação de uma base de dados nosql – o hbase. de forma a proteger os dados sensíveis que são guardados, foi implementada uma versão de hbase protegida com cifra simétrica. ambas versões – protegida e não protegida – foram implementadas, testadas, e comparadas com um tooltik open source. dos resultados obtidos a partir da execução de testes de performance é possível concluir que o hbase simples tem uma melhor performance em termos de latência e débito, em comparação com o toolkit open source. no entanto, a aplicação de cifra simétrica implica uma maior latência e um menor débito."
    ],
    0.12857142857142856
  ],
  [
    [
      "systems biology aims to integrate experimental and computational approaches with the purpose of explaining and predicting the organisms' behavior. the development of mathematical models in silico gives us a better in-depth knowledge of their biological mechanism. bioinformatics tools enabled the integration of a large amount of complex biological data into computer models, but also capable to perform computational simulations with these models, that can predict the organisms' phenotypic behavior in different conditions. up to date, genome-scale metabolic models (gsmms) include several metabolic components of an organism. these are related to the metabolic capabilities encoded in the genome. in recent years, multiple gsmms have been built by several research groups. with the increase in number, of these models, important issues regarding the standardization have arisen, a common problem is the different nomenclatures used by each of the research groups. in this work, the major focus is to address these problems, specifically for the human gssms. therefore, the two most recent human gsmms were selected to go through a data integration process. integration strategies of these models most important entities (metabolites and reactions), were defined based on an exhaustive analysis of the models. the broad knowledge of their attributes enabled the creation of effective and efficient integration methods, supported by a core database developed in the local research group. the final result of this work, is a unified repository of the human metabolism. it contains all the metabolites and reactions that were automatically integrated along with some manual curation.",
      "a biologia de sistemas pretende integrar abordagens experimentais e computacionais com o objetivo de explicar e prever o comportamento dos organismos. o desenvolvimento in silico de modelos matemáticos permite atingir um conhecimento mais aprofundado dos seus mecanismos biológicos. através de ferramentas bioinformáticas é possível integrar uma grande quantidade de dados complexos nestes modelos computadorizados, assim como, realizar simulações computacionais que permitem prever o comportamento fenotípico dos organismos em diferentes condições ambientais. até à data, os modelos metabólicos à escala genómica (mmegs) incluem muitos componentes metabólicos de um organismo, relacionando a codificação do seu genoma com as suas capacidades metabólicas. nos últimos anos, têm sido construídos vários mmegs, por diferentes grupos de investigação. com o crescente surgimento destes, tem-se denotado grandes falhas ao nível da padronização, uma vez que são utilizadas diferentes nomenclaturas por cada grupo de investigação. neste trabalho, pretende-se colmatar essas falhas especificamente para os mmegs humanos. deste modo, foram selecionados os dois mmegs humanos mais recentes, para passarem por um processo de integração de dados. as estratégias de integração das entidades mais importantes destes modelos (os metabolitos e as reações) foram definidas com base numa análise exaustiva dos modelos. o conhecimento dos atributos destes permitiu construir métodos eficientes e eficazes, tendo como núcleo uma base de dados desenvolvida no grupo de acolhimento. o resultado final deste trabalho é um repositório unificado do metabolismo humano. neste, estão contidos todos os metabolitos e reações que foram integrados automaticamente, com alguma verificação manual."
    ],
    [
      "this document reports the development of a master’s thesis, included in the second year of the master’s degree in informatics engineering at universidade do minho in braga, portugal. the main goal for this project was to identify which characteristics influence the recognition and identification of a programming language, considering both its typical source code elements and its linguistic style. in other words, which elements contribute the most to the characterization of a language? how many structural elements of a language may be modified without losing its identity? in order to achieve these goals, a comprehensive bibliographic research was made, ranging from basic concepts such as programming languages and how they work, to several state-of-the-art studies that have been conducted in the same context of this project. complementary to this research, a set of programming languages was also chosen as a study subject, which resulted in a detailed review and categorization of their characteristics. after the definition of a general approach, a survey was developed and conducted to gather programmers’ answers on how they identify and recognize programming languages. in addition to the survey, a machine learning model was also used to evaluate how these two facets (human versus machine) compared to each other. this dual approach provided insights into which syntactic and semantic elements have a greater influence on the identity of a programming language. this master’s project resulted in an overall picture of programming languages’ characteristics and the relative influence they have on both programmers’ and ai-driven recognition. this result may serve as support for language engineers and project managers who wish to reduce attrition when defining or designing new languages for a project, domain, or context.",
      "este documento é referente ao desenvolvimento de um projeto de mestrado, incluído no segundo ano do mestrado em engenharia informática da universidade do minho em braga, portugal. o objetivo principal deste projeto é identificar quais características influenciam o recon hecimento e a identificação de uma linguagem de programação, especificamente analisando o código fonte e o seu estilo linguístico. por outras palavras, quais elementos contribuem mais para a caracterização de uma linguagem? especificamente, quantos dos elementos estruturais de uma linguagem podem ser modificados sem que esta perca a sua identidade? para atingir os objetivos deste projeto, foi realizada uma pesquisa bibliográfica exaustiva, que abrange desde conceitos básicos, como linguagens de programação e seu funcionamento, até vários estudos de ponta realizados no mesmo contexto deste projeto. complementar mente a esta pesquisa, um conjunto de linguagens de programação foi escolhido como objeto de estudo, resultando numa revisão detalhada e categorização de suas características. após a definição de uma abordagem geral, foi desenvolvida e conduzida uma pesquisa para reunir respostas de programadores sobre como eles identificam e reconhecem lin guagens de programação. além da pesquisa, um modelo de machine learning também foi utilizado para avaliar como essas duas facetas (homem versus máquina) se comparavam entre si. esse duplo enfoque forneceu insights sobre quais elementos sintáticos e semânticos têm maior influência na identificação de uma linguagem de programação. este projeto de mestrado resultou em uma visão geral das características das linguagens de programação e da influência relativa que elas exercem tanto no reconhecimento de programadores quanto no reconhecimento impulsionado por ia. este resultado pode servir como suporte para engenheiros de linguagens e gerentes de projetos que desejam reduzir atrito ao definir ou projetar novas linguagens para um projeto, domínio ou contexto."
    ],
    0.3
  ],
  [
    [
      "despite the advances made in recent decades, information technologies still have a lot to offer to the health sector. in the scientific community, the idea of technology as a very important part of improving healthcare is already unanimous. health institutions are increasingly willing to invest in technologies that support the daily activities life of health professionals, especially at the time of decision making so that it is as fast and as accurate as possible. thus, the number of hospital information systems has increased, especially the sys tems supporting and easing the diagnosis, treatment and follow-up of the patient. however, these systems are also necessary for decision-making on the part of the ins titutions computer processes, such as intelligent agents, which indirectly influence the quality of the services provided. of all the choices that have been made within the developed systems, the use of bi has proved quite effective in the presentation of infor mation as well as in the construction of decision support systems. as a consequence, this dissertation project aims to develop a bi platform to continuously monitor the intelligent agents of the chp as well as their activities. the current digital revolution brings several challenges, the constant emergence of innovative technologies often put at stake the work done due to the eventual obsoles cence that it can present in comparison with those that are built with these innovations. thus, health institutions, should always be alert and keep an eye on their information systems, evaluating whether they have become archaic or even if there are new soluti ons that respond better to the problems they have daily at hand. in this way, they will be updated at technological level and competitive at market level since, inevitably, the quality of the provided services improves significantly. with all these innovations new problems arise, hospital units are increasingly com plex environments at the level of computer systems due to their heterogeneity. the information generated and stored in each system has characteristics and structures that can be quite different, which causes the information to be individualized. in this way, the main issue addressed in this dissertation emerges, the interoperability. , the interoperability. to answer all these challenges, aida was created, a system based on intelligent agents that aim to implement interoperability in health institutions. the in telligent agents have tasks of various types, but have the similarity of communicating with heterogeneous systems in order to exchange information of great importance or even manage and store information in databases. therefore, the need to monitor these agents as well as their activities arises in order to maintain the interoperability and quality of the services provided by the institution where they are implemented. thus, this dissertation aims to developa platform that monitors continuously and in real time the agents of the chp. this project was based on the dsr methodology, that initially defines the problem for which one intends to design a solution, outlining the objectives to be achieved. the remaining phases deal with the development and evaluation of the developed solution. as proof of concept, the swot analysis and the technology acceptance study based on tam3 were chosen. the results of the proof of concept were quite positive and revealed an excellent growth potential for the developed solution.",
      "apesar dos avanços registados nas últimas décadas, as tecnologias da informação (ti) ainda têm muito para oferecer ao setor da saúde. as instituições de saúde estão cada vez mais dispostas a investir em tecnologias que apoiem o dia a dia do profissional de saúde principalmente no momento de tomada de decisão para que este seja o mais rápido e acertado possível. assim sendo, o número de sistemas de informação hospitalar (sih) tem aumentado, principalmente em sistemas de ajuda no diagnóstico, tratamento e acompanhamento do paciente. no entanto, estes sistemas são também necessários para a tomada de decisões relativas aos processos informáticos das instituições, como por exemplo os agentes inteligentes, que influenciam a qualidade dos serviços prestados, de forma in-direta. como consequência, o presente projeto de dissertação visa o desenvolvimento de uma plataforma de business inteligence (bi) para a monitorização contínua dos agentes inteligentes, do centro hospitalar do porto (chp), assim como das suas ativi-dades. o constante aparecimento de tecnologias inovadoras põe muitas vezes em causa o trabalho desenvolvido devido à obsolescência que ele possa apresentar relativamente àqueles que são construídos com essas inovações. assim sendo, as instituições, neste caso de saúde, devem estar sempre alerta para se manterem atualizadas a nível tec-nológico e competitivas a nível de mercado uma vez que, inevitavelmente, a qualidade dos serviços prestados melhora significativamente. com todas estas inovações surgem novos problemas, as unidades hospitalares são ambientes cada vez mais complexos a nível de sistemas informáticos devido à hetero-geneidade dos mesmos. a informação gerada e guardada em cada sistema tem carac-terísticas e estruturas que podem ser bastante dispares o que faz com que a informação esteja individualizada. deste modo, surge o conceito sobre o qual se trabalhou neste projeto de dissertação, a interoperabilidade. para responder a todos estes desafios, foi criada a agência de interoperação, difusão e arquivo de informação médica (aida), um sistema baseado em agentes que visam a implementação da interoperabilidade nas instituições de saúde. os agentes inteligentes têm tarefas de vários tipos, mas que têm a semelhança de comunicar com sistemas heterogéneos a fim de trocar informações de grande importância ou até mesmo gerir e guardar essas informações em bases de dados. surge então a necessidade de monitorização destes agentes bem como das suas atividades para que, desta forma, seja mantida a interoperabilidade e qualidade da prestação de serviços na instituição onde estão implementados. deste modo, a pre-sente dissertação visa desenvolver uma plataforma de monitorização, contínua e em tempo real, para os agentes do chp. este projeto baseou-se na metodologia design sci-ence research (dsr), onde inicialmente se define o problema para o qual se pretende projetar uma solução, traçando os objetivos que se querem alcançar. as restantes fases tratam do desenvolvimento e avaliação do produto projetado. como prova de con-ceito, escolheu-se a análise strenghts, weaknesses, opportunities and threats (swot) e ainda um estudo de aceitação de tecnologia baseado no technology acceptance mo-del (tam)3. os resultados da prova de conceito foram bastante positivos e revelaram um excelente potencial de crescimento para a solução desenvolvida."
    ],
    [
      "desde o primeiro contacto da tecnologia com o universo da saúde, que esta agradável e harmoniosa relação tem vindo a ser cada vez mais poderosa e majestosa. a utilização de sistemas de informação na saúde tem servido diversos propósitos, seja na gestão de profissionais de saúde, na descoberta de novas curas, no apoio à tomada de decisão ou simplesmente no armazenamento de todas as informações respeitantes a este ambiente. a prática clínica é uma ciência multidisciplinar, na qual os profissionais de saúde de diferentes especialidades colaboram em prol do bem comum do utente. nesta linha de pensamento, emerge a cirurgia bariátrica, encarada como uma abordagem multidisciplinar no tratamento da obesidade. esta cirurgia engloba o diagnóstico de essencialmente cinco especialidades distintas, já que a “saúde é o estado de completo bem-estar físico, mental e social e não somente a ausência de doença, segundo a organização mundial de saúde”, como tal é imprescindível a constante partilha de informação entre estas, de modo a ser proporcionado um correto e eficaz tratamento ao utente. a quantidade extrema de informação clínica produzida diariamente e de forma constante, a diversidade de fontes de conteúdo e os diferentes formatos dos dados têm constituído grandes desafios de interoperabilidade entre sistemas de informação de saúde. por conseguinte, a presente temática é pautada pela integração da tecnologia openehr na plataforma desenvolvida aliada ao registo e à monitorização de informações intrínsecas à cirurgia bariátrica. face ao mencionado, o grande objetivo na conceção de um projeto desta dimensão concentra-se fundamentalmente em facilitar toda a especificidade do processo inerente à cirurgia bariátrica, quer para os profissionais de saúde (auxiliando-os nas suas tarefas diárias), quer para os utentes envolvidos (usufruírem de cuidados de saúde mais rápidos e orientados). desta forma, construiu-se uma plataforma que produziu resultados únicos e inovadores, arrastando consigo a solução útil, necessária e interessante para o preenchimento de uma profunda lacuna nesta área da saúde.",
      "since the first contact of technology with the health universe, this pleasant and harmonious relationship has become more and more powerful and majestic. the use of information systems in health has served several purposes, whether in the management of health professionals, in the discovery of new cures, in supporting decision-making, or simply in the storage of all the information concerning this environment. clinical practice is a multidisciplinary science, in which health professionals from different specialties collaborate for the common good of the patient. in this line of thought, bariatric surgery emerges, seen as a multidisciplinary approach to the treatment of obesity. this surgery encompasses the diagnosis of essentially five distinct specialties, since “health is a state of complete physical, mental, and social wellbeing and not merely the absence of disease, according to the world health organization”, and as such it is essential to constantly share information among them in order to provide the patient with correct and effective treatment. the extreme amount of clinical information produced daily and constantly, the diversity of content sources, and the different data formats have been great challenges to interoperability between health information systems. therefore, this theme is guided by the integration of openehr technology in the developed platform combined with recording and monitoring of information intrinsic to bariatric surgery. in view of the above, the main goal for the conception of a project of this dimension is fundamentally focused on facilitating all the specificity of the process inherent to bariatric surgery, both for health professionals (helping them in their daily tasks), and for the patients involved (enjoying faster and more oriented health care). in this way, a platform was built that produced unique and innovative results, bringing with it a useful, necessary, and interesting solution to fill a deep gap in this health area."
    ],
    0.12857142857142856
  ],
  [
    [
      "esta dissertação aborda o processo de implementação de interfaces web, mais concretamente, utilizando a framework react. as interfaces de utilizador são peças fundamentais de qualquer produto computacional interativo. uma boa interface consegue conquistar o utilizador e fazer com que este utilize o produto, enquanto uma interface de menor qualidade pode ser a causa para a pouca utilização de um software. por este motivo, existem abordagens e metodologias focadas na criação de interfaces, para proporcionarem uma boa experiência ao utilizador e fazer com que este utilize o software desenvolvido. após a conceção da interface, é necessário proceder à sua implementação. para isso existem diversas tecnologias e abordagens. entre as diferentes tecnologias há ainda múltiplas frameworks de desenvolvimento, cada uma com as suas características específicas, o que dificulta, por exemplo, a transição de uma tecnologia para outra. o ideal seria tornar o processo de desenvolvimento de uma interface o mais independente possível da tecnologia a ser utilizada. tendo em vista a resolução deste problema a dissertação apresenta duas contribuições principais. um processo de interpretação do design e da sua divisão em componentes, com o objetivo de maximizar a reutilização de código e consequentemente a eficácia no processo de implementação. a divisão do design é feita através de uma abordagem atómica, onde componentes mais atómicos se juntam e formam componentes mais complexos. a criação de uma arquitetura genérica capaz de representar uma aplicação react, com o objetivo de fornecer uma visão de mais alto nível, mostrando todas as diferentes entidades que existem na arquitetura de uma aplicação, e também a forma como estas entidades se relacionam. isto permite uma separação de responsabilidades, separando a definição da interface, da sua lógica de negócio, e da interação com serviços externos. além disso, a arquitetura genérica serviu de ponto de partida para a criação de uma estrutura de organização do código capaz de suportar o crescimento dos projetos ao longo do tempo. estrutura que facilita, e sistematiza, o trabalho dos programadores, dado que estes ficam a saber exatamente onde têm de inserir determinados novos ficheiros, ou onde está um qualquer ficheiro que precisa de ser alterado quando é necessário atualizar um componente da interface. por último, para provar que os conceitos descritos anteriormente são aplicáveis, para ajudar os programadores a aplicá-los, e para sistematizar o processo de implementação, criou-se uma ferramenta de geração de código. a ferramenta permite criar diferentes partes da arquitetura genérica automaticamente. é também possível gerar um componente react partindo de um protótipo de uma interface.",
      "this dissertation addresses the process of implementing web interfaces, more specifically, utilizing the react framework. the user interfaces are fundamental parts of any interactive computer product. a good interface is able to conquer its users and make them use the product, while a lesser quality interface may be the cause for the little usage of a software. for this reason, there are approaches and methodologies focused on the designing of interfaces, in order to provide a good experience to the user and make them use the developed software. after designing the interface, it is necessary to proceed with its implementation. for this, there are several technologies and approaches. among the different technologies, there are also multiple development frameworks, each with its characteristics, which makes it difficult, for example, to transition from one technology to another. the ideal would be to make the process of interface development as independent as possible of the specific technology used. intending to solve this problem, the dissertation presents two main contributions. a process of interpretation of the design and division into components, to maximize the reuse of code and consequently the efficiency of the implementation process. the division of the design is done through an atomic approach, where more atomic components come together and form more complex components. the creation of a generic architecture capable of representing a react application, with the goal of providing a higher-level view, showing all the different entities that exist in an application’s architecture, and also how these entities relate to each other. this allows a separation of responsibilities, separating the definition of the interface, its business logic, and the interaction with external services. furthermore, the generic architecture served as a starting point for the creation of a structure for code organization, capable of supporting the growth of projects over time. a structure that makes the programmer’s work easier and more systematic, since they know exactly where to insert new files, or where to find a file that needs to be changed, when it is necessary to update a component of the interface. finally, a code generation tool was created, to prove that the concepts described above are applicable, to help programmers apply them, and to systematise the implementation process. the tool allows pro grammers to create different parts of the generic architecture automatically. it is also possible to generate a react component starting from a prototype of an interface."
    ],
    [
      "nowadays, most companies resort to data analytics frameworks to extract value from the increasing amounts of digital information. these systems give substantial competitive ad vantages to companies since they allow to support situations such as possible marketing decisions or predict user behaviors. therefore, organizations tend to leverage the cloud to store and perform analytics over the data. database services in the cloud present significant advantages as a high level of efficiency and flexibility, and the reduction of costs inherent to the maintenance and management of private infrastructures. the problem is that these services are often a target for malicious attacks, which means that sensitive and private personal information can be compromised. the current secure analytical processing solutions use a limited set of cryptographic techniques or technologies, which makes it impossible to explore different trade-offs of performance, security, and functionality requirements for different applications. moreover, these systems also do not explore the combination of multiple cryptographic techniques and trusted hardware to protect sensitive data. the work presented here addresses this challenge, by using cryptographic schemes and the intel sgx technology to protect confidential information, ensuring a practical solution which can be adapted to applications with different requirements. in detail, this dissertation begins by exposing a baseline study about cryptographic schemes and the intel sgx tech nology, followed by the state-of-the-art revision about secure data analytics frameworks. a new solution based on the apache spark framework, called safespark, is proposed. it provides a modular and extensible architecture and prototype, which allows protecting in formation and processing analytical queries over encrypted data, using three cryptographic schemes and the sgx technology. we validated the prototype with an experimental evalu ation, where we analyze the performance costs of the solution and also its resource usage. for this purpose, we use the tpc-ds benchmark to evaluate the proposed solution, and the results show that it is possible to perform analytical processing on protected data with a performance impact between 1.13x and 4.1x.",
      "atualmente, um grande número de empresas recorre a ferramentas de análise de dados para extrair valor da quantidade crescente de informações digitais que são geradas. estes sistemas apresentam consideráveis vantagens competitivas para as empresas, uma vez que permitem suportar situações como melhores decisões de marketing, ou até mesmo prever o comportamento dos seus clientes. neste sentido, estas organizações tendem a recorrer a serviços de bases de dados na nuvem para armazenar e processar informação, uma vez que estas apresentam vantagens significativas como alto nível de eficiência e flexibilidade, bem como a redução de custos inerentes a manter e gerir uma infraestrutura privada. no entanto, estes serviços são frequentemente alvo de ataques maliciosos, o que leva a que informações pessoais privadas possam estar comprometidas. as soluções atuais de processamento analítico seguro utilizam um conjunto limitado de técnicas criptográficas ou tecnologias, o que impossibilita o balanceamento de diferentes compromissos entre performance, segurança e funcionalidade para diferentes aplicações. ainda, estes sistemas não permitem explorar a simultânea utilização de técnicas criptográficas e de hardware confiável para proteger informação sensível. o trabalho apresentado nesta dissertação tem como objetivo responder a este desafio, utilizando esquemas criptográficos e a tecnologia intel sgx para proteger informação confidencial, garantindo unia solução prática que pode ser adaptada a aplicações com diferentes requisitos. em detalhe, este documento começa por expor um estudo de base sobre esquemas criptográficos e sobre a tecnologia sgx, seguido de uma revisão do estado de arte atual sobre ferramentas de processamento analítico seguro. uma nova solução baseada na plataforma apache spark, chamada safespark, é proposta. esta providencia uma arquitetura modular e extensível, bem como um protótipo, que possibilita proteger informação e executar interrogações analíticas sobre dados cifrados, utilizando três esquemas criptográficos e a tecnologia intel sgx. o protótipo foi validado com uma avaliação experimental, onde analisamos a penalização de desempenho da solução, bem como a sua utilização de recursos computacionais. com este propósito, foi utilizada a plataforma de avaliação tpc-ds para avaliar a solução proposta, e os resultados mostram que é possível executar processamento analítico sobre dados protegidos, apresentando um impacto no desempenho entre 1.13x e 4.1x."
    ],
    0.3
  ],
  [
    [
      "o presente documento constitui o relatório de atividade profissional com vista à obtenção do grau de mestre ao abrigo do despacho rt-38/2011, regulamentado pela circular eeum-cc-02/2012. tendo realizado a minha licenciatura na universidade do minho em engenharia de sistemas e informática com especialização em de comunicações e redes, tendo trabalhando e complementada a formação desde então na referida área, qualificam-me para requerer a equivalência ao mestrado em engenharia de redes e serviços telemáticos. o presente documento irá centrar-se nas funções desempenhadas durante os anos de trabalho na fccn (fundação para a computação científica nacional) enquanto engenheiro de redes e de gestor da respetiva área. um primeiro capítulo introdutório descreve brevemente o meu percurso profissional assim como o académico, que servirá de enquadramento para o restante relatório. tal como especificado na circular, a parte seguinte do relatório descreve algumas atividades desenvolvidas no âmbito do trabalho, sendo enquadradas dentro de um contexto científico num tema em que a fccn foi pioneira – a implementação do ipv6 na rede académica. serão ainda apresentados alguns outros pequenos projetos desenvolvidos como ilustradores do que é a área da engenharia de redes. o terceiro capítulo descreve um grande projeto em que fui o principal responsável pelo seu desenvolvimento e implementação. trata-se da criação de uma rede de fibra ótica trazendo o estado da arte para a rede académica portuguesa. foi um processo longo e complexo que englobou diversas áreas da engenharia, requerendo conhecimentos profundos de telecomunicações, controlo financeiro, gestão de projetos e domínio de aspetos jurídicos associados às obrigações legais decorrentes do financiamento. no âmbito do relatório é ainda solicitada a apresentação de eventuais trabalhos de natureza científica. neste aspeto há a ressaltar que, durante o último ano em que trabalhei na fccn, integrei um projeto de investigação europeu – joint research activity. a temática era future transport networks e visava estudar os mais recentes desenvolvimentos na área das arquiteturas de rede. a componente em que trabalhei foi na evolution beyond 100g, nomeadamente no estudo das diversas codificações para débitos de 100g e superiores. no capítulo final é apresentada a conclusão e nos apêndices são indicados estágios e ações de formação realizados ao longo dos anos.",
      "this document represents my professional experience report to fulfil the requirements towards obtaining the master’s degree in accordance with eeum-cc-02/2012. having finished my degree at the university of minho in systems engineering and computer science, specialized in communications and networks, and having worked since then in this line of work as well as continued studies in this area, qualify me to apply for equivalency master’s on engineering of computer networks and telematics services. this document will focus on the tasks carried out during the years of work at fccn as a network engineer and manager of the network working group. the first chapter briefly describes my working career and the studies carried out since having finished the degree. as required, the following chapter describes some activities done in the line of work with scientific relevance in which the fccn was a pioneer - like the usage of ipv6. some other small projects aimed for improving the monitoring and stability of the network will be described. the third chapter describes a large project in which i had the main role for its development and implementation - the creation of a fiber optic network. it was a long and complex process that involved several areas of engineering, required deep knowledge of telecommunications, financial control, project management as well as legal obligations from the founding committee. in the report it is asked for presenting any work of scientific relevance. during the last year i worked at fccn, between mid-2012 and october 2013 i joined a european research project jra - joint research activity. the designation of this activity was future transport network and aimed to study the latest developments in the field of network architectures and that would be incorporated into the design of future géant network. the area i worked on was in evolution beyond 100g, studying various encoding schemes for 100gbps speeds and higher. in the final chapter is the conclusion. in the annexes some relevant internships and training activities conducted over the years are presented."
    ],
    [
      "internet of things (iot) systems generate massive amounts of time series data that need to be stored for historical analysis. as a result, database management systems (dbmss) for these scenarios have particular requirements in their ability to ingest large amounts of data and to optimise aggregation, filtering and time-ranged queries over this data, which are essential for historical analysis. through the use of fog computing, combining both edge and cloud layers, it is possible to achieve reduced latency and increased scalability, privacy and connectivity through the edge, while still benefiting from the enhanced computing and storage power of the cloud. this has led to the development of fog dbmss. database benchmarking allows standardising performance assessment and comparison of different solutions. however, current time series database benchmarking tools are not designed for multi-layer architectures, such as the ones used by edge-cloud hybrid dbmss. this thesis proposes mulletbench, a benchmarking tool that is able to evaluate the internal load balancing capabilities of a multi-layer time series database management system (tsdbms). this is achieved by integrating automated deployment features, per-node and per-layer performance and system resource metrics, allowing for a more detailed analysis of the suts’ performance than previously possible. the performance of influxdb and iotdb is evaluated using the developed tool, comparing their performance in multiple workloads and deployment scenarios. results show that the edge layer can be used to improve performance by distributing the workload over multiple layers and performing downsampling at the edge layer, increasing overall throughput and reducing latency at the cloud. these conclusions are enabled by mulletbench’s novel features, and would not have been possible with previously existing solutions.",
      "os sistemas de internet das coisas (idc) geram enormes quantidades de dados de séries temporais que precisam de ser armazenados para análise histórica. consequentemente, os sistemas de gestão de bases de dados (sgbd) para estes cenários têm requisitos particulares não só para a sua capacidade de ingerir grandes quantidades de dados mas também para a sua capacidade de otimizar queries de agregação ou filtragem e em intervalos de tempo sobre estes dados, que são essenciais para análise histórica. através do uso de computação em fog, combinando as camadas de edge e cloud, é possivel conseguir latência reduzida e maior escalabilidade, privacidade e conectividade através da edge, beneficiando simultaneamente do superior poder de computação e armazenamento da cloud. isto levou ao desenvolvimento de sistemas de gestão de bases de dados fog. o benchmarking de bases de dados permite a estandardização do método de avaliação de desempenho e a comparabilidadde de resultados de diferentes soluções. contudo, as atuais ferramentas de benchmarking para bases de dados de séries temporais não estão desenhadas para arquiteturas multi-camada, tais como as utilizadas por sgbds híbridos edge-cloud. a tese propõe o mulletbench, uma ferramenta de benchmarking capaz de avaliar as capacidades de balanceamento interno de carga de um sgbd de séries temporais multi-camada. esta ferramenta alcança o através da integração de funcionalidades de automação de deployment e métricas de desempenho e de recursos de sistema por nó e por camada, permitindo uma análise mais detalhada do desempenho dos sistemas testados do que era previamente possível. o desempenho das bases de dados influxdb e iotdb é avaliado usando a ferramenta desenvolvida, comparando o seu desempenho com multiplas cargas e em multiplos cenários. os resultados demonstram que a camada edge pode ser usada para melhorar o desempenho destas, através da distribuição da carga por múltiplas camadas e realização de downsampling na camada edge, aumentando o débito total e reduzindo a latência na cloud. estas conclusões são possíveis graças às funcionalidades inovadoras do mulletbench, e não seriam possíveis com soluções existentes anteriormente."
    ],
    0.06666666666666667
  ],
  [
    [
      "we are facing a period where software projects have a huge dimension involving small resources, high risk and a wide range of available approaches. in this scenario the software development methodologies (sdms) can prove to be a useful ally, but very dangerous and even fatal if misused. the big issue around this matter is how to choose the appropriated sdm that ts a speci c project. in the given scope, this dissertation describes a framework for comparing sdms delivering a set of procedures that should be followed when the choice of an sdm is made. the dissertation approaches the framework by applying it to a group of sdms that were selected by their popularity and signi cance. this exercise is done to prove the concept of the framework and to provide a base comparison, with each chosen sdm, that can, and should, be extended by those who choose to use the framework. the classi cation is achieved by de ning a scale that goes from total satisfaction to no satisfaction, with an intermediate level of partial satisfaction, that is applied to a set of keys. these keys are based in swebok (software engineering body of knowledge) that describes and explains the di erent knowledge areas (ka) stating their common issues and best practices. to explain the framework, the dissertation analyzes each ka and evaluates the selected sdms by assessing how their approach complies with swebok's knowledge areas, using the previous stated scale. the framework delivered can be enriched by its user who should provide weights to each ka regarding the project in which the sdm will be used and previous experiences",
      "actualmente atravessamos um período em que os projectos de software têm uma grande dimensão, envolvendo baixos recursos, alto risco e com um variado leque de abordagens a escolher. nestes casos as metodologias de desenvolvimento de software (mds) pode ser um bom aliado, contudo se mal escolhido pode ser extremamente perigoso ou até fatal. a questão que se levanta então é, qual a metodologia a escolher. neste contexto, este documento descreve um conjunto de procedimentos a seguir para comparar mds. os procedimentos são então aplicados a um conjunto de populares mds provando o conceito aqui apresentado, disponibilizando uma comparação de base com uma explicação para cada metodologia escolhida que pode, e deve, ser estendida por quem utilizar o conjunto de procedimentos aqui descritos. a classificação é conseguida através de uma escala que vai da satisfação total à não satisfação, com um nível intermédio de satisfação parcial, para cada uma das chaves. as chaves escolhidas são baseadas no swebok (sigla de software engineering body of knowledge), que descreve e explica as diferentes áreas de conhecimento da engenharia de software com referência ás melhores práticas e problemas comuns para cada uma delas. para o conjunto de procedimentos apresentado, cada uma das áreas de conhecimento é analisada e as mds são avaliadas de acordo com a forma como abordam cada uma das áreas de conhecimento do swebok utilizando a escala anteriormente referida. estes procedimentos podem ser enriquecidos por quem o escolha utilizar atribuindo pesos a cada uma das áreas de conhecimento com base no projecto a que a mds será aplicada e a experiências anteriores."
    ],
    [
      "o paradigma do controlo de acessos, em especial o controlo de acesso à informação, tem vindo a mudar nos últimos anos. controlo este que inicialmente era efetuado pelas próprias aplicações de forma isolada e autónoma, sem a possibilidade de consultarem ou se integrarem com qualquer sistema centralizado. todavia, com o crescente uso das tecnologias de informação nas organizações, novas soluções (tais como os serviços de diretoria ldap) têm vindo a ser adotadas com o intuito de dar resposta à necessidade de uma política de acessos unificada e coesa, transversal aos diversos serviços e aplicações. estas soluções representam uma mais-valia no desempenho das tarefas organizacionais. tendo em conta esta necessidade, este trabalho propõe uma nova solução para o controlo de acessos físicos e lógicos através da apresentação e implementação de um novo modelo de controlo de acessos baseado no par cargo-organização. é ainda apresentada e implementada neste projeto uma nova abordagem no controlo de acessos lógicos, sendo esta assim capaz de interagir e configurar aplicações que carecem do suporte de protocolos e mecanismos padrão para o controlo de acessos.",
      "the access control paradigm has been changing in the past years, specially what regards the access control to information. information access control was originally achieved independently by each application without querying or interacting with a central system. however, due to the increasing usage of information technologies, new solutions (such as ldap services) have been put in place in order to obtain a unified access policy across different services and applications. these solutions greatly improve the performance of organizational tasks. this project aims to present a new access control solution for both physical and logical layers. therefore, a new access control model based on the pair role/organization is presented, as well as a concrete implementation of this model. bearing in mind that not all applications support access control protocols, another approach (which allows the interaction with these applications) is also taken. in this context, after presenting the conceptual organization of the proposed solution, along with some implementation details, some illustrative evaluation tests are also presented and discussed."
    ],
    0.0
  ],
  [
    [
      "the simulation of object buoyancy is a very interesting topic that involves a lot of research in the area of fluid dynamics. to better understand how the simulation of floating objects is done, it is necessary to distill the articles found to make a simple simulation that still covers the main characteristics of the simulation. in this sense, the research required to develop the algorithm is focused on the main dynamic characteristics of the object’s movement in the water, that is, the main forces that are applied. although this type of simulation is widely used, as in video games, articles that address this topic in a simple way that can be easily adapted in a simulation are quite scarce in terms of the physics involved or focus on how to start and don’t develop much further. in this sense, there are few techniques or methods of implementation, most of which are focused on minimizing the costs of these same techniques and still providing a simulation as realistic as possible. faced with this problem, the objective is to have a robust algorithm and be as realistic as possible, being able to demonstrate a good representation of the buoyancy of objects according to their mass and the forces that are exerted. to accomplish this objective, an algorithm was adapted from kerner (2015) and kerner (2016). each added force contributes to a better representation of reality making the algorithm more realistic. with this accomplished, it is implemented in shaders and tested with multiple objects simultaneously for performance analysis, being possible to observe the capacity of the algorithm. the simulation made in this dissertation was tested in terms of performance and time spent on gpu, by the number of boats and by the number of triangles in the mesh of the boat.",
      "a simulação de flutuação de objetos é um tema bastante interessante que envolve imensa pesquisa na área de dinâmica de fluidos. de forma a compreender melhor como é feita a simulação de objetos a flutuar é necessário destilar os artigos encontrados para fazer uma simulação simples que ainda assim abranja as caracteristicas principais da simulação. neste sentido, a pesquisa necessária para a elaboração do algoritmo será focada nas principais características dinâmicas do movimento do objeto na água, ou seja, nas principais forças que são aplicadas. apesar deste tipo de simulação ser muito utilizado como, por exemplo, em videojogos, os artigos que abordam este tema de forma simples e que podem ser facilmente adaptados numa simulação são bastante escassos em termos de física envolvida ou simplesmente se focam em como começar e não são muito desenvolvidos. neste sentido, existem poucas técnicas ou métodos de implementação, sendo a maior parte focada em minimizar os custos dessas mesmas técnicas e mesmo assim proporcionar uma simulação o mais realista possível. considerando este problema, o objetivo será ter um algoritmo focado na simplicidade, mas que seja o mais robusto e realista possível, podendo demonstrar uma boa representação da flutuabilidade de objetos consoante a sua massa e as forças que são exercidas. para cumprir tal objetivo foi adaptado um algoritmo a partir de kerner (2015) e kerner (2016). cada força adicionada contribui para uma melhor representação da realidade tornando o algoritmo mais realista. com isto realizado, este será implementando em shaders e testado com vários objetos em simultâneo para uma análise de desempenho, sendo assim possível observar a capacidade do algoritmo implementado. a simulação feita nesta dissertação foi testada em termos de desempenho e tempo gasto em gpu, tendo em atenção o número de barcos e o número de triângulos na mesh do barco."
    ],
    [
      "the popularization of mobile devices and the characteristic pervasiveness of internet access have led to increased interest in developing potentially relevant services for citizens. the various solutions for identifying people using digital devices are an example of this. among these solutions is the iso/iec dis 18013-5 standard. this standard published by the international organization of standardization (iso) defines the technical requirements necessary for secure transmission, i.e. with guarantees of data integrity and authenticity, of identification attributes related to a driver’s license. the interest to implement the standard iso/iec dis 18013-5 has risen due to the increase in development and public interest in wearable devices, and the recent effort by the european union to encourage the use of digital identification documents. implementation of this standard is made easier by the existence of cross-development tools, e.g., xamarin, flutter, that allow code sharing between different wearable platforms. these tools serve as a catalyst for implementation while maintaining the functionality of the code.",
      "a popularização de dispositivos móveis e a pervasividade característica do acesso à internet, têm levado ao aumento do interesse em desenvolver serviços potencialmente relevantes aos cidadãos. as várias soluções para a identificação de pessoas recorrendo a dispositivos digitais, são exemplo disso. dentre estas soluções, encontra-se a norma técnica iso/iec dis 18013-5. esta norma publicada pela international organization of standardization (iso) define os requisitos técnicos necessários para que haja uma transmissão segura, ou seja, com garantias de confidencialidade, integridade e autenticidade de dados, de atributos de identificação relativos à carta de condução digital. tendo em conta o crescente desenvolvimento e interesse público de dispositivos wearable, e o recente esforço da união europeia para incentivar o uso de documentos digitais de identificação, seria de grande interesse a implementação da norma iso/iec dis 18013-5. a implementação vêm a ser facilitada pela existência de ferramentas de desenvolvimento cruzado, e.g., xamarin, flutter, que permitem a partilha de código entre diferentes plataformas wearable. estas ferramentas servem como catalisador da implementação, mantendo ao mesmo tempo a funcionalidade do código."
    ],
    0.3
  ],
  [
    [
      "a tecnologia tem revelado avanços significativos ao longo dos anos. porém os desenvolvimentos não se focaram apenas na área da computação, expandido-se para outras áreas como a saúde, permitindo o desenvolvimento de novas técnicas de diagnóstico ou o aperfeiçoamento das existentes. desta forma a qualidade de vida e os cuidados de saúde disponibilizados pelas instituições às pessoas foram melhorados. no entanto, estes progressos não permitem a total supressão de patologias, existindo algumas sem uma cura efetiva como é o caso das perdas cognitivas. o diagnóstico de perdas cognitivas pode potenciar algumas modificações na vida do paciente como, por exemplo, a presença de uma pessoa prestadora de cuidados, provocando a perda de independência do doente. de forma a diminuir a invasão de privacidade, vários investigadores desenvolveram projetos para permitir a orientação destas pessoas, porém exigiam algum esforço mental que pontualmente se poderia tornar demasiado complexo. o projeto desenvolvido pretende orientar as pessoas com perdas cognitivas, permitindo que estas tenham uma vida mais independente. por outro lado, a pessoa prestadora de cuidados pode desenvolver outro tipo de atividade sem descurar o tipo de serviço que presta através de aplicações que lhe permitem conhecer a posição atual da pessoa a seu cargo. com este projeto pretende-se que a pessoa com perdas cognitivas tenha uma maior independência tanto em trajetos comuns como em percursos de lazer. este grau de independência é possível através da utilização de uma aplicação informática para dispositivos móveis que permite a seleção de diversos pontos de interesse e posterior orientação até estes. este projeto utiliza conceitos de realidade aumentada e sinal gps para aquisição da localização atual, indicando o caminho através de símbolos simples. numa outra aplicação para dispositivos móveis o percurso pode ser visualizado em tempo real possibilitando uma constante monitorização da pessoa com perdas cognitivas. no caso de ocorrer alguma eventualidade é possível consultar os últimos pontos frequentados por esta.",
      "the technology has shown significant progress over the years. this progress was not exclusive to computers, expanding to other areas such as health. the use of technology in this field enabled the development of new diagnostic technics or the enhancement of existing ones. thus the life quality and healthcare provided by institutions to the people were also improved. however, all the progress has not suppressed diseases. there are a few that do not have an effective cure such as cognitive disabilities. when a patient is diagnosed with cognitive disabilities his life quality may suffer some changes. one of the major alterations is the presence of a caregiver. thus, the patient incurs in a decrease of its independence. in order to reduce this privacy invasion, researchers have developed several projects to enable the orientation of these people, but some of these projects may require some mental effort that sometimes could become very complex. the developed project intends to guide people with cognitive disabilities in a simplistic way, allowing them to become more independent. moreover, the caregiver may develop another type of activity without neglecting the type of service provided due to the applications that let him/her know the current position of the person with cognitive disabilities. with this project it is intended to provide a greater independence to the person with cognitive disabilities in common paths and leisure travels. this improvement in the independence of the person is achieved through an application for mobile devices that allows the selection of several points of interest and further orientation. this project uses concepts of augmented reality and the gps signal to get the user current location. through simple symbols the path is indicated. in another application for mobile devices the route can be viewed in real time enabling a constant monitoring of the person with cognitive disabilities. this application also provides the caregiver the visualization of the last points frequented by the person with cognitive disabilities."
    ],
    [
      "this dissertation introduces a logic aimed at combining dynamic logic and paraconsistent logic for application to the quantum domain, to reason about quantum phase properties: paraconsistent phased logic of quantum programs (phlqp◦ ). in the design phlqp◦ , firstly the dynamic was built first, phased logic of quantum programs (phlqp). phlqp is itself a dynamic logic capable of dealing with quantum phase properties, quantum measurements, unitary evolutions, and entanglements in compound systems , since it is a redesign of the already existing logic of quantum programs (lqp), [14], over a representation of quantum states restricted to a space b equipped with only two computational basis, standard and hadamard. as instances of applications of the logic phlqp, there is a formal proof of the correctness of the quantum teleportation protocol, of the 2-party and 4-party of the quantum leader election (qle) protocol, and of the quantum fourier transform (qft) operator for 1, 2 and 3 qubits . on a second stage, phlqp was extended with the connective ◦ known as the consistency operator, a typical connective of the paraconsistent logics logics of formal inconsistency (lfis), [8, 21, 22]. the definition of consistent quantum state and a set of proper para consistent axioms for the quantum domain, fundamental paraconsistent quantum axioms (fparqaxs), were provided. an example of application of phlqp◦ is the possibility of express and prove correctness of the universal quantum gate, the deustch gate.",
      "esta dissertação introduz uma lógica que tem como objectivo combinar lógica dinâmica e lógica paraconsistente com aplicação no domínio quântico, assim como expressar propriedades relacionadas com fases quânticas: phlqp◦. no projetar da phlqp◦, primeiramente concebeu-se a sua componente dinâmica, phlqp. phlqp por si só é uma lógica capaz de lidar com propriedades de fases quânticas, evoluções unitárias, e entrelaçamento em sistemas compostos, uma vez que é um redesenhar da já existente lqp, [14], sobre uma representação de estados quânticos restrita a um espaço b munido de apenas duas bases computacionais, standard e hadamard. como instâncias de aplicação da lógica phlqp, há uma prova formal para a correção do protocolo de teletransporte quântico, para o protocolo qle para uma party quer de 2 quer de 4 agentes, e para o operador de qft de 1, 2, e 3 qubits. numa segunda fase, phlqp é extendida com a conectiva ◦, conhecida como operador de consistência, uma conectiva característica das lfis, [8, 21, 22]. e a partir desta conectiva a definição de estado quântico consistente é um conjunto de axiomas paraconsistentes próprios para o domínio quântico, fparqaxs. um exemplo de aplicação da phlqp◦ é a possibilidade de expressar e permitir correção para o comportamento da gate quântica universal, a deutsch-gate."
    ],
    0.0
  ],
  [
    [
      "traffic classification is a fundamental task for various aspects of network management and monitoring. knowing traffic characteristics allows a better design of the network and services, contributing for an improved performance and quality of service. regarding anonymized traffic, this becomes an intricate procedure due to packet encryption. the presence of anonymous traffic in networks is noticeable, demonstrating user’s concern about privacy and anonymity at different levels. the growing use of anonymity tools creates the need of a secure and efficient experience. therefore, traffic classification is also relevant to developers of these tools (to improve user service robustness) and internet service providers (to understand what type of traffic is circulating in the network). this dissertation’s primary goal is to study the application of machine learning algorithms in multilevel anonymous traffic classification. this work proposes an adaptation of an intrusion detection system as proof of concept, using datasets containing mixed traffic (benign and anonymized). furthermore, it is presented a comparison to empirical studies carried out previously.",
      "a classificação do tráfego de rede é uma tarefa fundamental para múltiplos aspetos da gestão e operação das redes de comunicações. conhecer as características do tráfego permite dimensionar e configurar melhor os serviços de rede por forma a oferecer um melhor desempenho e qualidade de serviço a apli cações e serviços suportados. no contexto de redes anonimizadas, esta classificação é mais complexa devido à existência de encriptação de pacotes, tornando-se um grande desafio. é notável o crescimento da presença de tráfego anonimizado nas redes, evidenciando-se uma pre ocupação pela privacidade e anonimato em vários níveis. o uso crescente de ferramentas que fornecem estes serviços cria a necessidade de uma navegação segura e eficiente. a classificação de tráfego é então relevante tanto para quem desenvolve estas ferramentas, para melhorarem os seus serviços, como para os internet service providers, para compreenderem que tipo de tráfego circula na rede. esta dissertação tem como objectivo estudar a aplicação de algoritmos de machine learning no processo de classificação a vários níveis de tráfego anonimizado. propõe-se a adaptação de um sistema de deteção de intrusões como prova de conceito utilizando conjuntos de dados contendo tráfego normal e anonimizado. por fim, realizam-se ainda comparações a estudos realizados anteriormente."
    ],
    [
      "the main objective of this dissertation is to make a contribution in the automation of web applications' development, starting from prototypes of their graphical user interlaces. the integration of model-based user interface development concepts with the more traditional user-centred development approach allows for a rethinking of gui design development, independent of implementation details, and redefining models to realize these graphical interfaces. in the end, the intent is to increase the level of abstraction of the development process, promote better adaptation of applications to different devices and execution environments, and decrease the effort required to develop the graphical interlaces. due to the exponential increase in the use of internet-based services and applications, there is an also increasing demand for web designers and developers. at the same time, the proliferation of languages, frameworks and libraries illustrates the current state of immaturity of web development technologies. this state of affairs creates difficulties in the development and maintenance of web applications. an approach is presented that allows designers to use prototyping tools, in this case adobe xd, to design graphical interfaces, and then automatically converts them to vue.js + bootstrap code, thus creating a first version of the implementation. this is done through the interpretation of the svg file that adobe xd exports. the goal is not to produce the final version of the ul. instead, we aim to produce a first version of the code, which can then be refined by the developer. this enables us to place less requirements on the prototype, regarding the amount of information that it must contain. in the end, we get a skeleton of vue.js code that is easy to maintain and reuse to further improve the project.",
      "o principal objetivo da presente dissertação é contribuir para a automatização do desenvolvimento de aplicações web, a partir de protótipos das interfaces gráficas de utilizador. a integração de conceitos de desenvolvimento de interfaces de utilizador baseadas em modelos com a abordagem mais tradicional do desenvolvimento centrado no utilizador permite repensar o desenvolvimento do design da interface gráfica, independente dos detalhes de implementação, e redefinir modelos para concretizar essas interfaces gráficas. no fundo, o intuito é aumentar o nível de abstração do processo de desenvolvimento, promover uma melhor adaptação das aplicações aos diferentes dispositivos e ambientes de execução e diminuir o esforço necessário para desenvolver as interfaces gráficas. devido ao aumento exponencial da utilização de serviços e aplicações baseadas na internet, há uma procura crescente de web designers e programadores. ao mesmo tempo, a proliferação de linguagens de programação, frameworks e bibliotecas ilustra o atual estado de imaturidade das tecnologias de desenvolvimento web. este estado cria dificuldades no desenvolvimento e principalmente na manutenção de aplicações web. é apresentada uma abordagem que permite aos designers utilizar ferramentas de prototipagem, neste caso foi escolhido adobe xd, para desenhar interfaces gráficas e depois convertê-las automaticamente para código vue.js + bootstrap, criando assim uma primeira versão da implementação. esta geração é feita através da interpretação do ficheiro svg que o adobe xd exporta. o objetivo não é produzir a versão final da ui, mas sim produzir uma primeira versão do código que pode depois ser aperfeiçoada pelo programador. este fator permite colocar menos requisitos no protótipo, relativamente à quantidade de informação que este deve conter. no final, obtém-se um esqueleto do código vue.js, fácil de manter e de reaproveitar para melhorar cada vez mais o projeto."
    ],
    0.3
  ],
  [
    [
      "o desenvolvimento de aplicações tem sido alvo de recentes alterações, procurando cada vez mais rapidamente entregar as aplicações aos clientes. aliado a isto, a procura de uma integração mais eficaz com as várias equipas de desenvolvimento leva à procura por alternativas ao que era feito anteriormente. indo um pouco ao encontro do que é pretendido, acaba por surgir a arquitetura utilizando microserviços, que apresenta várias vantagens, sendo que muitas vezes é apresentada como a alternativa perfeita à arquitetura monolítica. por estas razões, em conjunção com a adoção desta arquitetura por grandes empresas, regista um grande crescimento e aceitação nos últimos anos, tanto no mercado, como em contextos académicos. a verdade é que com pesquisas profundas em vários artigos é possível verificar que esta arquitetura também apresenta vários inconvenientes, principalmente relacionados com a sua natureza distribuída, que muitas das vezes acabam por passar despercebidos devido às suas prometidas vantagens e por serem vagamente referidos na maioria dos trabalhos na comunidade cientifica. este paradigma distribuído acaba por levantar todo um novo conjunto de desafios e associado com o facto de ser uma arquitetura recente, muitas das equipas de desenvolvimento não estão preparadas para fazer a sua implementação corretamente. como resultado, as aplicações, tem dificuldades em cumprir os objetivos pretendidos. para mitigar estas questões, começaram a ser desenvolvidos vários padrões para problemas bastante comuns, para a grande maioria das aplicações. atualmente existem vários padrões para esta arquitetura já desenvolvidos, sendo que para cada problema, podem existir vários padrões desenvolvidos, como o problema de leitura de dados distribuídos em vários serviços. cada um destes padrões tem as sociados vários compromissos e por esse motivo é importante identificar cada um, de modo a escolher o padrão que melhor se adeque à aplicação que se aspira desenvolver. para a realização deste estudo empírico, foi definida uma aplicação referência que servirá como base. a seguir, são definidos vários casos de estudo onde são desenvolvidas as diversas aplicações com os padrões em questão. no final, foi feita uma comparação e uma análise dos compromissos relacionados com vários atributos de qualidade.",
      "the application’s development has been subject to recent changes, seeking to deliver applications to customers increasingly quickly. in addition to this, the search for more effective integration with the various development teams leads to the search for alternatives to what was done previously. going somewhat in line with what is intended, the architecture using microservices eventually emerges, which has several advantages, and is often presented as the perfect alternative to monolithic architecture. for these reasons, in conjunction with the adoption of this architecture by large companies, it has seen great growth and acceptance in recent years, both in the market and in academic contexts. the truth is that with in-depth research into several articles it is possible to verify that this architecture also presents several drawbacks, mainly related to its distributed nature, which often end up going unnoticed due to its promised advantages and because they are vaguely mentioned in most work in the scientific community. this distributed paradigm ends up raising a whole new set of challenges and associated with the fact that it is a recent architecture, many development teams are not prepared to implement it correctly. as a result, applications have difficulty fulfilling their intended objectives. to mitigate these issues, several patterns began to be developed for very common problems for a large set of applications. currently, there are several patterns already developed for this architecture, and for each problem, there may be several patterns developed, such as the problem of reading data distributed across several services. each of these patterns has several commitments associated with it and for this reason it is important to identify each one, in order to choose the patterns that best suits the application you intend to develop. to carry out this empirical study, a reference application was defined that will serve as a basis. next, several case studies are defined where different applications are developed with the patterns in question. at the end, there is a comparison and analysis of commitments related to various quality attributes."
    ],
    [
      "no mundo atual, com o crescimento da internet e o consequente aumento de informação e serviços que são oferecidos pelas empresas e organizações no seu ambiente torna-se premente desenvolver técnicas que facilitem a navegação dos utilizadores por este enorme espaço virtual. a forma como interagimos com os diversos sítios presentes na internet define um determinado comportamento, os nossos hábitos, os nossos costumes. de facto, no nosso dia-a-dia, e depois de frequentar durante bastante tempo um mesmo estabelecimento, apreciamos o cuidado com que, por vezes, sem nada dizer, o que mais apreciamos é posto à frente e à nossa disposição sem que sejamos consultados. simplesmente conhecem-nos. os sítios na internet cada vez mais tentam ter esse mesmo cuidado com os seus utilizadores. todavia, a comunidade cibernauta é, como sabemos, muito vasta e heterógena e, como tal, saber os hábitos e costumes de tantos indivíduos é uma tarefa complicada. o uso de perfis é uma ação normal na caracterização de utilizadores, seja por questões de segurança ou funcionais. um determinado utilizador pode sempre enquadrar-se num ou noutro perfil, em que cada um deles determina o acesso a este ou àquele tipo de informação ou funcionalidade usualmente oferecida por um sítio presente na internet. este tipo caracterização pode permitir o agrupamento de utilizadores por diversos perfis, facilitando a gestão de informação e serviços, aproximando-os às necessidades reais dos utilizadores. contudo uma das questões relacionadas com este tipo de caracterização de perfis é o facto de ela ser estática ao longo do tempo. os nossos comportamentos e hábitos, como é conhecido, podem não o ser. o conhecimento de “quem nós somos” num sítio pode sofrer alterações ao longo do tempo. as nossas características de consumo e as nossas preferências podem mudar, o que nos define perante ele, a nossa assinatura, pode ter variações. as características de utilização que os diversos sítios presentes internet valorizam mais varia de sítio para sítio. questões de consumo, por exemplo, serão provavelmente mais valorizadas em sítios que ofereçam produtos e serviços, em detrimento de outras questões, dependendo, obviamente dos objetivos de negócio do sítio em questão. com certeza que, a definição de quais as características mais importantes num determinado contexto irá definir os atributos da assinatura dos utilizadores para esse sítio, tendo cada utilizador um valor diferente de acordo com esses atributos. cada utilizador terá a sua assinatura. o valor da assinatura dos utilizadores ao longo do tempo tem que ser determinada por processos de cálculo específicos e ajustados ao contexto em questão e à informação disponível. o uso de técnicas de mineração de dados e de extração de conhecimento é, assim, essencial para este processo. a definição e o cálculo da variação de assinaturas de utilizadores para um dado sítio permitem a realização de várias análises. uma delas é a análise do perfil de um utilizador ao longo do tempo. a variação da assinatura de um utilizador poderá indiciar uma alteração no seu perfil de comportamento perante o sítio que frequenta. o sítio, sabendo dessa alteração, poderá reagir de forma dinâmica e de diversas formas. por exemplo, alterando o conteúdo ou a estrutura do próprio sítio. neste trabalho de dissertação foram exploradas estas temáticas. mais especificamente pretendeu-se aprofundar a definição de assinatura de um utilizador web e a sua associação aos diversos padrões de utilização de um sítio. no âmbito deste trabalho, esses padrões foram extraídos recorrendo a técnicas de mineração de dados a partir de diversas fontes de informação disponibilizadas pelos servidores que alojam os sítios. as técnicas utilizadas na extração desse conhecimento são também abordadas ao longo desta dissertação, com o objetivo de fornecer uma perspetiva global tanto do processo de mineração de dados em si, como da posterior associação do conhecimento extraído às assinaturas definidas para os utilizadores de um sítio específico que escolhemos como alvo para o nosso estudo.",
      "in today's world, with the continuous growth of the internet and the consequent increase of information and services that are offered by companies and organizations, it is urgent to develop techniques that ease users’ navigation throughout this virtual space. the way we interact with the various sites on the internet defines a particular behavior, our habits and our customs. in fact, in our daily life and after attending for a long time the same establishment, we appreciate that sometimes, without saying a word, the things that we like the most are put at our disposal without we being consulted. they just know us. the websites increasingly try to have that same care with their users. however and as we know, the cybernetic community is very large and heterogeneous and as so, knowing the habits and customs of so many individual users is a complicated task. the use of profiles is a normal procedure in user characterization, either for security or functional reasons. a given user can always be fitted in one profile and each profile will give access to various types of information or functionalities, usually given by a website. this characterization may allow the grouping of users by different profiles, easing the information and services management, bringing them closer to the real needs of users. however one of the issues with this type of profile characterization is that it is static over time. our behaviors and routines, as it is known, may not be. the knowledge of “who we are” in a website may change over time. our consumption habits and our preferences may change, that which defines us to that website, our signature, may have variations. the users’ characteristics that websites value the most, change from website to website. consumption issues, for example, will probably be more valued to websites that sell products and services over others, depending, obviously of the business goals of the website in question. it is certain that the definition of which are most important characteristics in a given context, will define the attributes of the signature for that website, having each user a different value according to those attributes. each user will have its signature. the value of the users’ signatures over a period of time must be determined by specific calculation processes and must be adjusted to the context in question and to the information available. the use of data mining techniques and knowledge extraction is thus, crucial to this process. the definition and calculation of users’ signature variation for a given website enables several analyses. one of those is the chance to analyze a users profile over a period of time. signature variation may indicate a change in its behavior profile to the website that he attends. the website, knowing of this change, may react to that change dynamically and in several ways. it can, for example, change its contents or structure. in this dissertation these issues were explored. more specifically it was intended to deepen the definition of a web user’s signature and its association with the usage patterns of a website. as part of this work, these patterns were extracted by data mining techniques from various sources of information, particularly those provided by the web servers that host these websites. the techniques used in the extraction of this knowledge are also addressed in this dissertation with the purpose of giving a global perspective of both the data mining process itself, and of the subsequent association of the extracted knowledge to the user´s signatures that were defined by a specific website that we selected as target for our study. keywords: signatures, web signatures, profiles, web, web profiling, personalization, web personalization, data mining, web mining, web usage mining, data warehouse, data warehousing, patterns, pattern discovery, pattern mining."
    ],
    0.0
  ],
  [
    [
      "desde a sua génese, os sistemas de informação hospitalar (sih) tem proporcionado um conjunto de métodos e ferramentas inovadoras que tem contribuído significativamente para o aumento da produtividade e eficiência dos processos hospitalares e, bem assim, para o incremento da qualidade dos serviços. com efeito, nos dias que correm, a sua utilização na área da saúde é mais do que uma simples funcionalidade, é uma necessidade. em particular, no centro da organização de qualquer unidade hospitalar, o agendamento representa um dos processos que maiores benefícios pode tirar da implementação e evolução tecnológica dos sistemas de informação, sobretudo quando estes possuem como desígnio principal a melhoria das condições dos serviços prestados aos utentes. é neste contexto que surge a presente dissertação, a qual possui como objetivo primordial o estudo e enquadramento da utilização de ontologias de última geração aplicadas no desenvolvimento de sih, com especial enfoque na criação de novos conceitos de agendamento de pedidos, suportados pelo desenvolvimento de uma plataforma assente em mecanismos e ferramentas inovadoras. para além disso, o desenvolvimento e implementação da plataforma em apreço pretendeu também contribuir para a otimização de fluxos de agendamento, através da simplificação de operações convencionais e da introdução de novas funcionalidades.",
      "the implementation of health information systems has been providing an increase in productivity and quality of service, ever since its adoption in different kinds of healthcare environments. nowadays, its use is considered a requirement rather than a feature. being a focal aspect of any healthcare environment’s organization, schedule management is one of the processes which can benefit the most from the implementation and evolution of health information systems, especially when their development takes into consideration the improvement that these solutions provide to patients. the main objectives of this dissertation are the assessment of state of the art ontologies suitable for the development of health information systems, especially those which define the resources needed for the implementation of scheduling workflows and the development of a platform capable of managing resources and scheduling requests, resorting to modern development methods and tools. this platform aims to optimize scheduling actions, by simplifying conventional operations and introducing new functionalities."
    ],
    [
      "cada vez é mais notória a importância que as ontologias têm vindo a ganhar no que toca ao desenvolvimento de sistemas baseados em conhecimento. para além de ainda haver alguma dificuldade em compreender o seu modo de implementação, a sua construção manual é muito dispendiosa tanto a nível de recursos como de tempo e, após a construção, é necessário manter a ontologia atualizada consoante os novos requisitos que poderão surgir. nesta dissertação apresentamos, numa primeira parte, a definição de ontologia, a sua utilidade e algumas das metodologias que podem ser utilizadas na sua construção manual, analisando a sua evolução ao longo do tempo. após esta introdução, apresentamos algumas técnicas de construção (semi-)automática de ontologias a partir de textos e abordamos o conceito de ontology learning, bem como tudo aquilo que este processo envolve. além disso, enunciaremos alguns dos sistemas que fazem uso dessas mesmas técnicas. por fim, apresentamos o trabalho desenvolvido na extração de uma ontologia a partir de um conjunto de textos relativos a testamentos antigos, que foram editados por barros e alves (2019) em o livro dos testamentos – picote, 1780-1803, detalhando o processo de extração realizado para a ontologia pretendida, bem como apresentando as técnicas e ferramentas utlizadas. neste processo, queremos relevar a importância da utilização de padrões léxico-sintáticos e o dependency parsing, que contribuíram de forma efetiva para a obtenção dos resultados que alcançámos.",
      "the importance that ontologies have gained in terms of knowledge-based systems develop ment is increasingly evident. in addition to the difficulties that still exist in understanding how to build ontologies, their manual construction is very costly not only in terms of resources but also in time and, after their construction, it is necessary to keep them updated according to new requirements that may arise. in this dissertation we’ll present, in the first part, the definition of ontology, its usefulness, and some methodologies for its manual cons truction as well as the possibility of its evolution. after this introduction to the concept of ontology some techniques for its (semi-) automatic construction from texts will be presented in which the concept of ontology learning will be introduced as well as everything that this process involves and some systems that make use of these techniques. finally, will be presented the work developed in the extraction of an ontology from old testaments, which were edited by barros and alves (2019), o livro dos testamentos – picote, detailing the process carried out to extract the ontology intended as well as presenting the techniques and tools used in this process. it is important to highlight, in this process, the importance of using lexical-syntactic patterns and the dependency parsing that effectively contributed to the achievement of the obtained results."
    ],
    0.3
  ],
  [
    [
      "topology (the density, organization and flow of a 3d mesh’s connectivity) constrains the suitability of a 3d model for any given purpose, be it surface showcasing through renders, use in real-time engines, posing or animation. while some of these use cases might not have very strict topology requirements, others may demand optimized polygon counts for performance reasons, or even specific geometry distribution in order to take deformation directions into account. many processes for creating 3d models such as sculpting try to make the user unaware of the inner workings of geometry, by providing flexible levels of surface detailing through dynamic geometry allocation. the resulting models have a dense, unorganized topology that is inefficient and unfit for most use cases, with the additional drawback of being hard to work with manually. retopology is the process of providing a new topology to a model such as these, while maintaining the shape of its surface. it’s a technical and time-consuming process that clashes with the rest of the artist’s workflow, which is mainly composed of creative processes. while there’s abundant research in this area focusing on polygon distribution quality based on surface shape, artists are still left with no options but to resort to manual work when it comes to deformation-optimized topology. this document exposes this disconnect, along with a proposed framework that attempts to provide a more complete retopology solution for 3d artists. this framework combines traditional mesh extraction algorithms with adapting manually-made meshes in a pipeline that tries to understand the input on a higher level, in order to solve deficiencies that are present in current retopology tools. our results are very positive, presenting an improvement over state of the art solutions, which could possibly steer discussion and research in this area to be more in line with the needs of 3d artists.",
      "a topologia (a densidade, organização e direções tomadas pela conectividade de uma mesh 3d) limita a adequação de um modelo 3d para um leque variado de usos, entre os quais, visualização da superfície através de renders, uso em motores real-time, poses ou animações. embora muitos destes usos não possuam requerimentos de topologia muito rigorosos, outros podem exigir número de polígonos mais baixos por questões de performance, ou até distribuição de geometria específica para acomodar direções de deformação corretamente. muitos processos de criação de modelos 3d, como escultura, permitem que o utilizador não esteja ciente do que se passa em termos de funcionamento da geometria por debaixo da utilização. isto é conseguido oferecendo níveis de detalhe flexíveis, alocando geometria de forma dinâmica. os modelos resultantes têm uma topologia densa e desorganizada, que é ineficiente e pouco apropriada para a maior parte dos casos de uso, com a desvantagem adicional de ser difícil de trabalhar com a mesma manualmente. a retopologia é o processo de gerar uma nova topologia para um modelo, ao mesmo tempo que se mantém a forma da superfície. é um processo técnico e demorado, que entra em conflito com o resto do fluxo de trabalho do artista, que é composto maioritariamente por processos artísticos. apesar de haver investigação abundante nesta área focada na qualidade da distribuição de polígonos baseada na forma da superfície, os artistas continuam a ter de recorrer ao trabalho manual quando se trata de topologia otimizada para deformações. este documento expõe esta divergência, propondo, em conjunto, uma framework que tenta oferecer uma solução mais completa para os artistas 3d. esta framework combina algoritmos de extração de meshes tradicionais com adaptação de meshes feitas manualmente, numa pipeline que tenta compreender o input a um nível superior, resolvendo as deficiências presentes nas ferramentas de retopologia atuais. os nossos resultados são bastante positivos, apresentando melhorias em relação a soluções de estado da arte, facto que poderá mudar o rumo da discussão e investigação neste campo, para melhor se adequar às necessidades dos artistas 3d."
    ],
    [
      "due to the highly complex nature of ftth networks’ infrastructure, a great deal of planning and documen tation is required to assure quality of service and ease the maintenance and upgrade of such structures. the creation of ftth networks projects is divided in four distinct phases one of which is the creation of synoptics maps. this dissertation focuses on these synoptics maps, particularly automating the creation of such documents. these are, in essence, schematics of different areas of the network and they allow for easier consultation of the network’s layout and details which are not easily observed on trace maps since they represent the entire network branch, often being too complex to interpret. currently, synoptics maps are created manually, through the help of computer aided design (cad) tools but this process is time consuming and prone to human error. it is estimated that each synoptics map takes up to two working hours to complete and each project requires multiple of these, depending on the area’s size, number of costumers in that area, and other variables. furthermore, errors can be costly for the company building the network since it can cause unnecessary materials to be allocated or insufficient quality of service for that area’s demands. as a way to help mitigate these issues, proef proposed the creation of a tool that, when provided with network data in the form of trace maps and other relevant information, creates synoptics maps automatically. the dissertation began with an initial study of the problem and it’s context and, in partnership with proef, the requirements where devised, discussed and agreed upon. then, after selecting the most ad equate development tools, a first prototype was developed. as specified, it was a desktop application that, as specified above, takes a network’s trace map and xslx tables and creates synoptics maps. the development process encountered some delays because it relied on examples provided by the company which had some errors that where difficult to spot and correct. this, along with the realization that trace maps do not contain all the necessary data for creating synoptics maps, caused successive changes to the requirements. as a result, a partial solution was achieved, serving as a foundational point for the development of a more comprehensive solution in the future.",
      "devido à natureza altamente complexa da infraestrutura de redes ftth, é necessário um elevado grau de planeamento e documentação para garantir qualidade de serviço e facilitar a manutenção e atualização dessas estruturas. a criação de projetos de ftth está dividida em quatro fazes distintas sendo uma delas os mapas de sinóticos. esta dissertação foca-se em mapas de sinóticos, particularmente em automatizar a criação dos mesmos. estes são, essencialmente, esquemas lógicos de diferentes áreas de uma rede e visam a facilitar a consulta de informação relativa à estrutura da rede ou detalhe, o que não é fácil de conseguir em mapas de traçado pois estes são uma representação de um ramo da rede, no seu todo, normalmente demasiado complexos para serem interpretados. atualmente, mapas de sinóticos são criados manualmente, com o auxílio de ferramentas de cad porém este processo é demorado e sujeito a erro humano. é estimado que cada mapa de sinóticos demore até duas oras de trabalho a criar e cada projeto nessessita múltiplos sinóticos, dependendo da dimensão da área, do número de clientes a servir entre outras vaiáveis. é também relevante de mencionar que o erro humano pode ter custos como a alocação de material em excesso ou qualidade de serviço abaixo do que é desejado. como forma de mitigar estes problemas, a proef, propôs a criação de uma ferramenta que, quando munida com dados da rede na forma de mapas de traçado e outros dados relevantes, cria mapas de sinóticos automaticamente. a dissertação começou com um estudo inicial do problema e o seu contexto e, em parceria com a proef, os requisitos foram concebidos, discutidos e concordados. após a escolha das ferramentas de desenvolvimento mais adequadas, um protótipo inicial foi desenvolvido. a solução apresentada foi uma aplicação de “desktop” que, tomando mapas de traçado e tabelas xslx cria mapas de sinóticos. o processo de desenvolvimento sofreu alguns atrasos pois este depende de exemplos fornecidos pela empresa que possuiam alguns erros que foram difíceis de detetar e corrigir. isto com o facto de ter sido detetado que os mapas de traçado não continham toda a informação necessária forçou a uma reavaliação dos requisitos e no final, uma solução parcial foi alcançada, solução esta que servirá como um ponto de partida para, no futuro, uma solução mais completa ser alcançada."
    ],
    0.3
  ],
  [
    [
      "para se alcançar um desenvolvimento ágil, as equipas de desenvolvimento devem estar munidas de ferramentas que facilitem a execução e a automatização dos processos ao longo do ciclo de vida de um produto de software. porém não são as tecnologias por si só que tornam as equipas e as organizações ágeis. as metodologias ágeis são essenciais para garantir o sucesso dos projetos de software, pois permite, numa fase embrionária, a participação dos stakeholders no processo, viabilizando a rápida deteção de problemas nos requisitos e no produto que se pretende desenvolver. para potenciar todo o investimento colocado nas tecnologias é necessário alterar o modo como as organizações operam, e adotar práticas e processos que permitam maximizar todos os recursos existentes. este documento propõe uma abordagem que consiste em criar uma harmonia entre o processo de desenvolvimento e operacional, recorrendo a tecnologias de última geração.",
      "to achieve an agile development, teams must be provided with tools to facilitate the implementation and automation of the processes throughout the software development life cycle. however the technologies aren’t itself the unique element that make teams and organizations agile. agile methodologies contribute to ensuring the success of software projects, as they allow, at an early stage, the participation of stakeholders in the process, allowing quick detection of problems in the requirements and at the product to be developed. to enhance all investment placed in the technologies, it is crucial to change the way that teams operate, and adopt practices and processes that allow to maximize all available resources. this thesis elaborates an approach which creates a harmony between the development and operational processes, taking advantage of state-of-the-art technologies and tools."
    ],
    [
      "since the ui is one of the first contacts that a user has with an interactive system being developed, it can include a user flow to be intuitive and effective. for this, prototypes are typically built in the early stages of the software development process so that both developers and users might have an early idea of what the final product will look like, smoothing development and testing in advance. on critical interactive systems, the standard ui prototyping method is not enough. these systems require more advanced and detailed analysis methodologies to avoid failures that can lead to accidents. for this, there are approaches based on models and formal analysis methods, complemented by the prototyping process, which enhance the process of looking for these usability flaws and support the analysis of the interaction made by users. the ivy workbench is a tool initially focused on modelling and verification, which recently accepted the goal of combining formal model-based analysis with prototype interfaces. this objective is still under development. however, it is already possible to produce prototypes with some level of interaction combining models and ui mock-ups. this work aims to develop improvements to this tool for these prototyping capabilities. the main objective is to automatically produce a web application capable of offering the expected behaviour and interaction as described in the model. the results of this work will include an analysis of current prototyping methods, problems with these methods and possible solutions; analysis of different prototyping tools where functionalities and characteristics that may be useful for the ivy workbench will be identified; development of an automatic system for generating web applications based on mock-ups and behaviour models.",
      "sendo a ui um dos primeiros contactos que um utilizador tem com um sistema interativo, esta deve conseguir fornecer ao utilizador um fluxo de tarefas intuitivo e efetivo. para isso, protótipos são normalmente construídos nas fases iniciais do processo de desenvolvimento de um software, para que seja feita a antecipação da fase de testes, e tanto os desenvolvedores como os utilizadores, tenham uma noção antecipada de como será o produto final, facilitando o desenvolvimento. em sistemas críticos, o método comum de prototipagem da ui não é suficiente. estes sistemas requerem metodologias de análise mais avançadas e detalhadas para que falhas que possam originar em acidentes sejam evitadas. para isso, existem abordagens com base em modelos e métodos de análise formal, complementadas com o processo de prototipagem, que suportam o processo de procura destas falhas de usabilidade, e a análise da interação feita pelos utilizadores. uma ferramenta originalmente focada na modelação e verificação, que recentemente aceitou o objetivo de combinar análise formal baseada em modelos com protótipos de interfaces, é a ivy workbench. este objetivo encontra-se ainda em processo de desenvolvimento. no entanto, já é possível produzir protótipos com algum nível de interação combinando modelos e mock-ups da ui. este trabalho visa a melhorar as capacidades de prototipagem desta ferramenta, sendo o objetivo principal a possibilidade de a produção automática de um protótipo web capaz de oferecer o comportamento e interação descritos no modelo. os resultados deste trabalho incluirão uma análise dos métodos atuais de prototipagem, problemas desses métodos e possíveis soluções; análise a diferentes ferramentas de prototipagem onde serão identificadas funcionalidades e caraterísticas que poderão ser úteis para a ivy workbench; desenvolvimento de um sistema automático de geração de protótipos web com base em mock-ups e modelos de comportamento da interface."
    ],
    0.06666666666666667
  ],
  [
    [
      "há cerca de duas décadas atrás, grande parte dos hospitais portugueses começaram a utilizar um sistema de notificação de pacientes, para os alertar dos seus eventos médicos nos hospitais, como consultas, cirurgias, exames, tratamentos, entre outros, através de mensagens de texto. este sistema de notificação é ainda usado nos dias de hoje, mas enfrenta um grande problema: a quantidade de dinheiro avultada que é gasta com as empresas de telecomunicações. ainda que cada mensagem custe uma fração de cêntimos, poderá facilmente ser representada num gasto superior a 50,000 euros anuais para um hospital. uma vez que a tecnologia e o uso de smartphones tem evoluído de forma tão rápida, é estimado que na próxima década, quase toda a população portuguesa possuirá ou poderá ter acesso indireto a um smartphone. por essas razões, o objetivo principal deste projeto de dissertação, é desenhar e desenvolver uma aplicação móvel de forma a substituir o serviço de notificação atual, por uma aplicação móvel que notificará o paciente através de notificações push, cuja informação poderá ser salva no calendário do próprio smartphone, traduzindo-se numa maior comodidade para o paciente, assim como a erradicação de custos no envio de notificações por parte do hospital. a principal motivação é, portanto, suprimir esses custos para o hospital, aproximar os pacientes do hospital, integrar outros sistemas na aplicação e tornar o sistema de notificação mais eficiente. de forma a gerir os utilizadores da aplicação considerou-se necessário o desenvolvimento de uma aplicação web. neste contexto, a presente dissertação apresenta então uma aplicação web, desenvolvida em angularjs, orientada para a gestão de utilizadores da aplicação móvel e apresenta também a aplicação móvel com a funcionalidade principal de notificar o paciente dos seus eventos médicos, oferecendo também funcionalidades como a consulta do seu histórico de eventos médicos passados, consulta de eventos médicos futuros, bem como a verificação dos eventos médicos próximos da data.",
      "around two decades ago, most hospitals in portugal started to use a patient notification system to alert patients of their medical events at the hospitals, such as appointments, surgeries, exams, treatments and so on, through text messages. this notification system is used nowadays, but it faces a big problem: a huge amount of money spent for the telecommunication companies involved. although each message cost a fraction of cents, it can easily reproduce its value in more than 50,000 euros per year per institution. since technology and the use of smartphones has been evolving in such a quick way, it is estimated that in no more than 10 years, almost all the portuguese population will use smartphones or have access to them. for those reasons, the main purpose of the present dissertation project, is to design and develop a mobile application in order to substitute the current notification service, through a mobile application notifying the patient through push up notifications, that can be saved on the smartphone calendar, translating in no costs associated with the notifications sent, for the hospital. the main motivation is, therefore, suppressing these costs for the hospital, bring the patients closer to the hospital, integrating other systems on the app and make the notification alert more efficient. in order to manage the users of the application, consider the development of a web application. in this context, this dissertation presents a web application, developed in angularjs, oriented to the management of users of the mobile application and also presents the mobile application with the main functionality of notifying the patient of their medical events, also offering functionalities, such as consulting your past medical history, consulting future medical events, as well as checking upcoming medical events."
    ],
    [
      "a utilização de inteligência sob forma de tecnologia no nosso dia-a-dia é uma realidade em crescimento e, portanto, devemos fazer uso da tecnologia disponível para melhorar várias áreas do nosso quotidiano. por exemplo, a tecnologia atual permite a conceção de sensores inteligentes, mais especificamente sensores de multidão, para detetar passiva mente dispositivos como smartphones ou smartwatches através de probe requests emitidos por estes dispositivos que, por sua vez, fazem parte de um processo de comunicação que ocorre sempre que o wi-fi dos dispositivos está ativado. adicionalmente, crowd sensing - uma solução de ambient intelligence (ami) - é estudada hoje em dia em várias áreas com bons resultados. portanto, esta dissertação visa investigar e utilizar sensores de multidão para capturar passivamente dados acerca da densidade de multidões, explorar as capacidades do sensor escolhido, analisar e processar os dados para obter melhores estimativas, e conceber e desenvolver modelos de machine learning (ml) para prever a densidade nas áreas sensorizadas. áreas nas quais o sensor de multidão está inserido - ami, smart cities, wi-fi probing - são estudadas, juntamente com a análise de diferentes abordagens ao crowd sensing, assim como paradigmas e algoritmos de ml. em seguida, é explicado como os dados foram capturados e analisados, seguido por uma experiência feita às capacidades do sensor. além disso, é apresentado como os modelos de ml foram concebidos e otimizados. finalmente, os resultados dos vários testes de ml são discutidos e o modelo com melhor desempenho é apresentado. a investigação e os resultados práticos abrem perspetivas importantes para a implementação deste tipo de soluções na nossa vida diária.",
      "bringing intelligence to our everyday environments is a growing reality and therefore we should take advantage of the technology available to improve several areas of our daily life. for example, current technology allows the conception of smart scanners, more specifically crowd sensors, to passively detect devices such as smartphones or smartwatches through probe requests emitted by such devices, that, in turn, are part of a communication process that happens every time the devices’ wi-fi is enabled. additionally, crowd sensing - an ambient intelligence (ami) solution - is being studied nowadays in several areas with good results. therefore, this dissertation aims to research and use crowd sensors to passively collect crowd density data, explore the capabilities of the chosen sensor, analyse and process the data to get better estimations and conceive and develop machine learning (ml) models to forecast the density of the sensed areas. areas in which crowd sensing is inserted - ami, smart cities, wi-fi probing - are studied, along with the analysis of different crowd sensing approaches and ml paradigms and algorithms. then, it’s explained how the data was collected and analysed together with the insights obtained from it, followed by an experiment done on the crowd sensor capabilities. moreover, it’s presented how the ml models were conceived and tuned. finally, the results from the ml several tests are discussed and the best performing model is found. the investigation, together with practical results, opens important perspectives for the implementation of these kinds of solutions in our daily lives."
    ],
    0.12857142857142856
  ],
  [
    [
      "machine learning (ml) and data science can solve different real-world problems. businesses are becoming increasingly interested in these approaches, and as technology evolves, new challenges can be identified, mostly regarding the ml models development, deployment cycle and data cleansing, which can significantly decrease the accuracy and viability of ml software systems. development and operations (devops) practices have become popular in operating software systems at scale successfully, but they need to be adapted to deliver the best results when applied to ml systems. this led to the emergence of machine learning and operations (mlops), a development culture specific for ml systems, derived from devops principles. what mlops attempts to address is the unification of the development cycle of ml based software systems while striving for automation and monitoring, in order to allow continuous integration and delivery. with this thesis, the goal is to study different available frameworks and methods for ml systems, in order to develop an automated ml pipeline to ingest and manipulate high volumes of data. a sensorial system, which simulates the interior of a vehicle, gathers enough data to feed the pipeline. alongside the development of the ml system, a visual interface which allows control over the overall system and its data is created.",
      "a aprendizagem automática (aa) e a ciência de dados são capazes de resolver diferentes problemas do mundo real. empresas estão cada vez mais interessadas nestas soluções, e à medida que a tecnologia evolui, novos desafios são identificados, maioritariamente no que diz respeito ao desenvolvimento de mo delos de aa, ao ciclo de implementação, e ao tratamento de dados, que podem significativamente diminuir a exatidão e viabilidade de sistemas de software com aa. práticas de development operations (devops) tornaram-se populares para operar sistemas de software de grande escala com sucesso, mas precisam de ser adaptadas para se obter os melhores resultados quando aplicadas a sistemas de aa. isto levou ao aparecimento de machine learning operations (mlops), uma cultura de desenvolvimento específica para sistemas de aa, derivada de princípios de devops. o que mlops tenta abordar é a unificação do ciclo de desenvolvimento de sistemas baseados em aa, ambicionando a automação e monitorização, de modo a permitir contínua integração e entrega. com esta tese, o objetivo é estudar diferentes frameworks e métodos para sistemas de aa, de modo a desenvolver um pipeline automatizado de aa para ingerir e manipular grandes quantidades de dados. um sistema de sensores, que simula o interior de um veículo, irá obter dados suficientes para alimentar a pipeline. juntamente com o desenvolvimento do sistema de aa, uma interface visual, que fornecerá controlo sobre o sistema em geral e os seus dados também será criada."
    ],
    [
      "this dissertation addresses the challenge of efficiently developing and testing software in hardware-dependent systems, with a specific focus on simulating hardware used in the vision software project of smartex. smartex is a company that applies ai solutions to the textile industry, particularly to the quality inspection cycle in fabric production. the primary motivation behind this research is to overcome delays in the software development life cycle caused by the software having physical hardware dependencies, whose unavailability can injure the software development life cycle. to achieve this, this dissertation aims to develop software capable of simulating the vision software’s hardware components and provide an application programming interface (api) for seamless interaction with the simulation software. the research objectives encompass several key aspects. firstly, the development of simulation software to faithfully simulate the hardware components of the vision system. secondly, the creation of an automated test pipeline that leverages the simulation software to enhance smartex’s quality assurance processes for the vision software. additionally, the reliability and effectiveness of the hardware simulation solution will be thoroughly tested. lastly, the impact of this work on smartex’s software life cycle, specifically for the embedded systems and quality assurance teams, will be carefully measured and evaluated. this dissertation offers a comprehensive approach to hardware simulation through the use of software as well as automated software testing, effectively addressing a critical challenge in modern software development. the achieved solution significantly improved the overall software life cycle for the embedded systems and quality assurance teams at smartex, enabling the testing and development of the vision software without the need for physical hardware dependencies.",
      "esta dissertação foca-se em abordar o desafio de desenvolver e testar com eficácia software em sistemas que dependem de hardware, com um foco específico na simulação do hardware usado no projeto vision software da smartex. a smartex é uma empresa que aplica soluções de inteligência artificial à indústria têxtil, em particular ao ciclo de inspeção de qualidade na produção de tecidos. a principal motivação por detrás desta investigação é ultrapassar atrasos no ciclo de vida de desen volvimento de software causados pelo facto de o software ter dependências físicas de hardware, cuja indisponibilidade pode prejudicar o ciclo de desenvolvimento de software. para tal, esta dissertação tem como objetivo desenvolver software capaz de simular os componentes de hardware do vision software e fornecer uma interface de programação de aplicações (api) para uma interação perfeita com o software de simulação. os objectivos da investigação abrangem vários aspectos fundamentais. em primeiro lugar, o desen volvimento de software de simulação para simular fielmente os componentes de hardware do sistema de visão. em segundo lugar, a criação de um pipeline de testes automatizados que aproveita o software de simulação para melhorar os processos de garantia de qualidade da smartex para o vision software. além disso, a fiabilidade e a eficácia da solução de simulação de hardware serão testadas exaustivamente. por último, o impacto deste trabalho no ciclo de vida do software da smartex, especificamente para as equipas de sistemas embebidos e quality assurance, será cuidadosamente medido e avaliado. esta tese oferece uma abordagem abrangente à simulação de hardware através da utilização de software, bem como de testes automatizados de software, abordando efetivamente um desafio crítico no desenvolvimento moderno de software. a solução alcançada melhorou significativamente o ciclo de vida global do software para as equipas de sistemas embebidos e quality assurance da smartex, permitindo o teste e o desenvolvimento do vision software sem a necessidade de ter disponíveis as suas dependências físicas de hardware."
    ],
    0.3
  ],
  [
    "o cloud computing tem emergido como sendo um novo paradigma para entrega de serviçoes através da internet. neste mercado em expansão, o serviço de paas (platform-as-a-service) tem sido objeto de grande interesse por parte das mais variadas organizações permitindo o fácil deployment de aplicações sem necessidade de uma infraestrutura dedicada, instalação de dependências ou configuração de servidores. no entanto, cada fornecedor de solução paas acaba por gerar um lock-in do utilizador às suas características proprietárias, tecnologias ou apis (application programming interfaces). além disso, dando como garantida a conectividade até aos clientes, a rede de operadores como seja o caso da portugal telecom (pt) acaba por servir apenas de dumb-pipe entre o fornecedor e os seus clientes. este projeto foca-se na especificação, desenvolvimento e avaliação de uma camada de abstração que visa unificar os processos de gestão e aquisição de informação sobre aplicações e bases de dados criadas atraves de diversos paas, de modo a combater o lock-in existente no mercado. neste sentido, um utilizador de paas pode selecionar a plataforma mais adequada para uma aplicação interagindo de forma idêntica com qualquer fornecedor suportado,tendo também a possibilidade de migrar aplicações entre fornecedores distintos. assim sendo, um operador como a pt tem agora a possibilidade de agir como um mediador entre os utilizadores e os fornecedores de paas.",
    [
      "nos dias de hoje presenciamos a uma mudança de paradigma no que diz respeito aos repositórios institucionais. passamos do repositório isolado no seu contexto institucional para os consórcios onde conjuntos de repositórios partilham ideias, políticas e tecnologias, contribuindo para o crescimento do conhecimento das comunidades em que estão inseridos. podia falar-se da importância que os repositórios institucionais mantêm junto da sua comunidade, mas com a mudança de paradigma torna-se relevante explorar os desafios de gestão que o novo contexto apresenta. para gerir um consórcio é necessário possuir indicadores que auxiliem na tomada de decisão e que atendam às necessidades de informação que as entidades de fomento possuem no que diz respeito ao impacto dos investimentos em cultura, investigação, inovação e desenvolvimento. esta dissertação apresenta o serviço centralizado de estatísticas de utilização de repositórios (sceur). trata-se de um projeto inserido no âmbito da iniciativa repositório científico de acesso aberto de portugal (rcaap) que visa a construção de uma arquitetura que permita recolher, processar e apresentar de uma forma intuitiva dados estatísticos de utilização em repositórios institucionais e também auxiliar a partilha de dados estatísticos quer pela disponibilização de add-ons que facilitem essa tarefa no software usado pelas instituições quer pela disponibilização de informação e recomendações variadas nesse contexto. são também apresentados projetos internacionais de referência neste contexto, normas existentes e tecnologias usadas para a implementação dos conceitos subjacentes.",
      "nowadays we witness a paradigm shift regarding to institutional repositories. repositories are no longer isolated in their institutional context. instead they participate in consortia where ideas, policies and technologies can be shared, contributing to the growth of knowledge of the communities where they are included. much could be said about the importance that institutional repositories do maintain in their community, but with this new paradigm it becomes relevant to explore the management challenges that the new context presents. to manage a consortium is necessary to have indicators that help in decision making and meet the needs of information that the promoting entities have with regard to the impact of investments in culture, research, innovation and development. this dissertation presents sceur. it’s a project (developed under the rcaap initiative) which aims to build an architecture that allows to collect, process and present in an intuitive manner statistics from institutional repositories usage data and also assist in the sharing of statistical data by developing add-ons that facilitate that task in software used by the institutions and by publishing information and various recommendations in this context. international projects are also presented, as well as existing standards and technologies used to implement the underlying concepts."
    ],
    0.0
  ],
  [
    [
      "this document presents a master’s thesis with researches focused on the teaching of computational thinking and present the development details of robi, a block-based visual programming language that is able to program a robot built with an arduino uno. these researches had the purpose of evaluating if the development of robi, a block-based program ming language that communicates with arduino, would really be needed. the researches have proved that from the popular programming environments that exist in the market, that were investigated, none have the requirements that robi requires. the platform will be used to teach computational think through a block-based programming environment and educational robotics. robi development is motivated by the intersection between the costs of educational robotics kits and the existing block-based programming language, in which simplicity and intuitiveness could be improved, so children with learning difficulties or even younger children, in the context of educational robotics, can leverage the learning benefits that the robi environment can bring. the educational robotics kit used with the block-based programming environment developed, is the one based on arduino uno, a microcontroller board that, together with electronic components, can be considered cheaper than some of the famous educational robotics kits. the main goal of this project is to provide a simpler and more intuitive visual programming language platform to program a robot based on arduino uno.",
      "este documento apresenta uma tese de mestrado com investigações voltadas ao ensino do pensamento computacional e apresenta os detalhes do desenvolvimento de robi, uma linguagem de programação visual baseada em blocos, que é possível programar um robô construído com um arduino uno. essas investigações tiveram o objetivo de avaliar se o desenvolvimento de robi, uma linguagem de programação baseada em blocos que se comunica com o arduino, seria realmente necessário. as investigações comprovaram que dos ambientes de programação populares existentes no mercado, que foram investigados, nenhum possui os requisitos que robi exige. a plataforma será utilizada para ensinar pensamento computacional por meio de um ambiente de programação baseado em blocos e robótica educacional. o desenvolvimento de robi é motivado pela combinação entre os custos dos kits de robótica educacional existentes no mercado e linguagens de programação baseada em blocos existentes, em que simplicidade e intuitividade poderiam ser aprimoradas, para assim, crianças com dificuldades de aprendizagem ou até crianças mais novas, no contexto da robótica educacional, poderiam fazer proveito dos benefícios da aprendizagem que o ambiente robi pode trazer. o kit de robótica educacional utilizado com o ambiente de programação baseado em blocos desenvolvido é um kit com o arduino uno, uma placa de microcontrolador que, junto com componentes eletrônicos, pode ser considerada mais barata que alguns dos famosos kits de robótica educacional. o objetivo principal deste projeto é fornecer uma plataforma de linguagem de programação visual mais simples e intuitiva para programar um robô baseado em arduino uno."
    ],
    [
      "in the past few years, due to the exponential growth of technology of the world, there has been an increasing interest in the field of data analytics. this interest comes from how important data is to everyday life, since data can significantly benefit and influence a lot of different areas, from security to health or even sports. large amounts of data allows analysts to predict possible outcomes, evaluate behavioral patterns and study trends. the main goal of this master’s thesis is to show that a data analytics tool can also be very useful in a company setting, since there is a considerable amount of data, generated by the software tools of the company, that can be collected and used to improve the services provided by the company. the master’s work here reported begun with a comprehensive study on different kind of analytics tools and also an investigation of the company’s product. after this investigation process, the next step was to design and implement a custom analytics tool that uses prometheus as a basis and takes into account the requirements of the company. the tool required the implementation of various processes starting with the collection of the generated data, the transformation and cleaning of that data and finally a process that provides different types of data visualizations. these visualizations present relevant knowledge to customers and also to different departments inside the company. while the implemented analytics tool met all of the proposed requirements and passed the tests made to verify it, there will be a need to further refine and improve the tool’s capabilities, specifically regarding scalability and data storage. this document provides a better understanding of the use of data analytics in a company setting by showcasing the full process of designing and developing a custom analytics tool using prometheus. through this document, it is possible to see that using product generated data and providing data visualizations is beneficial for the company.",
      "nos últimos anos, devido ao crescimento exponencial da tecnologia no mundo, tem havido um aumento no interesse pelo campo da análise de dados. esse interesse vem da im portância dos dados na vida quotidiana, já que os dados podem beneficiar e influenciar significiativamente várias áreas, desde a segurança até à saúde ou até o desporto. grandes quantidades de dados permitem aos analistas prever possíveis resultados, avaliar padrões comportamentais e estudar tendências. o principal objetivo desta dissertação de mestrado é demonstrar que uma ferramenta de análise de dados também pode ser muito útil num ambiente empresarial, uma vez que existe uma quantidade considerável de dados, gerados pelas ferramentas de software da empresa, que podem ser recolhidos e utilizados para melhorar os serviços prestados pela empresa. o trabalho de mestrado aqui relatado começou com um estudo abrangente de diferentes tipos de ferramentas de análise e também uma investigação do produto da empresa. após este processo de investigação, o passo seguinte foi realizar o design e implementação de uma ferramenta de análise de dados personalizada que utilizasse o prometheus como base e tivesse em consideração os requistos da empresa. a ferramenta exigiu a implementação de vários processos, começando pela recolha dos dados gerados pelo software da empresa, a transformação e limpeza destes e, por fim, um processo que providencia diferentes tipos de visualizações de dados. essas visualizações apresentam conhecimento relevante para os clientes e também para diferentes departamentos dentro da empresa. embora a ferramenta de análise de dados implementada tenha respondido a todos os requi sitos propostos e tenha passado nos testes realizados para a sua verificação, existe ainda uma necessidade de melhorar ainda mais as capacidades da ferramenta, especialmente em relação à escalabilidade e ao armazenamento de dados. este documento proporciona uma melhor compreensão do uso da análise de dados num ambiente empresarial, mostrando todo o processo de design e desenvolvimento de uma ferramenta de análise de dados personalizada utilizando o prometheus. ao longo deste documento, é possível observar que o uso dos dados gerados pelo software da empresa e a disponibilização de visualizações de dados são benéficos para a empresa."
    ],
    0.3
  ],
  [
    [
      "social networks have allowed, over the past few years, the appearance of new ways to share opinions and ideas in texts, providing a basis for studying opinions on a large scale. the tools for the retrieval and analysis of sentiments, contained in this information, are still under development, limited by access restrictions and technical difficulties in the development of new methods, which involve natural language processing and text mining. this work aims to develop tools to recover and analyze sentiments present in social networking texts. a case study using twitter will be used for validation. during this process, data obtained from this social network was stored in a document oriented database, elasticsearch, organized by topics helping its use in the following phases. then, the data went through a set of pre-processing steps, to maximize the value of their content, seeking to improve the chances of obtaining a correct classification. finally, the processed textual data were submitted to the algorithm chosen for classification, naive bayes. the results obtained over two different datasets show that the pre-treatment of data is very important regarding to the classification of sentiments in texts. overall, a computational architecture has been developed that can foster sentiment analysis applications over social network data from twitter",
      "as redes sociais têm permitido, ao longo dos últimos anos, o aparecimento de novas formas de partilhar opiniões e ideias em textos, fornecendo uma base para o estudo de opiniões em larga escala. as ferramentas para a recuperação e análise de sentimentos sobre este tipo de informação estão ainda em fase de desenvolvimento, sendo limitadas pelas restrições de acesso e dificuldade técnica no desenvolvimento de métodos de análise de sentimentos, que envolvem processamento de língua natural e mineração de textos. este trabalho desen-volveu ferramentas para recuperação e análise de sentimentos de textos de redes sociais. o trabalho teve como base de estudo a rede social twitter, e utilizou a sua informação para a validação dos resultados. durante o processo, os dados obtidos desta rede social foram armazenados numa base de dados orientado a documentos, elasticsearch, organiza-dos por tópicos, facilitando o seu uso nas fases seguintes. depois, os dados passaram por vários passos de pré-processamento, de forma a maximizar o valor do seu conteúdo, procurando melhorar as probabilidades de se obter uma anáçise de sentimentos mais pre-cisa. por fim, os dados textuais processados foram submetidos ao algoritmo escolhido para a classificação, o naive bayes. os resultados obtidos em dois conjuntos de dados distin-tos mostram que o pré-tratamento dos dados é muito importante no que diz respeito à classificação de sentimentos, quando comparamos os vários resultados obtidos. globalmente, uma arquitectura computacional foi desenvolvida que pode potenciar aplicações de análise de sentimentos sobre dados da rede social twitter."
    ],
    [
      "streaming content has completely changed the way we consume it. the arising of the internet, and streaming services, has allowed people to access a big collection of music without buying it, just by subscribing to a service or to use it for free with the consumption of publicity. but, and for those who have a local collection? they can’t use these platforms unless the service allows the user to buy the content, store it in a local environment, play it there, and the user could play it remote by streaming. the use of clouds appears to be a good solution for this group of people. unfortenelly, when the collection is really big, the additional costs of storage of the cloud could be a big problem. in addition, normally, the user interface of these services isn’t enjoyable, i.e., isn’t user-friendly. so, a hybrid system could be the ideal, a streaming service with cloud services that allow storing the private collection. this is the best for the user but will force them to store all the collection or at minimum a part of the collection which they want to access, in an external source, with a strict organizational structure. for a new user, this couldn’t be a problem but is for a user who already has a big collection. this dissertation has proposed a remote access service to these private collections so that with just a simple gadget (smartphone, pc, ...) the user can access the content stored on their private server anywhere on the planet, without change the original location of the collection. this service was built using only normalized and open-source technologies. based on the proposed architecture, was also developed a prototype with the main function of the system, like the possibility to play music from the private collection on a smartphone android. the conclusion of the tests made to the prototype was that this alternative solution could be very good for the people who want that their collection remains in one place, and, at the same time, can play it remotely.",
      "a transferência via streaming mudou completamente a forma como consumimos conteúdo audiovisual. o aparecimento de plataformas de streaming de áudio revolucionou o mundo da música, permitindo que um utilizador possa ter acesso a uma panóplia de conteúdo sem ter de adquirir os direitos dos mesmos, apenas subscrevendo o serviço, pagando uma mensalidade ou gratuitamente em troca do consumo obrigatório de publicidade. no entanto, este tipo de serviço não é útil a quem possui uma coleção musical privada armazenada localmente. neste caso não se pode recorrer a uma destas plataformas de streaming a não ser nos casos pouco comuns em que a música foi adquirida num serviço de streaming que permita a compra desse conteúdo para armazenamento e reprodução local e, em simultâneo, a sua reprodução remota através desse mesmo serviço de streaming. uma alternativa possível para reprodução remota duma coleção privada é o uso de clouds genéricas para o seu armazenamento, mas esta solução não é prática para coleções musicais de maior tamanho, pode acarretar custos adicionais no uso do serviço de cloud, a interface dos reprodutores musicais dos serviços de cloud são relativamente pobres e as exigências de qualidade de serviço podem ser maiores do que as que seriam necessárias apenas para streaming de música. existem também soluções híbridas, ou seja, serviços de streaming de música que são também serviços de cloud para coleções musicais privadas. esta tipo de solução mais recente é a que oferece melhores funcionalidades, mas obriga a que os utilizadores transfiram todos os ficheiros e mantenham a gestão da sua coleção completa duma forma remota. esta característica pode não ser relevante para novos utilizadores que estão a começar a sua coleção musical, ou a sua coleção é relativamente pequena, mas pode ser uma desvantagem importante para utilizadores que já possuam uma coleção grande, gerida duma forma local com ficheiros de diversos formatos. nesta dissertação foi proposto um serviço de streaming de acesso remoto a coleções privadas, para que qualquer utilizador com as devidas permissões de acesso, possa reproduzir o conteúdo da sua coleção em qualquer lugar e em qualquer dispositivo fixo ou móvel (desde que esteja assegurada uma ligação à rede internet com uma qualidade adequada para streaming de áudio), sem que seja necessário transportar ou transferir a coleção do seu armazenamento local. este serviço foi definido utilizando apenas tecnologias normalizadas e open-source. baseado na arquitetura do sistema proposto, foi desenvolvido um protótipo implementando as principais funcionalidades do sistema, incluindo a reprodução de música duma coleção privada num smartphone com o sistema operativo android. os resultados dos testes feitos com o protótipo permitiu concluir que a solução proposta pode constituir uma alternativa viável aos serviços já existentes, sobretudo quando o utilizador prefere não transferir (e manter atualizada) a sua coleção privada para um serviço na rede."
    ],
    0.3
  ],
  [
    [
      "most event data analysis tasks in the atlas project require both intensive data access and processing, where some tasks are typically i/o bound while others are compute bound. this dissertation work mainly focus improving the code efficiency of the compute bound stages of the atlas detector data analysis, complementing a parallel dissertation work that addresses the i/o bound issues. the main goal of the work was to design, implement, validate and evaluate an improved and more robust data analysis task, originally developed by the lip research group at the university of minho. this involved tuning the performance of both top quark and higgs boson reconstruction of events, within the atlas framework, to run on homogeneous systems with multiple cpus and on heterogeneous computing platforms. the latter are based on multicore cpu devices coupled to pci-e boards with many-core devices, such as the intel xeon phi or the nvidia fermi gpu devices. once the critical areas of the event analysis were identified and restructured, two parallelization approaches for homogeneous systems and two for heterogeneous systems were developed and evaluated to identify their limitations and the restrictions imposed by the lipminianalysis library, an integral part of every application developed at lip. to efficiently use multiple cpu resources, an application scheduler was also developed to extract parallelism from simultaneously execution of both sequential and parallel applications when processing large sets of input data files. a key achieved outcome of this work is a set of guidelines for lip researchers to efficiently use the available computing resources in current and future complex parallel environments, taking advantage of the acquired expertise during this dissertation work. further improvements on lip libraries can be achieved by developing a tool to automatically extract parallelism of lip applications, complemented by the application scheduler and additional suggested approaches.",
      "a maior parte das tarefas de análise de dados de eventos no projeto atlas requerem grandes capacidades de acesso a dados e processamento, em que a performance de algumas das tarefas são limitadas pela capacidade de i/o e outras pela capacidade de computação. esta dissertação irá focar-se principalmente em melhorar a eficiência do código nos problemas limitados computacionalmente nas últimas fases de análise de dados do detector do atlas, complementando uma dissertação paralela que irá lidar com as tarefas limitadas pelo i/o. o principal objectivo deste trabalho será desenhar, implementar, validar e avaliar uma tarefa de análise mais robusta e melhorada, desenvolvida pelo grupo de investigação do lip na universidade do minho. isto envolve aperfeiçoar a performance das reconstruções do top quark e bosão de higgs de eventos dentro da framework do atlas, a ser executada em plataformas homogéneas com vários cpus e em plataformas de computação heterogénea. a última é baseada em cpus multicore acoplados a placas pci-e com dispositivos many-core, tais como o intel xeon phi ou os dispositivos gpu nvidia fermi. depois de identificar e restructurar as regiões críticas da análise de eventos, duas abordagens de paralelização para plataformas homogéneas e duas para plataformas heterogéneas foram desenvolvidas e avaliadas, com o objectivo de identificar as suas limitações e as restrições impostas pela biblioteca lipminianalysis, uma parte integrante de todas as aplicações desenvolvidas no lip. um escalonador de aplicações foi desenvolvido para usar eficientemente os recursos de múltiplos cpus, através da extracção de paralelismo de execução em simultâneo de tanto aplicações sequenciais como paralelas para processamento de grandes conjuntos de ficheiros de dados. um resultado obtido neste trabalho foi um conjunto de directivas para os investigadores do lip para o uso eficiente de recursos em ambientes paralelos complexos. é possível melhorar as bibliotecas do lip através do desenvolvimento de uma ferramenta para extrair automaticamente paralelismo das aplicações do lip, complementado pelo escalonador de aplicações e outras alternativas sugeridas."
    ],
    "the growth of concepts such as intelligent environments and internet of things allows us to understand the habits of users and consequently act to improve people’s daily lives. through information gathering, it is thus possible to gather patterns about different kinds of human behavior and consequently build a learning model with predictive capabilities. in addition, there are increasing concerns from large companies about the influence, positive or negative, that aspects such as comfort and well-being have on the behavior and health of the population. in fact, as human beings, we are greatly influenced by the environment in which we are inserted. there are therefore conditions in a place that give us certain levels of comfort that will eventually interfere with our well-being. however, it is difficult to identify which of these factors are relevant and how they intervene in our daily lives. also, the habits we adopt as a result of the routines we follow can contribute to improving or worsening any of these indicators with the help of the various types of sensors present, for example, in the smart devices (smartphones, smartwatches, wristbands), it is increasingly possible to collect information on these factors, easily and comprehensively. in this sense, firstly the main objective of this dissertation is thus to collect data on factors that may influence the user in order to create a user profile. these factors can be inferred through its interests, the visited locations, and its main activities. this objective involves a large-scale analysis, where there are no geographical restrictions. furthermore, the study will be independent of the type of space (open or closed) that is explored. in that way, the perspective that will be used is from the user. then there is an exploration of the data so that some intelligence can be inferred, and in this sense, build a mobile application capable of providing smart notifications based on user needs.",
    0.0
  ],
  [
    [
      "the introduction of machine learning (ml) on the orbit of the resolution of problems typically associated within the human behaviour has brought great expectations to the future. in fact, the possible development of machines capable of learning, in a similar way as of the humans, could bring grand perspectives to diverse areas like healthcare, the banking sector, retail, and any other area in which we could avoid the constant attention of a person dedicated to the solving of a problem; furthermore, there are those problems that are still not at the hands of humans to solve - these are now at the disposal of intelligent machines, bringing new possibilities to the humankind development. ml algorithms, specifically deep learning (dl) methods, lack a bigger acceptance by part of the community, even though they are present in various systems in our daily basis. this lack of confidence, mandatory to let systems make big, important decisions with great impact in the everyday life is due to the difficulty on understanding the learning mechanisms and previsions that result by the same - some algorithms represent themselves as ”black boxes”, translating an input into an output, while not being totally transparent to the outside. another complication rises, when it is taken into account that the same algorithms are trained to a specific task and in accordance to the training cases found on their development, being more susceptible to error in a real environment - one can argue that they do not constitute a true artificial intelligence (ai). following this line of thought, this dissertation aims at studying a new theory, hierarchical temporal memory (htm), that can be placed in the area of machine intelligence (mi), an area that studies the capacity of how the software systems can learn, in an identical way to the learning of a human being. the htm is still a fresh theory, that lays on the present perception of the functioning of the human neocortex and assumes itself as under constant development; at the moment, the theory dictates that the neocortex zones are organized in an hierarchical structure, being a memory system, capable of recognizing spatial and temporal patterns. in the course of this project, an analysis was made to the functioning of the theory and its applicability to the various tasks typically solved with ml algorithms, like image classification, sound recognition and time series forecasting. at the end of this dissertation, after the evaluation of the different results obtained in various approaches, it was possible to conclude that even though these results were positive, the theory still needs to mature, not only in its theoretical basis but also in the development of libraries and frameworks of software, to capture the attention of the ai community.",
      "a introdução de ml na órbita da resolução de problemas tipicamente dedicados ao foro humano trouxe grandes expectativas para o futuro. de facto, o possível desenvolvimento de máquinas capazes de aprender, de forma semelhante aos humanos, poderia trazer grandes perspetivas para diversas áreas como a saúde, o setor bancário, retalho, e qualquer outra área em que se poderia evitar o constante alerta de uma pessoa dedicada a um problema; para além disso, problemas sem resolução humana passavam a estar a mercê destas máquinas, levando a novas possibilidades no desenvolvimento da humanidade. apesar de se encontrar em vários sistemas no nosso dia-a-dia, estes algoritmos de ml, especificamente de dl, carecem ainda de maior aceitação por parte da comunidade, devido a dificuldade de perceber as aprendizagens e previsões resultantes, feitas pelos mesmos - alguns algoritmos apresentam-se como ”caixas negras”, traduzindo um input num output, não sendo totalmente transparente para o exterior - é necessária confiança nos sistemas que possam tomar decisões importantes e com grandes impactos no quotidiano; por outro lado, os mesmos algoritmos encontram-se treinados para uma tarefa específica e de acordo com os casos encontrados no desenvolvimento do seu treino, sendo mais suscetíveis a erros em ambientes reais, podendo se discutir que não constituem, por isso, uma verdadeira inteligência artificial. seguindo este segmento, a presente dissertação procura estudar uma nova teoria, htm, inserida na área de mi, que pretende dar a capacidade aos sistemas de software de aprenderem de uma forma idêntica a do ser humano. esta recente teoria, assenta na atual perceção do funcionamento do neocórtex, estando por isso em constante desenvolvimento; no momento, e assumida como uma teoria que dita a hierarquização estrutural das zonas do neocórtex, sendo um sistema de memória, reconhecedor de padrões espaciais e temporais. ao longo deste projeto, foi feita uma análise ao funcionamento da teoria, e a sua aplicabilidade a várias tarefas tipicamente resolvidas com algoritmos de ml, como classificação de imagem, reconhecimento de som e previsão de series temporais. no final desta dissertação, após uma avaliação dos diferentes resultados obtidos em várias abordagens, foi possível concluir que apesar dos resultadospositivos, a teoria precisa ainda de maturar, não só a nível teórico como a nível prático, no desenvolvimento de bibliotecas e frameworks de software, de forma a capturar a atenção da comunidade de inteligência artificial."
    ],
    [
      "com a evolução natural da humanidade há uma preocupação crescente com o aumento das infraestruturas, por forma a acompanhar o aumento populacional. essas infraestruturas assumem dimensões de grande escala, o que torna a tarefa de navegação e posicionamento no seu interior complexa. a presente investigação, enquadrada no projeto where@um, procura desenvolver uma aplicação móvel que auxilie a navegação dos pedestres, no interior dos campi da universidade do minho. esta dissertação procura responder aos domínios da segurança e posicionamento indoor. para tal foi desenvolvido um sistema de segurança robusto, garantindo questões de privacidade e de controlo de acesso a alguns serviços. foi concebida uma aplicação móvel que faz uso deste sistema de segurança, a qual disponibiliza estratégias para que, de forma colaborativa, os utilizadores desta possam ter um contributo ativo na criação e manutenção do mapa de rádio do sistema. com objetivo de testagem da aplicação desenvolveu-se uma campanha de recolha de dados com 8 participantes, por forma a testar a credibilidade e funcionalidade do sistema concebido. foram encontrados dados que suportam a qualidade do sistema desenvolvido, com base na análise quantitativa dos dados recolhidos bem como por meio da opinião de satisfação dos participantes.",
      "with the natural evolution of humanity, there is a growing concern with the increase in the size of infrastructures in order to follow the population grow. these infrastructures assume large-scale dimensions, which increases the complexity of navigation and positioning in its interior. the present investigation, framed in the where@um project, has the goal of developing a mobile application that helps pedestrians to navigate within the campuses of the university of minho. this dissertation tries to find a response to the domains of security and indoor positioning. to this end, a robust security system was developed, guaranteeing privacy issues and access control to some services. a mobile application was designed that makes use of this security system, which provides strategies so that, in a collaborative way, its users can have an active contribution in the creation and maintenance of the radio map of the system. with the application testing in mind, a data collection campaign was developed with 8 participants, with the purpose of testing the credibility and functionality of the designed system. the quality of the developed system was corroborated by the gathered data as well as through the participants satisfaction survey."
    ],
    0.06666666666666667
  ],
  [
    [
      "os avanços tecnológicos verificados nos dia de hoje assim como a quantidade de informação e comunicação que lhes estão associados, atribuem um papel de grande importância aos sistemas de monitorização. é no seio desta evolução, que a competição existente entre os vários setores de mercado não estão acessíveis a erros e falhas, principalmente ao nível dos equipamentos tecnológicos. assim, neste contexto de intolerância, assiste-se a uma proliferação e a uma utilização cada vez maior de sistemas de monitorização e prevenção. mais importante que monitorizar é prevenir. ter a capacidade de evitar uma falha, e permitir a resolução de algum problema atempadamente é uma mais valia para o desempenho, atribuindo fiabilidade e qualidade ao serviço. a gestão e a verificação de equipamentos, bem como de processos associados aos mesmos permitem um maior controlo e domínio dum sistema. assim esta dissertação tem como objetivo principal a implementação dum sistema para monitorizar a atividade de um ou mais sistemas multi-agente, com capacidade para intervir e avisar o administrador do sistema quando ocorre um problema. o sistema construído, que assenta numa estrutura formada por três unidades distintas, unidade de análise, unidade de processamento e unidade de interface com o utilizador, permitem a implementação de processos para troca e integração de informação, exercendo assim uma comunicação fundamental entre sistemas e utilizadores . é um sistema direcionado principalmente à gestão e monitorização do desempenho de diferentes equipamentos assim como dos processos em execução nos mesmos. este trabalho foi desenvolvido em colaboração com um hospital no norte do país e o ambiente escolhido para a implementação da plataforma foi unicamente laboratorial. a concretização deste projeto esteve dividida em quatro fases: início, pesquisas, construção da aplicação e escrita da dissertação. na primeira fase, o início, foi feito o levantamento dos principais requisitos para o sistema, definição dos objetivos, e elaboração de um plano de trabalho. na segunda fase, pesquisas, procedeu-se ao levantamento de informação sobre os conceitos teóricos relacionados com o tema em causa, como artigos científicos que dão suporte ao tema, entre outros trabalhos já publicados na mesma área. no final desta etapa já se encontravam definidas as ideias base da aplicação a construir de acordo com as necessidades. na terceira fase deste projeto, a construção englobou a modelação e implementação da plataforma de monitorização, de acordo com as especificações definidas anteriormente. a quarta etapa englobou a escrita da dissertação, o que incluiu um enquadramento dos conceitos teóricos em função da aplicação desenvolvida.",
      "the technological advancements that are present today as well as the quantity of information and communication with them associated, attributes a major role to monitorization systems. it is in the heart of this evolution that competition exists between the various market sectors and so not giving any leeway to weaknesses or mistakes, above all by the technological equipments. and so, in this context of intolerence, a proliferation and utilization of system monitorization and prevention is all the more important. prevention is more important than monitorization. having the capacity to prevent a failure and timely access it with a resolution is of major worth to the quality of the service. the management and verification of these equipments, as well as the processes with them associated allows for a greater regulation and dominion of a system. having said this, this dissertation has for its main objective the implementation of a system to monitorize the activity of one or more multi-agent systems with the capacity to intervene and warn the system administrator of when a problem occurs. the system that was developed was so in a structure with three distinct stages, the stages are: data analysis, data processing, and user interface. together, they allow for an implementation of processes of exchange and integration of information, playing a major role in the communication between users and the system platform. it is a system primarily aimed at the management and monitorization of the performance of equipments as well as of the execution processes. this project was developed in collaboration with an hospital located on north of portugal and the chosen conditions for the implementation of this platform were solely achieved in the laboratory. the completion of this project was divided into four phases: beginning, investigation, application development, and the composition of the dissertation. in the first phase, beginning, the fundamental system requisites were collected, objectives were defined, and a work plan were elaborated. then, in the second phase, investigation, a collection of information regarding theoretical concepts and a vast and varied ammount of information related to the subject was also collected. in addition, information regarding cientific articles in relation to the subject were also collected, as well as information on other projects that were done in relation to the subject and also support it. in the final stages of this phase an application prototype had already been defined also congruent with the necessities that had been discovered within the previous phases. in the third phase of this project, the development is concerning the elaboration and implementation of the monitorization platform itself, corresponding to the specifications defined previously. the fourth and final phase involved the composition of the dissertation itself, which includes the framework of theoretical concepts directly related to the explanation of the developed application."
    ],
    [
      "automatic code generation is an increasingly recurring theme these days, constantly manifesting itself in new tools that allow you to generate code from top-level languages that try to make the programmer’s job faster and easier. uml is a long-standing modeling language that is primarily used to model applications during the specification phase and is sometimes also used for automatic code generation. in this dissertation we introduce umlayer, a middleware component that allows integration with certain types of applications through the provision of services. this layer accepts behavior specifications through uml sequence diagrams, allowing applications to access services that are specified from these diagrams. the goal is thus to allow an application, still under development, to have immediate access to services that correspond to its use cases, having the user to provide only the sequence diagrams that specify them. this, in the user’s view, allows his application’s service layer to be completely replaced by sequence diagrams and the application becomes immediately ready to use. the code generated from these diagrams will be inaccessible and unalterable on the part of the user, since it is only and exclusively through the diagrams that the user will specify all the logic of his use cases. all this mechanism of generation and subsequent access to the generated code becomes as transparent as possible to the user, having to only worry about the correct elaboration of his diagrams.",
      "a geração automática de código é um tema cada vez mais recorrente nos dias de hoje, manifestando-se constantemente em novas ferramentas que permitem gerar código a partir de linguagens de alto nível que tentam tornar mais rápido e fácil o trabalho do programador. o uml é uma linguagem de modelação que existe há bastante tempo, que é primeiramente utilizada para modelar aplicações durante a fase de especificação, sendo por vezes também utilizada para a geração automática de código. nesta dissertação introduzimos o umlayer, um componente de nziddleware que permite a integração com certo tipo de aplicações através da disponibilização de serviços. esta camada aceita especificações de comportamento através de diagramas de sequência uml, permitindo que as aplicações acedam a serviços que são especificados a partir destes diagramas. o objetivo passa assim por permitir com que uma aplicação, ainda em fase de desen-volvimento, tenha acesso imediato a serviços que correspondem aos seus use cases, tendo o utilizador de fornecer apenas os diagramas de sequência que os especificam. isto faz, na visão do utilizador, com que a camada de serviço da sua aplicação possa ser totalmente substituída por diagramas de sequência e a sua aplicação fique imediatamente pronta a ser utilizada. o código gerado a partir destes diagramas será inacessível e inalterável por parte do utilizador, uma vez que é apenas e exclusivamente através dos diagramas que o utilizador irá especificar toda a lógica dos seus use cases. todo este mecanismo de geração e posterior acesso ao código gerado torna-se assim o mais transparente possível ao utilizador, tendo este de se preocupar apenas com a correta elaboração dos seus diagramas."
    ],
    0.0
  ],
  [
    [
      "o synchronized multimedia integration language (smil) é um padrão definido pelo world wide web consortium (w3c), baseado na extended markup language (xml), usada no controlo de apresentações multimédia. esta linguagem é usada principalmente nos serviços de mensagens multimédia mas, actualmente, também é usada nos high definition dvd para interactividade e para vídeos na internet. este documento descreve de que forma o ambulant player, um animador de smil open-source, foi modificado para controlar apresentação multimédia fornecidas por um media server para dispositivos móveis. a implementação modular do ambulant baseada em code factories permitiu que o seu módulo de visualização fosse substituído por um mais simples. este novo módulo envia mensagens para um media server em vez de reproduzir os elementos de média. usando o ambulant player como base para este interpretador tornou o desenvolvimento mais rápido e permitiu obter uma ferramenta que respeite a recomendação do w3c de uma forma simples. como resultado final obteve-se um interpretador que demonstrou ser robusto, suportando cento e oitenta sessões concorrentes e servindo cerca de sessenta mil sessões sem erros.",
      "the synchronized multimedia integration language (smil) is a world wide web consortium (w3c) standard language based on the extended markupe language (xml), used to control multimedia presentations. this language was mainly used in multimedia messaging services but nowadays is also being used in high definition dvds for advanced interactivity and in internet video. this document describes how ambulant player, a pure opensource smil player, was modified to control the media presentation provided by a media streaming server for mobile devices. ambulant’s factory system allowed to replace the basic renderer by a simpler one. this new renderer sends messages to the media server instead of playing the media. using ambulant as a base for this interpreter made the development faster and made it easier to obtain a w3c compliant tool. the interpreter obtained as final result of this project showed to be robust, supporting one hundred and eighty concurrent sessions and serving about sixty thousand sessions without errors."
    ],
    [
      "desde tempos antigos que o ser humano tenta combater sentimentos negativos, como a tristeza e a solidão. uma outra emoção que sempre perturbou a humanidade é o aborrecimento. desta causa nascem vários tipos de arte e também diversos desportos, sendo que estes continuam a ser observados e/ou praticados até hoje. nos dias de hoje, dado o facto que os smartphones se tornaram dispositivos utilizados a nível global, faz com que as pessoas sejam submetidas a cada vez mais estímulos. assim sendo, quando as mesmas não estão ocupadas, sentem a necessidade de fazer algo que mantenha o seu cérebro activo. por esta razão, detectar aborrecimento quando se usa um smartphone, abre caminho para melhorar os índices de sucesso dos sinais de estímulo, com um sistema menos intrusivo e mais inteligente. numa fase inicial, este trabalho de investigação focou-se na recolha de dados. desenvolveu-se uma aplicação inicial, para cumprir este objectivo. o principal propósito desta aplicação protótipo é a recolha da gama de valores dos sensores físicos e virtuais de um dispositivo móvel, e dados que possam ilustrar o comportamento digital durante o seu uso. posteriormente à construção do conjunto de dados, foi realizado uma série de técnicas e processos relacionados com machine learning para eleger o melhor modelo possível. por fim, a última etapa foi a elaboração da aplicação final, já com o modelo ideal incorporado, que é capaz de indicar quando o utilizador de um smartphone está aborrecido, com base nos valores indicados pelos sensores e pelo estado do próprio dispositivo móvel. o modelo incorporado trata-se de uma rede neuronal artificial que tem a capacidade de prever o nível de aborrecimento da pessoa que está a interagir com o telemóvel. este modelo consegue prever o sentimento em causa com uma precisão de 70%.",
      "since ancient times, mankind has been trying to combat negative feelings such as sadness and loneliness. another emotion that has always disturbed humanity is boredom. this cause is probably the major factor that led to the birth of various types of art and sports, and that has led to the continuity of its practice and observation over the centuries. today, given the fact that smartphones have become globally used devices, people are increasingly subjected to stimuli. so when they are not busy, they feel the need to do something that keeps their brain active. for this reason, detecting boredom when using a smartphone paves the way for improving the success rates of stimuli signals, with a less intrusive, and more intelligent system. at an early stage, this research work focused on data collection. an initial application was developed, to fulfil this objective. the most relevant purpose of this prototype application is to collect the range of values of the physical and virtual sensors of a mobile device, and data that can illustrate the digital behaviour during its use. following the construction of the dataset, a series of techniques and processes related to machine learning was performed, to choose the best possible model. finally, the last step was the elaboration of the final application, already with the ideal model incorporated, which can indicate when the smartphone user is bored, based on the values of the sensors and the state of the mobile device itself. the built-in model is an artificial neural network that can predict the level of the boredom of the person interacting with the mobile phone. this model can predict the feeling in question with an accuracy of 70%."
    ],
    0.0
  ],
  [
    [
      "in the last decades, there has been an exponential development in the area of computing, which includes artificial intelligence (ai). the development of ai translates into the emergence of programs that replicate the ability to make decisions, perceive and solve problems in a similar way to humans. today, artificial intelligence is already part of various areas of society, such as security, health, or virtual assistants. this dissertation aimed to develop a web application that converts graphical interface sketches, elaborated with the balsamiq mockups application, into html, css and bootstrap code. converting a web page sketch into code is a task that developers typically perform. due to the time consuming of this task, it becomes impossible to devote more time to the application logic. on the other hand, it is a repetitive and tedious task. two deep neural network models were built, divided into two distinct approaches. the first approach, presenting poor results, uses a convolutional network and two recurring networks, according to an encoder-decoder architecture, similar to image captioning. it also uses a dsl language and a compiler that transforms dsl into code. the second approach is completely different and it is more focused on the spatial component of the addressed task. it uses yolo model and a layout algorithm that converts the output of yolo into code. in the same test set, the first approach achieves 71.30% accuracy, while in the second approach it yields 88.28% accuracy. the web application, which allows the user to upload images and automatically generate html, css and bootstrap code, is supported by the yolo based model as it gives better results.",
      "nas últimas décadas assistiu-se a um desenvolvimento exponencial na área da computação, na qual está incluída a inteligência artificial. o desenvolvimento da inteligência artificial traduz-se no aparecimento de programas que replicam a capacidade de decisão, perceção e resolução de problemas do ser humano. atualmente, a inteligência artificial já faz parte de várias áreas da sociedade, como a segurança, a saúde ou os assistentes virtuais. a presente dissertação tinha como objetivo desenvolver uma aplicação web que permitisse converter esboços de interfaces gráficas, elaborados com a aplicação balsamiq mockups, em código html, css e bootstrap. a conversão de esboços de páginas web em código e uma tarefa normalmente realizada pelos programadores. devido ao tempo necessário para realizar esta tarefa, reduz-se o tempo disponível para dedicar a lógica da aplicação. por outro lado, a aplicação a desenvolver eliminaria boa parte de uma tarefa repetitiva e tediosa. construíram-se dois modelos de aprendizagem profunda, resultado de duas abordagens distintas. a primeira abordagem, com piores resultados, utiliza uma rede neuronal convolucional e duas redes recorrentes segundo uma arquitetura codificador-descodificador, semelhante ao que se costuma adotar na legendagem de imagens. utiliza ainda uma linguagem dsl e um compilador que transforma a dsl em código html, css e bootstrap. a segunda abordagem, completamente diferente e mais focada na componente espacial do problema a resolver, consiste na utilização do yolo é um algoritmo de layout que converte a saída do yolo em código html, css e bootstrap. no mesmo conjunto de teste e de acordo com as métricas desenvolvidas para avaliar os modelos, a primeira abordagem resulta em 71.30% de correção, enquanto que a segunda abordagem permitiu alcançar resultados muito superiores (88.28%). a aplicação web permite ao utilizador carregar imagens e gerar automaticamente o código html, css e bootstrap. a aplicação é suportada pelo modelo que resultou da segunda abordagem e que apresenta melhores resultados."
    ],
    [
      "a população mundial está a envelhecer, sendo que, na europa, 5% da população tem mais de 80 anos e estima-se que este número venha a triplicar nos próximos 20 anos. esta evolução traz consigo um novo conjunto de desafios sociais e económicos, nomeadamente no âmbito da prestação de cuidados médicos. uma das vertentes mais importantes da prestação de cuidados médicos é a monitorização de pacientes. os sensores biomédicos atuais são dispendiosos e funcionam com sistemas computacionais protegidos pelos fabricantes, com tecnologias próprias não partilhadas. assim, torna-se útil a definição dum novo paradigma capaz de diminuir relevantemente os custos de aquisição, instalação e manutenção desses sistemas de monitorização. nesta dissertação apresenta-se um modelo genérico de monitorização biomédica capaz de ser implementado em unidades hospitalares, visando a recolha de dados sensoriais biomédicos, o seu pré-processamento e eventual integração numa base de dados global (numa cloud local, por exemplo). a arquitetura permite a monitorização automática de pacientes de variados serviços de saúde com a menor intervenção humana possível, independentemente do tipo de sensor utilizado, com baixo custo de implementação e de aplicação universal. o sistema desenvolvido utiliza mecanismos normalizados para a representação da informação a monitorizar assim como para a comunicação entre as entidades da arquitetura, e que são baseados nas tecnologias amplamente utilizadas para a gestão de redes internet. nomeadamente, foram criadas definições para novas bases de dados específicas para monitorização e configuração de sensores biomédicos utilizando o paradigma das management information bases. além disso, o protocolo de comunicação entre as entidades da arquitetura proposta é o simple network management protocol (snmp). como prova de conceito foi implementado, com sucesso, um protótipo que ilustra a arquitetura proposta, incluindo o hardware dum sensor biomédico básico de baixo custo e o software dum agente snmp e duma simples aplicação biomédica capaz gerar alertas em situações clinicamente pre-definidas por uma equipa médica.",
      "the world population is getting old, and, in europe, 5% of the population is over 80 years and it is estimated that this number will triple over the next 20 years. this development brings with it a new set of social and economic challenges, in particular the provision of medical care. one of the most important aspects of medical care is patient monitoring. current biomedical sensors are expensive and work with computer systems protected by manufacturers to own technologies not shared. thus, it is useful a new paradigm definition able to substantially reduce the costs of acquisition, installation and maintenance of these monitoring systems. this dissertation presents a generic model of biomedical monitoring to be set in hospitals in order to gather biomedical sensor data, its preprocessing and eventual inclusion in a global database (a local cloud, for example). the architecture allows automatic monitoring of various health care patients with the least possible human intervention, no matter the type of sensor used, with low cost implementation and universal application. the created system uses standard mechanisms for the representation of monitoring information, as well the communication between the entities of the architecture, which are based on widely used technologies for the internet networks management. in particular, definitions were created for a new basis of specific data for monitoring and configuration of biomedical sensors using the paradigm of management information bases. in addition, the communication protocol between the entities of the proposed architecture is the simple network management protocol (snmp). as proof of concept, a prototype that illustrates the proposed architecture, including the lowcost basic biomedical hardware sensor and software of an snmp agent and a simple biomedical application that can generate alerts in clinically pre-defined situations by medical team, has been successfully implemented."
    ],
    0.3
  ],
  [
    [
      "the goal of the master’s thesis work here reported is to develop a system capable of generating a sde for any given language definition. a sde is a type of source code editor that knows the programming language grammar and uses this knowledge to guide the editing and the execution of a program. this type of editing ensures that a program is syntactically correct. the editor is intended to provide both syntax-directed editing as well as manual text editing. a meta-language must be created to describe the grammar of the editor’s target language. the meta-language will provide annotations to change the display of the text in the editor. that specification, written in the referred meta-language will be the input to generate templates for the syntax directed editor. the sde generator is available in a standalone jar application as well as a web version.",
      "o objetivo do trabalho de mestrado aqui relatado é desenvolver um sistema capaz de gerar um editor dirigido pela sintaxe (sde) para uma qualquer linguagem. um sde é um editor de texto que tem conhecimento da gramática da linguagem e usa esse conhecimento para guiar o utilizador na edição e execução do programa assegurando assim que o programa esteja sintaticamente correto. o editor será capaz de dar ao utilizador a habilidade de editar o programa tanto através de comandos dirigidos pela sintaxe como digitando o programa. para isso é preciso criar uma meta linguagem para descrever a gramática da linguagem-alvo do dito editor. a linguagem permite ao utilizador anotar a gramática de modo a mudar a aparência do texto no editor. esta meta linguagem seria depois usada para gerar os templates necessários para o editor dirigido pela sintaxe. o gerador de sde está disponível numa versão jar standalone e numa versão web."
    ],
    [
      "resource allocation is critical to optimize billable allocation and avoid allocation conflicts. tools are needed to correctly book and share allocations during the normal operation of a company. zeus is an intranet built in-house by vilt, a digital solutions company, which is used to connect every employee in the company, containing important personal and shared information, as well as useful services. starting from scratch, it was developed according to the needs of vilt’s staff, making it a fully customized intranet. one of the many features in zeus that was requested by vilt’s professional services managers is the allocation map page. this feature allows managers to manage multiple human resources and their projects. each manager has their own team to manage, and they currently face some limitations related to the resource allocation management process. these limitations are noticeable to the managers, resulting in a lack of adherence from some of them, which resulted in a divergence of the methods used in resource allocation management inside the company. there is a need to create an improved solution that takes into account: the managers’ insights and ideas, inspiration from existing software, and inspiration from the managers’ alternative solutions. the main goal for the new solution is having an easy view of who is available and when, as well as the option to easily manage that information. with this dissertation, we want to study the ux/ui design process and different resource allocation solution, and use it to design a resource allocation solution for vilt.",
      "a alocação de recursos é fundamental para otimizar a alocação faturável e evitar conflitos de alocação. são necessárias ferramentas para registar e partilhar corretamente alocações no funcionamento normal de uma empresa. o zeus é uma intranet construída internamente pela vilt, uma empresa de soluções digitais, e é usado para conectar todos os funcionários da empresa, contendo informações importantes pessoais e partilhadas, bem como serviços. foi desenvolvido de acordo com as necessidades da equipa da vilt, tornando-se uma intranet totalmente personalizada. uma das características do zeus que foi solicitada pelos professional services managers da vilt foi a página do mapa de alocação. este recurso permite aos managers gerir múltiplos recursos humanos e os seus projetos. cada manager tem sua própria equipa para gerir, e estes atualmente enfrentam algumas limitações relacionadas ao processo de gestão de alocação de recursos. estas limitações são visíveis para os managers, resultando na falta de adesão de alguns deles, o que resultou numa divergência dos métodos utilizados na gestão de alocação de recursos dentro da empresa. é necessário criar uma solução melhorada que tenha em conta: as visões e ideias dos managers, tendo inspiração no software existente e nas soluções alternativas usadas pelos managers. o objetivo principal para a nova solução é ter uma visão fácil de quem está disponível e quando, bem como a opção de gerir facilmente essa informação. com esta dissertação, queremos estudar o processo de design ux/ui e a gestão de alocação de múltiplos recursos, e usá-la para criar o design de uma solução de alocação de recursos para a vilt."
    ],
    0.0
  ],
  [
    [
      "esta dissertação está centrada na paralelização massiva da biblioteca java evolutionary cornputation library), jecoli, que se foca no desenvolvimento de meta-heurísticas de otimização(e.g. algoritmos evolucionários (aes)) na linguagem java. os aes são um paradigma da computação evolucionária (ce) utilizados para resolver problemas complexos através de um método iterativo que evolui um conjunto de soluções (população) tendo em conta os princípios da teoria de evolução por seleção natural apresentada por charles darwin. estes algoritmos estão divididos em duas categorias, aes não estruturados e aes estruturados. os aes não estruturados são caracterizados por uma população centralizada onde existe apenas um conjunto de soluções ao qual é aplicado o processo evolutivo. por outro lado, os aes estruturados contêm várias populações onde os processos evolutivos são conduzidos de forma independente, embora existindo troca de informação. os algoritmos de ambas as categorias podem ser paralelizados de diferentes maneiras. nesta dissertação, foram implementadas quatro versões paralelas da plataforma jecoli de forma o menos invasiva possível, tendo em conta modelos paralelos já formulados: um modelo de paralelismo global; um modelo de ilhas em ambiente de memória partilhada; um modelo de ilhas em ambiente de memória distribuída; e um modelo híbrido. estas implementações paralelas foram executadas no cluster services and advanced research cornputing with htc/hpc clusters (search) utilizando o máximo de recursos computacionais possíveis de modo a realizar uma posterior análise dos resultados obtidos. foram utilizados dois casos de estudo reais para validar as implementações paralelas, um problema de otimização de um bioprocesso de fermentação fed-batch e outro de otimização dos pesos de um protocolo de encaminhamento (osp f). cada uma das implementações paralelas foi testada nos dois casos de estudo, aplicando o máximo de paralelismo possível tendo em conta as limitações de cada caso de estudo, dos modelos paralelos e dos recursos disponíveis. com estes testes concluí-se uma boa escalabilidade destes algoritmos, onde se destacam as implementações relativas ao modelo de ilhas em memória distribuída e ao modelo híbrido. contudo, algumas configurações que originam maiores ganhos foram descartadas pois não produzem valores de aptidão aceitáveis.",
      "this dissertation is centered in the massive parallelization of the java evolutionary computation library, jecoli, which focuses on the development of meta-heuristics optimizations (e.g. evolutionary algorithms (eas)) in the java programming language. the eas are a paradigm of evolutionary computation (ec) used to solve complex problems through an iterative method that evolves a set of solutions (population) taking into account the principles of the theory of evolution by natural selection by charles darwin. these algorithms are divided into two categories, unstructured eas and structured eas. the unstructured aes are characterized by a centralized population where there is only one set of solutions for which the iterative method is applied. on the other hand, structured aes contain several independent populations where evolutionary processes are applied, although there exchange information. the algorithms of both categories can be parallelized in different ways. ln this dissertation, it was implemented four parallel versions of the platform jecoli in a less invasive way, taking into account parallel models already formulated: a global parallelism model; a island model in shared memory environment; a island mo del in distributed memory environment; and a hybrid model. these parallel implementations were executed in the cluster search using the maximum computing resources in order to perform a further analysis of the results obtained. two case studies were used to validate the parallel implementations, a bioprocess optirnization problem of fed-batch fermentation and other weights optimization problem of a routing protocol (ospf). each of the parallel implementations were tested in the two case studies, applying the maximum parallelism taking into account the limitations of each case studies, parallel models and available resources. with these tests can be concluded a good scalability of these algorithms, which highlights the implementations on the island model in distributed memory and hybrid model. however, some settings that give greater gains were discarded because they produce no acceptable fitness values."
    ],
    [
      "parkinson’s disease (pd) is a neurodegenerative disorder of the central nervous system. resting tremor, akinesia, and bradykinesia (slow movements), rigidity, shuffling walking, and postural instability are some of the symptoms that not only negatively impacts patients’ life, but also the life of people around them. current approaches for monitoring patients’ motor autonomy are limited to the observer and self reported methods. the observer-based examinations, patients perform a set of standard pd examinations. the self-reported method relies on patients’ daily activities diaries. these approaches are commonly used, but are limited to a few sessions per year, they do not address common motor daily tasks, and their results are object of subjective interpretation by the clinical expert. by combining kinematic-driven data from wearable sensor with ai, the main goal of this dissertation is to develop an automatic software for recognition of human activities (e.g., walking, standing, turning, sitting, and lying) in pd to assist the clinical experts with objective and concrete data. a data collection protocol was developed and captured, resulting in a database comprised of data collected from eighteen pd patients who performed three trials of six different daily activities: walk; 180º turning; sit on chair; get up from chair; lay on bed and get up from bed. a deep learning (dl) framework based on convolutional neural network capable of recognizing daily activities was developed and attained a performance of f1 score equal to 0.90892. as a complementary goal an automatic software for human walk initial contact (ic) and final contact (fc) recognition using kinematic data was also developed. ic and fc are tremendously important to provide patient on-demand motor assistance and estimation of walking-associated metrics. a deep learning framework based on bidirectional long short-term memory neural network capable of walking ic/fc events detection was developed and attained a performance of mcc score equal to 0.538386. promising results were attained for both dl frameworks, however, this dissertation suggests that there is still room for further improvements. enriching the dataset with more data from different patient, data balancing and feature extraction techniques, experimenting new models’ architectures should be considered in future works.",
      "a doença de parkinson (dp) é uma doença neurodegenerativa do sistema nervoso central. o tremor em repouso, acinesia e bradicinesia (movimentos lentos), rigidez, marcha e postura instável são alguns dos sintomas que afetam negativamente a vida dos pacientes e também as pessoas à sua volta. as monitorizações da autonomia motora dos pacientes estão limitadas aos métodos presenciais e auto-relatados. em exames presenciais, os pacientes realizam um conjunto de exames padrão de dp. o método auto-relatado baseia-se nas agendas de atividades diárias dos pacientes. estas abordagens são comuns, mas são limitadas a algumas sessões por ano, não abordam tarefas motoras diárias comuns, e os seus resultados dependem da interpretação subjetiva do perito clínico. o principal objetivo desta dissertação é desenvolver um software automático para e reconhecimento de atividades humanas (por exemplo, andar, estar em pé, virar, sentar e deitar) na dp, que combine dados cinemáticos de sensores vestíveis com inteligência artificial para ajudar os especialistas clínicos a obterem dados objetivos e concretos. foi desenvolvido um protocolo de recolha de dados, resultando numa base de dados constituída por dados recolhidos de dezoito pacientes de dp que realizaram três ensaios de seis atividades diárias diferentes: caminhar; virar 180º; sentar-se na cadeira; levantar-se da cadeira; deitar-se na cama e levantar-se da cama. foi desenvolvida uma estrutura de deep learning (dl) baseada em convolutional neural network capaz de reconhecer as atividades diárias e atingir um desempenho de f1 score igual a 0,9089. como objetivo complementar, foi também desenvolvido um software automático para o reconhecimento do contacto inicial (ci) e final (fc) do andar humano, utilizando dados cinemáticos. o ci e a cf são tremendamente importantes para fornecer assistência motora em tempo real e estimativa da métrica associada à marcha do paciente. foi desenvolvida uma estrutura de dl baseada em bidirectional long short-term memory neural network capaz de detetar eventos de ic/fc durante o andar e atingir um desempenho mcc score igual a 0,5384. foram alcançados resultados promissores para ambas as estruturas dl, contudo, esta dissertação sugere que ainda há espaço para mais melhorias. enriquecer o conjunto de dados com mais dados de diferentes pacientes, técnicas de balanceamento de dados e extração de características, experimentar modelos com diferentes arquiteturas deve ser considerado em trabalhos futuros."
    ],
    0.06666666666666667
  ],
  [
    [
      "simulating quantum mechanical systems is one of the main applications envisioned for quantum com puters. in contrast with the first algorithms created for this purpose, that were devised to be implemented in a fault-tolerant quantum computer, the variational quantum eigensolver (vqe) aims to adjust to the con straints of noisy intermediate-scale quantum (nisq) devices. there is hope that the class of variational quantum algorithms (vqas), to which vqe belongs, will be the first to achieve quantum advantage. the choice of ansatz can dictate the success (or lack thereof) of a vqa: too deep ansätze can hinder near-term viability, or lead to trainability issues that render the algorithm inefficient. in this context, this dissertation aimed to analyse different ansätze for quantum chemistry, examining their noise-resilience and viability in state-of-the-art quantum computers. in particular, dynamic ansätze were explored, and their performance compared against predetermined ansätze, with a focus on susceptibility to noise. multiple variants of vqe, namely unitary coupled cluster singles and doubles (uccsd)-vqe (prede termined) and adaptive derivative-assembled pseudo-trotter (adapt)-vqe (dynamic), were implemented both in simulators and cloud quantum computers. using noise models, the impact of several noise sources on convergence was assessed. additionally, the importance of the operator pool in adapt-vqe was anal ysed, and strategies to manipulate the ansatz beyond the adapt-vqe algorithm were explored. several conclusions could be drawn from this work. adapting the ansatz to the problem and system was concluded to be fundamental in avoiding trainability issues, decreasing the circuit depth required for a given accuracy, and improving noise-resilience (against circuit depth dependent and independent sources alike). dynamic ansätze were shown to be capable of enduring significantly larger error rates than predetermined alternatives, and were thus proved to be better suited for nisq devices. for 𝐻2, as far as ground state energy calculations are concerned, adapt-vqe was shown to tolerate a 20 times lower shot count, 150 times larger error rates in state preparations and measurements, and 850 times lower coherence times than uccsd-vqe. the difference is expected to increase with the size of the system. additionally, it was observed that there is still a margin for improving upon adapt-vqe. further manip ulation of the ansatz was shown to be capable of producing yet shallower circuits for the same accuracy. using an idea previously proposed in the literature, a more conservative selection criterion was tested. additionally, removing operators on the fly based on available data was attempted as a new possibility. both approaches were shown to be capable of improving upon the adapt-vqe ansatz, resulting in an up to 35-fold decrease in the error for a similar circuit depth within the first 10 iterations of adapt-vqe.",
      "simular sistemas quânticos é uma das mais relevantes potenciais aplicações dos computadores quân ticos. os primeiros algoritmos criados para tal visam implementação em computadores quânticos toleran tes a falhas; em contraste, o eigensolver variacional quântico (vqe) procura adaptar-se às limitações dos dispositivos quânticos ruidosos de escala intermédia (nisq). há esperança que a classe de algoritmos variacionais quânticos (vqas), à qual o vqe pertence, seja a primeira a alcançar a vantagem quântica. o ansatz pode ditar o sucesso (ou falta de) de um vqa: a sua profundidade pode impedir a viabilidade a curto prazo, ou conduzir a problemas de treinabilidade que levam à ineficiência. neste contexto, esta dissertação pretendeu analisar diferentes ansätze para química quântica, examinando a sua resiliência ao ruído e viabilidade em dispositivos contemporâneos. em particular, exploraram-se ansätze dinâmicos, cujo desempenho se comparou ao de ansätze predeterminados com foco na suscetibilidade ao ruído. diferentes variantes do vqe, nomeadamente uccsd-vqe (predeterminado) e adapt-vqe (dinâmico), foram implementadas em simuladores e em computadores quânticos. com recurso a modelos de ruído, o impacto de diversas fontes de ruído na convergência foi analisado. inspecionou-se a importância do conjunto de operadores no adapt-vqe, e exploraram-se estratégias adicionais para manipular o ansatz. várias conclusões puderam ser retiradas deste projeto. concluiu-se que adaptar o ansatz ao problema e ao sistema é fundamental para evitar problemas de treinabilidade, diminuir a profundidade dos circuitos, e melhorar a resiliência ao ruído (seja a fonte dependente da profundidade do circuito ou não). mostrou se que ansätze dinâmicos toleram taxas de erro superiores aos predeterminados, sendo portanto mais adequados para dispositivos nisq. quanto aos cálculos da energia do estado fundamental, mostrou-se que, para a molécula 𝐻2, o adapt-vqe tolera um número de repetições do circuito 20 vezes inferior, taxas de erro na preparação e medição de estados 150 vezes superiores, e tempos de coerência 850 vezes inferiores do que o uccsd-vqe. é expectável que a diferença aumente com o tamanho do sistema. adicionalmente, observou-se que há ainda uma margem para melhorar além do adapt-vqe. mostrou se que outras formas de manipulação do ansatz são capazes de produzir circuitos ainda menos profundos para uma mesma precisão. usando uma ideia previamente sugerida na literatura, tentou-se usar um crité rio de seleção mais conservador. alternativamente, remover operadores com base em dados disponíveis ao longo da execução foi testado como uma nova possibilidade. mostrou-se que ambas as estratégias são capazes de superar o ansatz criado seguindo o protocolo do adapt-vqe, resultando num erro até 35 vezes menor para profundidades de circuito comparáveis dentro de 10 iterações do adapt-vqe."
    ],
    [
      "nowadays data can have many different shapes and relations between itself, ontologies try to formalize the semantics subjacent to this data and make it understandable by humans and code alike. while code succeeds at parsing and interpreting this formalization traditional ontology formats can be tough for a human to understand without previously deepened knowledge of the ontologic paradigm and, even then, directly analyzing a format like rdf would be, at the very least, very tedious. this problem is not exclusive to ontologic data either as to make sense of big datasets, even in famously human readable formats like json, humans need visualizations and abstractions. this dissertation is a study on graph visualization of ontologic data and how abstractions can be used to convey information to the end user in meaningful ways the information gathered is then used to implement an application called \"ulisses nextgen\" that can generate an easily navigable graph visualizing application with a strong focus to support ontological data but general enough to support any information that can be abstracted as a graph. the application is served as a javascript package to be used in anywhere on the web where it can be used best to reach the end user.",
      "hoje em dia os dados podem ter muitas formas e relações diferentes entre si, as ontologias tentam formalizar a semântica subjacente a estes dados e torná-los compreensíveis tanto para o ser humano como para o código. embora o código consiga análisar e interpretar facilmente esta formalização, os formatos tradicionais de ontologias podem ser difíceis de entender para um humano sem um con hecimento previamente aprofundado do paradigma ontológico e, mesmo assim, analisar directamente um formato como o rdf seria, no mínimo, muito tedioso. este problema não é exclusivo dos dados ontológicos, existe tradicionalmente uma grande dificulade por parte do ser humano em interpretar grandes conjuntos de dados precisando de visualizações e abstracções. esta dissertação é um estudo sobre a visualização gráfica de dados ontológicos e como as abstracções podem ser usadas para transmitir informação ao utilizador final de formas significativas a informação recolhida é então usada para implementar uma aplicação chamada \"ulisses nextgen\" que gera um grafo facilmente navegável com um grande foco para suportar dados ontológicos mas geral o suficiente para suportar qualquer informação que possa ser abstraída como um grafo. a aplicação é servida como um pacote javascript para ser usado em qualquer lugar na web onde possa ser melhor utilizada para chegar ao utilizador final."
    ],
    0.0
  ],
  [
    [
      "a imagem de ressonância magnética (irm) é uma técnica que visualiza as estruturas internas do corpo através de campos magnéticos poderosos e ondas de rádio, e ao contrário dos raios-x, um exame de ressonância magnética não usa radiação. normalmente esta técnica é usada para detecção de doenças, monitoramento de tratamento e diagnóstico. tem um grande impacto na neurociência desde que começou a ser desenvolvida nos anos 70 e 80, especialmente porque a ressonância magnética funcional (irmf) e a ressonância magnética de difusão (irmd) aumentam a eficiência da ressonância magnética, embora as técnicas de irmf e irmd tenham tido muito pouco impacto nas aplicações clínicas. os estudos e resultados recentes da irmf têm sido progressivamente aprovados por médicos e pesquisadores, pois são capazes de fornecer informações únicas sobre as funções cerebrais, no entanto, quando se trata de melhorar o atendimento clínico, os resultados têm sido muito limitados e a utilização de estímulos audiovisuais avançados de irmf foram quase nulos. uma das abordagens recentes que é adequada para criar um estímulo audiovisual avançado de irmf, e é útil em aplicações clínicas (por exemplo, transtorno de déficit de atenção e hiperatividade (tdah)) é a irmf cinematográfica. um dos problemas principais das técnicas cinematográficas de irmf é identificar uma determinada patologia que varia muito entre todos os filmes e patologias, ou seja, encontrar e fazer um filme adequado para cada patologia é muito caro e demora muito tempo. neste projeto, iremos explicar o desenvolvimento de programas de software que estão interligados a um cérebro-máquina para que os scripts possam ser transformados em animação por computador.",
      "magnetic resonance imaging (mri) is a technique that visualizes internal structures of the body through powerful magnetic fields and radio waves, and unlike x-rays, an mri exam doesn’t use radiation. normally this technique is used for disease detection, treatment monitoring and diagnosis. it has a great impact on neuroscience since it started to develop in the 1970s and 1980s, especially since functional magnetic resonance imaging (fmri) and diffusion magnetic resonance imaging (dmri) increase the efficiency of mri, even though, fmri and dmri techniques have had very little impact on clinical applications. the recent studies and results of fmri have been progressively approved by physicians and researchers as they are able to provide unique insights into brain functions, however when it comes to improve clinical care, the results have been very limited and the usage of advanced fmri audio-visual stimuli have been almost null. one of the recent approaches that is suitable to create an advanced audiovisual fmri stimulus and it’s usefull on clinical applications (e.g. attention deficit hyperactivity disorder (adhd)) is the cinematic fmri. one of the main issues of the cinematic fmri techniques is to identify how a given pathology varies greatly across all films and pathologies, so in other words, to find and make an appropriate film for each pathology is very expensive and time consuming. in this project, we will explain the development of software programs that are interconnected to a brain-machine so that scripts can be publicly transformed into computer animation. these programs can be used to detect a pathology that causes a change in film interfaces and other applications."
    ],
    [
      "every day, data is being collected from all different types of sources. according to the company domo, data is being collected from ad clicks, likes on social media, shares, transactions, streaming content, and so much more. their study, which focused on the data generated on the most popular platforms in 2020, shows that, every minute of the day, users sent 12m instant messages, shared 65k photos on instagram, and conducted 5.7m of searches on google. moreover, accordingly to statista, by 2025, the volume of data created, captured, copied, and consumed worldwide will increase up to 180 zettabytes. this enormous amount of data in itself may not be relevant. the real value of data lies in the information it hides about individuals and the world. as a result, it is more crucial than ever for businesses of all sizes to focus on the data they collect from diverse sources and use the insights they gain to become more competitive in their fields of expertise. in this scenario, companies rely on recruiting professionals to join data science teams capable of gleaning insights and extracting value from data. data science, as the name implies, can be seen as the science that studies data. it is a multidisciplinary field where professionals, commonly known as data scientists, transform data into insights and decisions. several researchers have focused on data science, intending to explain it and demonstrate its value in several contexts. however, in this research study, we shifted the focus to those who practice data science. this work aims to take advantage of the information collected through interviews and a public survey to fully understand who is doing data science, how they work, what skills they hold and lack, and which tools they need. based on the results, we argue that the academic past of data science professionals has little impact on the way they work and that the most difficult challenges they face are obtaining high-quality data and applying deep learning techniques. we also discovered evidence of a gender gap in data science, which the scientific community should address in order to make data science accessible to everyone.",
      "todos os dias são recolhidos milhões dos dados das mais distintas fontes. o último estudo realizado pela empresa domo sobre a quantidade de dados gerados nas principais plataformas online, mostrou que, a cada minuto de 2020, os utilizadores enviaram mais de 12 milhões de mensagens, partilharam cerca de 65 milhares de fotos no instagram, e fizeram mais de 5.7 milhões de pesquisas no google. para além disso, de acordo com um estudo realizado pela plataforma statista, até 2025, o volume de dados criados, guardados e consumidos a nível global atingirá 180 zettabytes. essa enorme quantidade de dados, por si só, pode não ser relevante. o valor real dos dados está nas informações que eles escondem sobre a sociedade e o mundo. assim, é mais crucial do que nunca que as empresas de todos as indústrias se concentrem nos dados que coletam e usar o conhecimento que obtêm para se tornarem mais competitivas nas suas áreas de atuação. perante este cenário, as empresas têm vindo a apostar cada vez mais no recrutamento de profissionais para integrarem equipas focadas em ciência de dados, capazes de utilizar dados para dar resposta a vários problemas que as afetam. ciência de dados, como o nome indica, pode ser vista como a ciência que estuda dados. é uma área multidisciplinar onde os profissionais, comumente conhecidos como cientistas de dados, transformam dados em conhecimento que auxilia a tomada de decisões. nos últimos anos, vários investigadores focaram-se no estudo da ciência de dados, com o objetivo de explicar e demonstrar o seu valor em diversos contextos. no entanto, neste trabalho, mudamos o foco para aqueles que praticam a ciência de dados. assim, o objetivo deste estudo é aproveitar as informações recolhidas por meio de entrevistas e de um inquérito para melhor conhecer quem trabalha em ciência de dados. com base nos resultados, argumentamos que o passado acadêmico dos profissionais de ciência de dados tem pouco impacto na forma como trabalham e que os maiores desafios que enfrentam são a obtenção de dados de qualidade e a aplicação de técnicas de deep learning. também encontramos evidências de uma lacuna de gênero, sendo esta uma questão que deve ser abordada pela comunidade cientifica de forma a tornar a ciência de dados igualmente acessível a todos."
    ],
    0.3
  ],
  [
    [
      "deep learning classifiers are capable of an outstanding performance. yet, they are vulnera ble to adversarial attacks, i.e. it is possible to craft a slightly modified version of a correctly classified image that, although its contents are still clearly recognisable to a human being, the classifier outputs an incorrect classification. in this thesis we evaluate the effectiveness of adversarial attacks, namely their trans ferability to other models, and some proposed defenses. transferability occurs when an adversarial sample is crafted with a model, and it succeeds in achieving a misclassification in another model. to make this study as comprehensive as possible, we explore several attack methods, namely: fast gradient sign method (fgsm), deepfool, jacobian saliency map attack (jsma), carlini, projected gradient descent (pgd) and few pixels. to evaluate the impact of the model’s architecture in the transferability rate we use sev eral common architectures: vgg16, three resnet with different depths, and a small con volution neural network. two common datasets were used for evaluation: cifar-10 and german traffic sign recognition benchmark (gtsrb). different attack methods use different approaches and parameters to craft adversarial samples. hence, it is not trivial to control the degree of perturbation. to be able to achieve the same level of perturbation with every method we resorted to an image comparison metric: structural similarity index measure (ssim). for each method we performed a search within its parameter space to find the parameters that on average attain a specific level of perturbation. to evaluate the impact of the level of perturbation on transferability rates, we evaluate two different values for the ssim metric. our results show that while it is possible to craft an adversarial sample in a particular model, the transferability rates vary considerably from method to method. regarding defensive methods we explored adversarial training and defensive distilla tion. the results show that the ability to prevent an adversarial attack, or robustness, varies significantly depending on the conditions that the attack is performed and on the defensive methods used. furthermore, there is a trade-off between robustness and accuracy, with defensive models having lower accuracy than non-defended models.",
      "modelos classificativos de deep learning são capazes de performances extraordinárias, su perando mesmo a capacidade humana, no entanto são vulneráveis a ataques adversariais, por exemplo é possível criar uma versão modificada de uma imagem bem classificada, que apesar do ser facilmente reconhecível por um ser humano, o modelo classifica incorrectamente. nesta tese avaliamos a transferabilidade dos ataques adversariais, e algumas propostas de defesa. a transferabilidade occorre quando um adversarial gerado por um modelo é capaz de fazer outro modelo o classificar incorrectamente. para fazer este estudo o mais compreensivo possível, explorámos vários tipos de métodos de ataques, nomeadamente: fgsm, deepfool, jsma, carlini, pgd and few pixels. para avaliar o impacto da arquitectura do modelo na taxa de transferabilidade, usamos várias arquitecturas conhecidas: vgg16, 3 resnet com diferentes números de camadas, e uma rede pequena de 3 camadas convolucionais. usamos 2 datasets conhecidos para esta avaliação: cifar-10 e gtsrb. diferentes métodos de ataques usam abordagens e parâmetros diferentes para gerar exemplos adversariais. por isso, não é trivial controlar o grau de perturbação gerada pelos exemplos adversariais. para sermos capazes de alcançar o mesmo nível de perturbação com todos os métodos recorremos a uma métrica de comparação de imagem: ssim. para cada método fizemos uma procura no domínio dos parâmetros para encontrar os valores que em média geram um nível específico de perturbação. para avaliar o impacto do nível de perturbação na taxa de transferabilidade, avaliámos 2 valores diferentes para a métrica ssim. os nossos resultados mostram que enquanto é possível gerar um exemplo adversarial num modelo particular, a taxa de transferabilidade varia consideravelmente de método para método. em relação às defesas exploramos adversarial training e defensive distillation. os resultados mostram que a capacidade de prevenir um ataque adversarial varia significativamente dependendo das condições em que o ataque é feito e nos próprios métodos."
    ],
    [
      "smoothed-particle hydrodynamics (sph) is a particle-based simulation considered by many to be the main candidate for fluid simulation. this model was developed by r.a. gingold and j.j. monaghan in 1977 and had the purpose of solving astrophysical problems. over the years, monaghan has revisited sph (1985, 1988, 1992 and so on) and it also gained traction with other researchers who discovered new applications for the model such as ballistics, volcanology, oceanography, and so on. among the fields there is one we are particularly interested in, and that is fluid simulation. this work aims to implement sph using efficient data structures that allow a real-time simulation to run on the graphics processing unit (gpu). according to the literature, the z-order indexing method and the hash map are the most suitable structures for this purpose. it is intended to see its impact and in which situations one will be better suited to use than the other. with said implementation, several tests were performed in order to analyze the robustness and stability of the method. with these tests it was possible to compare the two data structures used. the implemented sph showed realistic and robust results in most cases, being able to handle multiple scenes of varying levels of complexity. despite the good results, it showed some difficulties in maintaining stability in some boundaries (boundaries with great curvature or sharp edges) and also showed some difficulties in scenes with two fluids with different densities. as for the data structures, it was possible to observe that both are efficient and support real-time simulations with more than 1 million particles (using a nvidia rtx 3080). in the case of z-order, it proved to be the method with the best performance when compared to the hash map under the same conditions, that is, scenes with the same number of particles and the same simulation volume. this is due to the larger data locality that z-order has. on the other hand the hash map was a bit slower (when compared with the z-order under the same conditions) but allowed for greater freedom when creating a scene. when comparing the two methods with the same number of particles but different simulation volumes we can see that the hash map catches up with the z-order method as the particles spread across the simulation. with the two data structures analyzed it is possible to draw some conclusions. the z-order method is recom mended when we have a limited and relatively small simulation volume. in case there is no simulation volume, or it is very large, it is recommended to use a hash map since the performance deficit seems to disappear as the simulation volume gets bigger and the particles spread across the volume.",
      "o smoothed-particle hydrodynamics é um método baseado em partículas considerado por muitos como o prin cipal candidato para a simulação de fluidos. este modelo foi desenvolvido por r.a. gingold e j.j. monaghan em 1977 e tinha como objetivo resolver problemas astrofísicos. ao longo dos anos, monaghan revisitou o sph (1985, 1988, 1992, etc.) e este também ganhou tração com outros investigadores que descobriram novas apli cações para o modelo, como balística, vulcanologia, oceanografia, entre outros. de todos os campos, há um em que estamos particularmente interessados, que é o da simulação de fluidos. este trabalho visa implementar o sph usando estruturas de dados eficientes que permitem que a simulação corra em tempo real na gpu. de acordo com a literatura, o método de indexação z-order e o hash map são as estruturas mais adequadas para este fim. pretende-se ver o seu impacto e em que situações um deles será mais adequado do que o outro. com a referida implementação, vários testes foram realizados com o propósito de analisar a robustez e estabilidade do método. com esses testes foi possível comparar as duas estruturas de dados utilizadas. o sph implementado mostrou resultados realistas e robustos na maioria dos casos. sendo capaz de lidar com várias cenas de vários níveis de complexidade. apesar dos bons resultados, mostrou algumas dificuldades em manter a estabilidade em algumas fronteiras (fronteiras com grande curvatura ou extremidades bicudas) e também apresentou algumas dificuldades em cenas com dois fluidos com densidades diferentes. quanto às estruturas de dados, foi possível observar que ambas são eficientes e suportam simulações com mais de 1 milhão de partículas (utilizando uma nvidia rtx 3080). no caso do z-order, este mostrou-se o método com melhor desempenho quando comparado com o hash map nas mesmas condições, ou seja, cenas com o mesmo número de partículas e o mesmo volume de simulação. isso deve-se à maior localidade de dados que o z-order possui. por outro lado, o hash map foi um pouco mais lento (quando comparado com o z-order nas mesmas condições), mas permitiu maior liberdade ao criar a cena. ao comparar os dois métodos com o mesmo número de partículas, mas com diferentes volumes de simulação, podemos ver que o hash map alcança o z-order, em termos de performance, à medida que as partículas se espalham pela simulação. com as duas estruturas de dados analisadas é possível tirar algumas conclusões. o método de z-order é recomendado quando temos um volume de simulação limitado e relativamente pequeno. caso não haja um volume de simulação, ou seja muito grande, recomenda-se o uso de um hash map, pois o défice de desempenho parece desaparecer à medida que o volume da simulação aumenta e as partículas se espalham pelo mesmo."
    ],
    0.3
  ],
  [
    [
      "the future of vehicles is for them to become smarter: able to perceive the its surroundings, detect dangerous situations, and act accordingly. to realize this vision, vehicles must collect information and share it with others, allowing them to have shared knowledge of an event that their sensors could not yet detect. however, some vehicles may not have enough computational resources to process the information and comply with low-delay requirements, and may need to offload the data to edge computing platforms to extend their own processing capabilities. while offloading, a vehicle may have multiple access networks at their disposal and by choosing the best network it may maximize the amount of data it can offload. these access networks include the mobile 5g network and wi-fi access points, which the operator can integrate within the 5g system to take advantage of unlicensed spectrum in the 2.4, 5 and 60 ghz ranges. in the current work, we focused on leveraging wi-fi and allowing vehicles to decide which access network to use among three wi-fi 802.11n/ac/ad networks. we developed a wi-fi performance monitoring and decision-making system (wiperf) that can: (i) collect throughput measurements and channel state information for multiple wi-fi networks; (ii) estimate throughput using passive measurements; (iii) predict the next 40 seconds of throughput; (iv) decide which network to use, based on the throughput forecasts and on the time it takes to switch to another network. the system was implemented in a real-world setup with two vehicles and two tp-link talon ad7200 access points. we performed an initial set of experiments to collect a dataset to develop estimation and forecasting models, and a second set of experiments to validate our decision-making system. for throughput estimation, we developed the ukf-sr model, a novel approach that combines symbolic re gression with a non-linear recursive bayes filter. results showed that our solution was superior to nn, dt, and rf models, with these having higher rmse values by at least 4.94 %, 38.09 %, and 9.59 % for 802.11n/ac/ad, respectively. considering forecasting, we adapted previously developed spatial-clustering models, that forecast throughput based on a set of similar historic samples, and compared them with a time-series approach, us ing arima and var models. var showed the best results among the time-series models, but they were still outclassed by spatial-clustering, considering both mae and mase values. we integrated the estimation and forecasting models with a decision-making algorithm. the algorithm sched ules which networks the vehicle should use considering the time if takes to switch networks, to maximize the amount of data it can offload. compared with the optimal solution, based on the real throughput measurements and without forecasting (perfect prediction), the results show that our approach has near-optimal performance with an average throughput of only 4.43 % less than the optimal one.",
      "o futuro dos veículos passa por torná-los inteligentes: capazes de analisar o meio envolvente, detetar situações de perigo e agir de forma adequada. para concretizar esta visão, os veículos devem recolher informação e partilhá-la com os restantes, permitindo que todos tenham conhecimento partilhado de um evento que os seus sensores ainda não detetaram. no entanto, alguns veículos podem não dispor de recursos computacionais suficientes para processar a informação e cumprir com os requisitos de baixos atrasos, tendo como alternativa o envio de dados para plataformas de computação de borda, ampliando as suas capacidades de processamento. durante a transmissão, um veículo pode ter várias redes de acesso disponíveis e através da escolha da melhor rede pode maximizar a quantidade de dados que pode transmitir. estas redes de acesso incluem a rede 5g e pontos de acesso wi-fi, que a operadora pode integrar no sistema 5g para tirar proveito de espectro não licenciado nas gamas dos 2.4, 5 e 60 ghz. no presente trabalho, focamo-nos em utilizar wi-fi e permitir aos veículos escolherem qual a melhor rede a utilizar entre três redes wi-fi 802.11n/ac/ad. desenvolvemos um sistema de monitorização e tomada de decisão para redes wi-fi (wiperf) capaz de: (i) medir a taxa de transmissão e recolher informação do estado do canal para várias redes wi-fi; (ii) estimar a taxa de transmissão com base em medições passivas; (iii) prever a taxa de transmissão para os próximos 40 segundos; (iv) decidir qual a rede a utilizar, com base na previsão da taxa de transmissão e no tempo que demora a trocar de rede. o sistema foi implementado num cenário real, com dois veículos e dois pontos de acesso tp-link talon ad7200. realizamos um primeiro conjunto de experiências para recolher dados para desenvolver os modelos de estimação e previsão, e um segundo para validar o sistema de tomada de decisão. para estimar a taxa de transmissão, desenvolvemos o modelo ukf-sr, uma nova abordagem que junta regressão simbólica com filtros bayesianos recursivos e não-lineares. os resultados mostram que a nossa solução teve um melhor desempenho do que os modelos nn, dt e rf, sendo que estes apresentam valores de rmse superiores, de pelo menos 4.94 %, 38.09 % e 9.59 % para 802.11n/ac/ad, respetivamente. para a previsão adaptamos modelos de clustering espacial, que preveem a taxa de transmissão com base num conjunto de amostras históricas, com uma abordagem baseada em séries temporais, usando modelos arima e var. os modelos var tiveram os melhores resultados dentro dos modelos de séries temporais, mas foram superados pelos modelos de clustering espacial tendo em conta as métricas mae e mase. integramos os modelos de estimação e previsão juntamente com um algoritmo de tomada de decisão. o algoritmo planeia quais as redes que o veículo deve utilizar considerando o tempo associado a trocar de rede, para maximizar a quantidade de dados que poderá transmitir. comparativamente com a solução ótima, baseada nas medições reais da taxa de transmissão e sem previsão (previsão perfeita), os resultados mostram que a nossa abordagem tem um desempenho quase-ótimo, atingindo uma taxa de transmissão média de apenas menos 4.43 % do que a ideal."
    ],
    [
      "risk assessment is an important topic for financial institution nowadays, especially in the context of loan applications or loan requests and credit scoring. some of these institutions have already implemented their own custom credit scoring systems to evaluate their clients’ risk supporting the loan application decision with this indicator. in fact, the information gathered by financial institutions constitutes a valuable source of data for the creation of information assets from which credit scoring mechanisms may be developed. historically, most financial institutions support their decision mechanisms on regression algorithms, however, these algorithms are no longer considered the state of the art on decision algorithms. this fact has led to the interest on the research of new types of learning algorithms from machine learning able to deal with the credit scoring problem. the work presented in this dissertation has as an objective the evaluation of state of the art algorithms for credit decision proposing new optimization to improve their performance. in parallel, a suggestion system on credit scoring is also proposed in order to allow the perception of how algorithm produce decisions on clients’ loan applications, provide clients with a source of research on how to improve their chances of being granted with a loan and also develop client profiles that suit specific credit conditions and credit purposes. at last, all the components studied and developed are combined on a platform able to deal with the problem of credit scoring through an experts system implemented upon a multi-agent system. the use of multi-agent systems to solve complex problems in today’s world is not a new approach. nevertheless, there has been a growing interest in using its properties in conjunction with machine learning and data mining techniques in order to build efficient systems. the work presented aims to demonstrate the viability and utility of this type of systems for the credit scoring problem.",
      "hoje em dia, a análise de risco é um tópico importante para as instituições financeiras, especialmente no contexto de pedidos de empréstimo e de classificação de crédito. algumas destas instituições têm já implementados sistemas de classificação de crédito personalizados para avaliar o risco dos seus clientes baseando a decisão do pedido de empréstimo neste indicador. de facto, a informação recolhida pelas instituições financeiras constitui uma valiosa fonte de dados para a criação de ativos de informação, de onde mecanismos de classificação de crédito podem ser desenvolvidos. historicamente, a maioria das instituições financeiras baseia os seus mecanismos de decisão sobre algoritmos de regressão. no entanto, estes algoritmos já não são considerados o estado da arte em algoritmos de decisão. este facto levou ao interesse na pesquisa de diferentes algoritmos de aprendizagem baseados em algoritmos de aprendizagem máquina, capaz de lidar com o problema de classificação de crédito. o trabalho apresentado nesta dissertação tem como objetivo avaliar o estado da arte em algoritmos de decisão de crédito, propondo novos conceitos de optimização que melhorem o seu desempenho. paralelamente, um sistema de sugestão é proposto no âmbito do tema de decisão de crédito, de forma a possibilitar a perceção de como os algoritmos tomam decisões relativas a pedidos de crédito por parte de clientes, dotando-os de uma fonte de pesquisa sobre como melhorar as possibilidades de concessão de crédito e, ainda, elaborar perfis de clientes que se adequam a determinadas condições e propósitos de crédito. por último, todos os componentes estudados e desenvolvidos são combinados numa plataforma capaz de lidar com o problema da classificação de crédito através de um sistema de especialistas, implementado como um sistema multi-agente. o uso de sistemas multi-agente para resolver problemas complexos no mundo de hoje não é uma nova abordagem. no entanto, tem havido um interesse crescente no uso das suas propriedades, em conjunto com técnicas de aprendizagem máquina e data mining para construir sistemas mais eficazes. o trabalho desenvolvido e aqui apresentado pretende demonstrar a viabilidade e utilidade do uso deste tipo de sistemas no problema de decisão de crédito."
    ],
    0.0
  ],
  [
    [
      "atualmente vivemos numa sociedade que sofreu uma grande explosão tecnológica ao longo dos anos. graças a isso, muitos progressos ocorreram e como um dos resultados, surgiram os documentos de identificação em formato digital. neste momento, na união europeia, vários estados-membros possuem uma infraestrutura nacional que permite a emissão de documentos digitais e métodos que permitem autenticação digital, como no caso de portugal com a infraestrutura autenticacao.gov. mas as aplicações e infraestuturas desenvolvidas por cada estado-membro apenas funcionam num contexto nacional. para resolver esse problema, foi apresentado um projeto piloto denominado por european digital identity wallet (eudiw). a eu digital identity wallet, permitirá que os cidadãos e as empresas europeias partilhem dados de identificação de uma forma segura e conveniente. um dos formatos propostos para os documentos digitais é o selective disclosure jwt (sd-jwt), uma variante de json web token (jwt) que possibilita que alguns dos atributos presentes nele podem ser seletivamente divulgáveis, possibilitando ao titular do jwt o controlo de quais atributos deseja partilhar com as diferentes relying parties que o cidadão interagir. nesta dissertação será apresentado o serviço que suportará a emissão dos documentos e atestados digitais, eudi wallet provider backend, nomeadamente a componente que possibilitará a emissão desses documentos em formato sd-jwt em conformidade com as regulamentações e standards que deverão ser cumpridas para certificar a salvaguarda e controlo do dados por parte do utilizador final.",
      "we currently live in a society that has undergone a major technological explosion over the years. thanks to this, a lot of progress has been made and one of the results has been the emergence of identification documents in digital format. at the moment, in the european union, several member states have a national infrastructure that allows digital documents to be issued and methods that allow digital authentication, as in the case of portugal with the autenticacao.gov infrastructure. but the applications and infrastructures developed by each member state only work in a national context. to solve this problem, a pilot project called the european digital identity wallet (eudiw) was presented. the eu digital identity wallet will allow european citizens and businesses to share identification data in a secure and convenient way. one of the formats proposed for digital documents is the selective disclosure jwt (sd-jwt), a variant of the json web token (jwt) that allows some of the attributes present in it to be selectively disclosed, enabling the holder of the jwt to control which attributes they wish to share with the different relying parties that the citizen interacts with. this dissertation will present the service that will support the issuing of digital documents and certificates, eudi wallet provider backend, namely the component that will make it possible to issue these documents in sd-jwt format in compliance with the regulations and standards that must be met in order to certify the safeguarding and control of the data by the end user."
    ],
    [
      "in this thesis, a systematic review was conducted on the study of the use of vrss. vr is an immersive technology capable of simulating real life events through image, sound and headed mounted devices or technologies such as windows kinnect. these technologies can be used to evaluate the performance and evolution of iadls in older adults. an electronic data search was conducted, during january 2022. the final analysis includes 12 studies with 285 participants in total. the use of vrss is an innovative and feasible technique to support and improve the functional autonomy of older adults living in the community compared to conventional treatment. between 20% to 25% of community-dwelling people over 75 years old have limitations in the ability to perform adls. the ability to perform adls is extremely important as it enables individuals to have a good quality of life by creating a sense of competence, self-esteem, confidence, identify and realisation. in this thesis we present the concept of structural cognitive training, in which cognitive training tasks (executive functions and cognitive abilities) are combined with training of instrumental activities of daily living. the methodology adequacy is assessed by the design of a digital game to train older adults to conduct iadls.",
      "nesta tese, foi realizada uma revisão sistemática sobre o estudo do uso de sistemas de realidade virtual. a realidade virtual é uma tecnologia imersiva capaz de simular eventos da vida real através de imagem, som e head-mounted devices ou tecnologias como o windows kinnect. estas tecnologias podem ser utilizadas para avaliar o desempenho e a evolução dos iadls em adultos mais velhos. foi realizada uma pesquisa electrónica de dados, durante o mês de janeiro de 2022. a análise final inclui 12 estudos com 285 participantes no total. a utilização de sistemas de realidade virtual é uma técnica inovadora e viável para apoiar e melhorar a autonomia funcional dos adultos mais velhos que vivem na comunidade, em comparação com o tratamento convencional. entre 20% a 25% dos habitantes da comunidade com mais de 75 anos de idade têm limitações na capacidade de realizar adls. a capacidade de realizar adls é extrema mente importante, pois permite que os indivíduos tenham uma boa qualidade de vida, criando um sentido de competência, auto-estima, confiança, identificação e realização. nesta tese apresentamos o conceito de treino cognitivo estrutural, na qual as tarefas de treino cognitivo (funções executivas e capacidades cognitivas) são combinadas com o treino de actividades instrumentais da vida quotidiana. a adequação da metodologia é avaliada através da concepção de um jogo digital para treinar os adultos mais velhos a realizar actividades instrumentais da vida quotidiana."
    ],
    0.3
  ],
  [
    [
      "nitrogen is a fundamental element for all organisms. it is nevertheless predominantly found in the atmosphere, in the form of unreactive nitrogen. in the last century, a manmade method for nitrogen fixation improved the crops yield, fuelling a populational growth. the exponential increase of anthropogenic nitrogen in soils and water bodies has, however, affected the environment and deregulated the natural biogeochemical nitrogen cycle. currently, the costs of repairing the damage caused by the reactive nitrogen load from human activities have overcome the profits of the agricultural improvement, derived from the application of fertilizers. wastewater treatment plants remove the excessive amounts of nutrients such as carbon, nitrogen and phosphorus from wastewater to prevent environmental impacts derived from excessive nitrogen in the biosphere, like eutrophication. the current conventional wastewater treatment applied is nitrification coupled with denitrification. however, the requirement for an external carbon source and aeration render this process costly. furthermore, one of the intermediates of denitrification is nitrous oxide, a greenhouse gas with an effect three hundred times worse than carbon dioxide and with a lifespan of one hundred and twenty years in the ozone layer. the partial nitrification/anammox (pna) process combines aerobic ammonium oxidation with anaerobic ammonium oxidation while suppressing the activity of nitrite oxidizing bacteria. this efficient process of nitrogen removal from wastewater reduces the aeration cost and the need for external carbon with zero nitrous oxide emissions. unknown microbial interactions may, on the other hand, impair this process, resulting in suboptimal performance such as, excessive nitrate and nitrous oxide emissions. to better understand the microbial community and its interactions and to find the causes of the treatments instability, metabolic analysis and genomic annotation was performed, using two complementary binning methods. the biological samples used in this study were retrieved from a high-rate pna sequencing batch reactor, fed with carbon-free ammonium-rich synthetic wastewater. fifty-seven draft genomes making up about eighty percent of the total community metagenome were recovered. in addition to the three genomes each from nitrosomonas and candidatus brocadia, several genomes belonged to proteobacteria, chloroflexi, planctomycetes, bacteroidetes, armatimonadetes, ignavibacteriae, acidobacteria, chlorobi, verrucomicrobia, actinobacteria and gemmatimonadetes phyla. in this study, the heterotrophic organisms encoding partial denitrification could be divided into niches accordingly to their role in this pathway, describing their interactions as a community. the complexity of the community was also ascertained with the discovery of putative heterotrophic hydroxylamine oxidizing bacteria and putative heterotrophic nitrite oxidizing bacteria. overall, high quality genomes that constitute a high fraction of the metagenome were recovered, allowing for a precise description of the pna reactors community and the flow of nitrogen oxides. a complex community with high redundancy was uncovered basing the main interactions on the partitioning of the nitrogen oxides respiratory pathway.",
      "o azoto é um elemento essencial para todos os organismos. no entanto, este é predominantemente encontrado na atmosfera, sob a forma de azoto não-reativo. no século passado, um método artificial para a fixação de azoto melhorou o rendimento das culturas, alimentando um crescimento populacional. o aumento exponencial do azoto antropogénico nos solos e massas de água afetou o meio ambiente e desregulou o ciclo natural de azoto. atualmente, os custos de reparação dos danos causados pela inserção excessiva de azoto reativo, fruto das atividades humanas, superam os lucros da melhoria na produção agrícola, derivada da aplicação de fertilizantes. as centrais de tratamento de águas residuais eliminam quantidades excessivas de nutrientes, como o carbono, azoto e fosforo das águas residuais, para prevenir impactos ambientais derivados do excesso de azoto na biosfera, como a eutrofização. o tratamento convencional de águas residuais aplicado é a nitrificação juntamente com a desnitrificação. no entanto, a necessidade de uma fonte externa de carbono e de aeração tornam esse processo bastante caro. além disso, um dos intermediários da desnitrificação é protóxido de azoto, ou óxido nitroso, um gás de efeito de estufa com um efeito trezentas vezes pior que o dióxido de carbono, e uma vida útil na camada de ozono de cento e vinte anos. o processo de nitrificação parcial /anammox (pna) comina a oxidação aeróbica de amónia com a oxidação anaeróbica de amónia enquanto a atividade de bactéria que oxidam nitrito é suprimida. este eficiente processo de remoção de azoto de águas residuais reduz o custo de aeração, não requer uma fonte de carbono e gera zero emissões de protóxido de azoto. as interações microbianas desconhecidas podem, por outro lado, prejudicar este processo, resultando num desempenho inferior, como produção excessiva de nitrato e emissão de protóxido de azoto. para entender melhor a comunidade microbiana, as suas interações e as causas da instabilidade do tratamento, foi realizada uma análise metagenómica seguida de anotação genómica, utilizando dois métodos complementares de binning. as amostras biológicas utilizadas como dados neste estudo foram obtidas de um reator de pna de alta performance, alimentado com águas residuais sintéticas ricas em amónio sem adição de carbono. foram recuperados cinquenta e sete genomas que compõem cerca de oitenta por cento do metagenoma. além dos três genomas de cada de nitrosomonas e candidatus brocadia, vários genomas pertenciam aos filos de proteobacteria, chloroflexi, planctomycetes, bacteroidetes, armatimonadetes, ignavibacteriae, acidobacteria, chlorobi, verrucomicrobia, actinobacteria e gemmatimonadetes. neste estudo, os organismos heterotróficos que codificam a desnitrificação parcial podem ser divididos em grupos de acordo com o seu papel nesta via metabólica, descrevendo as suas interações na comunidade. a complexidade da comunidade também foi verificada com a descoberta das bactérias heterotróficas putativas que oxidam hidroxilamina e as bactérias heterotróficas putativas que oxidam nitrito. em geral, foram recuperados genomas de alta qualidade que constituem uma grande fração do metagenoma, permitindo uma descrição precisa da comunidade do reator e o fluxo de óxidos de azoto. uma comunidade complexa com enorme redundância e com as suas principais interações baseadas na partição da via respiratória de óxidos de azoto."
    ],
    [
      "there always have been a huge interest in working with public data from online social media users, with the exponential growth of social media usage, this interest and re searches on the area keep increasing. this thesis aims to address prediction and classification tasks on online social net work data. the goal is to predict psycho-demographic - personality and demographic - traits by doing text emotion analysis on social networks as twitter and facebook. our main motivation was to raise awareness to what can be done with users’ social media or network information or usual behaviours on the web, such as from text analysis we can trace their personality, know their tastes, how they behave and so on, and to spread the emotion-text relation on social networks subject, because it only started to be studied recently and there’s so much data and information to do it. to perform these tasks mentioned above we carried an extensive review of literature of previous works to define the state-of-art of the project and to learn and identify work strategies. almost all of the past researches, based their results on a vast sample of users and data, but because some frameworks and apis were shutdown in recent years, such as mypersonality from facebook adding to some frameworks being paid for, resulted in a small sample of users’ data to analyze in our thesis which can prejudice the results. we start by gathering data from twitter and facebook with users consent. on twit ter we focused on tweets and retweets, on facebook we focused on all of what the user typed by using the dataselfie plugin that stored all that data on a server that can be retrieved later. our next step was to find emotions on their text data with the help of a lexicon that categorized words by eight different emotions, two of them were put away because we focused only on the six major emotions - this is explained later - and we had to remove stopwords and apply stemming to all of the text and do a word-matching of every word of our data with every word from the lexicon. after this, we asked our participants to fulfill a \"big-five\" personality questionnaire and to provide us their age, so we added the big-five traits and age to each users individual dataset. we got their final versions, ready to apply machine-learning algorithms to find correlations between emotions and personality or demographic attributes. we focused on practical and methodological aspects of the user attribute prediction task. we used many techniques and algorithms that we thought it were best fit for the data we had and for the goal that we had to achieve. we gathered data in two datasets that we tested, one of them we called \"mixed lan guage dataset\", contains all text entries from each user, and the other \"user dataset\", contains one entry per user after we analyze every text entry for all users in order to have a more general view on each one. for the first mentioned dataset we achieve best results with the decision trees algorithms, from 58% on the agreeableness trait, to 68% on the neuroticism trait. this dataset had a problem with the way data was spread, so it was impossible to predict age and gender with efficiency. as for the lat ter, regarding demographic characteristics all of the classifiers had a good classifying percentage, from k-nearest’s 73% to naive bayes’ 95%. the most solid classifier for personality traits was the one using the cart decision tree algorithm, it ranged from 50% on the openness trait to 76% on the agreeableness one. there were classifiers with terrible results, there were others that were a bit dull, and there were some that stood out as we stated above. we had a small sample, and that was a problem as it wasn’t consistent or solid in terms of data value and that can change our results, we believe that our results would be way better if we applied the same mechanisms to a much bigger sample. concluding, we demonstrate how we can predict personality or demographic traits - bigfive traits, age or gender - from studying emotions in text. as stated above, we hope this thesis will alert people for what can be done with their online information, we only focus on psycho-demographic profiling, but there are many other things that can be done.",
      "sempre houve um enorme interesse em trabalhar com dados públicos dos utilizadores das redes sociais online, com o crescimento exponencial do uso das redes sociais, esse interesse e pesquisas na área continuam a crescer imenso. esta tese tem como objetivo abordar tarefas de previsão e classificação de dados de redes sociais online. o objetivo é prever traços psico-demográficos - de personalidade e demográficos - fazendo análises de emoções presentes no texto em redes sociais como twitter e facebook. a nossa principal motivação foi consciencializar os utilizadores sobre o que pode ser feito com as informações dos utilizadores ou com os seus comportamentos na web, por exemplo, com a análise de texto, podemos traçar a sua personalidade, conhecer os seus gostos, saber como eles se comportam e assim por diante, e para espalhar a relação texto-emoções nas redes sociais, porque só começou a ser estudado recentemente e há imensos dados e informações para isso. para realizar essas tarefas mencionadas acima, realizamos uma extensa revisão da literatura de trabalhos anteriores para definir o estado da arte do projeto, aprender e identificar estratégias de trabalho. quase todas as pesquisas anteriores basearam os seus resultados numa vasta amostra de utilizadores e dados, mas como algumas frameworks e apis foram encerradas nos últimos anos, como a mypersonality do facebook, adicionando a algumas frameworks que são pagas, o resultado foi que na nossa tese tivemos uma pequena amostra de dados de utilizadores para analisar o que pode prejudicar os resultados. começamos por recolher os dados do twitter e do facebook com o consentimento dos utilizadores. no twitter, concentramo-nos nos tweets e retweets, no facebook concentramo-nos em tudo o que o utilizador digitou usando o plugin dataselfie que armazena todos os dados num servidor que podem ser recuperados mais tarde. o nosso passo seguinte foi encontrar emoções no texto digitado por cada utilizador com a ajuda de um léxico que categoriza palavras por oito emoções diferentes, duas dessas emoções foram descartadas, concentrando-nos apenas nas seis principais emoções - o processo é explicado mais tarde - e tivemos que remover as stopwords e aplicar stemming a todo o texto e fazer uma correspondência de cada palavra dos nossos dados com cada palavra do léxico. depois disto, pedimos aos nossos participantes que preenchessem um questionário de personalidade \"big-five\" e nos dessem a conhecer a sua idade. adicionamos as 5 características do \"big-five\" e a idade ao dataset individual de cada utilizador e obtivemos as suas versões finais, prontas para aplicar algoritmos de aprendizagem de máquina para encontrar correlações entre as emoções e personalidade ou atributos demográficos. focamo-nos nos aspectos práticos e metodológicos da tarefa de predição e classificação de atributos do utilizador. muitas técnicas e algoritmos foram utilizados, aqueles que consideramos mais adequados para os dados que tínhamos e o objetivo que tínhamos que alcançar. obtemos dados para dois datasets diferentes que testamos no final, um deles chamado de \"mixed language dataset\", contém todas as entradas de texto de cada utilizador e o outro \"user dataset\" contém uma entrada por utilizador após analisarmos todas as entradas de texto de todos eles para ter informação mais concisa geral sobre cada um. para o primeiro conjunto de dados mencionado, os melhores resultados obtidos foram com os algoritmos de árvores de decisão, de 58% na característica de agreabilidade, para 68% na característica de neuroticismo. este conjunto de dados tinha um problema com a forma como os dados estavam compostos no dataset, por isso foi impossível prever idade e género com eficiência. quanto ao último dataset, em relação às características demográficas, todos os classificadores tiveram uma boa percentagem de classificação, de 73% de k-nearest para 95% com naive bayes. o classificador mais sólido para os traços de personalidade foi o que usou o algoritmo de árvore de decisão, cart, que varia apenas entre 50% no traço de \"abertura a experiências\" e 76% no de agreabilidade. tivemos classificadores com resultados terríveis, houve outros que foram um pouco \"aborrecidos\", e houve alguns que se destacaram como afirmamos acima. a nossa amostra era consideravelmente pequena e isso foi um problema para nós, pois não era consistente ou sólido em termos de valores de dados e isso provavelmente alterou alguns dos nossos resultados, com uma amostra bem maior, mais profunda, acreditamos que aplicando os mesmos processos e mecanismos, teríamos resultados mais sólidos e mais consistentes. concluindo, demonstramos como é possível prever traços de personalidade ou demográficos - traços bigfive, idade ou género - a partir do estudo de emoções presentes em texto. como foi dito acima, esperamos que esta tese permita que os utilizadores tenham mais consciência da importância dos seus dados e do que conseguimos atingir com eles."
    ],
    0.11249999999999999
  ],
  [
    [
      "public lighting, despite being one of the most important services a city must provide, it is also one of the main sources of energy consumption and consequently running costs. one way to drastically improve the efficiency of these systems is by using object detection in order to make each of the public lighting poles aware of their surroundings, giving them the ability to adapt to the environment. this is one of the main goals of an industry leader company's project in the area of public lighting, where this study is inserted. in this project, each public lighting pole is upgraded with a set of sensors: radar, microphone and camera (what we refer to as fog computing). by developing a multimodal machine learning model, the goal is to leverage the data from the different sensors to improve the object detection capabilities of traditional unimodal machine learning model. additionally, the developed model will be deployed into an edge device that is also installed in the public lighting pole, due to data privacy concerns and network latency problems that would otherwise occur with traditional server-side approaches. this constrain raises the main question that this study will try to answer, which is how to develop a complex multimodal machine learning model for low-power efficient edge devices. in this study, a multi-agent architecture will be proposed, that authors can adapt to their own multimodal machine learning problems with edge devices. to prove the efficient of the proposed system, a proof of concept implementation will be carried out that involves the aforementioned sensors, as well as the you only look once (yolo) object detection model, with a feature-level data fusion approach. finally, the implemented system will be deployed to an edge device, where the hardware performance will be tested and compared to similar work in the literature.",
      "a iluminação pública, apesar de ser um dos serviços mais importantes que uma cidade deve prestar, é também uma das principais fontes de consumo de energia e, consequentemente, de custos de funcionamento. uma das formas de melhorar drasticamente a eficiência destes sistemas é através da utilização da deteção de objetos, de modo que cada um dos postes de iluminação pública conheça o ambiente que lhe rodeia, dando-lhes a capacidade de se adaptarem a mudanças no ambiente. este é um dos principais objetivos de um projeto de uma empresa líder de mercado na área da iluminação pública, onde se insere este estudo. neste projeto, cada poste de iluminação pública é equipado com um conjunto de sensores: radar, microfone e câmara (o que designamos por fog computing). ao desenvolver um modelo de aprendizagem automática multimodal, o objetivo é aproveitar os dados dos diferentes sensores para melhorar as capacidades de deteção de objetos do modelo tradicional de aprendizagem automática unimodal. além disso, o modelo desenvolvido será deployed num dispositivo edge que também está instalado no poste de iluminação pública, devido a preocupações com a privacidade dos dados e a problemas de latência da rede que, de outro modo, ocorreriam com abordagens tradicionais do lado do servidor. esta limitação levanta a principal questão a que este estudo tentará responder, que é como desenvolver um modelo complexo de aprendizagem automática multimodal para dispositivos edge eficientes e com capacidade computacional limitada. neste estudo, será proposta uma arquitetura multi-agente, que os autores podem adaptar aos seus próprios problemas de aprendizagem automática multimodal com dispositivos edge. para provar a eficiência do sistema proposto, será realizada uma implementação de prova de conceito que envolve os sensores mencionados anteriormente, bem como o modelo de deteção de objectos you only look once (yolo), com uma abordagem de fusão de dados ao nível das características. por fim, o sistema implementado será deployed num dispositivo periférico, onde o desempenho do hardware será testado e comparado com trabalhos semelhantes na literatura."
    ],
    [
      "em plena revolução digital, seria inconcebível que o crescendo na procura de crédito ao consumo não fosse acompanhado pela maior preponderância que mecanismos como o scoring baseado em téc nicas de inteligência artificial representa para uma mais e melhor avaliação da solvabilidade em sede de decisões automatizadas. vislumbra-se o começo de um mundo novo em que a aceitação ou rejeição no pedido de empréstimo já não é tão-só conatural a decisões de puro julgamento dos analistas. e se, por um lado, a possível integração de megadados de crédito em modelos analíticos pode, até certo ponto, minorar o esforço das análises preditivas, em contramão, maiores serão os riscos de preterição do rango de tutela do bem jurídico proteção de dados, tal-qual prescrito no (ou pelo) direito dos livros. de facto, tópicas como esta exibem uma complexidade tal que é impossível olvidar debruçar-se sobre as mesmas tendo por base uma única lente jurígena, sem se ampliar o pendor multidisciplinar que a uma investiga ção desta índole deve subjazer. como tal, principiara-se o presente ensaio com o discorrer da temática em contexto histórico e sociotécnico. e porque a concessão de crédito ao consumo é, no atual estado de arte scoring, não só comandada pelos modelos de exploração e aprendizagem automática, mas antes, e sobretudo, pela qualidade dos conjuntos de dados que os alimentam, ainda em contexto prolegómeno, epitomaram--se os elementos-chave que concorrerão para um scoring de crédito mais exato e, espera- -se, cada vez mais transparente. foi, portanto, a partir destas premissas técnico-científicas que se abor dou a figura jurídica dos contratos de crédito ao consumo, com especial enfoque na contratação rápida em linha e nas diligências pré-contratuais concernentes à injuntividade da avaliação de solvabilidade. em última instância, porquanto o objeto de estudo suscita uma energética (ou antes, idiossincrática) tutela sobre a proteção de dados pessoais, foi à luz do emaranhado axiológico-normativo do regime das deci sões individuais e exclusivamente automatizadas, incluindo a definição de perfis, bem como das quimé ricas salvaguardas adequadas (ou antes, ilusórias) consagradas e sugeridas pelo legislador europeu, tanto no regulamento geral sobre proteção de dados pessoais, como na proposta de diretiva, relativa aos créditos aos consumidores, de 30 de junho de 2021, que se conclui a necessária adoção de políticas setoriais que primem por uma lhana infoliteracia financeira dos (ciber)consumidores.",
      "amid the digital revolution, it would be inconceivable that the greater preponderance would not accompany the growth in demand for consumer credit that mechanisms such as scoring based on artificial intelligence represent for a more and better assessment of creditworthiness in the background of exclusive automated decisions. we are glimpsing the beginning of a new world in which the acceptance or rejection of a loan application is no longer only con-natural to the pure judgemental decisions of analysts. and if, on the one hand, the possible integration of credit big data in analytical models may, to a certain extent, lessen the effort of predictive analyses, on the other hand, the risks of undermining the protection of the legal value of data protection are more noteworthy, as commonly stated in (or by) the law in books. in fact, topics such as credit scoring exhibit such complexity that it is impossible to avoid dealing with it based on a single legal lens, that is, without broadening the multidisciplinary nature which should underlie an investigation of this nature. as a result, this dissertation will begin with a discussion of the subject in its historical and socio-technical framework. and because consumer credit scoring, in its current state-of-the-art, is not only governed by exploitation and machine learning models but also, and above all, by the quality of the data sets that feed them, the key elements that will contribute to more accurate and, it is hoped, increasingly transparent credit scoring have been identified. therefore, based on these technical and scientific premises, the legal nature of consumer credit agreements was addressed, with a particular focus on online contracting and the pre-contractual stage concerning the injunctive nature of the solvency assessment. ultimately, the object of study raises an energetic (or somewhat idiosyncratic) tutelage on the protection of personal data. therefore, it was in the light of the axiological-normative framework of automated decision-making regime, including profiling, as well as the chimerical adequate (or rather, illusory) guarantees that the european legislator has enshrined and suggested - both in the general data protection regulation and in the proposal for a directive on consumer credits, of 30 june 2021 - that the conclusion traduces the necessity to adopt policies that strive for a better financial info literacy of (cyber)consumers."
    ],
    0.0
  ],
  [
    [
      "the development of cyber-physical systems (cpss) models is a complex process which requires deep multi-disciplinary knowledge of the intended topic to model. added to this complexity is the difficulty of combining multiple models, sometimes without access to their source code, and make them communicate in a harmonious and integrated way in order to represent the vicissitudes of the environment where the physical system is inserted into. functional mockup interface is a set of c headers that define a protocol that allows the interoperability of different models, independently of the programming languages and tools that generated them. a model that implements this interface is called functional mockup unit (fmu). this dissertation explores the usage of machine learning to generate automatically a fmu from parsing a dataset containing the inputs and outputs obtained during the observation of a physical system. a command-line interface (cli) tool named autofmu is also presented here, and it accepts as parameters a set of csv tables and the names of the column that correspond to the inputs and outputs, using several supervised learning algorithms to infer the relationships between these variables. its invocation results in a file containing a valid fmu ready to be used. in order to assess its feasibility in a real context, the tool autofmu was used to generate approximations of a controller of a line follower robot. the generated models were then simulated in the into-cps program and the robot movements under the purview of the new controller were observed. the values generated by the new models were also compared with the datasets of the original physical unit.",
      "o desenvolvimento de modelos de sistemas ciber-físicos é um processo complexo que exige profundos conhecimentos multi-disciplinares do tópico que se pretende modelar. a esta complexidade acresce ainda a dificuldade de combinar múltiplos modelos, por vezes sem acesso ao seu código fonte, e fazê-los comunicar de uma forma harmoniosa e integrada de forma a representar as vicissitudes do ambiente onde o sistema físico se insere. a functional mockup interface é um conjunto de cabeçalhos c que define um protocolo comum que permite a interoperabilidade de diferentes modelos, independente das linguagens de programação e ferramentas que os geraram. um modelo que implementa esta interface é chamado de fmu. esta dissertação explora a utilização de machine learning para gerar automaticamente um fmu a partir da análise de um conjunto de dados contendo os inputs e outputs obtidos durante a observação de um sistema físico. apresenta-se também uma ferramenta de linha de comandos de nome autofmu que aceita como parâmetros um conjunto de tabelas csv e os nomes das colunas que correspondem aos inputs e outputs, utilizando diversos algoritmos de aprendizagem supervisionada para deduzir as relações entre estas variáveis. da sua invocação resulta um ficheiro que contém um fmu válido pronto a ser utilizado. de forma a avaliar a sua viabilidade num contexto real, a ferramenta autofmu foi utilizada para gerar aproximações de um controlador de um robot que segue uma linha desenhada no chão. os modelos gerados foram depois simulados no programa into-cps tendo-se observado e comparado os movimentos efetuados pelo robot sob a alçada do novo controlador. os valores gerados pelos novos modelos foram também comparados com os datasets da unidade física original."
    ],
    [
      "atualmente, existe um forte crescimento das aplicações destinadas à aprendizagem de conteúdos no âmbito escolar. a proposta de dissertação, no âmbito do mestrado integrado em engenharia informática, presente neste documento, foca-se na criação e implementação de um tutor inteligente de modo a criar recursos para crianças do primeiro e segundo ciclos com o objetivo de monitorizar o progresso na aprendizagem desses mesmos conteúdos. o desenvolvimento desta aplicação visa ajudar os tutores e/ou encarregados de educação a planear o estudo dos seus tutorados, de acordo com as dificuldades ou necessidades apresentadas. o sistema desenvolvido acompanha não só o tutorado, mas também o seu tutor visto que são dadas sugestões e feitas análises de modo a agilizar o processo de criação de fichas, com o objetivo de os tutorados superarem as suas dificuldades, enquanto fazem uso das tecnologias. esta aplicação não só monitoriza a aprendizagem dos tutorados, como também tem presente uma componente para introdução de novo conteúdo escolar por parte dos tutores e/ou encarregados de educação, com o objetivo de armazenar grandes quantidades de dados, de modo a diminuir a ocorrência de perguntas semelhantes. relativamente ao sistema de monitorização, este apresenta dados estatísticos referentes às áreas do programa escolar em que o tutorado apresenta mais dificuldades, bem como as áreas em que este está mais à vontade, de modo a poder dinamizar o conteúdo das fichas para os seus intervenientes.",
      "actually, there is a strong growth of applications with learning propose in the school context. this dissertation, in the scope of the integrated master in informatics engineering presented in this document, focuses on the creation and implementation of an intelligent tutor system in order to create many resources for first and second cycle kids with the objective of monitoring your progress in learning context. the development of this application aims to help tutors and caregivers to plan the kids study, with difficulties and needs presented. this system follow not only the kid but also its tutor since there are made suggestions and analyzes in order to speed up the creating records process with the objective of the kids overcome their difficulties while use the technologies. this application not only monitors the tutors learning, but also has as a module for the introduction of new school content by tutors and parents with the purpose to store a large amounts of data, so as to reduce the occurrence of similar questions. this monitoring system presents statistical data referring to the areas of the school program in which the tutored presents the most difficulties, as well as the areas in which is more comfortable, in order to be able to improve the records content for yours stakeholders."
    ],
    0.3
  ],
  [
    [
      "hoje em dia, em qualquer unidade de saúde (hospitais, centros de saúde, clínicas, etc.), existem diversos equipamentos médicos, cada um com a sua complexidade. alguns destes equipamentos são cruciais para o tratamento de pacientes e requerem que a interação das equipas médicas com os mesmos seja precisa e exata, correndo-se o risco de, ao mínimo erro, causar danos fatais ao paciente. todos os dias, pelo mundo fora, ocorrem erros com dispositivos médicos e de dispositivos médicos. os erros com dispositivos médicos são causados devido a algum erro de utilização: uma sequência errada de botões, informação inserida em campos errados, falta de atenção às unidades de medida, etc. em alguns casos, estes erros ocorrem devido à falta de interação de um profissional com certo dispositivo. esta interação, pode ser reduzida, muitas vezes, devido a um dispositivo ter um custo elevado que não é justificável para um dispositivo disponível somente para testes. assim, as equipas médicas apenas interagem com o dispositivo quando um paciente precisa de cuidados médicos, o que pode ser perigoso caso um médico ou enfermeiro não esteja familiarizado(a) com o dispositivo. por outro lado, os erros de dispositivos médicos são causados por alguma falha no próprio dispositivo, não podendo ser evitada pelas equipas médicas, e que pode dever-se a uma má programação ou construção do dispositivo. uma solução, para a prevenção de erros na utilização de dispositivos médicos, passa pela simulação de dispositivos médicos, de modo a que os dispositivos possam ser tanto testados para prevenir eventuais erros de software na interface de utilizador que possam existir, como usados para a formação e treino de pessoal médico para diminuir os erros provocados na interação com os dispositivos. a presente proposta de dissertação, no âmbito do mestrado integrado em engenharia informática, visa tirar partido das aplicações móveis para a simulação de dispositivos médicos com que não podemos interagir todos os dias. com a criação destas simulações será possível ter inúmeras virtualizações de dispositivos médicos presentes nas unidades de saúde (centros de saúde, enfermarias, hospitais, clínicas, etc.) ao alcance de um tablet ou smartphone. isto irá também permitir uma maior mobilidade para simulações, permitindo aos utilizadores levar a cabo um treino diário ou semanal com um dispositivo simulado. passa também a haver uma maior mobilidade para quem desenvolve e cria protótipos de dispositivos inovadores, sendo que podemos apresentar a aplicação ao público alvo e ter um feedback instantâneo sobre os aspetos positivos e negativos do protótipo, bem como novas ideias para futuras funcionalidades que possam ser adicionadas ao protótipo.",
      "nowadays, in any health unit (hospitals, health centers, clinics, etc.), there are several medical equipment, each one with its complexity. some of these equipment are crucial for treatment of patients and require that the interaction of medical teams with them is precise and accurate, at the risk of causing fatal injuries to the patient at the slightest error. use errors with medical devices can be caused by human error: a wrong sequence of buttons, information entered in wrong fields, lack of attention to units of measurement, etc. various research studies revealed that these use errors are often caused by design errors in the user interface of the device, rather than negligence or lack of training of clinical practitioners. simulation of medical devices is a way to facilitate early detection of latent design errors. the same simulations can also be used for training of end users, to help them identify and avoid latent design problems that might affect a device, while waiting that the manufacturer fixes the problem. the present dissertation proposal, within the scope of the integrated master in computer science, aims to take advantage of the mobile applications for the simulation of medical devices with which we cannot interact every day. this interaction at the training level can be reduced, often because a device has a high cost and is not justifiable for a test-only device. with the creation of these simulations it will be possible to have numerous virtualizations of medical devices present in health facilities within a tablet or smartphone. this will also allow greater mobility for simulations, allowing practitioners to carry out daily or weekly training with a simulated device. it also gives a great advantage for those who develop and create prototypes of innovative devices. they can present the application to the target audience and have instant feedback on the positive and negative aspects of the prototype, as well as new ideas for future functionalities that can be added to the prototype."
    ],
    [
      "a televisão de hoje em dia disponibiliza um enorme lote de conteúdos. quando existe um leque de conteúdos de grande dimensão é difícil optar pela solução que mais nos agrada ou que melhor satisfaz as exigências dos espetadores. o tempo consumido na seleção pode mesmo esgotar o tempo que o espetador dispõe para visualizar o conteúdo selecionado. este facto influencia o nível de stresse nas pessoas. para tentar responder a este problema, recorreu-se a técnicas de inteligência artificial para a criação de um sistema inteligente que seja capaz de proporcionar ao utilizador uma melhor experiência televisiva, contribuindo para o seu bem estar e diminuição dos seus níveis de stress, reduzindo o tempo gasto.",
      "nowadays, television offers a large set of contents. when the viewers have a big range of content, it is difficult to choose something they really like. the time spent selecting the content may even consume all the time they have to watch the selected content. this fact influences the level of stress on people. to try to answer this problem, this project uses artificial intelligence techniques to create an intelligent system, providing a better tv experience and contributing to the decrease of stress levels on users, reducing the time consumed."
    ],
    0.06666666666666667
  ],
  [
    [
      "deoxyribonucleic acid (dna) is a biological macromolecule whose primary function is to store an individual’s genetic information. because of breakthroughs in sequencing technology, the number of dna sequences is now growing at an exponential rate. the assignment of a function to these sequences is a great obstacle in bioinformatics, and current methods rely on homologies, a solution that is slow and less accurate. machine learning (ml) has been widely employed as it is a relevant tool for processing huge amounts of data by learning on its own without explicit programming. using ml, it is now possible to speed up and automatically classify dna sequences into existing categories with the objective of learning their functions. however, building a machine learning classifier of biological sequences is a tough challenge due to the lack of numerical properties in the sequence that the model requires. therefore, it is still necessary to apply some pre-processing techniques so that the sequences are properly represented for the model. these techniques include feature extraction and feature selection, and they are the most difficult components because sequences lack explicit features. deep learning models have recently been developed that not only extract features from input automatically, but also improve the prediction and classification of dna sequences. the main goal of this project is to create a tool that can automatically classify dna sequences using machine and deep learning models and algorithms, followed by its integration into propythia, a python package developed by the host group. automated ml classifiers will also be developed to integrate in omniumai software platforms. transcription factor annotation and essential gene determination will be used as case studies for the platform validation. with this study, it is intended to encourage the use of such technologies to develop new tools that can manage vast volumes of biological data, thus boosting dna prediction understanding.",
      "o ácido desoxirribonucleico (adn) é uma macromolécula biológica cuja principal função é armazenar a informação genética de um indivíduo. devido aos avanços na tecnologia de sequenciamento, o número dessas sequências está a crescer a uma taxa exponencial. a atribuição de funções a estas sequências é um grande obstáculo na bioinformática, e os métodos atuais usam homologias, uma solução lenta e pouco precisa. machine learning tem sido bastante utilizado, pois é uma ferramenta capaz de processar grandes quantidades de dados aprendendo por conta própria sem programação explícita. desta maneira, é possível acelerar e classificar automaticamente as sequências de adn em categorias existentes com o objetivo de aprender as suas funções. no entanto, construir um classificador de machine learning de sequências biológicas é um grande desafio devido à falta de propriedades numéricas na sequência que o modelo exige. é necessário aplicar algumas técnicas de pré-processamento para que as sequências sejam devidamente representadas para o modelo. essas técnicas incluem extração e seleção de características, e são os componentes mais difíceis porque as sequências carecem de características explícitas. modelos de deep learning foram desenvolvidos recentemente que não só extraem características dos dados automaticamente, como também melhoram a previsão e classificação de sequências de adn. o principal objetivo deste projeto é criar uma ferramenta capaz de classificar automaticamente sequências de adn usando modelos e algoritmos de machine e deep learning, seguido da sua integração no propythia, um python package desenvolvido pelo grupo anfitrião. classificadores automáticos de machine learning também serão desenvolvidos para integração em plataformas de software omniumai. a determinação do fator de transcrição e de genes essenciais serão utilizados como casos de estudo para validação da plataforma. com este estudo, pretende-se incentivar o uso de tais tecnologias para desenvolver novas ferramentas que consigam lidar com grandes volumes de dados, permitindo avanços na área de previsão de adn."
    ],
    [
      "a utilização dos transportes rodoviários, nomeadamente o transporte rodoviário de pessoas, é um dos meios mais adoptados pela população para deslocação, tanto dentro da cidade como entre cidades. como é sabido de senso comum, existem horas e percursos mais críticos. estes, consequentemente, podem provocar desgastes ambientais e económicos tais como degradação do piso ou aumentos de consumos de combustível - que por sua vez aumentam as emissões de gases e também desgastes a nível pessoal, como mudanças de stress por estar no meio de um ambiente de trânsito intenso com possíveis horários a cumprir. este trabalho foca-se na recolha e na análise de dados através de dispositivos móveis como smartphones de forma a obter dados e informação que permita tirar conclusões sobre perfis de condução, caracterização de vias, e partilha de informação.",
      "roads and vehicular traffic are the most used travelling system by people so they can move in the city and between cities. as we know from common sense, there are some critical hours and routes. these can cause environmental and economic waste such as degradation of the pavement, and the increase of fuel consumption - which lead us to the increase of exhaust emissions. also the stress caused by these critical factors can be a problem. this study focus on gathering data from mobile devices, as smartphones and tablets, so we can obtain useful information to create driving pattern profiles, classify the road quality and share the information."
    ],
    0.0
  ],
  [
    [
      "i would like to thank the following people for their help and support over the course of the completion of this thesis. to joão saraiva, my supervisor, for his continued support and encouragement. to luís anjos and nuno vieira, my co-workers, that helped in everything i needed during my research time in primavera. to my parents, carlos and isabel, that, even though in my life i may have not done everything the way they wanted and wished, were always there for my best interest, caring for me, my life and my future. i feel eternal gratitude for the two of you. to my lovely sister, catarina, for giving me company even when i didn’t want it, for loving me unconditionally and for bringing a new cat into our household. life with kittens is always better than life without kittens! to my aunts, nocas, sissi and patrícia, that i am sure have always believed in me. to my grandfather, that age only made of him a more interesting, funny and reliable company. to my grandma, for all the love and food filled of love that fed me during all these years. to my girlfriend ana, for all the love, patience and understanding, and for staying by my side. to all my friends, who make my life outside of research so enjoyable. particularly, to my friends ricardo, renato, miguel, pedro nuno, bruno and sebastião, whose company has been a constant for many years. to zé and joão nuno, for the great time we have when we are together.",
      "primavera business software solution dedicou muito tempo e esforço nos últimos anos no desenvolvimento de uma framework que permite a um utilizador modelar uma aplicação e respectivos serviços. esta framework irá então gerar grande parte do código fonte, assim como a base de dados e o user interface. o objectivo deste projecto de dissertação é de adicionar uma nova função a esta framework - uma componente de automação de testes. desta forma, a framework será capaz de gerar casos de teste para validar os requisitos da aplicação. como gerar todas as possiveis combinações seria impraticável, um dos principais objetivos deste projeto é gerar um conjunto de casos de teste mais pequeno que possa garantir que o sistema está a ser a ser bem testado."
    ],
    [
      "the sampling of network traffic is a very effective method in order to comprehend the behaviour and flow of a network, essential to build network management tools to control service level agreements (slas), quality of service (qos), traffic engineering, and the planning of both the capacity and the safety of the network. with the exponential rise of the amount traffic caused by the number of devices connected to the internet growing, it gets increasingly harder and more expensive to understand the behaviour of a network through the analysis of the total volume of traffic. the use of sampling techniques, or selective analysis, which consists in the election of small number of packets in order to estimate the expected behaviour of a network, then becomes essential. even though these techniques drastically reduce the amount of data to be analyzed, the fact that the sampling analysis tasks have to be performed in the network equipment can cause a significant impact in the performance of these equipment devices, and a reduction in the accuracy of the estimation of network state. in this dissertation project, an evaluation of the impact of selective analysis of network traffic will be explored, at a level of performance in estimating network state, and statistical properties such as self-similarity and long-range dependence (lrd) that exist in original network traffic, allowing a better understanding of the behaviour of sampled network traffic.",
      "a análise seletiva do tráfego de rede é um método muito eficaz para a compreensão do comportamento e fluxo de uma rede, sendo essencial para apoiar ferramentas de gestão de tarefas tais como o cumprimento de contratos de serviço (service level agreements - slas), o controlo da qualidade de serviço (qos), a engenharia de tráfego, o planeamento de capacidade e a segurança das redes. neste sentido, e face ao exponencial aumento da quantidade de tráfego presente causado pelo número de dispositivos com ligação à rede ser cada vez maior, torna-se cada vez mais complicado e dispendioso o entendimento do comportamento de uma rede através da análise do volume total de tráfego. a utilização de técnicas de amostragem, ou análise seletiva, que consiste na eleição de um pequeno conjunto de pacotes de forma a tentar estimar, ou calcular, o comportamento expectável de uma rede, torna-se assim essencial. apesar de estas técnicas reduzirem bastante o volume de dados a ser analisado, o facto de as tarefas de análise seletiva terem de ser efetuadas nos equipamentos de rede pode criar um impacto significativo no desempenho dos mesmos e uma redução de acurácia na estimação do estado da rede. nesta dissertação de mestrado será então feita uma avaliação do impacto da análise seletiva do tráfego de rede, a nível do desempenho na estimativa do estado da rede e a nível das propriedades estatísticas tais como a long-range dependence (lrd) existente no tráfego original, permitindo assim entender melhor o comportamento do tráfego de rede seletivo."
    ],
    0.0
  ],
  [
    [
      "every program starts from a model, an abstraction, which is iteratively re ned until we reach the nal result, the implementation. however, at the end, one must ask: does the nal program resemble in anyway the original model? was the original idea correct to begin with? formal methods guarantee that those questions are answered positively, resorting to mathematical techniques. in particular, in this thesis we are interested on the second factor: veri cation of formal models. a trend of formal methods defends that they should be lightweight, resulting in a reduced complexity of the speci cation, and automated analysis. alloy was proposed as a solution for this problem. in alloy, the structures are described using a simple mathematical notation: relational logic. a tool for model checking, automatic veri cation within a given scope, is also provided. however, sometimes model checking is not enough and the need arises to perform unbounded veri cations. the only way to do this is to mathematically prove that the speci cations are correct. as such, there is the need to nd a mathematical logic expressive enough to be able to represent the speci cations, while still being su ciently understandable. we see the point-free style, a style where there are no variables or quanti cations, as a kind of laplace transform, where complex problems are made simple. being alloy completely relational, we believe that a point-free relational logic is the natural framework to reason about alloy speci cations. our goal is to present a translation from alloy speci cations to a point-free relational calculus, which can then be mathematically proven, either resorting to proof assistants or to manual proving. since our motivation for the use of point-free is simplicity, we will focus on obtaining expressions that are simple enough for manipulation and proofs about them.",
      "todos os programas partem de um modelo, uma abstração, que é iterativamente refinada até se atingir o resultado final, a implementação. no entanto, no m, não há garantia de a implementação representar o modelo original. nem sequer sabemos se o modelo inicial estava correcto. métodos formais garantem que estas questões são respondidas positivamente. nesta tese em particular estamos interessados no segundo factor: verificação de modelos formais. uma linha de pensamento nos métodos formais defende que estes devem ser leves, resultando numa diminuição da complexidade das especificações e na automação da sua análise. o alloy foi proposto como uma solução para este problema. em alloy, as estruturas são definidas usando uma notação matemática simples: lógica relacional. uma ferramenta para verificação de modelos, dentro de um dado limite, é também disponibilizada. no entanto, às vezes essa verificação não é suficiente, e surge a necessidade de efetuar verificações sem limites. a única maneira de fazer isto é provando matematicamente que as especificações estão correctas. sendo assim, surge a necessidade de encontrar uma lógica matemática que seja suficientemente expressiva para representar as especificações, mais ainda suficientemente compreensível. vemos o estilo\\point-free\", um estilo de programação sem variáveis, como uma espécie de transformada de laplace, onde problemas complexos se tornam mais simples. visto o alloy ser completamente relacional, acreditamos que um ambiente \\point-free\" relacional é o mais natural para lidar com as suas especificações. o nosso objectivo é apresentar uma tradução de especificações alloy para lógica relacional\\point-free\", onde podem ser matematicamente provadas, quer recorrendo a assistentes de prova, quer manualmente. visto a nossa motivação para o uso de \\point-free\" ser a simplicidade, vamos focar-nos na obtenção de expressões suficientemente simples para que possam ser manipuladas e verificadas."
    ],
    [
      "a capacidade de resolução de problemas pela inteligência artificial encontra-se em constante ex pansão, pelo que a otimização do seu funcionamento em sociedade está dependente da capacidade de extração das aplicabilidades da tecnologia. uma das áreas da inteligência artificial em que tem sido visível uma evolução significativa é a de machine learning, cujos algoritmos têm revelado um crescente nível de especialização na resolução de diversos problemas. na presente dissertação, pretendeu-se construir um modelo capaz de auxiliar na recomendação de configurações ideais para novas estações de carga para veículos elétricos, com a assistência de modelos de machine learning. a revisão da literatura revelou uma extensiva análise sobre problemas de previsão na área de machine learning, pelo que algoritmos tradicionais de machine learning e algoritmos da subárea de deep learning se demonstram adequados para a resolução do problema proposto nesta dissertação. o dataset empregue neste projeto, com dados referentes a portugal, foi construído com a assistência de diversas api e, posteriormente ao seu tratamento, foram aplicados seis algoritmos de machine learning, com o intuito de treinar um modelo que conseguisse prever a utilização futura de postos de carga. de entre os algoritmos avaliados, o random forest regressor e extreme gradient boosting são aque les que apresentam maior capacidade na resolução do problema em questão, com um mae 6.6220 e 6.6310, respetivamente. o modelo de random forest regressor é aquele que melhor se adequou para a previsão de utilização futura dos postos de carga, tendo sido utilizado para a construção do modelo de recomendação.",
      "the ability to solve problems through the use of artificial intelligence finds itself in constant expansion, its proper use in society at large is dependent on our ability to properly put this technology to use. one of the fields of artificial intelligence where significant progress has been noted, is the field of machine learning, where it’s algorithms have found themselves showing an increasing level of specialization in solving diverse tasks. in this dissertation, the main goal was to build a model capable of advising the user of ideal arrange ments for electric vehicle charging stations, with the assistance of machine learning models. the literature review revealed that there have been extensive studies about forecasting problems within the field of ma chine learning, where both machine learning algorithms and deep learning algorithms are up to the task presented in dissertation. the employed dataset used within this dissertation focused on portugal. its construction was assisted through the use of several different api that allowed garnering the necessary data. six machine learning algorithms were applied, the intent was to build a model that could predict occupancy rates for electric vehicle charging stations. of the six, random forest regressor and extreme gradient boosting were the ones that stood out the most in solving the task at hand, they obtained an mae of 6.6220 and 6.6310, respectively. having shown the best results, the random forest regressor model was the one employed in the creation of the final model that advises the user."
    ],
    0.0
  ],
  [
    [
      "hoje em dia, o recurso às novas tecnologias nas mais diferentes áreas do conhecimento tem sido evidente. a área da educação não foi, pois, exceção. cada vez mais se tentam arranjar ferramentas tecnológicas que permitam a qualquer aluno ter acesso a um processo de aprendizagem mais simples e eficaz. dentro desse leque variadíssimo de ferramentas encontramos os its (intelligent tutoring system). genericamente, estes sistemas têm como objetivo fornecer instruções a alunos sem a intervenção direta de um professor. para que isso possa acontecer, com sucesso, é necessário que estes sistemas possuam uma base de conhecimento fiável, com conteúdos adequados em todas as vertentes de ensino que promovem. nesta dissertação tivemos como base de trabalho a conceção e o desenvolvimento de um módulo de avaliação para um sistema de ensino inteligente, com capacidade para fazer a monitorização de todas as atividades desenvolvidas ao longo de um processo de aprendizagem ou de aferição de conhecimento. com esta monitorização pretende-se fazer a criação de perfis de aprendizagem para todos os alunos que utilizem o sistema, de forma a que se possa personalizar os processos de ensino, indo em contra às principais necessidades de aprendizagem dos alunos. nesta dissertação descrevemos o trabalho realizado no desenvolvimento do módulo de avaliação referido, dando particular atenção aos aspetos relacionados com a seleção e implementação dos mecanismos de profiling para a criação automática de perfis de aprendizagem, aos diferentes métodos de avaliação estudados, à preparação dos dados e serviços subjacentes ao seu armazenamento e processamento, à arquitetura do sistema de avaliação desenvolvido e, por fim, à demonstração das funcionalidades implementadas.",
      "nowadays, the usage of technology in the different sectors has been evident. the same applies, with no exception, in education. more and more we try to use it tools that allow any student to have access to a more simple and relying educational system. it’s within this wide range of tools that the its (intelligent tutoring system) arises. generically, these systems have the purpose to provide immediate instructions without the need for a direct teacher’s intervention. for a successful and fully functional system, it’s necessary that these have a reliable knowledge base, with suitable content in all educational features promoted. in this dissertation, we had as a work base the conception and development of an evaluation module for an intelligent tutoring system, with the ability to monitor all activity carried out during a learning process or the measurement of knowledge. the purpose of this monitoring is to create learning profiles for all students that use the system, so we can personalize the learning mechanisms, adapting to the special learning needs of every student. in this dissertation, we describe the accomplished work in the development of the evaluation module described, giving particular attention to the details related to the selection and implementation of the profiling mechanisms for the automatic creation of learning profiles, to the different evaluation modules studied, to the preparation of the data and its storage and processing, to the evaluation system’s architecture, and finally, to the demonstration of the implemented functionalities."
    ],
    [
      "o rápido crescimento da complexidade dos sistemas de software exige, agora mais do que nunca, uma validação rigorosa dos mesmos por forma a manter ou até mesmo aumentar a confiança nestes sistemas. em particular nos sistemas críticos, onde as falhas podem ter consequências catastróficas podendo até incluir a perca de várias vidas humanas, é de externa importância o desenvolvimento de técnicas capazes de garantir altos níveis de confiança para estes sistemas. nesta tese é proposta a utilização de uma técnica formal para a verificação de programas ada, que pretende aumentar a confiança em sistemas cuja implementação seja realizada nesta linguagem de programação. mais precisamente, pretende-se a aplicação da técnica de verificação de modelos para a análise do código fonte de programas concorrentes ada, com especial foco para o domínio dos sistemas críticos. a verificação de modelos é uma técnica bem-sucedida no que diz respeito à garantia de um aumento de fiabilidade destes sistemas. no entanto, a aplicação desta técnica a sistemas de software enfrenta ainda vários obstáculos, e as ferramentas e técnicas para ajudar a ultrapassar estes obstáculos estão ainda a ser desenvolvidas. a ferramenta desenvolvida no contexto desta tese (atos) visa responder a problemas como (i) a construção de modelos a partir de programas e (ii) a especificação de propriedades para estes modelos de acordo com as pretendidas para os programas. a construção manual de modelos que simulam o comportamento de programas é um processo complexo, temporalmente dispendioso, e sujeito a falhas devido à complexidade destes sistemas. de forma a ultrapassar este problema o atos propõe a extração automática de modelos a partir de programas ada. por outro lado, o mapeamento das propriedades desejadas dos programas em propriedades dos modelos pode ser urna tarefa com um grau de complexidade elevado, pois requer entre outros a utilização de um formalismo logico ao qual a maioria dos programadores não está acostumada. 0 atos ajuda no mapeamento destas propriedades, oferecendo vários mecanismos de suporte à sua especificação.",
      "the rapid growth of the complexity of software systems demands, flow more than ever, a rigorous validation of these systems in order to maintain or even increase their reliability. in particular in high-integrity systems, where failures may have catastrophic consequences which may even include the lost of human lives, the development of verification techniques capable of ensuring high degrees of confidence is seen as extremely important. in this thesis the use of a formal technique for the verification of ada programs is proposed, which aims to increase the reliability of systems whose implementation is based on this programming language. more precisely, we target the application of the model checking technique to the verification of source code of concurrent ada programs, with a special focus on the critical systems domain. model checking is a well-succeeded technique for providing increased levels of assurance regarding system correctness. however, its application to software systems still faces several obstacles, and the necessary tools and techniques to help in the overcoming of these problems are still being developed. the tool presented in this thesis (atos) addresses the problems of (i) constructing models from programs and (ii) specifying properties for models corresponding to the ones desired for programs. the manual construction of models that simulate the behavior of programs is a time-costly, complex and error-prone process, due to the complexity of these systems. in order to overcome this problem, atos proposes the automatic extraction of models from ada programs. on the other hand, the mapping of the desired properties from programs to models can be a task of high complexity, because it requires among others that they are expressed in a logical formalism that most programmers are not acquainted with. atos helps in this mapping task by providing several mechanisms aiming to support the specification of properties."
    ],
    0.0
  ],
  [
    [
      "as the complexity of the parallelism in high performance computing continues to increase, traditional tools such as mpi and openmp are starting to become inadequate. within this context, the hpx c++ library emerges as the first implementation of parallex, an execution model that describes message-driven and task-centric computing with the use of lightweight threading, futures, global address spaces and other mechanisms, in order to mitigate problems such as starvation, latency, management costs, and resource contention. in terms of monitoring hpx applications, this work includes the study of: i) hpx performance counters, a framework that provides real-time access to a number of performance metrics; and ii) apex, a tool with an event api that enables runtime auto-tuning of application parameters and the generation of execution profiles, with the aim of finding areas of application. this resulted in an extensive report on the inner workings of these tools, which, due to its importance, was incorporated into the dissertation itself. during the development of this project, a new hpx counter was defined that monitors component variables and exemplifies how the framework of counters can be adapted to handle situations outside of its initial scope. this dissertation’s main contribution is hpxtrace, a tracing tool for hpx applications that was modeled after dtrace and built with apex’s policy engine. hpxtrace provides a system of probes that trigger upon the occurrence of certain events. the user can associate actions with each probe to collect and manipulate event data. events include readings from hpx counters, execution of tasks, message exchanges, and events instrumented in the application by the users themselves. to describe the actions each probe should take, a scripting language was developed using the spirit library from the boost library collection. in addition to providing the basic operations and aggregations found in dtrace, the language had to be adapted to the distributed nature of hpx. this was accomplished by taking measures such as separating variables into local and global contexts and adding synchronization mechanisms. to validate hpxtrace, two implementations of stencil algorithms that solve heat distribution problems were used as case studies. hpxtrace detected the differences in the optimizations between the two versions and their impact on performance. following testing, it was concluded that hpxtrace can be used to analyse and measure the performance of hpx applications. additionally, it proved to be particularly useful in comprehending the internal behavior of hpx, which is valuable for tasks like finding performance optimizations, as well as debugging and learning the platform itself.",
      "com o aumento da complexidade do paralelismo na computação de alto desempenho, ferramentas tradicionais como o mpi e openmp começam a ser inadequadas. é neste contexto que surge o hpx, uma biblioteca c++ que é a primeira implementação do modelo de execução parallex. este modelo descreve computação message-driven e task-centric com o uso de mecanismos como lightweight threading, futuros e espaços de endereçamento global de forma a mitigar problemas como inanição, latência, custos de gestão e contenção de recursos. em termos de monitorização de aplicações em hpx, o trabalho incluiu o estudo: i) da framework de contadores de desempenho do hpx, que permite o acesso a diversas métricas de desempenho em tempo real; e ii) do apex, uma ferramenta cuja api de eventos permite afinar em tempo de execução os parâmetros da aplicação e gerar perfis de execução, com a intenção de encontrar áreas de aplicação. isto resultou num relatório extensivo sobre o funcionamento interno destas ferramentas, que pela sua importância foi incorporado na própria dissertação. durante o desenvolvimento deste projeto, foi definido um novo contador hpx que monitoriza as variáveis de componentes e que exemplifica como a framework dos contadores pode ser adaptada para lidar com situações fora do seu âmbito inicial. a principal contribuição desta dissertação é o hpxtrace, uma ferramenta de rastreamento de aplicações em hpx, inspirada no dtrace e construída com o sistema de políticas do apex. o hpxtrace disponibiliza um sistema de sondas que disparam mediante a ocorrência de certos eventos. o utilizador pode associar ações a cada sonda para colecionar e manipular os dados dos eventos. os eventos incluem leituras dos contadores do hpx, execução de tarefas, troca de mensagens e eventos instrumentados na aplicação pelos próprios utilizadores. para descrever as ações que cada sonda deve tomar, foi desenvolvida uma linguagem de scripting com a biblioteca spirit da coleção de bibliotecas boost. para além de disponibilizar as operações básicas e as agregações existentes no dtrace, a linguagem teve que ser adaptada à natureza distribuída do hpx. para isso foram tomadas medidas, tais como a separação de variáveis em contextos locais e globais e a adição de mecanismos de sincronização. para validar o hpxtrace, foram usadas como caso de estudo duas implementações de algoritmos stencil que resolvem o problema de distribuição de calor. o hpxtrace detetou as diferenças nas otimizações entre as duas versões e o impacto destas no desempenho. realizados os testes chegou-se à conclusão que o hpxtrace pode ser usado para a análise e medição do desempenho de aplicações em hpx. adicionalmente, provou ser particularmente útil na compreensão do comportamento interno do hpx, o que é valioso não só para procurar otimizações de desempenho como também para tarefas como debugging e aprendizagem da própria plataforma."
    ],
    [
      "human computer interaction (hci) is one of the most important aspects in software development. in order to produce valuable products, software companies are focusing more on the users and less on the technology behind their products. this calls for new prospects for development cycles. traditional methodologies are focused on the internals and there is little support to build a user interface (ui) in a more iterative manner [12]. model driven development (mdd) [21] is a technique that has been used to increase software quality and boost development time. with mdd organizations are able to implement iterative development methodologies that start with high level models that are iteratively transformed into lower level models and ultimately source code in an automated way. high level models have several advantages because they are platform independent, easier to maintain, easier to reuse and ultimately they serve as documentation for the project. unified modelling language (uml) is an industry standard language for modelling software. the problem with uml is that it’s not fit for ui models [4]. the ui requires a new modelling language that is able to represent ui aspects accurately. the hci community came up with several solutions for this problems, its [28], wisdom [16], unified modeling language for interactive applications (umli) [4] and user interface extensible mark-up language (usixml) [23] are some examples on this matter. this work proposes a method to reuse previous ui knowledge using patterns of high level models. the goal of this work is to improve the way developers build ui’s and maximize re-usability. patterns are tested and robust solutions that have been used in other contexts and can even persist between different projects and teams. this work integrates in the forward engineering method (fem) developed by the usixml community and uses the usixml user interface description language (uidl) to represent patterns of high level ui models. we developed a pattern definition using a set of descriptive fields and usixml models. with the information provided by the pattern we are able to perform model transformations from the domain and task models to an abstract user interface (aui) model. this gives developers the ability to reuse the structure of a ui developed in other context with a similar objective. this makes it easier for developers with little knowledge in hci to develop good ui’s and also helps development teams to maintain consistency across an application.",
      "a human computer interaction (hci) assumiu-se como um dos aspectos mais importantes no desenvolvimento de software. por forma a criar produtos com maior qualidade, as empresas de software estão a desviar o foco da tecnologia para os utilizadores, exigindo novas abordagens para os ciclos de desenvolvimento. as metodologias tradicionais são mais focadas em aspectos internos e há pouco suporte para construir a user interface (ui) de forma iterativa [12]. model driven development (mdd) [21] e uma técnica usada para aumentar a qualidade do software e diminuir o tempo de desenvolvimento. com a mdd as organizações são capazes de implementar metodologias de desenvolvimento iterativo que começam com modelos de alto nível que são transformados, iterativamente, em modelos de nível inferior e, até chegar ao código-fonte. modelos de alto nível têm várias vantagens porque são independentes de plataforma, mais fáceis de manter, mais fáceis de reutilizar e, finalmente, servem como documentação do projeto. unified modelling language (uml) é uma linguagem padrão da indústria para modelação de software. o problema do uml é que não está apto para modelos de ui [4]. a ui requer uma linguagem de modelação nova, que seja capaz de representar os aspectos próprios da ui com precisão. a comunidade à volta da hci desenvolveu várias soluções para este problema, its [28], wisdom [16], unified modeling language for interactive applications (umli) [4] e user interface extensible markup language (usixml) [23] são alguns exemplos de projetos dentro desta área. este trabalho propõe um método para reutilizar conhecimento passado sobre a ui usando padrões de modelos de alto nível. o objetivo deste trabalho consiste em abordar melhorias à forma como se constrõem ui 's e maximizar a sua reutilização. padrões são soluções testadas e robustas, que foram utilizadas em outros contextos e podem até persistir entre diferentes projetos e equipas. este trabalho integra-se no método forward engineering (fem) desenvolvido pela comunidade do usixml e usa a user interface description language (uidl) do usixml para representar padrões de modelos de ui de alto nível. neste trabalho desenvolveu-se uma definição de padrão usando um conjunto de campos descritivos e modelos em usixml. com a informação fornecidas pelos padrões e possível realizar transformações de modelos a partir dos modelos de domínio e tarefa e gerar um modelo de abstract user interface (aui). isto oferece aos engenheiros de software a capacidade de reutilizar a estrutura de uma ui desenvolvida noutro contexto com um objetivo similar. assim torna-se mais fácil para engenheiros com pouco conhecimento em hci desenvolver boas ui 's e também ajuda as equipes de desenvolvimento a manter consistência ao longo de uma aplicação."
    ],
    0.0
  ],
  [
    [
      "the recent advances in metabolomics experimental techniques have provided novel approaches for many research issues in the biological fields. indeed, the ability to identify and quantify numerous compounds in biological samples provides significant advances in functional genomics, biomarker identification, sample characterization or drug discovery and development. to take full advantage of these data advanced bioinformatics methods for data analysis and mining have been required. a number of methods and tools for metabolomics data analysis have been put forward recently, being one of the major limitations still faced the lack of integrated frameworks for extracting relevant knowledge from these data and being able to integrate these data with previous biochemical knowledge. also, the lack of reproducibility in many data analyses or data mining processes is a strong obstacle for biological discovery. in recent work from the host group, specmine, a metabolomics and spectral data analysis/ mining framework, in the form of a package for the r system, has been developed to address some of these issues. in this thesis, an integrated web-based platform for metabolomics data analysis and mining, named webspecmine, was designed and developed, based on the specmine package, thus providing an easier and friendly user interface. this website provides means for analysing metabolomics data from different formats, including tasks such as pre-processing, univariate and multivariate analysis and metabolite identification. this web-based platform was developed collaboratively and, therefore, this work focused mainly in data from nuclear magnetic ressonance and mass spectrometry. also, the package faced some limitations regarding types of analysis not yet provided, such as metabolite identification for other data formats besides mass spectrometry coupled to liquid chromatography. therefore, the extension of the metabolite identification feature was addressed, by implementing such analysis for nuclear magnetic ressonance data in the specmine package, as well as making it available in the website. the website was validated by applying it to reproduce the pipelines from previous studies that made use of the specmine package. furthermore, a case study involving banana peels and the analysis of their characteristics and potential made use of the newly created website to further validate its functionality. all the analyses here executed were stored and are available in the web application, as public projects.",
      "os mais recentes avanços nas técnicas experimentais metabolómicas têm levado a novas abordagens de muitas questões na investigação em áreas biológicas. de facto, a capacidade de identificar e quantificar os inúmeros compostos presentes nas amostras biológicas veio provocar enormes avanços na genómica funcional, identificação de biomarcadores, caracterização de amostras e descoberta e desenvolvimento de drogas. para tirar maior partido destes dados, é necessário a existência de métodos avançados de bioinformática para a análise e mineração de dados. vários métodos e ferramentas que permitem a análise de dados metabolómicos têm vindo a ser apresentadas, tendo no entanto como grande limitação a falta de extração de conhecimento relevante destes dados e integrá-los com conhecimento bioquímico anterior. para além disto, a falta de reprodutibilidade de muitas análises de dados ou de processamentos de mineração é um grande obstáculo à descoberta biológica. em trabalhos recentes do grupo de acolhimento foi desenvolvido um package para o sistema r por forma a abordar algumas destas questões. este package, denominado specmine, permite a análise e mineração de dados espectrais e de metabolómica. na presente tese, uma plataforma web integrada para a análise e mineracão de dados de metabolómica, denominada webspecmine, foi desenvolvida, baseada no package specmine, fornecendo assim uma interface simples e fácil para o usuário. este site permite a análise de dados de metabolómica de formatos diferentes, incluindo pre-processamento, análises univariada e multivariada, e identificação de metabolitos. esta plataforma web foi desenvolvida de forma colaborativa e, deste modo, o presente trabalho focou-se maioritariamente em dados provenientes das técnicas espectrometria de massa e ressonância magnética nuclear. para além disto, o package apresentava algumas limitações no que toca a tipos de análise ainda não disponíveis, como é o caso da identificação de metabolitos para outros formatos de dados que não a espectrometria de massa acoplada com cromatografia líquida. assim, a funcionalidade de identificação de metabolitos no specmine package foi estendida a dados de ressonância magnética nuclear, bem como também implementada no website. o site foi validado através da sua aplicação para reproduzir pipelines de estudos anteriores que fizeram uso do package specmine. para além disto, um estudo de caso envolvendo cascas de banana e a análise das suas características e potencial, fez uso do site recentemente criado para também validar a sua funcionalidade. todas as análises aqui executadas foram guardadas na aplicação web, estando disponíveis para consulta, como projectos públicos."
    ],
    [
      "the proliferation of social networks presents a significant amount of fake news and fake information every day and every second. the covid-19 pandemic confirms this situation. the general ignorance of this disease causes the spreading of misleading information, harming people's lives and governments' actions to contain it. to fight this infodemic, the populations resorted to the health services' phone lines, congesting them with questions, most of them repeated among different individuals and locations. a chatbot for covid-19- related questions would redirect this workload from the health services, mitigating such congestion. this chatbot should work for both the english and portuguese languages. this work provides a background overview about web crawlers, information processing and chatbot development, which are the three components of the application. a systematic literature review was done to provide an analysis of the existing literature on the mentioned thematics. the application presented in this work consists of three main modules: a web crawler, using the ache crawler application, which downloads the web pages from the trustworthy sources; a text processor, that parses the web pages and indexes them according to their language to the respective elasticsearch index; and a chatbot component, composed by a fine-tuned bert model with the squad 2.0 dataset and a web interface that queries the elasticsearch indexes for the most relevant pages and extracts the answers to the given questions by the users. to comply with the english and portuguese requirement, two sets of reliable sources were defined (one for each language) and a translated version of squad 1.1 dataset was used to train the portuguese bert model. the chatbot queries the correct model using the web browser's defined language. our system was evaluated using a set of covid-19 qa pairs extracted from the united nations website, and the obtained results are described in this work. these were far from the desirable outcomes, so some improvements were applied to the crawler and to the elasticsearch indexes. however the results were still not satisfactory, requiring a set of future modifications that are presented in this work.",
      "com a proliferação das redes sociais, um número significativo de fake news é disponibilizado às pessoas todos os dias, a cada segundo. isto foi confirmado durante a pandemia da covid-19, onde um desconhecimento geral da doença causou a difusão de informação enganosa, colocando em risco a vida das pessoas e as ações governamentais que visavam o controlo da doença. para combater esta infodemia, as populações recorreram às linhas telefónicas dos serviços de saúde nacionais, congestionando-as com questões muitas vezes repetidas. com o intuito de mitigar este con-gestionamento, um chatbot para a covid-19 ajudaria a redirecionar esta carga de trabalho dos serviços de saúde para a aplicação. este chatbot deve suportar as linguas portuguesa e inglesa. este trabalho apresenta uma visão geral acerca de web crawlers, de processamento de informação e de desenvolvimento de chatbots. uma revisão sistemática da literatura foi conduzida com o intuito de apresentar uma análise da literatura existente. a aplicação apresentada neste trabalho consiste em três componentes principais: um web crawler, usando a aplicação ache, que descarrega as páginas web das fontes confiáveis; um componente de processamento de texto, que processa as páginas e as indexa de acordo com a sua língua no respetivo índice de elasticsearch; e um chatbot, composto por um modelo bert treinado e refinado com o dataset squad 2.0 e uma interface web, que pesquisa no elasticsearch as páginas mais relevantes e extrai dai as respostas para as perguntas dos utilizadores. para satisfazer o requisito das duas línguas, dois conjuntos de páginas confiáveis foram definidos (um para cada lingua), e uma versão traduzida do squad 1.1 foi utilizada para treinar o modelo bert em português. o chatbot questiona o modelo correto consoante a língua configurada no browser utilizado. o sistema foi avaliado usando um conjunto real de perguntas e respostas sobre covid-19, sendo apresentados neste trabalho os resultados obtidos. estes ficaram longe do desejado, pelo que algumas melhorias foram aplicadas ao sistema. porém, os resultados permaneceram ainda assim insatisfatórios, necessitando de um conjunto de muras alterações que são apresentadas neste trabalho"
    ],
    0.0
  ],
  [
    [
      "this document presents a thesis and describes the underlying work which was developed along the second year of the master degree in informatics engineering offered by departamento de informática of universidade do minho and accomplished at syone sbs software – tecnologia e serviços de informática, s.a.. in the past few years, some attempts to automatically screening cvs with resource to natural language processing have been made not only to save recruiters’ time, but also to spare them the most tedious task of the recruitment process and, consequently, smooth their job. however, the majority is still very primitive, misclassifies a lot of cvs and needs a deeper study. therefore, the aim of this master’s project is precisely to develop an algorithm that is capable of automatically ranking candidates’ cvs according to their similarity regarding the job offer they applied for. thus, a general architecture was proposed where cvs and job offers are preprocessed, in order to obtain the respective texts proper to be further processed. that said, two different approaches were followed, in order to find the similarity between the documents in question. to do so, the first approach resorted to several machine learning algorithms and similarity measures, while the second approach structured the initial documents to compare their respective information. after that, tests were conducted to evaluate both approaches and enable the comparison between them. finally, the conclusions were drawn and also reported in this dissertation.",
      "este documento apresenta uma tese e descreve o trabalho subjacente que foi desenvolvido ao longo do segundo ano do mestrado em engenharia informática do departamento de informática da universidade do minho e realizado na syone sbs software – tecnologia e serviços de informática, s.a.. nos últimos anos, algumas tentativas de triagem automática de currículos com recurso a processamento de linguagem natural foram feitas não só para economizar o tempo dos recrutadores, mas também para os poupar da tarefa mais entediante do processo de recrutamento e, consequentemente, suavizar o seu trabalho. contudo, a maioria ainda é muito primitiva, classifica incorretamente muitos currículos e necessita de um estudo mais aprofundado. sendo assim, o objetivo deste projeto de mestrado é precisamente desenvolver um algoritmo capaz de classificar automaticamente os currículos dos candidatos de acordo com a sua similaridade relativamente à oferta de emprego a que se candidataram. deste modo, foi proposta uma arquitetura geral onde os cvs e as ofertas de emprego são pré-processados, de forma a obter os respetivos textos adequados para posterior processamento. dito isto, foram seguidas duas abordagens distintas, de forma a encontrar a semelhança entre os documentos em questão. para tal, a primeira abordagem recorreu a diversos algoritmos de aprendizagem automática e medidas de similaridade, enquanto a segunda abordagem estruturou os documentos iniciais para comparar as suas respetivas informações. de seguida, foram realizados testes para avaliar ambas as abordagens e possibilitar a comparação entre elas. por fim, as conclusões foram tiradas e também relatadas nesta dissertação."
    ],
    [
      "a evolução tecnológica das últimas décadas generalizou o uso de software para a substituição ou suporte de múltiplos processos das empresas e, evidenciou novas perspectivas para o desenvolvimento de soluções com altos níveis de performance, disponibilidade, escalabilidade e flexibilidade. no contexto vortal (empresa líder no mercado de contratação electrónica português com a plataforma vortalnext>), esta generalização levou à necessidade da existência mecanismos que permitam aos seus clientes a personalização/criação de áreas de trabalho dedicadas. tendo esta necessidade como foco, são avaliados os diferentes componentes da plataforma next>, a metodologia de desenvolvimento atualmente utilizada (model driven architecture) e quais as melhores aproximações para o desenvolvimento de aplicações no âmbito de uma plataforma web, focando as suas vantagens e desvantagens a nível arquitetural e aplicacional. concluiu-se que todas as soluções estudadas são adequados ao desenvolvimento de aplicações web, sendo o seu grau de adequação variável com o contexto de utilização. são soluções diferentes relativamente à complexidade de implementação, aos recursos necessários, aos riscos envolvidos e à simplicidade de utilização por parte do grupo de utilizadores finais. por fim, é apresentada a arquitetura de um software development kit (são estudadas outras opções, sendo esta a que oferece mais estabilidade aplicacional e mais vantagens competitivas) e a sua integração no ecossistema aplicacional e arquitetural da plataforma maximizando, não apenas a flexibilidade e funcionalidade para o cliente final, como também a segurança, robustez e fiabilidade do ecossistema da plataforma. a arquitetura definida em conjunto com o modelo de negócio apresentado formam a linha de ação indicada para garantir a existência de aplicações personalizadas a serem executadas no ecossistema vortalnext>.",
      "the technological evolution of the last decades, widespread the use of software for the replacement or support of many company processes and highlighted new perspectives for developing solutions with high levels of performance, availability, scalability and flexibility. in the case of vortal (leader in the portuguese e-procurement platforms with vortal next>), this generalization led to the need of designing a tool that allows customers to customize/create dedicated workspaces. focusing on this need, the different components of vortal next> platform are evaluated, the currently used development methodology is studied (model driven architecture) as well as what are the best approaches for developing applications within a web platform, focusing on the advantages and disadvantages at the architectural and applicational level. it was concluded that all the solutions are suitable for the development of web applications, having its degree of suitability to vary according with the context of use. they are different solutions regarding the implementation complexity, the resources required, the risks involved and the simplicity of use by the group of end users. finally, it is presented the architecture of a software development kit (other options are studied, but this is the one that offers more stability and more competitive advantages) and its integration into the platform, maximizing not only the flexibility and functionality to the end customer, but also the safety, robustness and reliability of the platform ecosystem. the architecture defined along with the business model presented, form the line of action indicated to ensure the existence of custom applications to be executed in the vortal next> ecosystem."
    ],
    0.0
  ],
  [
    [
      "database management systems have a long history of development and research, with systems like post gresql and languages like sql already being well-established in the industry. transactional memory emerges as a new concurrency control mechanism for concurrent programming, inspired by ideas and concepts from the database world. as is the case with database transactions, transactions in transac tional memory can (and will) conflict when multiple transactions try to modify the same data. this can lead to the appearance of hot spots in contended memory regions, quickly degrading the performance of an application. in this dissertation, we propose new optimisation techniques for transactional memory hot spots, based on previous research on splitting techniques for numeric database records. we implement the optimisations on an existing transactional memory system and measure their impact on performance, using custom-made and reference benchmarks.",
      "sistemas de gestão de bases de dados possuem uma longa história de desenvolvimento e investigação, estando sistemas como postgresql e linguagens como sql já bem estabelecidos na indústria. memória transacional surge como um novo mecanismo de controlo de concorrência para a programação concor rente, inspirada por ideias e conceitos do mundo das bases de dados. tal como se sucede nas transações em bases de dados, as transações em memória transacional podem (e irão) entrar em conflito quando múltiplas transações tentarem modificar os mesmos dados. isto leva ao surgimento de hot spots em regiões contendidas de memória, rapidamente degradando a performance de uma aplicação. nesta dissertação, propõem-se novas otimizações para hot spots em memória transacional, baseadas em investigação prévia de técnicas de divisão de registos numéricos em bases de dados. as otimizações foram implementadas sobre um sistema de memória transicional já existente, sendo o seu impacto na performance medido recorrendo tanto a um benchmark feito à medida como a um de referência."
    ],
    [
      "um obstáculo comum em vários domínios no processo de preparação de um modelo de machine learning (ml) é a escassez de labels (i.e., etiquetas dos dados). em aplicações reais, algures no processo de construção de um dataset existe um especialista a fazer anotação manual de cada instância dos dados para identificar a respetiva label. dentro do domínio de deteção de fraude, que é normalmente tratado como um problema de ml supervisionado, a existência de analistas de fraude a reverem todas as transações que ocorrem representaria um nível de custos em recursos humanos inexequível. isto leva a que apenas uma fração dos dados possam ser manualmente analisados. o sub-campo de ml conhecido como active learning (al) surgiu em resposta a este problema. em al são implementados algoritmos que selecionam de forma eficiente quais as instâncias dos dados que devem ser analisadas de forma a otimizarem-se os custos de anotação dos dados. o objetivo principal deste processo é a criação de um modelo de previsão eficaz treinado com a menor quantidade de dados possível. neste trabalho, apresentamos um estudo detalhado de diversas estratégias de al em que realizamos experiências com dados de aplicações reais. focamo-nos principalmente no cenário em que a anotação dos dados é iniciada a partir do primeiro dia de geração dos mesmos, não tendo à partida dados prévios para a construção de perfis dos utilizadores nem quaisquer labels. apresentamos avaliações de novos algoritmos e configurações de al, assim como métodos pré-existentes, através de múltiplas experiências. estas experiências são realizadas num ambiente em streaming (tal como nos sistemas de produção em causa), em que as transações ao processadas em tempo real. para além da escolha do algoritmo de al existem outros parâmetros a definir na configuração geral. realizamos estudos que nos permitem compreender quais os valores mais favoráveis de vários destes parâmetros, incluindo o impacto da escolha do método de pré-processamento de dados e do modelo de ml usado em avaliação. a maioria dos algoritmos de al existentes na literatura exigem um conjunto de dados já com labels que tenha elementos de todas as classes existentes (e.g., transações legítimas e fraudulentas). dado que no domínio da deteção de fraude é comum a ocorrência de transações fraudulentas ser rara, isto pode limitar quão rápido um algoritmo de al totalmente supervisionado pode começar a ser utilizado nas primeiras iterações do processo. em resposta a este problema nos apresentamos uma framework de al em três fases que utiliza, num período intermédio, um algoritmo de al que recorre à estrutura dos dados com labels sem utilizar as mesmas. isto resulta num aumento da eficácia do sistema de al. dada a hipótese de que dois algoritmos de al podem ser combinados de forma a produzir um que seja melhor que as suas partes, também desenvolvemos e estudamos vários métodos de combinação destes algoritmos. realizamos uma comparação com uma grande quantidade de combinações que nos levam à conclusão de que tais combinações não aumentam a eficácia relativamente aos algoritmos individuais numa framework de três fases. finalmente, realizamos um conjunto de experiências em larga escala que cobrem os diversos casos de uso da deteção de fraude. os resultados indicam que al é uma solução adequada para os casos de banking e merchant, principalmente quando utilizados algoritmos de al baseados em incerteza. contudo, o nosso estudo não demonstrou resultados positivos para um dataset de banking com ocorrências de fraude extremamente raras nem para o dataset de merchant acquirer.",
      "a problem that arises in many domains when preparing a machine learning (ml) model is label scarcity. in various real world applications, somewhere in the loop of building a dataset, there is a human expert manually annotating each dataset entry with the class label it belongs to. in fraud detection, which is usually addressed as a supervised machine learning problem, having fraud experts carefully reviewing every single transaction is often too expensive, so only a subset of them can be manually annotated. the sub-field of ml known as active learning (al) has emerged to address this problem. al implements policies that intelligently choose which instances should be labeled by a human annotator in order to optimize the data labelling costs. the ultimate goal of this procedure is to create a robust predictive model with as little data as possible [settles (2009)]. in this work, we present a detailed study of various proposed al strategies by performing experiments with real world data. we focus, primarily, on the scenario where the annotation starts from day-one with no previous data to build historical user profiles and, hence, no labeled data. we present evaluations of several new and already existing types of al policies and al configurations through various sets of experiments. the analysis is performed in a streaming setup (as required by the production systems under study) where transactions are processed in real-time. besides the choice of a policy, there are other parameters that must be chosen in our al setup. we conduct dedicated studies to assess the most suitable choices for several such parameters. these studies include the understanding of the impact on the choice of the data pre-processing methods and the ml model to use in evaluations. since most al policies proposed in the literature require that the pool of labeled instances contains labels from all classes, the extreme class imbalance in the fraud detection domain can limit how fast a fully supervised al policy can start being used in the first iterations of an al process. to address this issue, we introduce a three-phase al framework, which uses an intermediate stage policy that does not resort to the label values but can still exploit the labeled pool. this improves the overall performance of all policies used. based on the hypothesis that two al policies can be combined to produce one that outperforms each part, we also develop and study several policy combination methods. we perform a comparison on a large set of combinations that leads us to the conclusion that these do not increase performance when compared to the individual policies in a three-phase setup. finally, we perform a set of large-scale experiments that cover several business cases for fraud detection. the results support that al is an appropriate solution for the banking and merchant business cases, especially when using uncertainty sampling as final policy. however, our study did not demonstrate good results for a banking dataset with an extremely small fraud prevalence nor for a merchant acquirer dataset."
    ],
    0.3
  ],
  [
    [
      "information is nowadays an extremely valuable asset and takes on certain occasions a central role in many organizations. this information however may ﬁnd itself legally protected (e.g. medical records) or contain data of real people as individual preferences or transactions whose privacy people do not want to see broken. on the other hand, there are sometimes obvious applications for this information as for example to generate statistics for medical research or for simply as a way to provide a better service. there is thus a conﬂict of interest between those who hold information and intend to value it somehow and those who do not want to see their privacy compromised. two promising technologies arise in this context: techniques based on the notion of differential privacy and data anonymization algorithms. the goal of this dissertation is to explore the interaction between these two technologies and the possibility to utilize them together.",
      "a informação é hoje em dia um bem muito valioso e assume em certos casos um papel central em várias organizações. no entanto esta informação pode encontrar-se protegida legalmente (i.e. registos médicos) ou conter dados de pessoas reais como preferências individuais ou transações cuja privacidade as pessoas não querem ver quebrada. por outro lado existem por vezes aplicações óbvias para esta informação como por exemplo estatísticas para fins de investigação medica ou simplesmente para oferecer um melhor serviço. existe desta forma um conflito de interesses entre aqueles que detêm informação a pretendem de alguma forma valorizar e aqueles que não querem ver a sua privacidade comprometida. surgem neste contexto duas tecnologias promissoras questões técnicas baseadas na noção de privacidade diferencial e os algoritmos de anonimização de dados. o objetivo desta dissertação é explorar a interação entre estas duas tecnologias e a possibilidade de as utilizar em simultâneo."
    ],
    [
      "the access to internet services on a large scale, high throughput and low latency has grown at a very high pace over time, with a growing demand for media content and applications increasingly oriented towards data consumption. this fact about the use of data at the edge of the network requires the central offices (co) of telecommunication providers, to be pre pared to absorb these demands. cos generally offer data from various access methods, such as passive optical network (pon) technologies, mobile networks, copper wired and oth ers. for each of these technologies there may be different manufacturers that support only their respective hardware and software solutions, although they all share different network resources and have management, configuration and monitoring tools (fault, configuration, accounting, performance, and security management - fcaps) similar, but being distinct and isolated from each other, which produces huge investment in capital expenditure (capex) and operational expenditure (opex) and can cause barriers to innovation. such panora mas forced the development of more flexible, scalable solutions that share platforms and net work architectures that can meet this need and enable the evolution of networks. it is then proposed the architecture of software-defined network (sdn) which has in its proposal to abstract the control plane from the data plane, in addition to the virtualization of several net work function virtualization (nfv). the sdn architecture allows apis and protocols such as openflow, netconf / yang, restconf, grpc and others to be used so that there is communication between the various hardware and software elements that compose the net work and consume network resources, such as services aaa, dhcp, routing, orchestration, management or various applications that may exist in this context. this work then aims at the development of a virtualized network function, namely a vnf in the context of network security to be integrated as a component of an architecture guided by the sdn paradigm applied to broadband networks, and also adherent to the architecture ob-baa promoted by the broadband forum. such ob-baa architecture fits into the initia tive to modernize the information technology (it) components of broadband networks, more specifically the central offices. with such development, it was intended to explore the con cepts of network security, such as the ieee 802.1x protocol applied in ng-pon networks for authentication and authorization of new network equipment. to achieve this goal, the development of the applications was based on the golang language combined with grpc programmable interfaces for communication between the various elements of the architec ture. network emulators were initially used, and then the components were ”containerized” and inserted in the docker and kubernetes virtualization frameworks. finally, performance metrics were analyzed in the usage tests, namely computational resource usage metrics (cpu, memory and network i/o), in addition to the execution time of several processes performed by the developed applications.",
      "o acesso aos serviços de internet em larga escala, alto débito e baixa latência têm crescido em um ritmo bastante elevado ao longo dos tempos, com uma demanda crescente por conteúdos de media e aplicações cada vez mais orientadas ao consumo de dados. tal fato acerca da uti lização de dados na periferia da rede, obriga a que os central offices (co) dos provedores de telecomunicações estejam preparados para absorver estas demandas. os co geralmente re cebem dados de diversos métodos de acesso, como tecnologias passive optical network (pon), redes móveis, cabladas em cobre, entre outros. para cada uma destas tecnologias pode haver diferentes fabricantes que suportam somente suas respetivas soluções de hardware e software, apesar de todas compartilharem diversos recursos de rede e possuírem ferramentas de gestão, configuração e monitoração (fault-management, configuration, accounting, performance e segurança - fcaps) similares, mas serem distintas e isoladas entre si, o que se traduz em um enorme investimento em capital expenditure (capex) e operational expenditure (opex) e pode causar barreiras à inovação. tais panoramas forçaram o desenvolvimento de soluções mais flexíveis, escaláveis e que compartilhem plataformas e arquiteturas de redes que pos sam suprir tal necessidade e possibilitar a evolução das redes. propõe-se então a arquitetura de redes definidas por software (software-defined network - sdn) que tem em sua proposta abstrair o plano de controle do plano de dados, além da virtualização de diversas funções de rede (network function virtualization - nfv). a arquitetura sdn possibilita que api’s e pro tocolos como openflow, netconf/yang, restconf, grpc e outros, sejam utilizados para que haja comunicação entre os diversos elementos de hardware e software que estejam a compor a rede e a consumir recursos de redes, como serviços de aaa, dhcp, roteamento, orquestração, gestão ou diversas outras aplicações que possam existir neste contexto. este trabalho visa então o desenvolvimento de uma função de rede virtualizada nomeada mente uma (virtual network function - vnf) no âmbito de segurança de redes a ser integrada como um componente de uma arquitetura orientada pelo paradigma de sdn aplicado a re des de banda larga, e aderente também à arquitetura ob-baa promovida pelo broadband fo rum. tal arquitetura ob-baa se enquadra na iniciativa de modernização dos componentes de tecnologia da informação (ti) das redes de banda larga, mais especificamente dos cen tral offices. com tal desenvolvimento pretende-se explorar conceitos de segurança de redes, como o protocolo ieee 802.1x aplicado em redes ng-pon para autenticação e autorização de novos equipamentos de rede. para atingir tal objetivo, utilizou-se desenvolvimento de aplicações baseadas na linguagem golang aliado com interfaces programáveis grpc para comunicação entre os diversos elementos da arquitetura. para emular tais componentes, utilizou-se inicialmente emuladores de rede, e em um segundo momento os componentes foram ”containerizados” e inseridos nos frameworks de virtualização docker e kubernetes.por fim, foram analisadas métricas de desempenho nos testes executados, nomeadamente métricas de utilização de recursos computacionais (cpu, memória e tráfego de rede), além do tempo de execução de diversos processos desempenhados pelas aplicações desenvolvidas."
    ],
    0.06666666666666667
  ],
  [
    [
      "o estudo da conectividade cerebral, da integração e segregação das funções das diferentes partes do cérebro será no futuro próximo uma das ferramentas mais importantes na compreensão do cérebro humano. contudo, a variedade de processos e dinâmicas lá presentes tornam esta tarefa extremamente complexa. assim, o objectivo principal deste trabalho passa por desenvolver e testar um modelo capaz de representar redes de conectividade cerebral. a teoria de grafos, em conjugação com modalidades de neuroimagiologia como a ressonância magnética, tem-se mostrado uma ferramenta extremamente valiosa neste sentido. o trabalho aqui apresentado foca-se em três pontos: o pré-processamento das imagens de ressonância magnética; a definição dos elementos que constituem a rede, comparando diferentes estratégias, e construção das redes; a utilização de métricas e conhecimentos de teoria de grafos para caracterizar e comparar as redes. utilizando dados reais foi possível construir redes esparsas, eficientes, resilientes, com forte divisão em comunidades e arquitetura small-world. foi observado o efeito das diferentes estratégias nas características das redes, e mesmo na falta de fortes conclusões sobre qual a mais adequada, foi possível compreender a dificuldade inerente á comparação de redes complexas e dados passos importantes no sentido de melhorar essa comparação.",
      "the study of the brain connectivity, the integration and the segregation of the functions of the different parts of the brain will, in the near future, be one of the most important tools in the understanding of the human brain. however, the variety and dynamic of processes that can be found there make this task extremely complex. thus, the main objective of this work is to develop and test a model able to represent networks of brain connectivity. the discipline of graph theory in conjunction with neuroimaging modalities, such as magnetic resonance imaging, has proven to be an extremely valuable tool in this regard. the work presented here focuses on three points: the pre-processing of mri images; the definition of the elements of the network, comparing different strategies, and construction of networks; the use of metrics and knowledge of graph theory to characterizing and comparing networks. using real data it was possible to build sparse, efficient and resilient networks, with a strong division in communities and small-world architecture. we observed the effect of different strategies on characteristics of networks, and even in the absence of strong conclusions about the best one, it was possible to understand the inherent difficulty in comparing complex networks as well as important steps were taken to improve this comparison."
    ],
    [
      "technology has become essential for society’s every-day-life and with the recent increase in artificial intelligence’s interest, this area has gained more and more relevance for both people (e.g., due to the increasing number of users of personal assistants, such as siri, alexa and google assistant) and service providers (e.g., google search engine and social networks’ recommendation algorithms to keep users busy and active on their platforms - facebook, youtube, tiktok, etc.). nevertheless, artificial intelligence has been applied to many other areas, such as targeted advertising to specific users, cybersecurity, medicine, and the automobile industry. although artificial intelligence has not been the perfect solution in the aforementioned applications, it has been responsible for several significant improvements in the last decade. for example, in the automobile industry, there are more and more companies offering solutions for autonomous vehicles, being tesla the most notorious. this evolution was driven by several factors, including need and interest in improving road safety, growing traffic problems that exist due to the increase of vehicles circulating, more reliable sensors, and recent advances in various areas of artificial intelligence, such as object detection, semantic segmentation, and object tracking. these three areas are interconnected. however, they have different purposes - the first two (detection and segmentation) more related to static frame analysis (e.g., image based analysis), while object tracking is usually applied in dynamic environments (e.g., sequence of frames, such as a video) where its input is processed in order to track objects over time, allowing an intelligent system to be “aware” of its environment. that said, this dissertation aims to study and explore the applicability and feasibility, as well as to develop and implement an object tracker in the context of autonomous driving. furthermore, it is also intended to make a benchmark with state-of-the-art approaches and identify their main limitations. the input data will be focused on light detection and ranging (lidar) based 3d point cloud, as there are several datasets available, in particular kitti [1], which, in addition to being widely used in the state-of-the-art, has also achieved positive results, even in real-time execution situations. however, these solutions usually require a lot of computational resources and, which can be a hurdle for its application in real-life settings.",
      "a tecnologia tornou-se essencial para o normal funcionamento da sociedade e com o recente aumento do interesse na inteligência artificial, esta área tem ganho cada vez mais relevância tanto para as pessoas (por exemplo, devido ao aumento do número de utilizadores de sistemas como assistentes pessoais, como siri, alexa e google assistant) como para os prestadores de serviços (por exemplo, o motor de busca da google, os algoritmos de recomendação de várias redes sociais para manterem o utilizador ocupado e ativo nas plataformas em questão - facebook, youtube, tiktok, entre outros). ainda assim, a inteligência artificial tem sido aplicada a muitas outras áreas, como, por exemplo, publicidade adaptada a cada utilizador, cibersegurança, medicina e indústria automóvel. embora a inteligência artificial não tenha sido a solução perfeita nas aplicações mencionadas acima, esta tem sido responsável por vários avanços significativos na última decada. por exemplo, na indústria automóvel, existem cada vez mais empresas que oferecem soluções para veículos autonómos, sendo a tesla a mais reconhecida. esta evolução foi alimentada por vários fatores, como a necessidade e interesse em melhorar a segurança na estrada, os crescentes problemas de trânsito que existem devido ao aumento de veículos a circular, sensores mais fidedignos e os avanços recentes em várias áreas da inteligência artificial, como, por exemplo, a deteção de objetos, segmentação semântica e tracking de objetos. estas três áreas estão interligadas. contudo têm focos diferentes - as duas primeiras (deteção e segmentação) mais relacionadas com análise de frames estáticas (por exemplo, análise baseada em imagens), enquanto que o tracking de objetos é, usualmente, aplicado em ambiente dinâmicos (por exemplo, em sequência de frames, sendo vídeo um desses casos). este input é então processado para executar a tarefa de monitorização, permitindo que um sistema inteligente esteja “ciente” do ambiente onde se encontra. posto isto, esta dissertação tem como objetivo estudar e explorar a aplicabilidade e viabilidade, assim como desenvolver e implementar um tracker de objetos no contexto da condução autónoma. para além disso, também se pretende efetuar uma comparação com abordagens do estado de arte e identificar as suas principais limitações. os dados de input serão focados em light detection and ranging (lidar) baseado em point cloud 3d, uma vez que existem vários datasets disponíveis, em particular o kitti [1], que, para além de ser muito utilizado no estado de arte, tem também alcançado resultados positivos, mesmo em situações de execução em tempo real. no entanto, estas soluções necessitam, normalmente, de muitos recursos computacionais, o que pode ser um entrave para a sua aplicação em contextos reais."
    ],
    0.0
  ],
  [
    [
      "a área de ehealth/assisted living tem vindo a crescer com a necessidade de melhorar a qualidade de vida de pessoas que precisam de acompanhamento médico permanente (e.g., doença crónica) ou ocasional (e.g., pós-operatório, pandemia), de forma presencial ou à distância. no entanto, a tendência é manter as pessoas em casa o máximo tempo possível e não nas instituições, evitando o perigo de contaminação, precavendo a rutura dos serviços de saúde, e garantindo simultaneamente maior conforto e bem-estar ao utente. contudo, estas pessoas precisam de ser acompanhadas e ter acesso a serviços de saúde de qualidade, tal como se estivessem nas instituições. os serviços de telemedicina e telemonitorização servem este propósito, permitindo a cuidadores formais e/ou informais estar em contacto com os seus doentes, acompanhá-los remotamente em tempo real e prevenir situações de maior risco, agindo rapidamente em casos de urgência. no âmbito específico da telemonitorização, esta pode ser clínica e/ou não clínica, mais focada no controlo de sinais vitais ou apenas no acompanhamento da pessoa com base noutro tipo de informação pessoal (e.g., atividade, sono, localização), respetivamente. no caso particular desta dissertação, o objetivo é desenhar, especificar e implementar um serviço que ofereça conforto e segurança usando essencialmente informação não clínica e assegure o acompanhamento remoto de pessoas de idade avançada que vivam ou passem muito tempo sozinhas. a necessidade de criação de uma ferramenta deste tipo deve-se ao facto de, cada vez mais, se pretender apostar na prevenção e segurança das pessoas, sobretudo das pessoas de idade que se encontram em situações de alguma fragilidade (e.g., por doença, por isolamento), mas simultaneamente tentar passar um sentimento de companhia, mesmo à distância, procurando preservar ao máximo a sua autonomia. neste contexto, o projeto pretende explorar cenários mistos de telemonitorização, através de informação proveniente de sensores ligados à casa edispositivos wearables, criando o conceito de confort@home, ou seja, um conceito de saúde e bem-estar centrado na pessoa, na sua casa e no ambiente circundante. a oferta de serviços a considerar incluirá in formação de localização dentro e fora de casa, dados adicionais provenientes de dispositivos a selecionar e notificações/alertas; na lógica da aplicação serão definidas as situações e condições que espelhem os possíveis riscos e falta de segurança do idoso, e enviados alertas para o seu cuidador/familiar, de forma a facilitar a vida de ambos no dia-a-dia. como demonstração do conceito, será desenvolvida uma das vertentes do confort@home que se materializa numa aplicação web fornecida em ambiente cloud que permita a cuidadores informais monitorizar os seus familiares/dependentes com base na sua localização.",
      "the area of ehealth/assisted living has been growing with the need to improve the quality of life of people who need permanent (e.g., chronic disease) or occasional (e.g., post-operative, pandemic) medical monitoring, in person or remotely. however, the trend is to keep people at home for as long as possible, rather than in institutions, avoiding the danger of contamination, preventing the breakdown of health services, and at the same time ensuring greater comfort and well-being for the user. nevertheless, these people need to be accompanied and have access to quality health services, just as if they were in the institutions. telemedicine and telemonitoring services serve this purpose, allowing formal and/or informal caregivers to stay in touch with their patients, to monitor them remotely in real time, and to prevent higher risk situations by acting quickly in cases of emergency. in the specific scope of telemonitoring, this monitoring may be clinical and/or non-clinical, i.e., more focused on monitoring vital signs or only on monitoring the person based on other types of personal information (e.g., activity, sleep, location), respectively. in the particular case of this dissertation, the goal is to design, specify and implement a service that offers comfort and security using essentially non-clinical information and ensures remote monitoring of elderly people who live or spend a lot of time alone. the need to create a tool of this type is due to the fact that, increasingly, we want to focus on the prevention and safety of people, especially the elderly who are in situations of some fragility (e.g., due to illness, isolation), but simultaneously trying to pass on a sense of companionship, even at a distance, trying to preserve their autonomy as much as possible. in this context, the project aims to explore mixed telemonitoring scenarios, through information from sensors connected to the house and wearable devices, creating the concept of confort@home, that is, a concept of health and well-being centered on the person, their home and the surrounding environment. the service offering to be considered will include location information inside and outside the home, additional data from devices to be selected, and notifications/alerts; in the application logic, situations and conditions that mirror possible risks and lack of safety of the elderly person will be defined, and alerts will be sent to the caregiver/family member, in order to facilitate the life of both on a daily basis. as a demonstration of the concept, one of the aspects of the confort@home will be developed, materialized in an web application provided in an cloud environment that will allow informal caregivers to monitor their relatives/dependents based on their location."
    ],
    [
      "technological evolution is impacting several industries, e.g., by allowing them to deliver higher levels of functionality. the automotive industry is an example of how technology is supporting the development of new solutions in vehicle safety and comfort. advanced driver assistance systems (adas) are cases of solutions that evolved significantly in recent years. this is possible not only due to the progress of electronic solutions but also because of higher quality in software. the smartphone is an example of this evolution with a broad range of applicability since these devices have been used to develop adas, making them an interesting cost-effective platform to develop such systems. previous research has shown smartphones’ ability to output sensors data with the necessary quality for a broad number of applications with special focus in inertial sensors. however, such studies tend to be difficult to reproduce or lack the desired detail levels of their experimental methods. concerns about how good are smartphone sensors and their use to develop adas emerge when reading existing literature, particularly, how the context of collecting data is controlled and which variables impact the collection process. in order to assess the feasibility of using smartphones as sensing devices, questions arise on how different parts of the collection setup affect the quality of data collected. motivated by those questions, a study considering four different hypotheses is proposed to assess the impact of a controlled set of variables, namely: brands of inertial sensors, car mounts, sensor sampling rates, and vehicles. a set of controlled experiments is performed to assess the impact of each variable in the collection process of inertial sensors, more precisely the vertical acceleration. to perform the experiments, three special-purpose tools were developed. smartphones used in the experiments feature an application to collect and export their sensors data. a researcher of an experiment operates another smartphone application to annotate road anomalies found while driving. a desktop application automates the computation and statistical validation of the vertical acceleration correlation from different setups. dynamic time warping was used to compute the correlation coefficient of vertical acceleration as measured by different devices. results show a baseline correlation coefficient of 0.892 with a standard configuration of software and hardware. when one of the independent variables is changed, the resulting coefficients range from 0.827 to 0.848. randomization tests were executed to statistically validate experiments results, making use of a random shuffle algorithm on surrogate data. such tests rejected all four proposed null hypotheses regarding dissimilarities on vertical acceleration sensed by different setups. from the controlled experiment a deeper understanding of the variables influencing data collection with smartphones was obtained. results showed that varying the inertial sensors, car mounts, rates of sampling, or vehicles had a low impact on vertical acceleration sensed by smartphones. this is a good indicator that smartphones can be used to develop adas without the need to standardize every part of the collection setup. thus, it possible to foresee the deployment of a system to a wider audience by taking advantage of existing equipment.",
      "a evolução tecnológica está a afectar várias indústrias, por exemplo, ao capacitá-las para fornecer níveis mais elevados de funcionalidade. a indústria automóvel é um exemplo da forma como a tecnologia está a apoiar o desenvolvimento de novas soluções de conforto e segurança automóvel. os sistemas avançados de assistência ao condutor – advanced driver assistance systems (adas) – são casos de soluções que evoluíram significativamente nos últimos anos. para tal, não só contribuiu o progresso de soluções electrónicas, mas também o aumento de qualidade do software. os smartphones são um exemplo desta evolução de ampla aplicabilidade, sendo já utilizados para desenvolver adas e uma interessante plataforma para desenvolver tais sistemas com baixo custo. estudos anteriores demostraram a capacidade dos smartphones para fornecer dados de sensores com a qualidade necessária para um grande número de aplicações, com especial foco nos sensores inerciais. no entanto, tais estudos tendem a ser de difícil reprodução ou não possuem o nível de detalhe desejado nos seus métodos experimentais. questões sobre a qualidade dos sensores dos smartphones e o seu uso para desenvolver adas surgem do estudo da literatura existente, particularmente como a recolha de dados pode ser controlada e que variáveis têm impacto nesse processo. para avaliar a viabilidade do uso de smartphones como dispositivos sensoriais, nascem questões sobre como as diferentes partes do sistema afetam a qualidade dos dados recolhidos por ele. motivado por essas questões, é proposto o estudo de quatro hipóteses para medir o impacto de um conjunto de variáveis, a saber: sensores inerciais, suportes de telemóvel, taxas de amostragem dos sensores, e veículos. experiências controladas são realizadas para estudar o impacto de cada variável no processo de recolha de dados de sensores, mais precisamente a aceleração vertical. foram desenvolvidas três ferramentas de software para a realização das experiências. os smartphones usados possuem uma aplicação para recolher e exportar os dados dos seus sensores. durante a experiência, um investigador utiliza outra aplicação de smartphone para anotar as anomalias da estrada encontradas durante a condução. uma aplicação de desktop automatiza a computação e validação estatistica da correlação da aceleração vertical medida por diferentes dispositivos. o coeficiente de correlação da aceleração vertical medida por diferentes dispositivos fez-se usando o algoritmo dynamic time warping. os resultados mostram um coeficiente de 0.892 com uma configuração padrão de software e hardware, que serve como base de análise. quando uma das variáveis independentes é alterada, os coeficientes resultantes variam entre 0.827 e 0.848. testes de permutação foram executados para validar estatisticamente os resultados experimentais, usando o algoritmo random shuffle sobre dados substitutos. esses testes rejeitaram as quatro hipóteses nulas relativas à diferença de aceleração vertical detetada por diferentes dispositivos. a partir das experiências obteve-se uma compreensão aprofundada das variáveis que influenciam a coleção de dados com smartphones. os resultados mostram que variar os sensores inerciais, suportes de telemóvel, taxas de amostragem, e veículos tem baixo impacto na aceleração vertical detetada. isto indica que estes dispositivos podem ser usados para desenvolver adas sem a necessidade de padronizar cada peça da recolha de dados. assim, é possível antever o desenvolvimento de um sistema para um público mais amplo, tirando partido de equipamentos já existentes."
    ],
    0.06666666666666667
  ],
  [
    [
      "in this dissertation we cover the implementation in haskell of an interpreter for a while-language capable of handling both hybrid and probabilistic effects. the interpreter is supported by both operational and denotational semantics which were devised in this dissertation as well. we started by studying a pre-existing syntax and operational semantics of a programming language capable of performing wait calls and probabilistic choices through a random-number-generator. we then redefined this semantics to another one that is more suitable for statistical analysis in programming. next we performed another iteration over these two semantics, more specifically we extended them to support full hybrid behaviour, traditionally used to encode interactions between digital devices and physical processes such as movement and time. we also devised two denotational semantics corresponding to the operational semantics mentioned before, as a way of providing a mathematical abstraction, through the use of monads, to the programs of our language. not only this, we also implemented a domain specific language embedded into haskell, which thus provides to the hybrid programmer all the expressive power that haskell offers in addition to a palette of combinators designed specifically for the hybrid domain. such gives rise to an expressivity power much greater than what the aforementioned while-language can provide. lastly, we presented and analysed several deterministic hybrid programs, such as cruise controllers, and added subtle probabilistic elements to them that reflect certain real-word scenarios. such an addition lead from one possible execution to several possible executions; and most notably some of the latter revealed safety issues introduced by the probabilistic elements. all in all this dissertation has both theoretical and practical contributions that form a stepping stone towards a rigorous engineering discipline of probabilistic hybrid systems.",
      "através desta dissertação descrevemos uma implementação em haskell de um interpretador para uma linguage while capaz de lidar tanto com o efeito híbrido como com o efeito probabilístico. o interpretador é suportado por semânticas operacionais e denotacionais que foram definidas também nesta dissertação. primeiramente, abordamos uma sintaxe e semântica operacional pré-existentes de uma linguagem capaz de realizar chamadas de espera e escolhas probabilísticas, através de um gerador de números aleatórios. de seguida, redefinimos esta semântica numa outra que é mais apropriada para a análise estatística dos programas. consequentemente, realizamos uma outra iteração sobre estas semânticas sendo que as estendemos para suportarem totalmente o comportamento híbrido, tradicionalmente utilizado para representar interações entre dispositivos digitais e físicos, tais como movimento e tempo. criámos também duas semânticas denotacionais, correspondentes às semânticas anteriores, de modo a fornecer uma abstração matemática, através da utilização de mónadas, para os possíveis programas da nossa linguagem. para além disto, foi implementada uma domain specific language incorporada em haskell, o que providencia ao programador híbrido todo o poder que o haskell oferece conjugado com combinadores definidos especialmente para o domínio híbrido. tal dá origem a um poder expressivo muito maior do que aquele que a linguagem while supramencionada pode providenciar. por fim, abordaram-se vários exemplos de programas híbridos determinísticos, tais como sistemas de cruise control, aos quais foram adicionados elementos probabilísticos subtis que refletem cenários do mundo real. esta adição fez com que fossemos de uma execução para uma variedade de execuções possíveis; e mais notávelmente alguns dos sistemas revelaram problemas relacionados com a segurança quando os elementos probabílisticos foram introduzidos. em suma, esta dissertação apresenta contribuições teóricas e práticas que se traduzem num passo em direção a uma dísciplina de engenharia rigorosa sobre sístemas híbridos probabilísticos."
    ],
    [
      "currently, in the health area, there is a need for systems that provide support for the decision of health professionals through specific recommendations for each patient based on clinical practice guidelines (cpgs) for automatic interpretation. cpgs are documents that have enormous importance in the daily life of health professionals, playing a key role in reducing variations in medical practice, improving the quality of health care, and reducing health care costs. these documents reflect knowledge about how best to diagnose and treat diseases in the form of a list of clinical recommendations. however, there may be conflicts and interactions in the application of these clinical recommendations, that which in their maximum exponent may impair the patient’s clinical condition. these conflicts are transported to decision support systems, creating the need to develop computational methods to solve these same conflicts. in the case of multimorbid patients, this resolution of conflicts can be very problematic because these patients suffer from several pathologies at the same time, and that the use of a drug for one particular pathology may have a detrimental effect on the application of another drug in another pathology. therefore, the objective of this dissertation topic is the determination of conflicts and interactions between drugs and the determination of these same alternatives.",
      "atualmente na área da saúde, existe uma necessidade de existirem sistemas que forneçam apoio à decisão dos profissionais de saúde através de recomendações específicas para cada paciente com base em protocolos clínicos para interpretação automática. os protocolos clínicos são documentos que têm enorme importância no dia-a-dia dos profissionais de saúde, desempenhando um papel fundamental na redução das variações na prática médica, na melhoria da qualidade dos cuidados de saúde e na redução dos custos de saúde. estes documentos reflectem o conhecimento sobre a melhor forma de diagnosticar e tratar doenças na forma de uma lista de recomendações clínicas. contudo, podem existir conflitos e interações na aplicação destas recomendações clínicas, que no seu expoente máximo poderão levar a um agravamento do estado clínico do paciente, nomeadamente no caso da aplicação de diferentes fármacos. estes conflitos são transportados para os sistemas de apoio à decisão, criando a necessidade de desenvolver métodos computacionais de resolução destes mesmos conflitos. no caso dos pacientes multimórbidos esta resolução de conflitos pode ser bastante problemática devido ao facto destes pacientes sofrerem de várias patologias ao mesmo tempo, e que a utilização de um fármaco para uma determinada patologia possa vir a ter um efeito nocivo na aplicação de outro fármaco noutra patologia. sendo assim, o objetivo deste tema de dissertação é a determinação dos conflitos e interações entre fármacos e a determinação dessas mesmas alternativas."
    ],
    0.3
  ],
  [
    [
      "uma das consequências do problema emergente das alterações climáticas, devido ao aumento do stress antropogénico, é a falta de informação sobre o impacto específico destas alterações nos ecossis temas e as potenciais medidas para as atenuar. este facto cria uma necessidade premente de melhores ferramentas de biomonitorização para avaliar o estado de diversos ecossistemas. especificamente, os ecossistemas de água doce que desempenham um papel crucial na prestação de bens e serviços essen ciais à humanidade. no entanto, os esforços de investigação e biomonitorização concentram-se frequen temente nos invertebrados e algas, deixando uma lacuna na monitorização dos microrganismos que são chave para o funcionamento dos ecossistemas aquáticos. muitos microrganismos que habitam nestes ambientes não são cultiváveis em meios tradicionais, o que constitui um desafio para a compreensão das suas respostas a vários gradientes de stress ambiental. para melhorar a compreensão e facilitar a tomada de decisão para a proteção destes ecossistemas, torna-se imperativo dispor de dados mais precisos so bre os microrganismos presentes nestes ecossistemas. este trabalho teve como objetivo colmatar essa lacuna, usando a metagenómica para ajudar a compreender como a comunidade de fungos e bactérias é afetada pelos diferentes fatores de stress ambiental. as amostras biológicas foram recolhidas em 50 locais localizados em 4 bacias hidrográficas do norte de portugal, dos rios ave, cávado, lima e minho. a amostragem consistiu em recolher folhada naturalmente acumulada no leito dos rios. as amostras foram recolhidas durante o verão de 2020. para além do material biológico, foram também recolhidos dados de variáveis ambientais nos mesmos locais de amostragem. o material genético foi extraído e foi feita uma sequenciação completa do genoma das comunidades presentes. a identificação taxonómica foi feita com recurso ao servidor web kaiju. alguns taxa destacaram-se pela sua dominância nas amostras em relação aos restantes. a riqueza de taxa de fungos apresentou uma resposta negativa à presença combinada dos fatores de stress ambiental, à intensificação do uso do solo, e ao aumento da condutividade e da concentração de azoto inorgânico dissolvido. por outro lado, a riqueza taxonómica dos fungos aumentou com o aumento da altitude. a intensificação do uso do solo foi o fator de stress que melhor explicou a riqueza em taxa. a diversidade taxonómica das bactérias apresentou uma resposta à velocidade da corrente de acordo com o modelo de gauss. isto significa que um distúrbio moderado pode favorecer a biodiversidade, o poderá ser explicado pela hipótese do distúrbio intermédio.",
      "the environmental degradation driven by increased human activities has presented numerous press ing challenges that require urgent attention and careful resolution. one such challenge is the limited understanding of how these changes affect ecosystems and which measures can be taken to mitigate these impacts. this creates a pressing need for better biomonitoring tools to assess the state of ecosys tems. freshwater ecosystems are among the most endangered ecosystems on earth, despite harboring a large proportion of biodiversity on earth and providing essential goods and services for humanity. how ever, biomonitoring in freshwaters tends to focus on benthic macroinvertebrates and diatoms, thus leaving a gap in using microorganisms despite they play a key role in ecosystem functioning. many microorgan isms that inhabit these environments cannot be cultivated in traditional media, which poses a challenge for understanding their responses to environmental stress. to improve our understanding and facilitate informed decision-making to protect freshwaters, obtaining more accurate data on the microorganisms present in these ecosystems are needed. this work aimed to fill this gap, using metagenomics to better understand how fungal and bacterial communities are affected by multiple environmental stressors. the biological samples were collected from 50 sites located in 4 river basins in northern portugal, from the ave, cávado, lima and minho rivers. sampling consisted in retrieving leaf litter naturally accumulated in the riverbed at each site. the samples were collected during the summer of 2020. in addition, environ mental variables were characterized at the same sites. subsequently, the samples’ genetic material was extracted from leaf litter, and dna was subjected to shotgun sequencing using illumina miseq technology. taxonomic identification was performed for each read using the kaiju web server. the richness of fungal taxa showed a negative response to a combination of environmental stress factors. at the individual level, the intensification of land use, conductivity and dissolved inorganic nitrogen negatively affected fungal taxon richness. conversely, richness increased with altitude. the intensification of the land use index was the stress factor that best explained the richness of fungal taxa. diversity of bacterial taxa responded to current velocity according to a gaussian model. this suggests that moderate disturbances may contribute to greater bacterial diversity as found in the intermediate disturbance hypothesis."
    ],
    [
      "spectre attacks pose a significant threat to modern computer systems, exploiting speculative execution to leak sensitive information from a program. since speculative execution is present in modern cpu it is of high priority to protect programs against these spectre attacks without reducing performance. this study presents a comprehensive comparison of mitigation strategies employed by different tools to counteract the effects of spectre attacks. however, this paper focuses its analyses on the type system from the jasmin framework and on the blade tool. the type system uses three main primitives that work together to protect vulnerable variables from leaking. on the other hand, the blade tool allows for an automatic mitigation strategy that implements a min-cut algorithm to a graph representing the different outputs of a program in order to find the minimal cut points that stop speculative execution from leaking vulnerable variables. aside from these strategies, this study also presents in detail the oo7 tool that identifies spectre vulnerable patterns which are used to easily identify if a program is vulnerable to spectre attacks. through an in-depth analysis of their techniques, performance implications, and applicability, this research evaluates the suitability of both strategies to protect cryptography functions against spectre attacks by protecting the blake2b hash function. in the end, this comparative analysis between these two mitigation strategies determines in which scenario or purpose which technique should be used.",
      "os ataques spectre representam uma ameaça significativa para os todos os computadores modernos, já que exploram a execução especulativa para obter informações sensíveis de um programa. uma vez que a execução especulativa está presente em todos os cpus modernos, é de alta prioridade proteger programas contra estes ataques sem reduzir a performance de um programa. este estudo apresenta uma comparação compreensível das estratégias de mitigação realizadas por diferentes ferramentas para combater os efeitos dos ataques spectre. no entanto, este artigo concentra a sua análise no type system da framework jasmin e na ferramenta blade. o type system utiliza três primitivas principais que trabalham juntas para proteger variáveis vulneráveis de ser lidas por entidades externas. por outro lado, a ferramenta blade fornece uma estratégia automática de mitigação que implementa o algoritmo min-cut num gráfico que representa os diferentes outputs de um programa, a fim de encontrar os cortes mínimos que impeçam a execução especulativa de ler variáveis vulneráveis. além destas estratégias, este estudo também apresenta em detalhe a ferramenta oo7, que identifica padrões vulneráveis a ataques spectre que são usados para identificar facilmente se um programa é vulnerável a ataques spectre. através de uma análise aprofundada das técnicas, implicações de performance e aplicabilidade, esta pesquisa avalia o quão adequadas são as estratégias em proteger funções de criptografia, através da proteção da função de hash, blake2b. no final, esta análise comparativa irá determina em qual cenário ou propósito cada estratégia de mitigação deve ser usada."
    ],
    0.0
  ],
  [
    [
      "knowing what lies around us has been a goal for many decades now, and the new advances in sequencing technologies and in meta-omics approaches have permitted to start answering some of the main questions of microbiology - what is there, and what is it doing? the exponential growth of omics studies has been answered by the development of some bioinformatic tools capable of handling metagenomics (mg) analysis, with a scarce few integrating such analysis with metatranscriptomics (mt) or metaproteomics (mp) studies. furthermore, the existing tools for meta-omics analysis are usually not user friendly, usually limited to command-line usage. because of the variety in meta-omics approaches, a standard workflow is not possible, but some routines exist, which may be implemented in a single tool, thereby facilitating the work of laboratory professionals. in the framework of this master thesis, a pipeline for integrative mg and mt data analysis was developed. this pipeline aims to retrieve comprehensive comparative gene/transcript expression results obtained from different biological samples. the user can access the data at the end of each step and summaries containing several parameters of evaluation of the previous step, and final graphical representations, like krona plots and differential expression (de) heatmaps. several quality reports are also generated. the pipeline was constructed with tools tested and validated for meta-omics data analysis. selected tools include fastqc, trimmomatic and sortmerna for preprocessing, metaspades and megahit for assembly, metaquast and bowtie2 for reporting on the quality of the assembly, fraggenescan and diamond for annotation and deseq2 for de analysis. firstly, the tools were tested separately and then integrated in several python wrappers to construct the software meta-omics software for community analysis (mosca). mosca performs preprocessing of mg and mt reads, assembly of the reads, annotation of the assembled contigs, and a final data analysis. real datasets were used to test the capabilities of the tool. since different types of files can be obtained along the workflow, it is possible to perform further analyses to obtain additional information and/or additional data representations, such as metabolic pathway mapping.",
      "o objectivo da microbiologia, e em particular daqueles que se dedicam ao estudo de comunidades microbianas, é descobrir o que compõe as comunidades, e a função de cada microrganismo no seio da comunidade. graças aos avanços nas técnicas de sequenciação, em particular no desenvolvimento de tecnologias de next generation sequencing, surgiram abordagens de meta-ómicas que têm vindo a ajudar a responder a estas questões. várias ferramentas foram desenvolvidas para lidar com estas questões, nomeadamente lidando com dados de metagenómica (mg), e algumas poucas integrando esse tipo de análise com estudos de metatranscriptómica (mt) e metaproteómica (mp). além da escassez de ferramentas bioinformáticas, as que já existem não costumam ser facilmente manipuláveis por utilizadores com pouca experiencia em informática, e estão frequentemente limitadas a uso por linha de comando. um formato geral para uma ferramenta de análise meta-ómica não é possível devido à grande variedade de aplicações. no entanto, certas aplicações possuem certas rotinas, que são passíveis de serem implementadas numa ferramenta, facilitando assim o trabalho dos profissionais de laboratório. nesta tese, uma pipeline integrada para análise de dados de mg e mt foi desenvolvida, pretendendo determinar a expressão de genes/transcriptos entre diferentes amostras biológicas. o utilizador tem disponíveis os resultados de cada passo, sumários com vários parâmetros para avaliação do procedimento, e representações gráficas como gráficos krona e heatmaps de expressão diferencial. vários relatórios sobre a qualidade dos resultados obtidos também são gerados. a ferramenta foi construída baseada em ferramentas e procedimentos testados e validados com análise de dados de meta-ómica. essas ferramentas são fastqc, trimmomatic e sortmerna para pré-processamento, megahit e metaspades para assemblagem, metaquast e bowtie2 para controlo da qualidade dos contigs obtidos na assemblagem, fraggenescan e diamond para anotação e deseq2 para análise de expressão diferencial. as ferramentas foram testadas uma a uma, e depois integradas em diferentes wrappers de python para compôr a meta-omics software for community analysis (mosca). a mosca executa pré-processamento de reads de mg e mt, assemblagem das reads, anotação dos contigs assemblados, e uma análise de dados final foram usados dados reais para testar as capacidades da mosca. como podem ser obtidos diferentes tipos de ficheiros ao longo da execução da mosca, é possível levar a cabo análises posteriores para obter informação adicional e/ou representações de dados adicionais, como mapeamento de vias metabólicas."
    ],
    [
      "os sistemas de informação (si) das unidades de saúde recorrem, cada vez mais, a ferramentas informáticas para gerir a grande quantidade de informação, e assim garantir a qualidade e a segurança da mesma. a interoperabilidade semântica é uma urgência nos sistemas de informação de saúde (sis), pois, através de normas, permite a uniformização dos termos médicos, assegurando registos clínicos com informação fiável, sem redundância e ambiguidade, conferindo qualidade e segurança à informação. sem terminologias médicas a prestação de cuidados de saúde pode tornar uma tarefa complexa e conduzir a erros médicos, pelo que a utilização das mesmas é fulcral para o registo de diagnósticos, procedimentos e peças anatómicas no registo clínico eletrónico (rce) de cada utente. como tal, o desenvolvimento de uma plataforma de interoperabilidade semântica vai permitir uniformizar termos médicos e conduzir à diminuição de erros. recorrendo às tecnologias mais avançadas de ehealth e mhealth, pretende-se implementar o systematized nomenclature of medicine clinical terms (snomed ct) em contexto hospitalar real, num serviço de anatomia patológica. a solução consiste numa aplicação independente da plataforma de registo de relatórios médicos, utilizando web services, que proporcionam a interação humana com diferentes interfaces para diferentes tipos de dispositivos eletrónicos.",
      "healthcare information systems use software tools to manage the vast amount of information, to ensure safety and quality information. semantic interoperability is an urgency in healthcare information systems, and through standards, enables the standardization of medical terms, securing medical records with reliable information without redundancy and ambiguity, providing quality and safety information. without medical terminologies, healthcare tasks can become complex and lead to medical errors, so the use of them is essential for the record of diagnoses, procedures and body parts in electronic health record. thus, the development of a semantic interoperability platform will allow standardizing medical terms and lead to fewer errors. to this end, we use the most advanced technologies of ehealth and mhealth to implement the systematized nomenclature of medicine clinical terms (snomed ct) in a real hospital environment. in conclusion, the solution consists of an independent platform for the medical records using web services, which provide human interaction with diferent interfaces for di erent types of eletronic devices."
    ],
    0.023076923076923078
  ],
  [
    [
      "a recente popularidade dos ambientes de grelhas introduziu a necessidade de suportar a execução robusta de aplicações numa gama alargada de recursos computacionais. em contextos de grelhas computacionais, onde a fiabilidade e disponibilidade dos recursos não é garantida, as aplicações deverão ser capazes de suportar dois requisitos fundamentais: 1) tolerância a faltas; 2) adaptação aos recursos disponíveis. as técnicas tradicionais utilizam uma abordagem \"caixa-negra\", onde a camada intermédia de software (mediador) é a única responsável por assegurar estes dois requisitos. estes tipos de abordagens possibilitam o suporte a estes serviços com uma intervenção mínima do programador, mas limitam a utilização de conhecimento sobre as características da aplicação, visando a otimização destes serviços. nesta tese são apresentadas abordagens orientadas aos aspetos para suportar tolerância a faltas e adaptação dinâmica aos recursos em grelhas computacionais. nas abordagens propostas, as aplicações são aprimoradas com capacidades de tolerância a faltas e de adaptação dinâmica através da ativação de módulos adicionais. a abordagem de tolerância a faltas utiliza a estratégia de ponto de controlo e restauro, enquanto a adaptação dinâmica utiliza uma variação da técnica de sobre-decomposição. ambas são portáveis entre sistemas operativos e restringem a quantidade de alterações necessárias no código base das aplicações. além disso, as aplicações poderão adaptar de uma execução sequencial para uma configuração multi-cluster. a adaptação pode ser realizada efetuando o ponto de controlo da aplicação e restaurando-a em diferentes máquinas, ou então, realizada em plena execução da aplicação.",
      "grids’ recent popularity introduced the necessity of supporting robust execution of applications on a wide range of computing resources. in computational grids’ context, where reliability and availability are not granted, applications must support two fundamental requirements, namely, fault tolerance and adaptation to available resources. traditional techniques use a \"black-box\"approach, where middleware is the only sponsor for those requirements. these kind of approaches enable this services’ support with a minimum programmer’s intervention, but limits knowledge utilization of application’s features in order to optimize services. this thesis presents aspect-oriented approaches to support fault tolerance and dynamic adaptation to resources in computational grids. in the proposed approaches, applications are enhanced with the ability of fault tolerance and dynamic adaptation through additional modules activation. fault tolerance approach uses a check point and restore strategy while dynamic adaptation uses a variation of the over-decomposition technique. both are portable between operating systems and minimize alterations to base code of applications. moreover, applications can adapt from a sequential execution to a multi-cluster configuration. adaption can be performed by checkpointing the application and restarting on a different mode or can be performed during run-time."
    ],
    [
      "atualmente, o setor hoteleiro vive uma intensa expansão devido ao crescimento da atividade turística. este constitui um sector de atividades económicas onde as tecnologias têm vindo a manifestar-se como primordiais para a eficiência das organizações. de modo a enfrentar a competitividade, as organizações devem adquirir recursos necessários de maneira a tornarem-se os benfeitores do mercado em que estão inseridas. nesta dissertação descreve-se o desenvolvimento de duas plataformas na área de hotelaria que visam integrar o leque dos produtos cloud comercializados pela [wintouch]. as duas plataformas inserem-se em dois domínios distintos mas complementares. a primeira deverá conter as funcionalidades necessárias para ajudar na gestão eficientemente das operações de um hotel. para além disso, deverá, de acordo com o tamanho e a natureza do hotel, conseguir adaptar-se às as necessidades individuais de cada estabelecimento, mostrando-se flexível e abrangente. este componente deverá estar integrado com a solução de gestão comercial fornecida pela [wintouch]. a segunda plataforma deverá complementar a primeira, e visa ajudar as governantas de um hotel com a gestão da limpeza do mesmo, bem como a estabelecer um canal de comunicação e registo entre as governantas e os outros departamentos da unidade hoteleira. tratando-se de plataformas que têm como fim serem comercializadas no mercado, estas terão de cumprir todos os requisitos de performance e funcionalidade associados, de modo a cumprir os padrões de qualidade esperados pelos parceiros comerciais da [wintouch]. este documento demonstra a aplicação de uma design science research na abordagem de um problema na área de hotelaria. o foco na conceção e desenvolvimento de soluções inovadoras permitiu a criação de uma solução prática e aplicável ao problema. além disso, a utilização da metodologia agile no processo de desenvolvimento assegurou que o trabalho fosse desenvolvido de forma incremental, com feedback e melhoria contínua. esta abordagem permitiu uma forma eficiente e eficaz de gestão, resultando num projeto estruturado e abrangente.",
      "currently, the hotel sector is undergoing an intense expansion due to the growth of the tourism activity. this is a sector of economic activities where technologies have been manifesting themselves as primordial for the efficiency of organizations. in order to face all the competitiveness, organizations must acquire the necessary resources so as to become the benefactors of the market in which they are inserted. this dissertation describes the development of two platforms in the hospitality sector that aim to integrate the range of cloud products marketed by [wintouch]. the two platforms fall into two distinct but complementary areas. the first should contain the necessary functions to help manage a hotel’s operations efficiently. in addition, depending on the size and nature of the hotel, it should be able to adapt to the individual needs of each establishment, proving flexibility and inclusivity. this component should be integrated with the commercial management solution provided by [wintouch].the second platform should complement the first, and aims to help hotel housekeepers manage the hotel’s cleaning, as well as establishing a communication and registration channel between the housekeepers and the hotel’s other departments. as these platforms are intended to be sold on the market, they will have to meet all the associated per formance and functionality requirements in order to meet the quality standards expected by [wintouch]’s commercial partners. this paper demonstrates the application of a design science research approach to a problem in the hospitality industry. the focus on the design and development of innovative solutions allowed the creation of a practical and applicable solution to the problem. furthermore, the use of the agile methodology in the development process ensured that the work was developed incrementally, with feedback and continuous improvement. this approach allowed for an efficient and effective form of management, resulting in a structured and comprehensive project."
    ],
    0.0
  ],
  [
    [
      "as aplicações mobile health (m-health) desenvolvidas para dispositivos móveis têm sido alvo de muitas investigações, sendo a área da saúde um dos principais focos de desenvolvimento, em que estas aplicações pretendem auxiliar nas práticas médicas. um dos principais desa os da área da saúde é melhorar a acessibilidade e a disponibilidade da informação clínica dos pacientes. com este desa o em mente, foi desenvolvida a agência para a integração, difusão e arquivo de informação médica (aida) de forma a garantir a interoperabilidade entre os vários sistema de informação hospitalar (sih), estando esta implementada num dos maiores centros hospitalares de portugal, o chp. o desenvolvimento de uma aplicação para dispositivos móveis que permita a visualização e consulta da informação clínica dos pacientes, providenciando um acesso rápido e fácil a esta informação aos pro ssionais de saúde, é o principal objetivo do presente projeto. a aplicação pode ser considerada como uma ferramenta adicional do processo clínico eletrónico da aida (aidapce) e é para uso dos pro ssionais de saúde pertencentes ao chp. este projeto expõe assim uma nova metodologia que se baseia nos princípios da computação calma para a apresentação dos relatórios dos meios complementares de diagnóstico e terapêutica (mcdt) dos pacientes e pretende melhorar a qualidade e a e ciência na prestação dos cuidados de saúde. um dos objetivos deste projeto incide na realização de uma prova de conceito à metodologia abordada, de forma a avaliar o impacto da implementação da aplicação desenvolvida no chp. desta forma, foi realizada uma análise strengths weaknesses opportunities and threats (swot), com o intuito de identi car as suas forças e fragilidades, e foram realizados testes de e ciência e usabilidade, em que para este último recorreu-se ao método de avaliação de usabilidade do tipo inquérito. a realização destes estudos permitiu averiguar os resultados que teria a implementação da aplicação, veri cando que esta seria bem aceite e utilizada pelos pro ssionais de saúde.",
      "the m-health applications developed for mobile devices have been the target of many researches, being the area of health a major focus of development, in which these applications are intended to support the healthcare practices. one of the major challenges in the area of health is to improve the accessibility and availability of patients' clinical information. with this challenge in mind, it has been developed the agency for integration, di usion and archive of medical information (aida) in order to ensure interoperability among the various hospital information systems, which is implemented in one of the major portuguese hospital centre, the chp. the development of an application for mobile devices that allows the visualization and consultation of patients' clinical information, providing quick and easy access to this information for healthcare professionals. this is the main objective of this project. the application can be considered as an additional tool of electronic health record of aida (aida-ehr) and it was created for the chp healthcare professionals. the current project thereby presents a new methodology based on the principles of calm computing applied to diagnostic and therapeutic procedure reporting of patients and it aims the improvement of quality and e ciency in healthcare providing. one of the aims of this project focuses on the realization of a proof of concept (poc) to the discussed metodology, in order to assess the impact of the developed application in the chp. thus, it was performed a swot analyses, in order to identify its strengths and fragilities. an e ciency and usability tests were performed too. it was adopted the inquiry method. these studies allowed the prediction of the application implementation results, verifying that it would be accepted and used by the healthcare professionals."
    ],
    [
      "since the dawn of times, curiosity and necessity to improve the quality of their life, led humans to find means to understand everything surrounding them, aiming at improving it. whereas the creating abilities of some was growing, the capacity to comprehend of others follow their steps. disassembling physical objects to comprehend the connections between the pieces in order to understand how they work together is a common human behavior. with the computers arrival, humans felt the necessity of applying the same techniques (disassemble to comprehend) to their programs. traditionally, these programs are written resorting to general-purpose programming languages. hence, techniques and artifacts, used to aid on program comprehension, were built to facilitate the work of software programmers on maintaining and improving programs that were developed by others. generally, these generic languages deal with concepts at a level that the human brain can hardly understand. so understanding programs written in this languages is an hard task, because the distance between the concepts at the program level and the concepts at the problem level is too big. thus, as in politics, justice, medicine, etc. groups of words are regularly used facilitating the comprehension between people, also in programming, languages that address a specific domain were created. these programming languages raise the abstraction of the program domain, shortening the gap to the concepts of the problem domain. tools and techniques for program comprehension commonly address the program domain and they took little advantage of the problem domain. in this master’s thesis, the hypothesis that it is easier to comprehend a program when the underlying problem and program domains are known and a bridge between them is established, is assumed. then, a program comprehension technique for domain specific languages, is conceived, proposed and discussed. the main objective is to take advantage from the large knowledge about the problem domain inherent to the domain specific language, and to improve traditional program comprehension tools that only dealt, until then, with the program domain. this will create connections between both program and problem domains. the final result will show, visually, what happens internally at the program domain level, synchronized with what happens externally, at problem level.",
      "desde o início dos tempos a curiosidade e a necessidade de melhorar a qualidade de vida impeliram o humano a arranjar meios para compreender o que o rodeia com o objectivo de melhorar. à medida que a habilidade de uns foi aumentando, a capacidade de compreensão de outros seguiu-lhe os passos. desmontar algo físico de modo a compreender as ligações entre as peças e assim perceber como funcionam num todo, é um acto bastante normal dos humanos. com o advento dos computadores e os programas para ele codificados, o homem sentiu a necessidade de aplicar as mesmas técnicas (desmontar para compreender) ao código desses programas. tradicionalmente, a codificação de tais programas é feita usando linguagens genéricas de programação. desde logo técnicas e artefactos que ajudam na compreensão desses programas (nessas linguagens) foram produzidas para auxiliar o trabalho de engenheiros de software que necessitam de manter ou alterar programas previamente construídos por outros. de um modo geral estas linguagens mais genéricas lidam com conceitos a um nível bastante abaixo daquele que o cérebro humano, facilmente, consegue captar. previsivelmente, compreender programas neste tipo de linguagens é uma tarefa complexa pois a distância entre os conceitos ao nível do programa e os conceitos ao nível do problema (que o programa aborda) é bastante grande. deste modo, tal como no dia-a-dia foram surgindo nichos como a política, a justiça, a informática, etc. onde grupos de palavras são usadas com maior regularidade para facilitar a compreensão entre as pessoas, também na programação foram surgindo linguagens que focam em domínios específicos, aumentando a abstracção em relação ao nível do programa, aproximando este do nível dos conceitos subjacentes ao problema. ferramentas e técnicas de compreensão de programas abordam, geralmente, o domínio do programa, tirando pouco partido do domínio do problema. na presente tese assume-se a hipótese de que será mais fácil compreender um programa quando os domínios do problema e do programa são conhecidos, e entre eles é estabelecida uma ponte de ligação; e parte-se em busca de uma técnica de compreensão de programas para linguagens de domínio específico, baseada em técnicas já conhecidas para linguagens de carácter geral. o objectivo prende-se com aproveitar o conhecimento sobre o domínio do problema e melhorar as ferramentas de compreensão de programas existentes para as linguagens genéricas, de forma a estabelecer ligações entre domínio do programa e domínio do problema. o resultado será mostrar, visualmente, o que acontece internamente ao nível do programa, sincronizadamente com o que acontece externamente ao nível do problema."
    ],
    0.0857142857142857
  ],
  [
    [
      "a crescente evolução tecnológica verificada nas últimas décadas e a necessidade de criação e implementação de novas soluções de software, levam a que o papel dos testes de software assuma, cada vez mais, uma maior relevância no respetivo ambiente de desenvolvimento. boas ferramentas de gestão e monitorização de testes, assim como, de geração de relatórios de estados e resultados associados aos mesmos testes, melhoram o processo de desenvolvimento tornando-o melhor e mais completo a todos os níveis. o principal objetivo da presente dissertação de mestrado é criar uma nova aplicação de geração de relatórios em páginas confluence, utilizando como fonte de dados, os testes gerados a partir do xray. o xray é uma aplicação de gestão e monitorização de testes de software em instâncias jira. o desenvolvimento da aplicação implica o estudo das diferentes estratégias de desenvolvimento de software disponíveis no ecossistema atlassian. a aplicação xray foi analisada em detalhe, com especial foco nos relatórios de testes de software disponibilizados, o que permitiu a formulação de uma proposta de solução para o problema em questão. de seguida, foram analisadas diferentes frameworks de desenvolvimento de software para produtos atlassian, culminando na escolha da ferramenta forge face a todas as vantagens e desvantagens a ela associadas. de igual modo, é apresentado todo o processo de desenvolvimento da aplicação, assim como as estratégias adotadas para a correta implementação de cada um dos relatórios propostos.",
      "the growing technological evolution of recent decades and the need to create and implement new software solutions mean that the role of software testing is becoming increasingly important in the development environment, in the development environment. good tools for managing and monitoring tests, as well as for generating status reports and results associated with the same improve the development process, make this process better and more complete at all levels. the main objective of this master’s thesis is to create a new application for generating reports on confluence pages, using the tests generated from xray as a data source. xray is an application for managing and monitoring software tests on jira instances. the development of the application involves studying the different software development strategies available in the atlassian ecosystem. the xray application was analyzed in detail, with a special focus on the reports provided, which allowed the formulation of a proposed solution to the problem in question. different software development frameworks for atlassian products were then analyzed, culminating in the choice of the forge tool in view of all the advantages and disadvantages associated with it. the entire application development process is presented, as well as the strategies adopted for the correct implementation of each of the proposed reports."
    ],
    [
      "the goal of this dissertation is to explore techniques to improve the efficiency and performance level of scientific applications on computing platforms that are equipped with multiple multi-core devices and at least one many-core device, such as intel mic and/or nvidia gpu devices. these platforms are known as heterogeneous servers, which are becoming increasingly popular both in research environments as in our daily gadgets. to fully exploit the performance capabilities of the heterogeneous servers, it is crucial to have an efficient workload distribution among the available devices; however the heterogeneity of the server and the workload irregularity dramatically increases the challenge. most state of the art schedulers efficiently balance regular workloads among heterogeneous devices, although some lack adequate mechanisms for irregular workloads. scheduling these type of workloads is particularly complex due to their unpredictability, namely on their execution time. to overcome this issue, this dissertation presents an efficient dynamic adaptive scheduler that efficiently balances irregular workloads among multiple devices in a heterogeneous environment. to validate the scheduling mechanism, the case study used in this thesis is an irregular scientific application that has a set of independent embarrassingly parallel tasks applied to a very large number of input datasets, whose tasks durations have an unpredictable range larger than 1:100. by dynamically adapting the size of the workloads that were distributed among the multiple devices in run-time, the scheduler featured in this dissertation had an occupancy rate of every computing resources over 97% of the application’s run-time while generating an overhead well below 0.001%.",
      "o objetivo desta dissertação é o de explorar técnicas que possam melhorar a eficiência e o nível de performance de aplicações cientificas em plataformas de computação que estão equipadas com vários dispositivos multi-core e pelo menos um dispositivo many-core, como por exemplo um intel mic e/ou um gpu da nvidia. estas plataformas são conhecidas como servidores heterogéneos e estão a se tornar cada vez mais populares, tanto em ambientes de investigação como em nossos gadgets diários. para explorar completamente as capacidades de desempenho dos servidores heterogéneos, é crucial ter uma distribuição eficiente da carga de trabalho entre os vários dispositivos disponíveis; no entanto a heterogeneidade do servidor e a irregularidade das cargas de trabalho aumentam drasticamente o desafio. a maioria dos escalonadores mais avançados são capazes de equilibrar eficientemente cargas de trabalho regulares entre dispositivos heterogéneos, embora alguns deles não disponham de mecanismos adequados para cargas de trabalho irregulares. o escalonamento desse tipo de cargas de trabalho é particularmente complexo devido à sua imprevisibilidade, nomeadamente ao seu tempo de execução. para superar este problema, esta dissertação apresenta um escalonador dinâmico e adaptativo que equilibra de forma eficiente cargas de trabalho irregulares entre vários dispositivos de uma plataforma heterogénea. para validar o escalonador, o caso de estudo utilizado nesta tese é uma aplicação científica irregular que possui um conjunto de tarefas independentes, que são embaraçosamente paralelas, aplicadas a um grande número de conjuntos de dados, cujas tarefas têm durações com um n´nível de imprevisibilidade maior do que 1:100. ao adaptar dinamicamente o tamanho das cargas de trabalho, que são distribuídas entre os vários dispositivos, em tempo de execução, o escalonador apresentado nesta dissertação apresenta uma taxa de ocupação de cada dispositivo acima de 97 % do tempo total de execução da aplicação e tem um peso que é bem abaixo dos 0,001 %."
    ],
    0.3
  ],
  [
    [
      "esta dissertação foi desenvolvida sobre no contexto de um projeto proposto pela empresa altice labs, para o desenvolvimento de um aplicação móvel para o catálogo e gestão de equipamentos e infraestruturas dispersos no terreno, que integrará o sistema de gestão de inventário, provisionamento de clientes e projeto de rede, netwin. estudou-se também a possibilidade de complementar a solução através da utilização de tecnologia de realidade aumentada. o projeto surge devido à necessidade de uma solução adaptada à utilização no terreno. o sistema netwin possui uma interface web, mas esta não se destina à utilização no exterior por dispositivos móveis, não só por não ser capaz de funcionar offline, mas também pela dificuldade que a complexidade e extensão desta impõem a um operacional que apenas pretenda catalogar/consultar infraestrutura e equipamento de rede. o projeto foi dividido em duas partes: a primeira do projeto consiste na conceção de uma aplicação que permite cumprir um conjunto de use cases definidos, que passam pelas operações de catálogo/consulta de infraestrutura e equipamento de rede. a segunda parte corresponde a um estudo, especificação e prototipagem de algumas soluções de realidade aumentada utilizando diferentes ferramentas e abordagens de modo a investigar a viabilidade destas no projeto e de que modo podem acompanhar a aplicação. a solução permite aos operacionais no terreno consultar/catalogar e atualizar o estado dos equipamentos, o que permite que o sistema tenha uma visão mais completa e atualizada sobre o estado da rede.",
      "this dissertation was developed in the context of a project proposed by the company altice labs, for the development of a mobile application for the catalog and management of equipment and infrastructure dispersed in the field. this application which will integrate the inventory management system, provisioning of customers and network project, netwin. the possibility of complementing the solution through the use of augmented reality technology was also studied. the project arises due to the need for a solution adapted to use in the field. the netwin system has a web interface, but this is not intended for outdoor use by mobile devices. not only because it is not able to work offline, but also due to the difficulty that its complexity and extension impose on an operator who only wants to catalog/consult infrastructure and network equipment. the project is was divided into two parts: the first part of the project starts with the design of an application that allows the fulfillment of a set of defined use cases, which include catalog/consult operations of infrastructure and network equipment. the second part corresponds to a study, specification and prototyping of augmented reality solutions, using different tools and approaches, in order to investigate their viability in the project, and how they can accompany the application. the solution will allow field operators to consult/catalog and update the status of the equipment, which will allow the system to have a more complete and up-to-date view of the network status."
    ],
    [
      "atualmente, o maior desafio no desenvolvimento de software é referente à a portabilidade das aplicações para as várias plataformas disponíveis, especialmente pela crescente heterogeneidade nos componentes de hardware, de middleware e de software base. o desenho de modelos abstratos de software é uma das formas mais elegantes e eficientes para solucionar este desafio. a model-driven software engineering (mdse) ́é uma metodologia de desenvolvimento em que os modelos são chave em todo o ciclo de vida do projeto, desde a captura de requisitos, passando pelas fases de modelação e desenvolvimento, e por fim nos processos de teste e instalação. o objetivo primário desta dissertação foca-se na construção de uma ferramenta, o mda smart, capaz de interpretar modelos abstratos de software, parametrizáveis, e de gerar automaticamente código fonte para várias plataformas. a ferramenta, caracterizada por uma arquitetura robusta e extensível, é idealizada para permitir a manipulação de modelos de forma ágil, para ser modular o suficiente para integrar novos perfis meta-modelo e para escalar eficientemente para novas plataformas. o mda smart resulta da articulação de uma domain-specific language (dsl) para a gestão dos meta-modelos e consequentes processos de transformação. na utilização da dsl são obtidos processos de transformação rigorosos, com elevado desempenho e que visam maximizar a consistência e portabilidade dos modelos através de medidas ajustadas a destoarem a heterogeneidade entre as plataformas. adicionalmente, a ferramenta visa compatibilizar os modelos de lógica de negócio com os referentes às interfaces gráficas que, conjugados, vão permitir a obtenção de modelos e código fonte com alto nível de consistência e completude.",
      "the current problem of software development stays on solutions portability for the rising number of platforms. this happens because the hardware high speed evolution, as well as middleware and base software has become more complete, efficient, and in more standardized ways. to port a software product for many platforms it demands the use of several technical specifications, such as wireless connections, advanced electronics, and the internet. using a model-driven approach it is possible to reuse software solutions between different targets, since models are not affected by the platform diversity and its evolution. the model-driven software engineering (mdse) is a development methodology where models are the key for all project lifecycle, from requisites gathering, through modeling and development stage, as well as on testing. this dissertation reports on a tool, the mda smart, which is highly parameterizable and driven to support model-2-model and model-2-code transformations. also, instead of using a predefined technology, the tool was built to be scalable and extensible for many different targets. the tool core is based on a domain-specific language (dsl) definition to ensure models consistency and transformations. with a dsl approach it is possible to achieve rigorous and high performance transformations procedures. unlike other tools, this tool is targeted to ensure the models consistency and to provide high independency between abstraction layers, maximizing the source code correctness and portability. the ultimate objective is to support other model-driven frameworks on mda smart. here, to make compatible logic models with interface models and generate new models and source code at higher level of completion and consistency."
    ],
    0.0
  ],
  [
    "este relatório resume o meu percurso profissional centrado em funções de direcção técnica, gestão de projectos, consultoria e formação em tecnologias e sistemas de informação, apresentado para efeitos de obtenção do mestrado em engenharia informática, de acordo com o despacho rt-38/2011, de 21 de junho. a minha experiência centrou-se nas a áreas de infra-estrutura, tecnologia e plataformas de ti, com um grande ênfase na prestação de serviçs de desenho, implementação, operação e manutenção de serviços internet e intranet. entre as entidades empregadoras (directa ou indirectamente) encontram-se universidades, empresas tecnológicas e de marketing, bancos, instituições governamentais, entre outras. a nível pessoal, tive ainda a oportunidade de publicar duas obras técnicas, sobre serviços de ti open-source.",
    [
      "the constant growth of integration and popularity of “internet of things” devices is affecting home automation systems, where new technologies were introduced, in the recent years for this particular sector. these automation systems integrate devices that can be anywhere in the house, connected to a home network, either through a wire or wireless connection. a home automation system can be used to control air conditioning, lighting, pool control systems, home-entertainment systems and much more. within the field of home-entertainment systems, the best known technologies are the digital living network alliance and the digital audio access protocol, which provide interoperability to allow sharing of digital media content between devices across a home network. however, these technologies have the disadvantage of being proprietary, maintaining restrict documentation access, complex architectures and concepts and not optimal to specific purposes, like audio distribution. the main goal of this project was to prove that is possible to use standardized protocols, such as the simple network manager protocol and open source tools in order to develop a music distribution service that allows the implementation of similar features than the ones already existing proprietary technologies. as such, the implementation prototype system allows a user to manage and play audio from a music collection that is stored in a single home audio server. the system architecture enables audio streaming between the server and the various devices in the same local network. further more, the music collection, can integrate virtual audio files that are available from external music sources, like itunes, etc.",
      "o constante crescimento de integração e popularidade da “internet das coisas” tem atualmente afetado sistemas de domótica, onde cada vez mais tecnologias têm vindo a ser desenvolvidas nos últimos anos para este sector em particular. estes sistemas de domótica integram dispositivos que podem estar em qualquer parte de uma casa, ligados à rede seja através de um cabo ou por wireless. um sistema de domótica pode ser usado para controlar: ar condicionado, iluminação, sistemas de controlo de piscinas, sistemas de entretenimento, entre outros. na área de sistemas de entretenimento, as tecnologias mais conhecidas são digital living network alliance e digital audio access protocol, que fornecem interoperabilidade de modo a permitir a partilha de conteúdos digitais multimédia entre dispositivos que se encontram na mesma rede local. contudo, possuem a desvantagem de serem tecnologias proprietárias, com documentação e manuais restritos, arquiteturas e conceitos complexos, e não otimizados para fins específicos, tal distribuição de áudio. o principal objetivo deste projeto foi provar que é possível usar protocolos normalizados, como o simple network manager protocol e ferramentas open source de forma a desenvolver um serviço de distribuição de música que permite a implementação de funcionalidades semelhantes às tecnologias proprietárias já existentes. assim, o protótipo implementado permite a um utilizador gerir e reproduzir áudio de uma coleção de música que se esteja armazenada num servidor de áudio domestico. a arquitetura permite streaming de áudio entre o servidor e os diferentes dispositivos que se encontram na mesma rede local. consequentemente, a coleção de música pode integrar ficheiros de áudio visuais que estejam acessíveis através de fontes externas de música, como por exemplo: itunes, etc."
    ],
    0.0
  ],
  [
    [
      "this dissertation explores the potential of quantum computing, particularly in the context of variational quantum algorithms (vqa), like the variational quantum classifier (vqc). it focuses on overcoming chal lenges such as noise in quantum circuits and optimization complexities. the research introduces adaptive strategies for vqc, enabling dynamic adjustments to circuit depth and expressibility during training. this flexibility aims to improve classification accuracy on datasets. the dissertation starts with a literature review of vqa algorithms, especially adaptive strategies, draw ing insights from various domains, including chemistry. next, it details the proposed adaptive approaches for vqc and presents rigorous experiments to evaluate their performance across diverse datasets, comparing them to the standard vqc. the results show promise with improved accuracy and reduced circuit depth. however, it’s important to note that this work primarily serves as an introduction to the concept of adaptive approaches for classification, focusing on enhancing vqc within its existing context. in summary, this dissertation provides insights into enhancing vqc performance and contributes incrementally to quantum computation in the realm of classification. adaptive vqc addresses challenges posed by noisy quantum devices and offers opportunities for further research in quantum classification algorithms.",
      "esta dissertação explora o potencial da computação quântica, particularmente no contexto de algoritmos quânticos variacionais (vqa), como o classificador quântico variacional (vqc). ele se concentra na superação de desafios como ruído em circuitos quânticos e complexidades de otimização. esta dissertação introduz estratégias adaptativas para vqc, permitindo ajustes dinâmicos na pro fundidade e expressibilidade do circuito durante o treino. esta flexibilidade visa melhorar a precisão da classificação de um conjunto de dados. a dissertação começa com uma revisão da literatura sobre algoritmos vqa, especialmente estratégias adaptativas, extraindo insights de vários domínios, incluindo da química. a seguir, detalha as abordagens adaptativas propostas para vqc e apresenta experimentos rigorosos para avaliar seu desempenho em diversos conjuntos de dados, comparando-os com o vqc padrão. os resultados são promissores pois monstrarm uma maior precisão e profundidade de circuito reduzida. no entanto, é importante notar que este trabalho serve principalmente como uma introdução ao conceito de abordagens adaptativas para classificação, focando no aprimoramento do vqc dentro do seu contexto existente. em resumo, esta dissertação fornece ideos sobre como melhorar o desempenho do vqc e contribui de forma incremental para a computação quântica no domínio da classificação. o vqc adaptativo aborda desafios colocados por dispositivos quânticos ruidosos e oferece oportunidades para pesquisas adicionais em algoritmos de classificação quântica."
    ],
    [
      "nowadays, when it comes to achieving goals in business environments or educational environments, the successful on a person performing a task has an important role. however, this performance can be affected by several factors. one of the most common is the lack of attention. the individual’s attention in performing a task can be determinant for the final quality or even at the task’s conclusion. in this project is intended to design a solution that can help on the reduce or even eliminate the lack of attention on performing a task. the idea consists on develop a software that capture the user behaviour through the mouse and keyboard usage. furthermore, the system will analyse how the devices are used. it will be quantified the attention level and, after several captures for each user, it will be defined for each user an user profile. through standardization of user’s behaviour it will be possible to determine the learning style of each user.",
      "atualmente quando se fala sobre atingir objetivos em ambiente de negócio ou educacional fala-se sobre o desempenho com sucesso, que determinado individuo teve para a realização dos mesmos. contudo, esse desempenho pode ser afetado por diversos fatores, mas um dos mais frequentes, é a falta de atenção. a atenção que um individuo despende para a realização de uma tarefa pode ser determinante para o nível de qualidade que a mesma vai possuir no final, ou mesmo de conseguir que esta seja finalizada. o que se pretende neste projeto é a conceção de uma solução que consiga reduzir ou mesmo eliminar a falta de atenção existente na realização de tarefas. a ideia consiste na criação de um software que irá captar comportamentos dos utilizadores a partir da utilização de um computador ou portátil, através da utilização do rato e do teclado. para além disso, o sistema irá analisar a forma como estes periféricos são utilizados. será também quantificada o nível de atenção dos alunos e, após diversas captações dos dados dos alunos, será definido um perfil para cada utilizador, através da padronização dos seus comportamentos e, poder-se-à determinar o estilo de aprendizagem mais propício a cada utilizador."
    ],
    0.06666666666666667
  ],
  [
    [
      "a un1qnx, s.a., soluções de autenticidade ciber-físicas, é uma empresa sediada em braga, que desenvolve e comercializa sistemas físicos, eletrónicos e cibernéticos de validação e autenticação de produtos, sendo o objetivo a proteção da marca e o combate à contrafação. neste momento, a empresa possui um serviço de autenticação de produtos localizado numa máquina virtual na cloud, mais especificamente na microsoft azure. contudo, a utilização deste serviço é intermitente e passa por períodos de inatividade. porém, quando utilizado, cada execução do serviço é computacionalmente custosa, o que obriga à utilização de uma máquina virtual que tem em conta o caso de máxima utilização. assim, nos intervalos entre utilizações os custos acumulam-se sem aproveitar os recursos alocados. deste modo, esta tese passa por otimizar a utilização dos recursos na cloud, tendo em vista tirar proveito da escalabilidade e elasticidade das tecnologias de computação na nuvem, bem como melhorar a latência dos pedidos. a otimização dos recursos passa por comparar diferentes serviços de diferentes forne cedores e selecionar o que se apresenta como a melhor opção. a fim de realizar estas comparações, fez-se antes uma investigação baseada na metodologia design science research. primeiramente, explorou-se o ambiente da solução (computação na nuvem) e o ambiente do problema, isto é, qual a situação atual da empresa no que diz respeito ao funcionamento do serviço de validação e dos recursos afetos ao mesmo. em segundo lugar, fez-se uma averiguação sobre o estado da arte das tecnologias usadas, das tecnologias que poderiam vir a ser usadas e de outras empresas da mesma área, sobre quais os seus produtos e o seu modo de funcionamento. por último, investigaram-se métodos de seleção e comparação entre várias opções. em terceiro lugar, realizou-se a parte mais trabalhosa e demorada: o desenvolvimento prático. nesta fase realizaram-se testes de performance, a colocação do serviço num docker container e a utilização de kubernetes. ainda nesta última parte, houve vária experimentação com diversas arquiteturas. por fim, o sistema estabilizou numa arquitetura assíncrona, que fez reduzir os custos e, permitiu com que o serviço se adequasse melhor à quantidade de trabalho a processar.",
      "un1qnx, sa, cyber-physical authenticity solutions, is a company headquartered in braga, which develops and markets physical, electronic and cyber systems for validating and authenticating products, with the aim of protecting the brand and combating counterfeiting . at this moment, the company has a product authentication service located in a virtual machine in the cloud, more specifically in microsoft azure. however, the use of this service is intermittent and goes through periods of inactivity. however, when used, each execution of the service is computationally expensive, which requires the use of a virtual machine that takes into account the case of maximum use. thus, in the intervals between uses, costs accumulate without taking advantage of the allocated resources. thus, this thesis involves optimizing the use of resources in cloud, with a view to taking advantage of the scalability and elasticity of cloud computing technologies, as well as improving the latency of requests. resource optimization involves comparing different services from different providers and selecting the best option. in order to make these comparisons, an investigation based on the design science research methodology was carried out. first, the solution environment (cloud computing) and the problem environment were explored, that is, the current situation of the company with regard to the functioning of the validation service and the resources allocated to it. secondly, an inquiry was made about the state of the art of the technologies used, the technologies that could be used and other companies in the same area, about their products and how they work. finally, selection and comparison methods between various options were investigated. thirdly, the most laborious and time-consuming part was carried out: practical deve lopment. in this phase, performance tests were carried out, the service was placed in a docker container and kubernetes was started to being used. also in this last part, there was a lot of experimentation with different architectures. finally, the system stabilized in an asynchronous architecture that reduced costs and allowed the service to be better suited to the amount of work to be processed."
    ],
    [
      "as more projects are financed by public institutions, demonstrating tasks and other information developed is increasingly required. traditional monitoring tools like prometheus are used in data centers to collect data from the machines and supervise them as the devices can malfunction and make the service unavailable. currently, applications like prometheus that display the machines’ performance are limited to a restricted set of people: the data center managers. these applications have limited data visualization methods since their focus is on retrieving the data from the machines. these applications are associated with specialized frameworks in showing the machines’ performance. these frameworks present the data in several visualization methods, such as graphic lines and gauge graphics. however, the forms of exposing data are not attractive for people in general. so, other ways to expose the data need to be developed. data visualization in three dimensions can expose data more attractively. besides, 3d has some advantages over traditional ways. with a data friendlier exposition, catching the attention of people who do not manage the data center is easier. this project aims to build an application to expose the data center’s data in a 3d scenario. the data exposed are the machines’ tasks, components, and performance. by exposing the data center’s tasks and other information to the general public, the application can present to the viewer the usefulness of the data center. the application must have its components flexible, so any data center can use it. moreover, those data centers should expose any visualization they desire through plugins. to complete the goals, first, different techniques to explore and view the data are investigated. several applications that expose data from a data center are analyzed to know the current status of these applications. furthermore, different scenarios are constructed based on the research made. using a tool capable of handling web requests makes the application available to everyone. besides, the application is flexible in some parts of the architecture to be adaptable to any framework. so, any data center can use the application. those parts are the server that contains the machines’ performance data and the database management system. the system allows the creation of a plugin to communicate with the machine’s performance server. following a simple interface, a new plugin can be developed with relative ease. besides, the webserver is replicable, making it adaptable to the data center’s needs. moreover, the application allows the creation of arbitrary 3d scenarios. by following a set of steps a simple 3d scenario can be built, including the visualization and communication server stages. such a scenario can be expanded freely, as long as the communication api is observed. the created scenario functions as a plugin that can be inserted into the application effortlessly. the application’s usefulness is validated through an experience with information from a real data center. finally, the application’s performance is corroborated, supporting a considerable amount of concurrent requests.",
      "como há um número maior de projetos financiados por instituições públicas, é necessário revelar que tarefas e outras informações a instituição desenvolve. ferramentas de monitorização como o prometheus são usadas em centros de dados para recolher o desempenho das máquinas e supervisioná-las uma vez que podem ter problemas tornando o serviço indisponível. aplicações como o prometheus que exibem o desempenho das máquinas são limitados a um conjunto restrito de pessoas: os gestores dos centros de dados. estas aplicações têm métodos de visualização limitados, uma vez que se focam em obter os dados das máquinas. estas aplicações são associadas com aplicações especializadas em mostrar o desempenho das máquinas. os dados são apresentados em vários métodos de visualização, como gráficos de linhas e de área. no entanto, estas formas não são atraentes para o público em geral. portanto, é preciso usar outras formas de expor os dados. visualização de dados em três dimensões pode expor os dados de uma forma mais eficiente. além disso, 3d tem algumas vantagens em relação às formas tradicionais. com um cenário mais amigável, é mais fácil captar a atenção das pessoas. este projeto tem o objetivo de construir uma aplicação para expor os dados do centro de dados em 3d. os dados expostos são as tarefas, o desempenho e os componentes das máquinas. ao expor as tarefas para o público em geral a aplicação pode apresentar a utilidade do centro de dados. a aplicação deverá ter os componentes flexíveis para que qualquer centro de dados o possa usar. além disso, os centros de dados deverão expor qualquer tipo de visualização que desejarem. para completar os objetivos, são investigadas diferentes técnicas de exposição de dados. são analisadas várias aplicações que expõem os dados de um centro de dados para conhecer o estado atual das mesmas. além do mais, são construídos vários cenários com base nos dados da investigação. ao usar uma ferramenta capaz de lidar com pedidos web torna-a disponível para todos. a aplicação também deve ser flexível em alguns dos componentes para serem adaptados a qualquer ferramenta. desta forma qualquer centro de dados pode usar a aplicação. as partes flexíveis devem ser o servidor que contém os dados do desempenho das máquinas e a base de dados. o sistema permite o uso de diferentes plugins para comunicar com esse servidor. ao seguir um conjunto de passos a criação do plugin é relativamente fácil. o servidor aplicacional é replicável, tornando o sistema adaptável para as necessidades do centro de dados. a aplicação permite o desenvolvimento de novos cenários 3d. ao seguir um conjunto de passos é criado um cenário 3d simples, incluindo os passos da visualização e comunicação com o servidor. o cenário pode ser expandido, desde que siga a api de comunicação. o cenário criado funciona como um plugin que pode ser adicionado na aplicação facilmente. a utilidade da aplicação é validada através de uma experiência com dados reais de um centro de dados. por fim, o desempenho da máquina é validado, uma vez que suporta uma quantidade considerável de pedidos concorrentes."
    ],
    0.3
  ],
  [
    [
      "no seguimento do regulamento europeu de identificação electrónica (eidas), tem vindo a ser promovida a carteira de identidade digital europeia (european digital identification wallet - eudiw). trata-se de um mecanismo de identificação desmaterializado que oferece garantias de segurança e privacidade aos cidadãos, e que se pretende interoperável nos vários estados membros. esta dissertação aborda a emissão de personal identification data (pid) para diferentes países no contexto da eudiw, e ainda uma emissão da versão digital da carta de condução automóvel (mobile driver license — mdl). o estudo compreende uma análise de soluções semelhantes disponibilizadas em portugal, assim como uma análise dos standards e regulamentos que se enquadram no âmbito desses sistemas. foca-se depois no desenho e implementatção do protótipo para a componente de personal identification data provider e da mdl.",
      "following the european electronic identification regulation (eidas), the european digital identification wal let (eudiw) has been promoted. it is a dematerialized identification mechanism that offers guarantees of security and privacy to citizens, and which is intended to be interoperable in the various member states. this dissertation addresses the issuance of personal identification data (pid) for different countries in the context of eudiw, and also the issuance of the digital version of the automobile driving license (mobile driver license — mdl). the study comprises an analysis of similar solutions available in portugal, as well as an analysis of the standards and regulations that fall within the scope of these systems. it then focuses on the design and implementation of the prototype for the personal identification data provider and mdl components."
    ],
    [
      "os dispositivos móveis têm ganho uma enorme relevância ao longo dos últimos anos, especialmente os smartphones, que nos dias de hoje, são indispensáveis para a maioria das pessoas, tanto para a sua utilização pessoal como para a sua utilização profissional. neste contexto e com algumas dificuldades encontradas nas unidades hospitalares, especialmente a falta de comparência dos utentes às respetivas consultas médicas e a falta de uma relação fluída entre o utente e a sua unidade hospitalar, foi proposta uma possível solução, de modo a combater esses problemas e a integrar os dispositivos móveis no acesso do utente aos cuidados de saúde. no levantamento dos problemas enumerados surgiu a ideia de criar uma aplicação mobile, precisamente para que a interação dos utentes com as suas unidades hospitalares sejam facilitadas e fluídas. para complementar este projeto e de modo a obter um protótipo completamente funcional, foi criada ainda uma aplicação web, para os profissionais de saúde, com a capacidade de responder aos pedidos dos utentes, feitos através da aplicação mobile. para procurar as funcionalidades a implementar nesta aplicação foi feita uma análise de mercado e um levantamento do que os utentes mais necessitam nas deslocações aos hospitais. relativamente à estrutura da solução, a aplicação mobile foi pensada para ser utilizada em qualquer sistema operativo de um smartphone, quer seja ios ou android, por essa razão foi desenvolvida em react native, a aplicação web em react, o servidor em node.js e a base de dados relacional em mysql.",
      "mobile devices have gained enormous relevance over the last few years, especially smartphones, which nowadays are indispensable for most people, both for their personal use and for professional use. within this context and with some difficulties encountered within the healthcare facilities, especially the lack of attendance of the patients to the respective medical appointments and the lack of a fluid relationship between the patient and his healthcare facility, a solution was devised in order to overcome these problems and to integrate mobile devices in the patient’s access to health care. in surveying the listed problems, an idea to creating a mobile application emerged, precisely so that the interaction of patients with their healthcare facilities is facilitated. to make this this project functional, a prototype of the proposed solution was designed and developed, a web application was also created for health professionals, with the ability to respond to user requests made through the mobile application. to look for the features to be implemented in this application, a market analysis was carried out and a survey of what patients need most when visiting hospitals. regarding the structure of the solution, the mobile application was designed to be used in any operating system of a smartphone, whether ios or android, for that reason it was developed in react native, the web application in react, the server in node.js and the relational database in mysql."
    ],
    0.3
  ],
  [
    [
      "domotics represent a field of consumer electronics related primarily to home automation. although smart environments have existed for decades and even for longer in the imaginary of people and sci-fi, the new age of internet of things (iot) and the low-cost do it yourself (diy) electronics/maker market has brought smart homes and the understanding of domotics closer to everyone. in recent years, the interest in the fields of domotics and iot has increased. this recent academic and industrial interest has contributed to the evolution of the smart home concept, being more and more appealing to our civilization. this has prompted new commercial solutions on the market, which are discovering ways to expand and increase their own value by integrating new features, especially from the iot market. the proliferation of iot devices leads to new sources of information. in addition, the relation between the environment occupant and the environment is responsible for adding value to this data. afterwards, the data can then be used in a meaningful way, by machine learning algorithms to learn usage patterns. furthermore, this data may or may be unrelated to the data incoming from sensors. this masters project will focus on learning strategies to allow smart homes to become intelligent, in a sense that they anticipate needs and actions, thus enabling extended features to the home automation system. the user should be able to give feedback on his ”feeling” regarding the decision making and suggestions, possible by our system implementation.",
      "domótica é um campo da eletrónica de consumo relacionado principalmente com automatização de casas. embora a domótica exista há décadas, e ainda há mais tempo nas nossas imaginações e na ficção científica, a nova era de internet das coisas (idc) e o mercado de aparelhos eletrónicos de baixo custo permitiu a aproximação da domótica com todos nós. com o aumento de interesse nas áreas da domótica e idc, soluções comerciais procuram formas de expandir e aumentar o seu valor no mercado, integrando novas funcionalidades, especialmente oriundas do mercado idc. a proliferação dos dispositivos idc conduz a novas fontes de dados. através da interação do utilizador com o ambiente, os dados são gerados, podendo assim serem usados de forma significativa, por algoritmos de aprendizagem máquina, conduzindo à aprendizagem de padrões de utilização. os dados, podem ou não, estar relacionados com informação oriunda dos sensores. este projeto de mestrado irá focar em estratégias de aprendizagem para permitir que as casas inteligentes se tornem realmente inteligentes, no sentido em que elas antecipam necessidades e ações, permitindo assim, uma ampliação de funcionalidades para o sistema de automação residencial. o utilizador deve ser capaz de dar feedback sobre o seu ”sentimento” em relação à tomada de decisão e às sugestões do sistema, sendo possível através da nossa implementação."
    ],
    [
      "the constant development of the internet and underlying transmission technologies, together with the increasing popularity of provided services, such as multimedia applications and applications using p2p technologies, are contributing to the continuous growth of the network traffic in volume and diversity. to be able to handle such amount of data while assuring the quality and operation of provided services, traffic-measuring tools are required to implement mechanisms that scale and have a minimum interference on the normal network behaviour. one of the most common solutions for this purpose involves the implementation of measurement techniques based on traffic sampling. these techniques aim to provide accurate estimations of traffic behaviour and characteristics by processing fractions of the original network traffic. another fundamental area of traffic monitoring concerns the traffic classification and characterization, as it supports important tasks such as resource allocation, planning and management, security and quality of service. attending to this, added up to the mentioned growth in traffic volumes, it is likely that traffic classification and characterization will be increasingly supported by traffic sampling mechanisms. this work aims to study the impact of traffic sampling mechanisms on the accuracy of traffic flows characterization. this was carried out through the application of classical and adaptive traffic sampling techniques to real traffic traces, which were captured in different real scenarios and opened to public access. the resulting sampled data is then organized under the form of flow records, which are then classified according to their transport and application protocols. the performance of the distinct traffic sampling techniques in enabling a correct characterization of traffic flows was then assessed taking into account multiple metrics applied to the flow records of sampled traffic, compared to the metrics of the full original traffic.",
      "o constante desenvolvimento das tecnologias relacionadas com a internet e transmissão de dados, juntamente com a crescente popularidade de serviços fornecidos, como aplicações multimédia e aplicações que utilizam tecnologias p2p, contribuem para um contínuo crescimento quer da diversidade do tráfego quer do volume de dados que circulam nas redes. para permitir lidar com tais quantidades de dados e ao mesmo tempo garantir a qualidade e o funcionamento dos serviços prestados, as ferramentas de medição de tráfego necessitam de implementar mecanismos escaláveis e de interferir o mínimo possível no comportamento normal da rede. uma das soluções mais comuns envolve a implementação de técnicas de medição baseadas em amostragem de tráfego. estas técnicas têm como objectivo proporcionar estimativas precisas do comportamento do tráfego e suas características através do processamento de fracções do tráfego real. outra área fundamental da monitorização de tráfego refere-se à sua classificação e caracterização. esta suporta tarefas importantes, tais como a alocação de recursos, o planeamento e gestão, a segurança e a qualidade de serviço. dada a importância desta área adicionado ao mencionado facto do crescimento dos volumes de tráfego, os mecanismos de classificação e caracterização conjugados com mecanismos de amostragem de tráfego constituem um possível cenário cuja adopção futura é tida não só como útil mas também como necessária. através da realização deste trabalho, pretende-se contribuir para o estudo do impacto dos mecanismos de amostragem de tráfego na acurácia da caracterização de fluxos de tráfego de rede. para isso, foram aplicadas técnicas clássicas de amostragem e amostragem adaptativa a colectas de tráfego reais, capturados em diferentes cenários e disponíveis para acesso ao público. os conjuntos de dados resultantes foram então organizados sob a forma de registos de fluxos e classificados de acordo com o protocolo de transporte e protocolo aplicacional. o desempenho das diferentes técnicas de amostragem em sustentarem a correcta caracterização de fluxos de tráfego foi avaliado tendo em conta um conjunto de métricas aplicadas aos registos de fluxos do tráfego amostrado, comparando-as com as métricas do tráfego total original."
    ],
    0.0
  ],
  [
    [
      "alcançar o consenso distribuído é fundamental para que se consigam construir sistemas tolerantes a faltas, pois permite que uma coleção de processos opere como um grupo coerente que pode sobreviver a falhas de alguns dos seus membros. o algoritmo raft resolve o problema de consenso e visa ser o mais compreensível possível, porém as atuais implementações deste algoritmo são dedicadas a casos de uso específicos. consequentemente, quem desenvolve sistemas que tenham que contemplar componentes replicados vê-se obrigado a construir o seu próprio mecanismo de replicação, o que pode ser contraproducente e até ter certas implicações no código da aplicação. paralelamente, as arquiteturas de microsserviços passaram a ser o novo normal, fazendo oposição à construção de sistemas monolíticos. dada a sua natureza, este tipo de arquiteturas permite endereçar problemas que dizem respeito à resiliência e coerência dum dado serviço, existindo por isso uma oportunidade para cruzar algoritmos de consenso distribuído com microsserviços. nesta dissertação propõe-se a construção de duas implementações de raft num toolkit aplicacional típico de microsserviços, mais especificamente spring boot. cada implementação deverá utilizar uma das diferentes stacks da framework, nomeadamente, a serviet stack ou a stack reativa. ambas as implementações deverão ser modulares e genéricas o suficiente, para que possam ser simultaneamente configuráveis e aplicáveis a diferentes casos de uso. para o efeito, começa-se por delinear as configurações em que o middleware poderá operar, assim como a arquitetura interna do mesmo, seguindo-se da fase de implementação, que detalha decisões tomadas ao longo da mesma. a fase de avaliação começa com a implementação, em ambas as stacks, de uma aplicação de armazenamento chave-valor que é configurada com diferentes parâmetros, para que finalmente possa ser comparada com o etcd que é um armazenamento chave-valor replicado. desde logo, os resultados recolhidos fazem prever o desempenho de ambas as soluções de middleware construídas, que ficam aquém dos desempenhos alcançados por um cluster etcd, mas que dão garantias de viabilidade e extensibilidade, uma vez que as soluções são modulares para integrarem novas otimizações, e são genéricas e úteis para qualquer aplicação que necessite de assegurar garantias de coerência forte.",
      "achieving distributed consensus is fundamental to building fault-tolerant systems, as it allows a collection of processes to operate as a coherent group that can survive the failure of some of its members. the raft algorithm solves the consensus problem and aims to be as understandable as possible, but current implementations of this algorithm are dedicated to specific use cases. consequently, those developing systems that have to contemplate replicated components are forced to build their own replication mechanism, which can be counterproductive and even have certain implications for the application code. at the same time, microservices architectures became the new normal, opposing the construction of monolithic systems. due to its nature, this type of architecture allows addressing problems that concern the resilience and coherence of a given service, therefore there is an opportunity to cross distributed consensus algorithms with microservices. this dissertation proposes the construction of two raft implementations in a typical microservices application toolkit, more specifically spring boot. each implementation must use one of the different framework stacks, namely, the servlet stack or the reactive stack. both implementations should be modular and generic enough to be simultaneously configurable and applicable to different use cases. for this purpose, we begin by outlining the configurations in which the middleware will operate, as well as its internal architecture, followed by the implementation phase, which details decisions made throughout. the evaluation phase begins with the implementation, in both stacks, of a key-value storage application that is configured with different parameters, so that it can finally be compared with etcd which is a replicated key-value storage. from the outset, the results collected predict the performance of both middleware solutions, which fall short of the performance achieved by an etcd cluster, but which provide guarantees of viability and extensibility, since the solutions are modular to integrate new optimizations, and are generic and useful for any application that needs to ensure strong consistency guarantees."
    ],
    [
      "as assinaturas olap podem ser vistas como uma forma de caracterização de um dado perfil de exploração analítica. porém, ao contrário de um perfil de exploração típico, uma assinatura olap não tem uma natureza estática. uma assinatura olap congrega de uma forma única todos os elementos de informação recolhidos ao longo do tempo nas várias sessões de exploração olap desenvolvidas por um dado utilizador, caracterizando de uma forma bastante concreta esse utilizador ao longo do tempo. num sistema olap as assinaturas podem ser utilizadas para traçar um perfil de exploração de dados de um dado utilizador, baseado nas queries que este coloca ao longo do tempo sobre um dado sistema de processamento analítico e dos seus hábitos e tendências de exploração. através da análise das assinaturas olap podemos otimizar as estruturas multidimensionais – cubos - de um dado sistema analítico, de forma a reduzir o seu tamanho, guardando apenas informação relevante, e prever quais as operações que podem ser despoletadas a partir da ocorrência de uma dada querie. desta forma é possível escolher a priori quais as partes do cubo que devem ser carregadas para memória ou aquelas que podem ser transferidas para a máquina do próprio utilizador. tudo isto, para que seja possível minimizar a carga do servidor e reduzir o tráfego de dados no sistema de comunicação que suporta os processos de exploração analítica. neste trabalho de dissertação exploraremos esta temática e definiremos um método sustentado para definição e manutenção de assinaturas olap.",
      "olap signatures can be viewed as a way of characterizing one analytical exploration profile. however, unlike a typical scan profile, an olap signature does not have a static nature. an olap signature uniquely brings together all the information elements collected over time in the various olap exploration sessions developed by one user, characterizing that user in a very concrete way along the time. in an olap system the signatures can be used to trace a data exploration profile of a user, based on the queries that this one places along the time on an analytical processing system and its exploration habits and trends. through the analysis of olap signatures we can optimize the multidimensional structures - cubes - of an analytical system, so as to reduce its size, keeping only relevant information, and predict which operations can be triggered from the occurrence of a specific query. in this way it is possible to choose primarily which parts of the cube should be loaded into memory or those that can be transferred to the user's own machine. with this we can minimize server load and reduce data traffic in the communication system that supports analytical scanning processes. in this dissertation we will explore this theme and define a sustained method for defining and maintaining olap signatures."
    ],
    0.3
  ],
  [
    [
      "na atualidade, graças às elevadas capacidades computacionais e gráficas existentes, é possível dotar os sistemas de processamento analítico com ferramentas de visualização e manipulação de informação muito atrativas e de fácil utilização, em particular quando utilizamos para isso dashboards. os dashboards tornam a interação com a informação proveniente de um sistema de processamento analítico mais interativa e eficaz, muito graças à modularidade inerente aos seus componentes gráficos e à sua qualidade inata de representar a informação graficamente. a modularidade também é uma característica importante uma vez que permite modificar o sistema utilizando apenas cliques do rato, enquanto que, por sua vez, a representação gráfica da informação facilita a sua análise e interiorização (few, 2006a). estas qualidades, entre outras, fazem com que os dashboards sejam uma ferramenta fulcral na análise da informação e no suporte à tomada de decisão no seio de uma empresa, tendo sempre em mente que o sucesso de uma empresa está dependente da capacidade que os seus responsáveis e funcionários têm de tomar decisões acertadas em tempo útil. em geral, os dashboards podem ser utilizados para monitorizar o desempenho de uma empresa, tanto a nível global como a nível individual, definir estratégias de marketing, analisar tendências, entre outros. nesta dissertação pretendeu-se investigar a utilização de dashboards em sistemas de processamento analítico, abordando desde o seu desenho até à sua implementação e exploração prática. complementarmente, de forma a demonstrar a utilidade e vantagens desse tipo de instrumentos, procedeu-se à implementação de um sistema piloto, incorporando na sua estrutura uma coleção de dashboards providos de mecanismos de auto-adaptabilidade aos requisitos dos utilizadores.",
      "nowadays, thanks to the existing high computational and graphical capabilities, we can endow systems with analytical processing tools for information visualization and manipulation that are very attractive and easy to use, especially when using dashboards for the purpose. dashboards make the interaction with the information from an analytical processing system more interactive and effective, mainly due to the modularity inherent to its graphical components and their innate quality of representing the information graphically. modularity is also an important characteristic since it allows modifying the system using only mouse clicks, whereas the graphical representation of the information facilitates it analysis and internalization (few, 2006a). these qualities, among others, make dashboards a central tool to the analysis of information and support for decision making within a company, always bearing in mind that the success of a company is dependent on the ability of its managers and employees have to make good decisions in a timely manner. in general, dashboards can be used to monitor the performance of a company, both globally and individually, define marketing strategies, analyze trends, among others. this thesis aimed to investigate the use of dashboards in analytical processing systems, covering from design to implementation and practical exploration. in addition, to demonstrate the utility and advantages of this type of instruments, it was implemented a pilot system incorporating in its structure some dashboards provided with mechanisms of self-adaptability to the requirements of the users."
    ],
    [
      "the application of data science techniques, specifically natural language process ing (nlp) and machine learning, in financial markets is of immense interest to in vestors, as these techniques can have a potential economic impact. in particular, stock markets represent an opportunity that has been exploited in several ways, such as us ing market opinions (e.g., news, blogs) to predict the direction of price movement or even volatility. this study analyses the 10-k documents of the s&p 100 index for 10 years (2008-2017), which contains the 102 largest companies in the united states of america. the 10-k is an annual financial report required by the united states securities and ex change commission (sec), which describes the financial performance of a company. recent research suggests that the readability of a company’s 10-k text document may influence its future financial performance, since the way the market perceives textual information also depends on the readability of that text. in this sense, this work aims to understand the relationship between 48 readability metrics applied to these reports and the corresponding future financial performance of these companies. a clustering approach was applied over these readability metrics, aiming to identify distinct and valuable readability clusters. as an external evaluation, we assessed the information value of the clusters by analyzing 3 future crash risk metrics, that are often used to assess the companies’ financial performance.",
      "a aplicação das técnicas de ciência de dados, especificamente processamento de linguagem natural e machine learning, nos mercados financeiros é de imenso interesse para os investidores, uma vez que podem ter um potencial impacto económico. em particular, os mercados de ações representam uma oportunidade que tem sido explorada de várias formas, como no uso de informações de mercado (por exemplo notícias, blogs) para prever a direção do movimento dos preços ou mesmo o movimento da volatilidade. este estudo analisa os documentos io-k do índice s&p 100 durante 10 anos (20°8- 2017), que contém as ioz maiores empresas dos estados unidos da américa. o 10-k é um relatório financeiro anual exigido pela comissão de valores mobiliários dos estados unidos (sec), que descreve o desempenho financeiro de uma empresa. pesquisas recentes sugerem que a legibilidade do documento de texto 10-k de uma empresa pode influenciar o seu desempenho financeiro futuro, uma vez que a forma como o mercado perceciona as informações textuais também depende da legibilidade desse texto. neste sentido, este trabalho visa compreender a relação entre 48 métricas de legibilidade aplicadas a esses relatórios e o desempenho financeiro futuro correspondente dessas empresas. uma abordagem de agrupamento de dados foi aplicada nestas métricas de legibilidade, com o objetivo de identificar grupos de legibilidade distintos e relevantes. com uma avaliação externa, avaliamos o valor das informações desses grupos analisando três métricas de crash risk futuro, que são frequentemente usadas para avaliar o desempenho financeiro das empresas."
    ],
    0.0
  ],
  [
    [
      "numa determinada empresa, está a ser desenvolvido um sistema que visa identificar e classificar certos objetos em embalagens paletizadas o sistema é composto por um conjunto de câmaras colocadas numa estação de carregamento que executa várias capturas de imagens. a isto segue-se um processamento que termina numa deteção e identificação de certos objetos que determinam as regras de empacotamento. estas regras devem estar de acordo com as regras definidas pelo cliente. a solução atual passa por anotar manualmente as imagens exemplo (processo lento e demorado) para serem utilizadas para o treino do algoritmo de deteção de objetos. depois do treino estar terminado, o modelo de deteção fica imutável, o que significa que este é incapaz de evoluir com os resultados de verificação que se podem seguir. o âmbito desta dissertação é o de estudar,propôr e desenvolver uma solução para automatizar partes do processo de treino de um sistema de visão por computador que executa deteção de objetos de items paletizados através da implementação de geração automática de imagens e a automatização da sua anotação. já foram desenvolvidos e cientificamente aprovados alguns métodos que produzem anotações automáticas recorrendo a processos de active learning, classificação e feature transferring, bem como softwares de anotação de imagens que são capazes de executar tal tarefa. a proposta apresentada introduz uma estratégia para gerar imagens sintéticas e as anotações associadas para serem utilizadas para treinar modelos de deteção de objetos de modo a tornar mais ágil o processo de coleção e anotação de imagens. os resultados mostram que, no geral, os modelos treinados com imagens sintéticas demonstram melhores resultados que o modelo existente que foi treinado com imagens reais anotadas por um anotador. demonstrando que é possivel criar modelos de deteção de objetos que obtenham bons niveis de desempenho, que foram treinados apenas com dados sintéticos.",
      "in company x, an automated system is being developed in order to identify and classify certain objects in palletized packages. the system is comprised of a set of cameras in a loading station that executes several image captures. this is followed by image processing and terminates in detection and identification of certain objects that determine packaging rules. these must be in compliance with the defined shipping costumer standards.the current solution passes by manually annotating the image examples (pain-staking and time-costly process) to be used by the model’s training algorithm. once training has ended, the detection and classification model remains unchanged, which means that it is incapable of evolving with the verification results. the aim of this dissertation is to study, propose and develop approaches to automate parts of the process of training of a computer vision system that performs object detection in palletized items by implementing automatic generation of images and automated image annotation. there have been developed and scientifically approved approaches and methods to develop automatic image annotation recurring to processes of active learning, classification and feature transferring, as well as some image labeling software that is capable of perform this task. the presented proposal introduces the approach of generating synthetic images and its associated image annotations to train object detection models in order to expedite the collection and labeling of images. the results show that, overall, the models trained on synthetic image data performed better than the existing model that was trained on human labeled data. this indicates that is possible to create good performing object detections models trained only on synthetic data."
    ],
    [
      "na era atual dos avanços tecnológicos, a blockchain emergiu como uma grande inovação, ganhando reconhecimento generalizado pelas suas capacidades de manutenção de registos seguros e transparentes. ao mesmo tempo, a gestão e a comunicação eficazes de dados críticos da infraestrutura de tecnologias da informação (ti) continuam a ser uma necessidade premente. esta dissertação aborda a intersecção destes domínios, com o objetivo de aproveitar o potencial da tecnologia blockchain para otimizar e proteger os processos de comunicação de dados. patrocinada pela dstelecom, um operador de rede de fibra ótica neutra em portugal, esta investigação investiga o papel transformador que a blockchain pode desempenhar na monitorização da rede. com base numa análise abrangente da literatura, o estudo examina a capacidade de aplicação e os desafios da blockchain, apresentando um caso de estudo prático, que utiliza o hyperledger fabric, para destacar o seu potencial na monitorização da rede.",
      "n the present era of technological advancements, blockchain has emerged as a leading innovation, gaining widespread acclaim for its secure and transparent record-keeping capabilities. concurrently, the effective management and reporting of critical data from information technology (it) infrastructure remain a pressing need. this dissertation addresses the intersection of these domains, aiming to leverage the potential of blockchain technology to optimize and secure data reporting processes. sponsored by dstelecom, a neutral fiber optic network operator in portugal, this research delves into the transformative role blockchain can play in network monitoring. through a comprehensive literature review, the study examines blockchain’s applicability and challenges, presenting a practical case study using hyperledger fabric to highlight its potential in network monitoring."
    ],
    0.06666666666666667
  ],
  [
    [
      "minium is a framework for automating testing of web applications. it provides an api for automating tests that combines the capabilities of the selenium webdriver api for automating interactions with the browser with the convenience of the jquery api for identifying web elements. the goal of this dissertation was to develop a plugin for minium that could generate automation scripts by recording user interactions with the browser. the main requirement was that it should be capable of generating a list of expressions, ordered by ease of understandability, to identify each of the elements upon which an interaction is performed. prior to the development of the solution, a research work was conducted. this research work focused on the study of the algorithms for generating expressions to identify web elements and of the techniques and tools for recording interactions with web pages. the developed solution, which will be described in detail, was tested in some web applications with good results.",
      "o minium é uma ferramenta para automatizar testes de aplicações web. esta ferramenta disponibiliza uma api que combina as capacidades da api do selenium webdriver para automatizar interações com o browser com a conveniência da api do jquery para identificar elementos em páginas web. o objetivo desta dissertação era desenvolver um plugin para o minium que fosse capaz de gerar scripts de automação a partir do registo das interações de um utilizador com o browser. o principal requisito era que fosse capaz gerar uma lista de expressões, ordenada por facilidade de compreensibilidade, para identificar cada um dos elementos sobre os quais uma interação é realizada. antes do desenvolvimento da solução, foi realizado um trabalho de pesquisa que se focou no estudo dos algoritmos de geração de expressões para identificar elementos e sobre as técnicas e ferramentas existentes para registo de interações com páginas web. a solução desenvolvida, que será descrita em detalhe, foi testada em algumas aplicações web com bons resultados."
    ],
    [
      "the brain is the most amazingly powerful and complex organ in the human body. constituted by approximately 86 billion of highly interconnected neurons, it allows us to have unique cognitive capabilities, such as language production and comprehension, memory, judgement and problem solving, or even experience feelings. the full understanding of the organization and functioning of the human brain is receiving increasing attention but remains an exciting challenge for all neuroscientists. to assist in this quest, fmri stands as a safe and powerful non-invasive neuroimaging tool providing high visualization quality of the location of activity in the brain resulting from, for example, sensory stimulation, cognitive or motor function, or even resting state fluctuations, being then widely used for mapping the human brain. it allows the study of how the healthy brain works, how it is affected by different diseases, how it recovers from damage and how drugs can modulate activity or post-lesion recovery. starting purely as a research tool, fmri was quickly adopted for clinical purposes and has now a growing role in clinical neuroimaging. constantly gaining increased popularity among clinicians and researchers, fmri is presently a promising tool for studying the brain function in living humans. however, it has a complex workflow that implicates knowledge of paradigm design, imaging artifacts, complex mri protocol definition, a multitude of preprocessing and analysis methods in several software packages, statistical analyzes, and in results interpretation. in addition, fmri data can be analyzed with a large quantity of commonly used tools, with minor consensuses on how, when, or whether to apply each one. this dissertation aims to compile a practical guide of crucial information and essential references to consider in setting up fmri studies, optimizing data quality, and interpreting results. all the major stages are covered with the aim to ultimately help the fmri beginner researcher, clinician to consider and overcome the most significant difficulties along the process and expand the use of this imaging technique. to validate this guide two examples of fmri studies were analyzed, with real data, obtaining results according to similar studies literature.",
      "o cérebro é o órgão mais poderoso e complexo do corpo humano. constituído por aproximadamente 86 mil milhões de neurões altamente interligados, confere-nos capacidades cognitivas únicas, tais como a criação e compreensão de linguagem, formação de memória, resolução de problemas, ou experienciar sentimentos. o conhecimento profundo da organização e funcionamento do cérebro humano continua a ser um desafio para os neurocientistas, apesar da crescente atenção que tem recebido. para auxiliar na pesquisa deste conhecimento, a imagem por ressonância magnética funcional (irmf) posiciona-se como uma poderosa ferramenta que para além de ser segura, é não-invasiva e proporciona visualizações de alta qualidade das localizações de actividade cerebral resultante de, por exemplo, estimulação sensorial, função cognitiva, ou até mesmo de fluctuações em repouso, sendo por isso amplamente utilizada para o mapeamento do cérebro humano. permite desta forma o estudo do funcionamento do cérebro saudável, de como é afectado por diferentes doenças, como recupera de lesões, e como fármacos influenciam a actividade cerebral e recuperação de lesões. apesar de começar por ser uma ferramenta usada para investigação, a irmf foi rapidamente adoptada para aplicações clínicas, tendo agora uma crescente importância em neuroimagem clínica. ganhando crescente popularidade entre médicos e investigadores, irmf é neste momento uma ferramenta muito promissora no estudo in-vivo do funcionamento do cérebro em humanos. contudo implica um conhecimento e domínio do seu fluxo de trabalho, nomeadamente desenho do paradigma, artefactos de imagem, definição de protocolos complexos de irm, uma multitude de métodos de pré-processamento e análise estatística, e finalmente na interpretação dos resultados. tudo isto usando uma ampla colecção de diferentes softwares, sem que haja consenso em quais são os mais adequados. esta dissertação tem como objectivo compilar um guia prático contendo a informação crucial e referencias essenciais à definição, optimização e interpretação de estudos de irmf. todas as principais etapas estão cobertas de forma a auxiliar os principiantes em irmf a considerar e ultrapassar as principais dificuldades normalmente encontradas e assim expandir a utilização desta técnica de imagem médica. para validar este guia, dois exemplos de estudos de irmf foram analisados com dados reais e foram obtidos resultados de acordo com os de estudos semelhantes já publicados."
    ],
    0.06666666666666667
  ],
  [
    [
      "atrial fibrillation affects millions of individuals worldwide, posing a major threat to public health due to the variety of comorbidities that constitute by-products of the disease. in light of this epidemic, new means of diagnosis, prognosis and therapy are pressing. biomarkers, particularly protein markers, are important tools in this process but lack validation, which is essential before clinical translation. several appraisal benchmarks have been developed to determine the relative potential of biomarkers, but these present multiple limitations. we developed a bioinformatic-oriented scoring function aimed at weighing the importance of proteins and mitigating the limitations of the currently known scores. after taking an extensive literature search and mining a massive volume of reports, data was organized into several subsets, according to the sample major characteristic and atrial fibrillation type. a mathematical scoring function was proposed, based on the consensus of studies supporting the protein-disease association (incoherence), median of the reported fold-changes and importance of each study according to the number of diseased individuals, and applied to each subset in the form of an algorithm implemented in python 3.5. the developed ranking method performed well regarding both the degree of alteration and the inconsistency parameters. our results portray a set of proteins with the highest biomarker potential (highest scores) for atrial fibrillation. we also selected the top five potential biomarkers for atrial fibrillation in general and for each type of disease. the main biological functions in which they are involved were retrieved for comparison with the state of the art. alterations in the expression levels of proteins involved in either of these functions seem to agree with af’s pathophysiology and clinical presentation, showing the effectiveness of the developed algorithm. overall, the developed pipeline seems to improve the processes of biomarker ranking and selection for a target disease, allowing a leap towards clinical translation.",
      "a fibrilhação auricular afeta milhões de indivíduos em todo o mundo, representando uma grande ameaça à saúde pública devido à grande variedade de comorbidades que constituem subprodutos da mesma. face a esta epidemia, novos métodos de diagnóstico, prognóstico e terapêutica são prementes. os biomarcadores, em particular marcadores proteicos, tornam-se importantes ferramentas neste processo, mas carecem de validação, passo essencial antes da tradução clínica. vários meios de avaliação foram desenvolvidos para determinar o seu potencial relativo, mas estes apresentam inúmeras limitações. neste trabalho desenvolvemos uma função de classificação orientada para a bioinformática, destinada a calcular a importância de proteínas e a mitigar as limitações dos métodos já conhecidos. após uma extensa pesquisa de literatura e análise de um volume enorme de artigos, os dados foram organizados em vários subconjuntos, de acordo com a principal característica da amostra e tipo de fibrilhação auricular. uma função matemática de classificação foi proposta, baseada no consenso de estudos que suportam a associação proteína-doença (incoerência), mediana dos fold-changes e importância de cada estudo de acordo com o número de indivíduos afetados, e aplicada a cada subconjunto por meio de um algoritmo implementado em python 3.5. o método de classificação desenvolvido teve uma boa performance relativamente a ambos os parâmetros, nomeadamente o grau de alteração e a coerência. os resultados retratam um conjunto de proteínas com o potencial de biomarcador mais elevado (classificações mais elevadas) para a fibrilhação auricular. também selecionamos as cinco proteínas com o maior potencial de biomarcador para a fibrilhação auricular geral e para cada tipo da doença. procedeu-se um rastreio das principais funções biológicas nas quais as proteínas estão envolvidas para comparação com o estado da arte. alterações nos níveis de expressão de proteínas envolvidas em qualquer uma destas funções parecem estar de acordo com a patofisiologia e apresentação clínica desta arritmia, o que demonstra a eficácia do algoritmo desenvolvido. de forma geral, todo o processo aqui delineado parece melhorar os processos de classificação e seleção de biomarcadores para uma doença alvo, permitindo progressos na direção da tradução clínica."
    ],
    [
      "para processar dados utilizando sql, em geral, é necessário carregá-los previamente para uma base de dados. a unicage propõe uma solução personalizada para processamento de dados que permite fazer a gestão de todo o ciclo de vida dos dados sem que seja necessário efetuar o seu carregamento prévio. esta solução oferece um conjunto de comandos baseados numa filosofia unix, sendo a sua sintaxe bastante diferente de uma interrogação sql convencional. a conversão de uma interrogação sql para comandos unicage é um processo manual, custoso e demorado, que requer a intervenção de um perito. torna-se, então, pertinente a construção de uma plataforma que efetue a tradução e otimização de uma interrogação sql para operações de acordo com a filosofia unicage, passíveis de serem aplicadas diretamente sobre ficheiros de texto. o objetivo desta dissertação é a implementação ou adaptação de um motor de interrogações para receber uma interrogação sql e a transformar em operações sobre ficheiros de texto, utilizando os co mandos da shell e comandos unicage. em particular, utiliza-se o otimizador do motor de interrogação para gerar um plano de execução otimizado, tendo em conta as especificidades da execução sobre fi cheiros alterando, por exemplo, a ordem de execução de comandos de forma a minimizar o tempo de resposta. a avaliação mostra que as regras e estratégias tradicionais de otimização de interrogações sql conti nuam a ter um impacto positivo apesar da conversão de sql para unicage, diminuindo o custo cumulativo das interrogações e o tempo de execução das mesmas. a avaliação de desempenho, mostra ainda que a aplicação desenvolvida se revela vantajosa comparativamente ao sgbd postgresql, quando comparados os tempos de execução de uma mesma interrogação sobre os mesmos dados.",
      "in order to process data using sql, it is usually necessary to preload it into a database. unicage offers a customized data processing solution that allows you to manage the entire data lifecycle without having to preload it. this solution offers a set of commands based on a unix philosophy, the syntax of which is quite different from that of a conventional sql query. converting an sql query into unicage commands is a manual, costly and time-consuming process that requires the intervention of an expert. it is therefore important to build a platform that translates and optimizes sql queries into operations with the unicage philosophy, which can be applied directly to text files. the aim of this dissertation is to implement or adapt a query engine to receive an sql query and transform it into operations on text files, using shell commands and unicage commands. in particular, the query engine optimizer is used to generate an optimized execution plan, taking into account the specificities of execution on files by, for example, changing the order of command execution in order to minimize response time. the evaluation shows that the traditional rules and strategies for optimizing sql queries continues to have a positive impact despite the conversion from sql to unicage, reducing the cumulative cost of queries and their execution time. the performance evaluation also shows that the application developed is advantageous compared to the postgresql dbms, when comparing the execution times of the same query on the same data."
    ],
    0.02727272727272727
  ],
  [
    [
      "as crianças são uma população especialmente vulnerável, nomeadamente no que diz respeito à administração de medicamentos e necessidade de nutrição. estima-se que os doentes neonatais e pediátricos são pelo menos três vezes mais vulneráveis a danos causados devido a eventos adversos e erros de medicação do que a população adulta. o desenvolvimento de uma plataforma que suporte os médicos pediatras no exercício das suas funções diárias, de forma a reduzir o erro médico, é o principal objetivo deste projeto. a sua necessidade foi identificada por um médico pediatra em exercício de funções no hospital de santo antónio no porto, de forma a que falhas existentes na ferramenta em uso fossem colmatadas e ainda novas funcionalidades fossem desenvolvidas. com a presente dissertação foi procurada ainda uma abordagem que permitisse o desenvolvimento de um canal de passagem de informação entre os médicos e a farmácia hospitalar, e que este sistema pudesse ser altamente escalável, sendo facilmente replicado em qualquer instituição de saúde. o desenvolvimento do sistema foi sempre acompanhado por um médico pediatra, sendo este testado e refinado ao longo desse período. por fim, uma versão para testes da aplicação é lançada assim como um questionário que pretende avaliar a mesma.",
      "children are a particular vulnerable population, namely when it comes to drug’s administration and nutricional needs. it is estimated that neonatal and pediatric patients are three times more vulnerable to damage from adversal events and medication errors than the adult population. the main objective of this project is the development of a platform that supports the pediatrician in their daily functions, in order to reduce the medical error. this need was identified by a pediatrician working in the hospital de santo antónio, in porto, to improve the tool’s weaknesses and develop new funcionalities. in this dissertation was also searched an approach that would allow the development of a link between the doctors and the hospital pharmacy, and that this system could be repicable and easily reproduced in every other health institution. the system development was supervised by a pediatrician, being tested and refined all along. at last, an aplication version to tests is launched as well as an avaliation questionnaire."
    ],
    [
      "computational thinking has been increasingly explored in the area of teaching. many researchers believe that the early introduction of this concept leads to a better understanding of multiple fields like computer science, math and engineering. however, the inclusion of computational thinking as part of the educational program needs to be carefully done. for that, we need to choose the right learning resources. as game-based learning was proven to be effective by numerous researchers, in this project is argued that games are proper learning resources to develop computational thinking. with game-based learning, this work aims to improve students’ motivation and learning experience on computational thinking by choosing the most suitable games for each student. to find the relation between students and games, it is necessary to analyze each of them. first, to differentiate types of games, ontojogo, an ontology for game classification, was built. the usability and coverage of ontojogo were tested in an experiment conducted with five participants. secondly, it was required to profile the students through the analysis of sociodemographic, competencies, and psychological factors. for that, a profile questionnaire was developed with the collaboration of two child psychologists. lastly, a game evaluation questionnaire was designed for the students to complete, making it possible to connect game classifications with students’ profiles. with these tools, it was possible to develop a platform for games suggestion, fulfilling the primary goal of this project. the platform adequa supports the registration of games and students and the evaluation of games. additionally, adequa recommends the most suitable games for each student. for the recommendation of games, it was designed an algorithm that uses the data collected from the questionnaires and returns a list of suitable games. the algorithm was developed from the results of an experiment conducted with twenty-four participants, where it was searched patterns between the participants and game types. from the results, it was found that variables like gender, gaming habits, and emotional factors can influence the motivation a student feels towards a game. this experiment was essential to prove the hypothesis that it is possible to relate students and games. based on this conclusion, it is right to affirm that the future of education must pass through a personalized experience, starting with the learning resources used.",
      "o pensamento computacional tem sido cada vez mais explorado na área de ensino. muitos investigadores acreditam que a introdução precoce deste conceito leva a uma melhor compreensão de várias áreas como ciências da computação, matemática e engenharia. contudo, a inclusão do pensamento computacional como parte do programa escolar deve ser feita com sensatez. para isso, precisamos de escolher os recursos de aprendizagem apropriados. tendo a aprendizagem baseada em jogos sido provada eficaz por vários investigadores, neste projeto argumenta-se que os jogos são recursos de aprendizagem adequados para desenvolver o pensamento computacional. com a aprendizagem baseada em jogos, este trabalho visa melhorar a motivação e a experiência de aprendizagem dos alunos no pensamento computacional, escolhendo os jogos mais adequados para cada aluno. para encontrar a relação entre alunos e jogos, é necessário analisar cada um deles. primeiramente, para diferenciar os tipos de jogos, foi construída a ontojogo, uma ontologia para classificação de jogos. a usabilidade e abrangência da ontojogo foram testadas num experimento realizado com cinco participantes. em segundo lugar, era necessário traçar o perfil dos alunos pela análise de fatores sociodemográficos, de competências e psicológicos. para tal, foi elaborado um questionário de perfil com a colaboração de dois psicólogos infantis. por fim, foi elaborado um questionário de avaliação de jogos para os alunos preencherem, permitindo relacionar as classificações dos jogos aos perfis dos alunos. com estas ferramentas foi possível desenvolver uma plataforma para sugestão de jogos, cumprindo o objetivo principal deste projeto. a plataforma adequa suporta a inscrição de jogos e alunos, e a avaliação de jogos. adicionalmente, adequa recomenda os jogos mais adequados para cada aluno. para a recomendação de jogos, foi desenhado um algoritmo que utiliza os dados recolhidos dos questionários e retorna uma lista de jogos adequados. o algoritmo foi desenvolvido a partir dos resultados de um experimento realizado com vinte e quatro participantes, onde foram pesquisados padrões entre os participantes e tipos de jogos. a partir dos resultados, constatou-se que variáveis como género, hábitos de jogo e fatores emocionais podem influenciar a motivação que um aluno sente em relação ao jogo. este experimento foi essencial para comprovar a hipótese de que é possível relacionar alunos e jogos. com base nesta conclusão, é correto afirmar que o futuro da educação deve passar por uma experiência personalizada, partindo dos recursos de aprendizagem utilizados."
    ],
    0.09999999999999999
  ],
  [
    [
      "computing infrastructure management is increasingly demanding and has to comply with regulatory requirements. to comply with these requirements, the existence of a configuration management database (cmdb) is fundamental. one of the challenges that any team has when starting it service management (itsm) is to create the organization’s cmdb. cmdb is a database that stores information about the components, usually called configu ration items (cis), of the infrastructure and the relationships between them. thus, the cmdb creation implies discovering information about the infrastructure, saving it in the cmdb chosen by the organization. this dissertation presents a tool for the automatic creation of a cmdb that uses automatic discovery, mapping, and population mechanisms, to find information about the infrastructure components and store these results in the cmdb. it was also necessary to adapt the populate operation according to the database structure of the selected cmdb. this tool uses several discovery mechanisms to explore different types of configuration items (cis), discovering information about them and their dependencies. it also uses an automatic mapping mechanism to adapt the types of discovered data with the cmdb structure where they will be stored. finally, it populates the cmdb using its application programming interface (api) to create the cis and relationships.",
      "a gestão de infraestruturas computacionais é cada vez mais exigente e tem de cumprir cada vez mais com requisitos normativos. para estar de acordo com estes requisitos, a existência de uma cmdb é fundamental. um dos desafios que qualquer equipa tem ao iniciar a gestão de uma infraestrutura é a criação da sua cmdb. uma cmdb é uma base de dados que guarda informação acerca dos componentes, usualmente denominados de itens de configuração (cis), de uma infraestrutura computacional e dos relacionamentos entre si. assim, a criação de uma cmdb implica descobrir informação acerca dos componentes que fazem parte da infraestrutura e armazenar esta na cmdb escolhida pela organização. nesta dissertação é apresentada uma ferramenta de criação automática de uma cmdb que recorre a mecanismos automáticos de descoberta, mapeamento e povoamento, de forma a encontrar informação acerca da infraestrutura, e armazenar estes resultados na cmdb. tendo em conta que existem produtos de software distintos que implementam cmdbs, foi necessário adaptar o povoamento de acordo com a estrutura da base de dados selecionada. esta ferramenta recorre a diversos mecanismos de descoberta de forma a explorar diferentes tipos de componentes, com a finalidade de descobrir informação acerca destes e das dependências entre estes. recorre também a um mecanismo de mapeamento automático entre modelos de dados, de forma a adaptar os tipos de dados descobertos com a estrutura da cmdb onde estes vão ser armazenados. finalmente, utiliza um mecanismo para efetuar o povoamento da cmdb que utiliza a api desta para efetuar a criação dos componentes e dos relacionamentos."
    ],
    [
      "o cloud computing tem sido amplamente adotado na área das tecnologias de informação na última década, devido às diversas vantagens que providencia, entre elas a possibilidade de redução de custos com infraestruturas. embora a utilização da cloud possa minorar os custos de operação de aplicações web, verifica-se que a definição dos preços praticados pelos fornecedores de serviços tem-se tornado cada vez mais complexa, ameaçando uma das principais razões que leva os utilizadores a migrar para a cloud: a redução de custos. derivado deste aumento de complexidade, o surgimento de soluções de monitorização e otimização de custos de cloud tem vindo a aumentar por forma a combater este problema. apesar de existirem algumas soluções capazes de auxiliar na otimização de custos, verifica-se que a visibilidade sobre os custos e dados de utilização é limitada, não sendo possível consultar a informação com a granularidade que os utilizadores pretendem. por todos estes motivos, a equipa de investigação e desenvolvimento da eurotux informática, s.a. decidiu investir no desenvolvimento de uma solução que auxiliasse os seus colaboradores e clientes num problema que enfrentam no dia a dia. após estudar as soluções existentes, identificou-se, junto dos principais intervenientes, os requisitos que a solução deveria cumprir. a criação de uma aplicação em flask em conjunto com uma elas& stack constitui a base tecnológica da solução. a modularidade, escalabilidade e robustez da solução foi tida em conta em todo o processo de elaboração da solução. o resultado final é uma ferramenta totalmente funcional que permite satisfazer as necessidades impostas. a integração com os principais fornecedores de cloud estudados foi amplamente conseguida. a avaliação da mesma foi realizada tendo por base diversos casos de estudo de clientes reais da empresa.",
      "cloud computing has been widely adopted in the area of information technology (it) over the last decade due to the many advantages it provides, including the possibility of reducing infrastructure costs. although the adoption of the cloud can mitigate the costs of operating web applications, it appears that the definition of prices practiced by service providers has become increasingly complex, threatening one of the main reasons that leads users to migrate to the cloud: cost savings. due to this increased complexity, the number of cloud cost monitoring and optimization solutions has been growing in order to face this problem. while there are some solutions that can help with cost optimization, it is verified that the visibility into costs and usage data is limited, and it is not possible to query the information with the granularity that users want. for all these reasons, the research and development team (r&d) of eurotux informatica, s.a. decided to invest in the development of a solution that would help its employees and customers in a problem they face every day. after studying existing solutions, the key actors identified the requirements that the solution should sastified. creating a flask application together with an elastic stack is the technology foundation of the solution. the modularity, scalability and robustness of the solution has been taken in consideration throughout the solution design process. the final result is a fully functional tool that allows you to meet the requirements imposed. the integra-tion with the major cloud providers studied has been largely achieved. the evaluation was made based on several study cases of real company customers."
    ],
    0.3
  ],
  [
    [
      "o oracle retail oferece um conjunto de aplicações de software que ajuda os retalhistas a gerir os seus negócios, incluindo ponto de venda, gestão de inventário, gestão de relações com clientes e gestão da cadeia de fornecimento. no entanto há necessidade de adaptar a solução oracle retail à realidade do cliente, isto é, fazer algumas personalizações à solução. estas necessidades dos clientes são várias vezes idênticas, sendo o volume de alterações por vezes elevado e repetitivo, mudando pequenas especificidades de cliente para cliente. isto, por sua vez, resulta numa perda de tempo valioso que poderia ser alocado a tarefas mais produtivas. combinando esta realidade de trabalhos semelhantes com a pressão do mercado onde a disponibilidade e continuidade de recursos é cada vez mais complexa, as organizações devem procurar formas de automatizar estes processos e beneficiar da redução do tempo gasto tanto no desenvolvimento como na resolução de problemas. à luz destes desafios, o desenvolvimento de uma aplicação destinada a gerar código oracle retail personalizado é uma solução promissora para responder às necessidades dos clientes na adaptação da solução oracle retail aos seus requisitos.",
      "oracle retail offers a suite of software applications that help retailers manage their businesses, including point of sale, inventory management, customer relationship management and supply chain management. however, there is a need to adapt the oracle retail solution to the customer’s reality, i.e. to make some customizations to the solution. these customer needs are often identical, and the volume of changes is sometimes high and repetitive, changing small specifics from customer to customer. this, in turn, results in a waste of valuable time that could be allocated to more productive tasks. however, there is a need to adapt the oracle retail solution to the customer’s reality, i.e. to make some customisations to the solution. these customer needs are often identical, and the volume of similar changes, configurations and/or solution extensions that are applied is very high. this, in turn, results in a waste of valuable time that could be allocated to more productive tasks. combining this reality of similar work with market pressure where resource availability and continuity is increasingly complex, organisations must look for ways to automate these processes and benefit from reduced time spent on both development and troubleshooting. in the light of these challenges, the development of an application designed to generate personalised oracle retail code is a promising solution for responding to the similar needs of customers in adapting the oracle retail solution to their requirements."
    ],
    [
      "a gestão de acesso e permissões afeta várias áreas como a da saúde, mais concretamente os registos de saúde eletrónicos, que se espera que nos próximo anos cresça exponencialmente e alcancem um valor no mercado de $39.7 biliões no ano 2022. a utilização de blockchain aparece como uma solução para estes cenários onde existem diversos domínios em que os dados são potencialmente sensíveis, quer como dados pessoais, quer como dados que podem revelar segredos de negócio. algumas tecnologias, como o hyperledger fabric, já prometem resolver estes problemas, mas sempre com uma granularidade baixa, com bastantes limitações ao nível da definição de políticas de acesso e de transformações dos dados. no contexto de hyperledger fabric vamos implementar um mecanismo de gestão de permissões flexível, que além de diferenciar o acesso com base na identidade de quem faz o pedido permite considerar um conjunto de atributos configurável. adicionalmente, além de decisões binárias sobre o acesso, o sistema que implementamos permite implementar políticas que definem transformações a ser aplicadas aos dados acedidos.",
      "the management of access and permissions affects several areas such as health, more specifically elec tronic health records, which are expected to grow exponentially in the coming years and reach a market value of $39.7 billion in the year 2022. the use of blockchain comes up as a solution for these scenarios where there are several domains where data is potentially sensitive, either as personal data or as data that can reveal business secrets. some technologies, such as hyperledger fabric, already promise to solve these problems, but always with a low granularity, with many limitations in terms of definition of access policies and data transformati ons. in the context of hyperledger fabric, we are going to implement a flexible permissions management mechanism, which, in addition to differentiating access based on the identity of the person making the request, allows us to consider a set of configurable attributes. additionally, in addition to binary access decisions, the system we implement allows us to implement policies that define transformations to be applied to the accessed data."
    ],
    0.3
  ],
  [
    [
      "the objective of this project is to develop a mobile application in order to aid pediatricians performing their work. the necessity of this application was initially identified by a pediatrician working in santo antónio hospital of oporto, after also verifying the interest of some of his coworkers. the bibliography also states some situations where mobile applications may be helpful, such as: errors in the administration of drugs or the difficulty pediatricians face in performing needed mathematical operations. it is made a review of pediatric applications, mobile mostly, in order to know what kinds of applications are already available for pediatricians. it is presented the analysis of 5 distinct applications, from medical calculators for emergency situations to decision support systems that given a set of clinical characteristics it is provided a list of diagnosis to consider. following it is done a study of requirements elicitation and prioritization. its objective is to know the techniques and tools already studied in the bibliography, as well as to identify the most appropriate ones for this project. several elicitation and prioritization techniques were used in this project. it is also used a tool to register the requirements. in order to develop a mobile application that may run on the majority of smartphones in the market, it is made an analysis of the smartphone operating systems market share, as well as of market share projections for the next few years. after identifying the target operating systems for the app it is made a study of the mobile cross-platform development frameworks. the framework choice considered the elicited requirements and the operating systems with the greatest market share. after a learning period of the involved technologies, the pediatric app is developed using the gathered requirements and following the results of the requirements prioritization. the development of the application was always followed by a pediatrician, and as a result the application was tested and refined during that time. finally, the application is released as well as a questionnaire to evaluate it.",
      "este projecto tem o objectivo de criar uma aplicação para os dispositivos móveis que auxilie os médicos pediatras no exercício das suas funções. a necessidade desta aplicação foi inicialmente identificada por um médico pediatra que trabalha no hospital de santo antónio do porto, após também verificar o interesse de alguns colegas de trabalho. a bibliografia também evidencia algumas situações em que as aplicações móveis podem dar o seu contributo, assim como: erros na administração de medicamentos ou dificuldades na realização de operações matemáticas necessárias. é feita uma análise a aplicações móveis na sua maioria, vocacionadas para pediatria por forma a conhecer o trabalho já realizado nesta área. é apresentada a análise de 5 aplicações diferentes, que vão desde calculadoras médicas para situações de emergência até sistemas de suporte à decisão em que é apresentada uma lista de possíveis diagnósticos dado um conjunto de características do paciente. de seguida é feito um estudo sobre a elicitação e priorização de requisitos. este estudo teve por objectivo conhecer as técnicas e ferramentas já estudadas até ao momento, assim como identificar e aplicar as que melhor se adequam a este projecto. são aplicadas várias técnicas tanto de elicitação como de priorização de requisitos. é também utilizada uma ferramenta para o registo dos requisitos. para desenvolver uma aplicação móvel que atinja a grande maioria dos dispositivos móveis, é realizada uma análise ao market share dos sistemas operativos móveis, assim como a previsões para os próximos anos. depois de identificados os sistemas operativos preferenciais para o desenvolvimento da aplicação é feito um estudo das frameworks de desenvolvimento de aplicações móveis multiplataforma. a escolha da framework teve em conta os requisitos adquiridos e os sistemas operativos com mais peso no mercado. passado um período de aprendizagem das tecnologias envolvidas neste projecto, é desenvolvida a aplicação em causa, utilizando os dados recolhidos na elicitação de requisitos e seguindo a ordem resultante da priorização dos requisitos. o desenvolvimento da aplicação foi sempre acompanhado por um médico pediatra, sendo a aplicação testada e refinada ao longo desse período. por fim, a aplicação é lançada assim como um questionário que pretende avaliar a mesma."
    ],
    [
      "we are now living in a digital world where almost anything, or something is saved somewhere with very few considerations for determining if that was in fact relevant to be saved or not. hence, it is predictable that most information systems are facing an information management problem. to overcome this issue, it is vital the creation of new and more specific data management techniques that will enforce the established governance policies and manage the information systems in order to maintain their ideal performance and quality. currently, a solution that is able to cope with this problem efficiently is “pure digital gold”, especially for the biggest players that have to handle an astonishing amount of data, which needs to be properly managed. nevertheless, this is a problem of general interest for any database administration, because even if shrinking the dimension of the information is not a major concern in some cases, the data assessment efficiency and its quality assurance are certainly two subjects of great interest for any system administrator. this work tackles the data management problem with a proposal for a solution that uses machine learning techniques and other methods, trying to understand in an intelligent manner the data in a database, according to its relevance for their users. thus, identifying what is really important to who uses the system and being able to distinguish it from the rest of the data, is a great way for creating new and efficient measures for managing data in an information system. through this, it is possible to improve the quality of what is kept in the database as well as increase, or at least try to ensure, system performance. basically, what its users expect from it throughout its lifetime.",
      "estamos a viver num mundo digital onde praticamente tudo que alguém ou algo faça é capturado e guardado em algum sítio, com muito pouca consideração que determine se esse evento é ou não relevante para ser guardado. como tal, é previsível que grande parte dos sistemas de informação tenha, ou venha a ter, um problema de gestão de informação no futuro. isto obriga a que sejam criados novos tipos de técnicas de gestão de dados mais eficientes e específicos para cada caso, que sejam capazes de governar os sistemas de forma a assegurar o desempenho e qualidade desejados. atualmente, uma solução capaz de lidar com este problema eficientemente nos tempos que correm é “ouro digital”, especialmente para os grandes intervenientes neste domínio que têm de lidar com uma quantidade exorbitante de dados e que, por sua vez, precisam de ser devidamente geridos. apesar disso, este é um problema de interesse global para qualquer equipa de administração de bases de dados, porque mesmo que a diminuição da dimensão da base de dados não seja uma preocupação fulcral para certos casos, o eficiente acesso e a qualidade dos dados existentes numa base de dados serão sempre dois assuntos de grande preocupação para qualquer administrador de sistemas. neste trabalho, é investigado o problema da gestão de dados através de uma proposta de solução, na qual através de técnicas de machine learning, tenta com inteligência perceber, aprender e classificar os dados em qualquer base de dados, de acordo com a sua relevância para os utilizadores. identificar o que realmente é importante para quem usa o sistema e ser capaz de distinguir esta informação da restante, é uma excelente forma para se criarem novas e eficientes medidas de gestão de dados em qualquer sistema de informação. assim, certamente, irá aumentar a qualidade de tudo o que é mantido no mesmo, bem como aumentar, ou pelo menos tentar assegurar, que o desempenho do sistema é o esperado pelos utilizadores."
    ],
    0.0
  ],
  [
    [
      "network management evolved in a way where implementing complex, high level network policies, implies dealing with some attributes that depend on low-level specific configuration. this reflects on a difficulty of changing the underlying infrastructure. sdn (software- defined networking) concept opens a road for new developments due to the centralized non vendor-specific control of the network, most of it related with the separation of data and control planes. since collecting actual data to create information is important at the time of taking decisions, network operators need to understand the dynamic of their network through monitoring and sampling. an sdn approach offers different possibilities to solve network managing problems, raising new points of view on how networks can operate and, consequently, how they can be managed and monitored. this study is mainly focused on exploring the sdn architecture, and its elements, for applying sampling techniques through flexible network measurements. to pursue this, sdn elements will be presented and explained, alongside with existing monitoring solutions. these solutions, after explored and analysed, will lead to a new approach on applying and configuring flexible sampling techniques on sdn.",
      "a gestão de redes evoluiu de forma a que, para implementar políticas de rede de alto nível complexas, é comum lidar com alguns atributos que dependem de configuração específica de baixo nível. isto reflete-se numa dificuldade: mudar a infra-estrutura subjacente. o conceito sdn (software-defined networking) abre caminho para novos desenvolvimentos devido ao controlo de rede centralizado que não depende do fornecedor de equipamentos de rede, em grande parte devido à separação das camadas de dados e controlo. para que os operadores de rede possam compreender a dinâmica da rede, recorrem às tarefas de monitorização e amostragem, uma vez que a captura de dados reais da rede, com finalidade de criar informação, torna-se importante no momento de tomar decisões. uma abordagem sdn oferece diversas possibilidades para resolver problemas de rede, apresentando novos pontos de vista sobre como as redes podem operar e, consequentemente, como podem ser geridas e monitorizadas. este estudo está principalmente focado na exploração da arquitetura sdn, e os seus elementos, para a aplicação de técnicas de amostragem através de medições de rede flexíveis. para alcançar o objetivo final, os elementos de uma sdn serão apresentados e explicados, juntamente com as soluções de monitorização existentes. essas soluções, depois de exploradas e analisadas, irão guiar o trabalho para uma nova abordagem na aplicação de técnicas de amostragem flexível em sdn."
    ],
    [
      "every artist is somewhat limited by the mean by which they expose their art. this is also true for the field of computer graphics, where there are many limiting factors that developers must go out of their way to avoid. the most limiting of these factors is the computing performance, which directly limits the complexity of what an artist can fabricate in a piece of hardware. as such, computer graphics’ investigators keep an eye out for the improvements made in the hardware department that enables them to introduce more complexity to the scenes they create on their computers. three years ago, a novel approach to compute the geometric complexity of three-dimensional (3d) scenes was introduced: mesh shaders. mesh shaders pose as an alternative to the traditional geometric processing method and can be a more performant approach to handle specific geometric workloads. notwithstanding, little attention has been given to these shaders. thus, this thesis presents an investigative effort to evaluate the value proposition of these shaders across different scenarios. to do so, this thesis puts mesh shaders against traditional implementations and measures their differences both in method and performance. by the end of this thesis, the reader should have a concise understanding of mesh shaders, but not a clear cut answer regarding their use. these shaders can provide performance benefits in specific scenarios over the traditional approach, but not without considerable care by the developer. in fact, the flexibility provided by the mesh shaders’ approach gives the developer a significant responsibility regarding their final performance. when incorrectly set up, these shaders can result in mediocre performances compared to those of the traditional pipeline. ultimately, these shaders should be used by experienced users intending to avoid specific bottlenecks of the traditional approach. for others, the traditional pipeline offers a more streamlined approach, thoroughly optimised by default.",
      "todos os artistas são de alguma forma limitados pelo meio de exposição da sua arte. isto não deixa de ser verdade com computação gráfica, onde existem vários fatores limitadores que os programadores têm de con tornar. entre estes, o mais impeditivo é a velocidade de computação, que limita diretamente a complexidade da arte que pode ser produzida por uma peça de hardware. deste modo, os investigadores da área de computação gráfica mantêm-se atentos às inovações que ocorrem no campo do hardware e lhes permitem introduzir mais complexidade nos cenários que criam. há três anos, um método inédito para tratar a complexidade geométrica de cenas tridimensionais foi intro duzido: mesh shaders. os mesh shaders apresentam-se como uma alternativa ao método tradicional de pro cessamento de geometria, que pode obter melhor desempenho em certos cenários geométricos. no entanto, não tem sido dada muita atenção a esta alternativa. assim, esta tese apresenta uma investigação destes shaders com o intuito de avaliar a sua proposta de valor em diferentes situações. para o fazer, esta tese irá colocar estes shaders frente a frente com os shaders tradicionais e medirá as diferenças entre ambos, tanto em desempenho como em método. no final, o leitor deverá possuir uma ideia coesa sobre os mesh shaders, mas não terá uma perceção binária quanto ao uso dos mesmos. isto porque estes shaders podem oferecer um benefício em termos de desempenho em certas situações, mas requerem cuidados adicionais por parte do programador. da flexibilidade oferecida pelos mesh shaders advém uma responsabilidade significativa para o programador no que toca ao desempenho final dos mesmos. quando programados incorretamente, estes shaders resultarão num desempenho medíocre comparado ao desempenho oferecido pelo método tradicional. fundamentalmente, estes shaders deverão ser utilizados por utilizadores mais experientes que pretendem evitar bottlenecks específicos do método tradicional. para todos os outros, o pipeline tradicional oferece um método mais simples que possui por predefinição otimizações acentuadas."
    ],
    0.06666666666666667
  ],
  [
    [
      "in our current lifestyle we often face situations that push us a bit further, over our ordinary limits. these situations can be considered as stress moments that make us react differently from a normal situation, both in a physical or psychological way. while a break can be beneficial to our mind and body when we guarantee proper rest after experiencing this types of moments, when, for instance, we do a physical activity, a chain of stressful moments can have negative impact on human beings leading to serious health problems on the long run if there has not been any preventive action. whereas the physical aspect is easily understood when looking at, the psychological side is usually forgotten or undervalued because one cannot waste time and focuses on better achievements or simply lacks understanding of this matter. one of the reasons behind this inevitable fatigue is the high competitiveness in the markets, which forces employees to work harder or for longer periods so as to accomplish the same results as in shorter periods of time. in theory working harder can lead to more productivity thanks to challenging factors or it can have the opposite effect when workers suffer from situations like stress or increase of fatigue. in this dissertation we will examine the relation between performance and mental fatigue and will prove how this association works with the help of a simulation environment created for this purpose.",
      "à medida que os mercados se tornam mais competitivos, os funcionários são pressionados pelos seus superiores para executar tarefas cada vez mais complexas durante o mesmo período de tempo, de forma a aumentar sua produtividade. isto pode levar a situações de risco, pois o indivíduo não está habituado a sentir tanta pressão. são situações que não permitem erros, pelo que a atenção e a performance necessitam de estar ao mais alto nível; por exemplo, com um controlador de voos, essas situações precisam de ser seguramente evitadas. o mesmo conceito aplica-se a outras posições em que situações críticas consecutivas podem levar à deterioração das capacidades cognitivas do sujeito e, em casos extremos, onde o espaço de tempo é consecutivamente maior podem provocar de saúde que irão prejudicar severamente a expectativa de vida do trabalhador tanto a longo prazo como no seu dia a dia. tendo em conta o pressuposto anterior, tentaremos provar a ideia de que situações críticas, distintas umas das outras, precisam de ser abordadas adequadamente, para facilitar a deteção de situações de fadiga mental dos utilizadores, e que existe uma associação entre a fadiga mental e o desempenho ou produtividade de cada pessoa. para este efeito, criamos um ambiente de simulação que pode ser usado para estudar estes casos de forma a obter os resultados pretendidos. a simulação concentra-se na deteção de desempenho enquanto que para a deteção de fadiga mental será utilizado um software externo, neste caso da empresa performetric."
    ],
    [
      "nesta dissertação iremos estudar uma abordagem para a análise, criação e descrição de música através de uma domain specific language (dsl). trata-se de uma linguagem dinâmica, com todas as funcionalidades a que estamos habituados, tais como variáveis, funções, ciclos, condicionais. para além disso, os dois fatores de diferenciação passam pela sintaxes especializadas para declaração de acom panhamentos musicais e de teclados virtuais. esta linguagem deve depois poder ser avaliada e os seus resultados convertidos para diversos formatos, desde ficheiros de som, midi, ou reproduzir diretamente as notas para as colunas do computador. com este intuito vamos analisar as linguagens já existentes neste espaço, bem como quais as funci onalidades que já implementam, e aquelas que consideramos estarem em falta. como esses aspetos em mente, de seguida propomos uma linguagem que tente aproveitar as boas ideias daquilo que já existe, mais as nossas soluções para os novos desafios que encontramos. introdu zimos também vários casos de estudo para demonstrarem as vantagens que acreditamos existirem na nossa abordagem. finalmente descrevemos também o processo de desenvolvimento da linguagem, dividido em três fases principais: 1. o desenho da sintaxe, da sua gramática, e do parser. 2. a implementação do interpretador. 3. o desenvolvimento de uma biblioteca standard para ser incluída com a linguagem. a nível da sintaxe e da gramática, descrevemos sucintamente toda a linguagem. damos particular atenção às expressões de declarações de acompanhamentos musicais e de teclados. em termos grama ticais, são apenas expressões, ou seja, as suas sintaxes devem integrar-se homogeneamente no resto da linguagem. e como tal, podemos utilizá-las em qualquer sítio que onde podemos introduzir uma expressão, seja ela um número, uma string ou o que quer que for. esta integração sem separação significa que todos os aspetos da linguagem têm de ser pensados de forma a coexistir sem problemas. iremos por isso analisar quais os desafios encontrados pela introdução destas novas classes de expressões na gramática, e quais as soluções que foram tomadas para contornar essas situações. a nível do interpretador, discutimos várias das opções que poderiam ser escolhidas (interpretadores tree walk, máquinas bytecode, compilação jit) bem como justificamos a nossa escolha de utilizar um interpretador tree-walk. a nível da biblioteca standard, descrevemos os vários formatos suportados, quer de input, quer de output, bem como os mecanismos providenciados para a utilização de teclados, como grelhas e buffers. no final, descrevemos como correr scripts escritos na nossa linguagem: através de uma aplicação de linha de comandos desenvolvida em python, chamada musikla, publicada no python package index (pypi) 1, e cujo código é disponibilizado livremente no github2.",
      "in this dissertation we’ll study an approach to the analysis, creation and description of music through a domain specific language (dsl). it is a dynamic language with all the features we are used to, such as variables, functions, loops and conditionals. furthermore, the two diferrentiating factors about this language are the specialized sintax for declaration of musical arrangements and virtual keyboards. once evaluated, the results of this language whould be able to be converted into multiple formats, ranging from sound files, midi files, our even sounds played directly by the computer’s speakers. to accomplish that, we’ll analyze existent languages in this space, as well as what functionalities they already implement, and which ones we consider missing from them. with those aspects in mind, we’ll start by proposing a language that tries to reuse the good ideas that are already in use by other projects, plus our own solutions to the challanges we find. we’ll also list several case studies that demonstrate what we believe are the main advantages in our approach. finaly we’ll describe as well the process of developing said language, divided in three main phases: 1. the design of the syntax, its grammar and parser. 2. the interpreter’s implementation. 3. the development of a standard library to be included in the language. with regards to the syntax and the grammar, we’ll briefly describe the entire language, giving particular attention to the musical arrangements and keyboards’ declaration expressions. gramattically, those are regular expressions, and so their syntaxes must integrate seamlessly in the rest of the langfuage. this means being able to use them anywhere we could use an expression, be it a number, a string our anything else. this integration without any specific separation means that all the aspects of the language must be thought of in a way to coexist without issues. because of that, we’ll discuss the challanges we faces by introducing these new classes of expressions in our grammar, and what solutions we found to go around those situations. in terms of the interpreter, we discuss several options that could be chosen (tree-walk interpreters, bytecode machines, jit compilation), as well as the justification for our ultimate choice of building a tree walk interpreter. in terms of the standard library, we describe the multiple formats supported, both for input and output, as well as the provided facilities to the use of our virtual keyboards, such as grids and buffers. in the end, we describe briefly how to run scripts written in our language: through a command line application developed in python, called musikla, published in python package index (pypi)3, and whosesource code is freely available on github4."
    ],
    0.02727272727272727
  ],
  [
    [
      "currently, information technology (it) is a driving factor in the process of globalization and in the innovative use of resources to promote new products and ideas, creating efficient and effective channels to exchange information. products based upon, or enhanced by, technology are used in nearly every aspect of life in contemporary industrial societies. business productivity software ensures that organizations have the tools to overcome the challenges of executing on strategy every day and prosper in an increasingly challenging era. it has been the catalyst for global integration, thus it is hard to imagine a company that dispenses the support of these services. therefore, it allows the return of investment of all the logistics businesses, shipping scheduling, procurement of materials and suppliers. the use of it tools is considered a competitive advantage, taking companies to invest in research and development in order to increase profits, thus influencing their strategy and organizational model. logistics is the process of planning, implementing, and control the efficient, cost effective flow and storage of raw materials, in-process inventory, finished goods and related information from point of origin to point of consumption, for the purpose of conforming to customer requirements. this project looks at the integration of logistics with it, since information is a key element of logistics. the dissertation proposal presented in this document has as main goal, the development of a continuous finished good (fg) monitoring system, also known as outbound logistics. given the responsibility and importance of such system different techniques and methodologies are carefully addressed. techniques necessary to ensure the reliability and the correct organization of information present in the system, as well as a projection for expansion in the near future. an intelligent environment is proposed that is able to track and provide real-time information related to fg in transit. the major focus is that the user should not be forced to actively search for deviations in the supply chain,but be able to pursue other tasks,and still be notified of any change in the transportation process. to develop this work a prototype was devised on which the behaviour and external system interactions were thoroughly tested and validated. the result of the project is a system capable of monitoring fg in transit, as well as provide updated information to the users in order to better predict or assess any deviations.",
      "atualmente, a tecnologia da informação (ti) é um fator impulsionador na globalização e no uso inovador de recursos para promover novos produtos e ideias, criando canais eficientes e efetivos para trocar informações. os produtos baseados ou aprimorados pela ti são utilizados em quase todos os aspetos nas sociedades industriais contemporâneas. o desenvolvimento de software garante que as organizações possuam as ferramentas para superar os desafios diários e prosperam numa era cada vez mais desafiante. a ti tem sido o catalisador da integração global, por isso é difícil imaginar uma empresa que dispense o suporte desses serviços. assim sendo, possibilita assegurar o retorno do investimento em conjunto com a logística do negócio, agendamento de compras, obtenção de matéria-prima, contratação e fornecedores. o uso de ferramentas deste tipo ´e considerado um diferencial competitivo, levando estas a investirem maciçamente em pesquisa e desenvolvimento de forma a aumentar os lucros, influenciando desta forma seu modelo de estratégia e organização. a logística é o processo de planear, implementar e controlar de forma eficiente o fluxo e o armazenamento de produtos, bem como os serviços de informação associados, cobrindo desde o ponto de origem até o ponto de consumo, sendo que o foco será sempre de atender os requisitos do consumidor. este projeto aborda a integração da logística com a ti, uma vez que a informação é um elemento-chave da logística. esta atividade visa analisar as aplicações da ti e sua influência no modelo de estratégia organizacional. a proposta de dissertação apresentada neste documento tem como objetivo principal o desenvolvimento de um sistema de monitorização contínua de produto acabado, também conhecido como logística externa. dada a responsabilidade e a importância de um projeto destes, diferentes técnicas e metodologias são cuidadosamente abordadas, para garantir a confiabilidade e a organização correta das informações presentes no sistema, bem como uma projeção para expansão num futuro próximo. é proposto um ambiente inteligente de forma a rastrear e fornecer informação em tempo real dos produtos em transito. o principal foco ´e que o utilizador não deve ser forçado a procurar ativamente por desvios, mas ser capaz de realizar outras tarefas e ser avisado de qualquer alteração no processo de transporte. foi concebido um protótipo no qual o comportamento e as interações externas do sistema foram exaustivamente testadas e validadas. o resultado do trabalho é um sistema capaz de acompanhar o produto acabado em trânsito, bem como fornecer relatórios adequados para os utilizadores, a fim de prever melhor ou avaliar com mais precisão eventuais desvios."
    ],
    [
      "nos dias de hoje, tem-se notado um aumento do número e diversidade de dados digitais que circulam, são tratados, analisados e utilizados à escala global. os números são significativos, e, por isso, as empresas começam a tomar partido de serviços de terceiros, para beneficiar das vantagens de computação que estes proporcionam. posto isto, serviços de nuvem disponibilizados pela amazon, google ou microsoft, são utilizados por essas empresas, que procuram garantias não só de disponibilidade mas também de proteção dos seus dados. como temos observado ao longo dos anos, os serviços de nuvem têm vindo a sofrer imensos ataques, onde falhas de segurança nos servidores de armazenamento acabam por ser responsáveis pela libertação de enormes quantidades de informação confidencial. de modo a resolver as preocupações existentes de aplicações que lidam com dados sensíveis e confiáveis foram propostas várias bases de dados capazes de armazenar e processar dados de forma segura na cloud. contudo, o maior esforço de investigação encontra-se em desenvolver novos esquemas criptográficos que protegem os dados em texto cifrado de tal modo que as bases de dados consigam processar interrogações como se fosse texto simples. esta abordagem apesar de eficiente acaba por libertar informação sensível que pode ser utilizada para quebrar a segurança dos sistemas. para além disso, a investigação existente tem dado prioridade às bases de dados sql devido à sua grande aplicabilidade. esta dissertação toma uma abordagem diferente e apresenta uma nova base de dados nosql com processamento seguro, trustnosql, assente nas propriedades de segurança de hardware confiável. mais precisamente, este trabalho tem três contribuições principais. o primeiro é uma análise compreensiva do estado da arte atual em base de dados com processamento seguro. este estudo permite posicionar o sistema apresentado em relação às capacidades e propriedades de segurança dos sistemas existentes. a segunda contribuição é a base de dados nosql com processamento seguro, trustnosql, a primeira base de dados nosql que processa de forma segura as interrogações utilizando a tecnologia intel sgx. a última contribuição é uma extensa avaliação do sistema apresentado com uma plataforma de avaliação de base de dados reconhecida pela indústria.",
      "in today’s world, there is a notorious increase of digital data that is handled, processed and analyzed at a global scale. to handle this data surge, organizations have started to outsource storage and computation to cloud services. these services can store vast amounts of data and handle thousands of concurrent users at a fraction of the cost of what organizations would have to spend to have the same computational power. amazon, google and microsoft are just some of the cloud market players that over the years gained their clients trust by having highly available and ubiquitous services. however, this trust has been affected by data leaks that compromised organization’s confidential data and individuals’ privacy. to address the concerns of existing applications that deal with sensitive and confidential data, several privacy-aware databases technologies have been proposed to securely outsource storage and computation to the cloud. however, the core research effort has been to develop new cryptographic schemes that protect data in encrypted text, so that databases can process queries as if they were plaintext. furthermore, most of the research has explored sql databases due to its wide applicability. in this dissertation we propose a different approach to secure databases by proposing a novel privacy-aware nosql database, trustnosql that leverages secure hardware processing technologies. in detail, this work has three main contributions. the first, is a detailed state-of-the-art of the current privacyaware databases, sql and nosql, and the security guarantees ensured by these systems. the second contribution is the system trustnosql, the first nosql privacy-aware database that securely processes queries with intel sgx. the final contribution is a detailed system evaluation with an industry-standard benchmark."
    ],
    0.3
  ],
  [
    [
      "a crescente utilização dos sistemas de informação (si) nas unidades de saúde tem um papel muito importante para garantir a qualidade das mesmas. com as tecnologias da informação e da comunicação (tic), os dados armazenados estão estruturados e organizados de forma a possibilitar uma utilização rápida e e caz. o aumento de informações em formato eletrónico no processo de registo clínico (rc) apesar de diminuir em grande escala erros que resultavam da utilização de dados mal entendidos, trouxe um desa o aos técnicos de informática médica. esse desa o passa por melhorar a qualidade da prestação de cuidados de saúde utilizando a informação armazenada. é neste âmbito que surgem as normas e sistemas de nomenclatura que possibilitam uma uniformização do rc de forma a evitar dados ambíguos e permitir a comunicação entre diferentes pro ssionais de saúde e serviços hospitalares. estas normas são divididas conforme a sua nalidade, havendo normas de comunicação, imagem e representação. pretende-se, neste contexto, implementar o systematized nomenclature of medicine (snomed) na agência de interoperação, difusão e arquivo (aida) no centro hospitalar do alto ave (chaa) de forma a utilizar as suas potencialidades no processo de uniformização do registo clínico eletrónico (rce).",
      "the increasing use of information systems in health units has a very important role to ensure their quality. with information and comunication technologies, the stored data are structured and organized in order to enable a fast and e ective utilization. the increase of information in electronic format in the process of health record while decreasing large-scale errors that resulted from the use of data misunderstandings brought a technical challenge to medical informatics. this challenge involves improving the quality of health care using the stored information. it is in this context that arise standards and naming systems that enable uniformity in the medical record in order to avoid ambiguous data and allow communication between di erent healthcare professionals and hospital services. these standards are divided according to their purpose, having communication, image and representation standards. it is intended, in this context, implementing the snomed in aida at the chaa in order to use their potentialities in the process of standardization of the electronic health record."
    ],
    [
      "cada vez mais a relação entre as tecnologias da informação e a saúde se estreitam. concretamente, na neuroimagiologia, essa ligação tem vindo a tornar-se cada vez mais importante principalmente após o surgimento da imagem de ressonância magnética (mri – magnetic ressonance imaging). com o desenvolvimento da tecnologia, para além das aquisições mri convencionais, surgiram outras técnicas como a aquisição de imagens de tensor de difusão (dti – diffusion tensor imaging) e da mri funcional (fmri). estas técnicas permitem a obtenção de uma imagem interior do corpo. um dos órgãos mais estudados com estas imagens é o cérebro, que é alvo de vários estudos mas que devido à sua complexidade ainda é bastante desconhecido. enquanto que com a mri estrutural pode-se efetuar uma análise volumétrica às diferentes estruturas do cérebro, com a dti é possível verificar a integridade da substância branca através das fibras virtualmente criadas que representam o movimento das moléculas de água. vários estudos referem os benefícios de uma análise multimodal com estas duas técnicas. para tratamento e análise destas imagens é necessário uma gestão de várias aplicações informáticas que processam os dados e corregistam as imagens de forma automática. um dos grandes desafios consta, não só na utilização individual de cada ferramenta na qual é exigido algum conhecimento técnico, como na combinação das várias aplicações que apresentam os dados resultantes em diferentes formatos. uma solução passa pela pesquisa e definição de fluxos de trabalho para que exista uma abordagem simples dos procedimentos a ter com as várias ferramentas e da sua combinação com outras. no entanto, esta solução não impedirá o gasto de recursos de tempo e o trabalho moroso de um estudo que contenha vários sujeitos. assim, neste trabalho, para além de serem apresentados os vários fluxos de trabalho possíveis para análise multimodal, será exposto um módulo automatizado que será inserido numa aplicação de multimodalidade já existente: braincat. a presente dissertação apresenta um meio de facilitar as análises multimodais para que a qualidade quer a nível de investigação científica quer a nível dos diagnósticos clínicos aumente.",
      "currently, the relationship between information technology and health has been increasingly stronger. in the neuroimaging area, this connection has become increasingly important mainly after the emerging of magnetic resonance imaging (mri). with technology development, in addition to conventional mri acquisitions, other techniques have arisen: diffusion tensor imaging (dti) and functional mri (fmri). these techniques allow the capture of an inside body image. one of the most studied organs is the brain which is the target of several studies. however, due to its complexity, it still has much to reveal. on the one hand, with structural mri it is possible to analyze different volumes and structures of the brain. on the other hand, with dti, it is possible to verify the integrity of the white matter through the fibers virtually created that represent the motion of water molecules. several research papers report the benefits of a multimodal analysis that combine these two techniques. for imaging processing and analyzing, the management of the various informatic tools is needed in data processing and imaging co-registration. some challenges are associated with the use of individual tools that require some technical knowledge. the combination of the various applications returns data with different formats. one solution involves the research and definition of existing workflows for a simple approach about the procedures required with various tools and their combination. however, this solution does not prevent the wasting of time resources and the hardly work of a multiple subjects study. this work presents not only various possible workflows for multimodal analysis but also it exposes an automated module that will be inserted into existing application multimodality: braincat. this dissertation presents a way to facilitate multimodal analysis to improve the quality,both in scientific research and in clinical diagnoses."
    ],
    0.3
  ],
  [
    [
      "the importance of making predictions in health is mainly linked to the decision-making process. make survival predictions accurately is a very difficult task for healthcare professionals and a major concern for patients. on the one hand, it can help physicians decide between palliative care or other medical practice for a patient. on the other hand, the notion of remaining lifetime could help patients in the realization of dreams. however, the prediction of survivability is directly related to the experience of health professionals and their ability to memorize. most decisions are made based on probability and statistics, but these are based on large groups of people and may not be suitable to predict what will happen in particular cases. consequently, the use of machine learning techniques have been explored in healthcare. their ability to help solve diagnostic and prognosis problems has been increasingly exploited. the main contribution of this work is a prediction tool of survival of patients with cancer of the colon and/or rectum, after treatment and a few years after treatment. the characteristics that distinguishes it is the balance between the number of required inputs and their performance in terms of prediction. the tool is compatible with mobile devices, includes a online learning component that allows for automatic recalculation and flexibly of the prediction models, by adding new cases. the tool aims to facilitate the access of healthcare professionals for instruments that enrich their practice and improve their results. this increases the productivity of healthcare professionals, enabling them to make decisions faster and with a lower error rate.",
      "a importância de fazer previsões na área da saúde está sobretudo ligada ao processo de tomada de decisão. fazer previsões de sobrevivência de forma precisa é uma tarefa muito difícil para os profissionais de saúde e uma grande preocupação para os pacientes. por um lado, pode ajudar os médicos a decidir entre cuidados paliativos ou outra prática médica para um paciente. por outro lado, a noção do tempo de vida remanescente poderia ajudar os pacientes na concretização de sonhos. no entanto, este tipo de previsão está diretamente relacionada com a experiência do profissional de saúde e da sua capacidade de memorizar. a maior parte das decisões são tomadas com base em probabilidades e estatística, mas estas têm como base grandes grupos de pessoas, podendo não ser adequadas para prever o que vai acontecer em casos particulares. por conseguinte, a utilização de técnicas de machine learning têm sido exploradas na área da saúde. a sua capacidade para ajudar a resolver problemas de diagnóstico e prognóstico tem sido cada vez mais explorada. a principal contribuição deste trabalho é uma ferramenta de previsão da sobrevida de pacientes com cancro do cólon e/ou do reto, após o tratamento e alguns anos após o tratamento. as características que a distingue são o equilíbrio entre o número de entradas necessárias e o seu desempenho a nível da previsão. a ferramenta, compatível com dispositivos móveis, possui uma componente de aprendizagem em tempo real que permite recalcular de forma automática e evolutiva os modelos usados para fazer a previsão, através da adição de novos casos. a ferramenta tem como propósito facilitar o acesso dos profissionais de saúde a instrumentos capazes de enriquecer a sua prática e melhorar os seus resultados. esta aumenta a produtividade dos profissionais de saúde, permitindo que estes tomem decisões mais rapidamente e com uma taxa de erro menor."
    ],
    [
      "this document reflects the work developed for a master’s dissertation in scope of smart autonomous mobile units (samu) project. this project is inserted in the ifactory programme where exists a partnership between bosch car multimedia (cm) and university of minho (um). the samu project main objective is the creation and development of a system that supports the internal movement of materials along the supply chain using autonomous vehicles (avs). with this type of system is possible to increase the efficiency and productivity of the logistics processes. in this context, the present document makes a brief description about the project area of intervention and a summary about the current state of material flows in bosch brgp. the work developed in this dissertation is integrated in samu project and is fundamental to achieve samu system goals. briefly, it consists of a solution responsible for managing and tracking a fleet of indoor autonomous vehicles. in addition, the integration process of samu system in bosch infrastructure was necessary to validate the management system. this document has a brief contextualization about samu project and the detailed description of all the work made in this dissertation. it is also presented the analysis of the state of art for this type of systems, the problems that the project intends to address and the challenges encountered throughout the work. finally, it is described the solution found with the mechanisms and strategies adopted, the results analysis and the final conclusions.",
      "o presente documento reflete o trabalho desenvolvido referente a uma dissertação de mestrado no âmbito do projeto samu, sendo este um projeto inserido no programa ifactory no qual existe uma parceria entre a bosch car multimedia (cm) e a universidade do minho (um). o projeto samu tem como principal objetivo a criação e desenvolvimento de um sistema de movimentação interna de materiais ao longo da cadeia de abastecimento com recurso a dois tipos de veículos autónomos internos (avs). com este tipo de sistema é possível ter processos logísticos mais eficientes que consequentemente vão aumentar os níveis de produtividade da fábrica. neste contexto, ´e feita uma breve descrição onde o projeto vai intervir, assim como o estado atual do fluxo de materiais na bosch brgp. o trabalho desenvolvido nesta dissertação é parte integrante do projeto samu sendo fundamental para o seu funcionamento e para alcançar os objetivos definidos inicialmente. resumidamente, o principal objetivo desta dissertação consiste em desenvolver uma solução capaz de gerir e monitorizar um conjunto de veículos autónomos internos. adicionalmente, o processo de integração do sistema samu na infraestrutura da bosch foi necessário para validação do sistema de gestão criado. neste documento ´e feita uma breve contextualização ao projeto samu, assim como a descrição detalhado de todo o trabalho desenvolvido nesta dissertação. alem disso, foi feito um levantamento e análise do estado de arte deste tipo de sistemas, os problemas que o projeto pretende tratar, e todos os desafios encontrados ao longo do trabalho. por último, é apresentada a solução encontrada com os mecanismos e estratégias adotadas juntamente com os resultados obtidos e respetiva analise finalizando com uma conclusão."
    ],
    0.06666666666666667
  ],
  [
    [
      "digital preservation is the sum of activities necessary to ensure the long-term access to digital information. the oais standard(iso, 2012a) was developed in order to ease the communication between the various entities involved in the preservation of digital objects and regulate the long-lasting storage of digital information. the preservation process begins when the producer creates submission information packages (sip) and uploads them to the archive’s repository. to create these packages, the producer must choose which files to archive and provide extra information (metadata) to describe and to allow finding the information. as the production of digital content increases exponentially, the creation of sip by current methods can be too onerous and even unfeasible. this work focuses on creating a semi-automatic way of producing sips by employing a simple and well-defined workflow. using the file system as the source of content, the producer defines aggregation and metadata association rules and specify how the sips are created. the application that was developed to support this work, roda-in, was designed to be able to create thousands of sips with gigabytes of data in an easy to use way. additionally, it has multiple features that ease the work of the producer, such as metadata templating and mass edition.",
      "a preservação digital define-se pelo conjunto das atividades necessárias para garantir o acesso a longo prazo à informação digital. a norma oais(iso, 2012a) foi desenvolvida para facilitar a comunicação entre as várias entidades envolvidas na preservação de objetos digitais e regular o armazenamento duradouro da informação digital. o processo de preservação começa quando o produtor cria pacotes de informação de submissão (sip) e os envia para um repositório de um arquivo. para criar estes pacotes, o produtor tem que escolher os ficheiros que pretende arquivar e fornecer informação extra (metadados) para descrever e permitir a descoberta da informação. uma vez que a produção de conteúdo digital tem vindo a crescer exponencialmente, a criação de sip pelos métodos atuais pode ser demasiado oneroso ou mesmo impraticável. este trabalho foca-se na criação de uma forma semi-automática the produzir sips empregando um workflow simples e bem definido. usando o sistema de ficheiros como fonte do conteúdo, o produtor define regras de agregação e de associação de metadados que especificam como os sips são criados. a aplicação que foi desenvolvida para suportar este trabalho, roda-in, foi desenhada para ser capaz de criar milhares de sips com gigabytes de dados e para ser de fácil utilização. adicionalmente, possui múltiplas funcionalidades que facilitam o trabalho do produtor, como por exemplo a criação de metadados a partir de um modelo ou a edição em massa de metadados."
    ],
    [
      "the internet is always evolving. the way content is generated, displayed and ac cessed is constantly changing with the advancements of technology. as such, new tools, frameworks and technologies are constantly surfacing as a way to deal with the challenges of this evolution. keeping up with an increasingly large number of options is, for web developers, as important as it is challenging. with this challenge in mind, this dissertation aims to offer a deep look into the current state of front-end web development by going through relevant concepts and doing an in-depth, comparative analysis of the different frameworks. in the process, data will be collected, a case study will be developed and a developed ap proach will be validated, thus obtaining results and taking conclusions that will help make the best possible decisions in the development process. the author will be, during the work period, part of the developer team at jumpseller, working hands-on with these technologies",
      "a internet está sempre a evoluir. a forma como o conteúdo de uma página é criado, visualizado, e acedido está constantemente a alterar graças aos avanços tecnológicos. como tal, existem novas ferramentas, frameworks, e tecnologias em constante surgimento, oferecendo métodos para lidar com os desafios desta evolução. o de safio de se manter a par de um número cada vez maior de opções é, para os web developers, algo tão importante como é exigente. com este desafio em mente, o objetivo desta dissertação é oferecer uma análise a fundo para o atual estado da arte do desenvolvimento web front-end, percorrendo vários conceitos relevantes e realizando uma análise comparativa detalhada sobre as várias frameworks. para atingir este fim, dados serão obtidos, será desenvolvido um caso de estudo e uma abordagem será desenvolvida e validada, obtendo assim resultados e tirando conclusões que irão auxiliar a tomada de decisões no processo de desenvolvimento. o autor irá integrar, durante o período de trabalho, a equipa de desenvolvimento na jumpseller, trabalhando diretamente com estas tecnologias."
    ],
    0.3
  ],
  [
    [
      "na atual conjuntura em que vivemos é crucial que seja realizado um estudo aprofundado sobre a utilização dos recursos da cidade, especialmente em zonas urbanas. além disso, uma rede de estradas bem conservada deveria ser uma prioridade para o desenvolvimento económico e bem-estar dos habitantes de qualquer país. com o desenvolvimento da tecnologia, em particular na área das cidades inteligentes (“smart cities”), é importante implementar sistemas de apoio, de modo a centralizar toda a informação existente sobre um aspeto de interesse, de maneira a “virtualizar” as cidades. neste trabalho pretende-se desenvolver esse suporte em torno de uma rede de transportes públicos, compilando toda a informação que essa rede nos poderá fornecer para que possa ser útil a várias entidades. neste contexto, a presente dissertação pretende estudar duas vertentes no âmbito das cidades inteligentes. na primeira será realizado um estudo do estado do pavimento através de sensores de aceleração (acelerómetros), sendo estes sensores colocados numa rede urbana de transportes públicos já existentes, assim a rota será conhecida pela entidade responsável, como por exemplo a câmara municipal do distrito em questão. assim, espera-se que com esta informação seja possível uma tomada de decisão mais adequada, face ao estado do pavimento, de maneira a que possam ser realizados os trabalhos necessários para a reconstrução do mesmo. na segunda vertente será realizado um estudo da monitorização do trânsito em áreas em que o fluxo é elevado, com base na mesma rede de transportes públicos, sendo a informação obtida através dos mesmos sensores.",
      "in the current conjuncture that we are living in, it is crucial to reflect and carry out a thorough study on the resources being used by the population, especially in urban areas. with the increasing developments in technology, particularly in the smart cities domain, it is important to implement information systems to help sustaining the growth of a city. this involves gathering and centralizing data in order to “virtualize” the cities, providing enhanced services to citizens in general. this work intends to develop a system that could potentially assist daily life in cities, resorting to a public transport network. the idea is to take advantage of all the information that this network can provide, compiling and delivering it to be useful to various entities. in this context, this dissertation intends to study two strands within smart cities. the first will be carrying out a study of road conditions through accelerometers sensors; these sensors will be used in an existing urban public transport network. the routes to consider may be determined by the responsible entity, such as the town hall of a particular district. thus, based on the data collected, decision making can be improved, for instance, starting road reconstruction more rapidly if the state of the pavement is in poor condition. in the second part, a traffic monitoring study will be carried out in urban areas where the traffic flow is high, based on sensorial data from the same public transport network."
    ],
    [
      "non-fungible tokens have emerged as a trend in recent times and have aroused interest among developers and companies due to their potential in a wide range of application areas. this dissertation arose from the initiative of the company dstelecom, and its main objective is to study the use of nfts in the context of document certification and the development of a software module that issues certificates based on nfts, thus providing a viable alternative to the methods currently used to issue certificates. in order to meet the objectives of the dissertation, it was divided into several stages, starting with the study of blockchain and nft technologies, application examples and related work. since creating and managing nfts requires a blockchain base, one of the important decisions was choosing the blockchain platform that would support the creation and maintenance of nfts. once the technological decisions had been made, the next step was to implement a software module which, in the future, could be used as a tool for managing and issuing certificates for documents based on nfts.",
      "os non-fungible tokens emergiram como uma tendência nos tempos recentes o que despertou interesse entre desenvolvedores e empresas, pelas suas potencialidades nas mais diversas áreas de aplicação. esta dissertação surgiu da iniciativa da empresa dstelecom, sendo o seu principal objetivo estudar a utilização de nfts no contexto de certificação de documentos e o desenvolvimento de um módulo de software que emita certificados baseados em nfts, proporcionando assim uma alternativa viável aos métodos utilizados atualmente para emissão de certificados. para que os objetivos da dissertação fossem cumpridos, a mesma foi dividida em várias etapas, co meçando pelo estudo das tecnologias blockchain e nft, exemplos de aplicação e trabalhos relacionados. uma vez que, para criar e gerir nfts é necessário uma blockchain base, uma das decisões importantes foi a escolha da plataforma blockchain que iria suportar a criação e manutenção dos nfts. depois de tomadas as decisões tecnológicas, o passo seguinte centrou-se na implementação de um módulo de software que, no futuro, poderá ser utilizado como uma ferramenta de gestão e emissão de certificados de documentos baseados em nfts."
    ],
    0.3
  ],
  [
    [
      "interstitial lung diseases (ild) are defined as a set of more than 200 pulmonary disorders. among these, the ones broadly termed as pneumonia represent a major cause of morbidity and mortality in the world. the chest radiograph (cxr) was the first x-ray based lung imaging technique to emerge and is still widely used as a diagnostic method for pneumonia and other lung diseases. however, correct interpretation of cxr requires analysis by experts and stays vulnerable to errors and observer-related variation. to counteract these problems, artificial intelligence (al) methods have been applied for the automated analysis of cxr and other medical images. the deep learning (dl) branch of ai and in the particular the methods based on convolutional neural networks (cnn), recently obtained impressive results in these tasks. this dissertation presents a dl approach to classify pneumonia from medical cxr image datasets. two different models based on the development of cnn were trained from a preprocessed dataset of cxr images obtained from 8562 individuals classified as normal (n=7214) or with pneumonia (n=1348) (dataset xp1’). model 1 applied a normal cross entropy loss function, and model 2 an alternative loss function aiming at counteracting the unbalance in normal/pneumonia class frequency. for performance enhancing both models underwent a hyper optimization procedure. the optimized model 1 and 2 were tested on a test set from pi'. to better understand the predictability and generalization potential we then tested both models on an unrelated test set of 624 images (dataset xp2). interestingly, model 1 obtained better performance when tested on xp2 than in xp1', scoring an accuracy of 85%, recall of 93% and precision of 85% for the detection of the pneumonia class. the higher homogeneity present on dataset xp2 compared with dataset xp1' could be a plausible justification. as for model 2, it correctly predicted more pneumonia cases an test set xp1' than model 1. however, on test set xp2 the results were poor, predicting most cases as pneumonia and scoring a recall value of only 26% for the pneumonia class. testing the dl models on unseen data is a relevant but not always performed validation. overall, the higher accuracy, recall and precision levels of model 1 in xp2 suggests it has a higher potential to be applied for real-word application although its performance should be further improved and evaluated. this work opened promising new lines of research for the future development of a high-performance cnn-based automated method to classify cxr and assist in the diagnostic of pneumonia.",
      "doenças intersticiais pulmonares são definidas como um conjunto de mais de 200 doenças pulmonares. dentro deste grupo de doenças, as doenças denominadas como pneumonia ou pneumonite representam uma condição inflamatória que afecta o interstício pulmonar e representam uma das principais causas de morbidade e mortalidade no mundo. a radiografia torácica foi a primeira técnica de imagiologia pulmonar baseada em raios-x a surgir sendo, ainda amplamente utilizada como método de diagnóstico de pneumonia e outras doenças pulmonares. no entanto, a correcta interpretação de radiografias torácicas requer uma análise de pessoal especializado e encontra-se vulnerável a erros e variações relacionadas com o observador. de modo a contrariar estes problemas, métodos de inteligência artificial têm sido aplicados na análise automatizada de radiografias torácicas e outro tipo de imagens médicas. métodos de \"deep iearning\" e em particular, métodos baseados em redes neuronais convolucionais, obtiveram recentemente resultados impressionantes quando aplicados nesta área de estudo. esta dissertação apresenta uma abordagem de \"deep learning\" que permite classificar imagens de pneumonia a partir de \"datasets\" de radiografias torácicas. dois modelos diferentes baseados no desenvolvimento de redes neuronais convolucionais foram treinados a partir de um \"dataset\" preprocessado de radiografias torácicas obtido a partir de 8562 indivíduos classificados como normais (n=7214) ou como doentes de pneumonia (n=1348) (\"dataset\" xp1'). ao modelo 1 foi aplicada uma função \"loss\" de entropia cruzada normal, e ao modelo 2 foi aplicada urna função \"loss\" alternativa que visa contrariar o desbalanceamento entre casos normais e de pneumonia presente no \"dataset xp1’”. para melhorar o desempenho, ambos os modelos foram submetidos a um procedimento de hiper optimização. os modelos 1 e 2 otimizados foram de seguida testados no conjunto de teste do \"dataset xp1’. para entender melhor a capacidade de previsão e generalização, os dois modelos foram também testados num conjunto de teste não relacionado de 624 imagens (dataset xp2). curiosamente, o modelo 1 obteve melhor desempenho quando testado no xp2 do que no xp1', obtendo uma \"accuracy\" de 85%, sensibilidade de 93% e precisão de 85% durante a deteção de casos de pneumonia. a maior homogeneidade de informação presente no xp2 em comparação com o xp1', é vista como a justificação mais plausível. quanto ao modelo 2, ele previu correctamente mais casos de pneumonia no xp1 do que o modelo 1. no entanto, quando testado no xp2, os resultados ficaram abaixo das expectativas, prevendo a maioria dos casos como pneumonia e obtendo um valor de sensibilidade de apenas 26% para a classe de pneumonia. testar os modelos de \"deep learning\" em dados não relacionados é uma técnica de validação relevante, no entanto nem sempre é realizada. os níveis elevados de precisão, sensibilidade e \"accuracy\" do modelo 1 quando aplicado no xp2 sugerem que possuí um grande potencial de utilização em aplicações de carácter real, no entanto, o seu desempenho pode ainda ser melhorado. este trabalho abriu novas e promissoras vias de pesquisa para o desenvolvimento futuro de um método automatizado baseado em cnn de alto desempenho que sela capaz de classificar radiografias torácicas e auxiliar no diagnóstico de pneumonia."
    ],
    [
      "nos últimos anos tem-se verificado um crescimento tecnológico, empreendedor e de inovação no cluster de braga. dadas estas circunstâncias, tem-se verificado um aumento na oferta e procura de produtos e serviços de software, o que incentiva o surgimento de novas start-ups. contudo, devido à elevada competitividade do mercado tecnológico e à atual crise económica mundial, nem todas estas empresas sobrevivem nos seus primeiros anos de vida. neste projeto de investigação procurou-se compreender como as start-ups entram no mercado e o que distingue as que vingam no mercado das restantes, através da avaliação dos seus determinantes internos e externos. deste modo, será possível contribuir para a compreensão das condições em que os empreendedores devem construir as suas start-ups, aumentando as possibilidades de sucesso na construção de produtos e serviços de software com viabilidade de mercado. nesta dissertação foi explorado o early-life decision model, um modelo composto por diversos tipos de decisão que devem ser tomados pelos empreendedores para a sustentabilidade do negócio. através do modelo, procurou-se analisar e avaliar os determinantes internos e externos de diversas vertentes do negócio das start-ups. a metodologia de investigação adotada consistiu na entrevista semi-estruturada, por apresentar características mais apropriadas para a finalidade deste estudo. foram preparadas e realizadas entrevistas em 15 empresas do cluster de braga, foi efetuada uma análise e tratamento dos dados recolhidos e, posteriormente, foi feita uma análise dos resultados. obtiveram-se diversas conclusões como o impacto de cada determinante nos grupos de empresas identificados, bem como a quantidade de determinantes internos e externos identificados nas diversas vertentes do negócio. também foi possível verificar que a existência de determinantes que dificultam a entrada das empresas no mercado, influencia o rumo do negócio, e que a aprendizagem adquirida deles e da experiência vivida durante a atividade da empresa, é essencial para a sustentabilidade do negócio.",
      "in recent years it has been found a technological, entrepreneurial and innovation growth in braga cluster. given these circumstances, it has been found an increase in supply and demand for software products and services, which encourages the emergence of new start-ups. however, due to the high competitiveness of the technology market and the current global economic crisis, not all of these companies survive in their first years of life. in this research project it was sought to understand how start-ups enter the market and what distinguishes those that survive in the market from the remaining ones, through the evaluation of their internal and external determinants. thus, it will be possible to contribute to the understanding of the conditions in which entrepreneurs should build their start-ups, increasing the chances of success in the construction of software products and services with market viability. in this dissertation it was explored the early-life decision model, a model composed of several decision types that should be taken by entrepreneurs for the business sustainability. through the model, it was sought to analyze and evaluate internal and external determinants of several aspects of the start-ups business. the research methodology adopted was the semi-structured interview, for presenting characteristics more appropriate for the purpose of this study. it was prepared and realized interviews in 15 companies of the braga cluster, it was made a data analysis and treatment and, posteriorly, it was made a result analysis. several conclusions were obtained, such as the impact of each determinant on the groups of companies identified, as well as the quantity of internal and external determinants identified in the various aspects of the business. it was also possible to verify that the existence of determinants that make it difficult for companies to enter the market, influence the course of business, and that the learning acquired through them and the experience lived during the activity of the company, is essential for business sustainability."
    ],
    0.02727272727272727
  ],
  [
    [
      "this thesis aims to develop a new methodology that combines model-based testing and bidirectional transformations. more precisely, the method of software testing used is blackbox testing, where the system under test is a black-box. without knowledge of the blackbox’s internal structures or implementation, the focus is on the inputs and outputs. to infer a model for this black-box, machine learning algorithms are used by submitting test cases against the black-box and observing the correspondent output. the resulting model is a finite state machine that produces the same outputs of the black-box when submitted the same inputs used in its making. usually, in this approach, new test cases are provided to infer better models. in this thesis, bidirectional techniques will be studied in order to guarantee the conformity between both the model and the instance evolution. this way, it is allowed not only the evolution of the test cases and co-evolution of the model, but also the evolution of the model and the co-evolution of the test cases.",
      "esta tese visa desenvolver uma nova metodologia que combina model-based testing (mbt) e bidirectional transformations (bx). mais precisamente, o método de teste de software usado é black-box testing (bbt), onde o system under test (sut) é uma black-box. sem o conhecimento das estruturas internas da black-box ou da sua implementação, o foco está nos inputs e outputs. para inferir um modelo para esta black-box, são usados algoritmos de aprendizagem através de interrogações à black-box (i.e., casos de teste) e da observação do output correspondente. o modelo resultante é uma finite state machine (fsm), que produz os mesmos outputs da black-box, quando lhe são submetidos os mesmos inputs usados na sua criação. geralmente, nesta abordagem, novos casos de teste são fornecidos para inferir melhores modelos. nesta tese, serão estudadas técnicas bidireccionais com o objetivo de garantir a conformidade entre as evoluções do modelo e dos casos de teste. desta forma, é permitida não só a evolução dos casos de teste e co-evolução do modelo, mas também a evolução do modelo e a co-evolução dos casos de teste."
    ],
    [
      "nos dias de hoje verifica-se cada vez mais que vivemos num mundo onde a tecnologia tem grande relevância em todas as áreas. com este constante avanço, pode salvar-se milhares de vidas todos os dias devido à sua união com a medicina, melhora-se a capacidade de prever o tempo na área da meteorologia, apoia-se a química com o desenvolvimento de software para o estudo molecular, entre outros. a tecnologia está em todo o lado, como se pode notar, e requer uma constante adaptação por parte do ser humano para melhorar o seu desenvolvimento. uma área que já utiliza este avanço é a educação, e é onde este projeto se irá focar. com a ajuda de um sistema de e-learning (moodle) pretende-se criar um sistema inteligente para reduzir a distância entre os professores e os alunos durante a sua vida académica. pretende-se, também, dar a possibilidade aos professores de perceber mais facilmente e mais objetivamente as dificuldades dos seus alunos através do desempenho que os mesmos apresentam durante os momentos de avaliação.",
      "nowadays, it is verified increasingly that we live in a world where technology is having great relevance in all areas. through this constant advance, we can today save thousands of lives every day due to the union with medicine, improving the ability of temporal diagnosis in the field of meteorology, supporting the chemistry in the development of software for molecular study, among many others. technology is everywhere, as already noted, and requires constant adaptations from us humans to improve our development. one area that already uses much of this technological advancement is education, and it is here that this project will focus. with the help of an e-learning system (moodle) is intended to create an intelligent system to reduce the distance between teachers and students during their academic life. it is intended to also give the possibility to teachers to perceive more easily and objectively the difficulties of their students through the performance they present during the evaluation moments."
    ],
    0.3
  ],
  [
    [
      "a correta previsão e anotação de genes bacterianos é essencial para a aplicação da informação contida no adn em muitos tópicos de pesquisa (bio)médica, como microbiologia, imunologia e doenças infeciosas. embora existam vários softwares de previsão de genes bacterianos como genemarkhmm, glimmer e prodigal e pipelines completos como isga, xbase, maker e consensus prediction, a previsão de genes pode ser melhorada. o principal objetivo deste trabalho foi o desenvolvimento de um pipeline de previsão de genes bacterianos, o prokaryote gene prediction (pgp), que combina métodos de ab initio e de homologia. uma vez que o software ab initio prodigal mostrou um melhor desempenho relativamente a outros softwares estudados, foi usado como o passo inicial para o pgp. considerando as proteínas previstas pelo prodigal, o pgp a) analisa os alinhamentos obtidos, b) determina a necessidade de encurtar ou estender genes, c) introduz as correções necessárias, d) faz a previsão de arnr e arnt utilizando os programas rnammer e trna-scan2 e e) determina a existência de eventuais genes não identificados nas regiões intergénicas, através de um blastx. quando comparados os resultados do pgp com os dados produzidos pelo prodigal utilizando 4 genomas com conteúdo g+c% moderado e 3 com conteúdo em g+c% extremo, o pgp apresentou melhorias de 1% tanto na taxa de erro como na especificidade, exibindo a mesma sensibilidade. foi observado que para genomas com conteúdos g+c% extremos, o pgp tem mais impacto e portanto realiza mais correções. os resultados do pgp ainda foram comparados com os pipelines isga, xbase e consensus prediction. o pgp melhorou a previsão de genes corretos em 4,4%, comparativamente com isga e xbase e ainda 3,1% em relação à previsão do consensus prediction, mantendo uma sensibilidade idêntica entre previsões. no que respeita à deteção de genes na região intergénica verificou-se um acréscimo na ordem de 9 falsos positivos em 12 genomas modelo, necessitando esta vertente de um melhor desenvolvimento. concluiu-se que o pgp melhora a correta previsão de genes, especialmente em genomas bacterianos com conteúdos g+c% extremos, contribuindo para a anotação automática de genomas bacterianos de elevada qualidade.",
      "the correct bacterial gene prediction and annotation is essential for the application of the information contained in dna in several areas of (bio)medicine, like microbiology, immunology and infection diseases. although there are several softwares to perform bacterial gene prediction, like genemarkhmm, glimmer and prodigal and also full pipelines as isga, xbase, maker and consensus prediction, gene prediction can be improved. the main objective of this work was the development of a bacterial gene prevision pipeline, the prokaryote gene prediction (pgp) which combines ab initio and homology methods. since the ab initio software prodigal showed a better performance relatively to others studied softwares, it was used as the beginning step for the pgp. taking into account the proteins predicted by prodigal, the pgp a) analyses the results of the alignment, b) determines if it is necessary to shorten or extend or extension of genes, c) introduces the necessary corrections, d) predictsrrna and trna using the rnammer and trna-scan2 programs and e) determines possible missing genes in intergenics regions through blastx. when comparing the results of pgp with data produced by prodigal, the pgp showed improvements in both the error rate, and in the specificity, while displaying the same sensitivity. for genomes with extreme g+c% content, the pgp has higher impact and therefore performs more corrections. the results obtained with pgp were also compared with isga, xbase and consensus prediction pipelines. the pgp improved the precision of correct genes in 4,4%, comparatively with isga and xbase and 3,1% relative to the prediction of consensus prediction, keeping a similar sensibility among predictions. as regards the detection of genes in the intergenic region there was an increase in the range of 9 false positive in 12 model genomes, requiring this part a better development. it was concluded that pgp improves the correct prediction of genes, especially in bacterial genomes with extreme g+c% content, contributing to a high quality in automatic bacterial gene annotation."
    ],
    "atualmente, com a expansão das tecnologias de informação, muito do conhecimento humano passou a estar registado em suportes digitais, o que nos remete à utilização de intermediários para a leitura dessa informação: hardware e software. devido a esses intermediários estarem em constante evolução e havendo o perigo da sua descontinuidade, essa informação poderá ser perdida, não pelo desaparecimento do objeto digital, mas por ficar ilegível para os novos equipamentos e aplicações. devido a estes problemas emergiu uma nova problemática no universo digital, a preservação digital. este projeto estuda a problemática da preservação digital, mas foca-se numa única classe de objetos digitais: as bases de dados relacionais. as bases de dados relacionais são de extrema importância, particularmente para as organizações, pois é nas suas bases de dados que se encontra informação essencial às suas atividades. por essa razão, é fundamental não comprometer a sua longevidade, integridade e autenticidade. o presente trabalho visa o desenvolvimento de um sistema de exploração de ontologias, que recebe ficheiros em formato siard, transformando-os em ontologias (owl) e acrescentando-os ao repositório. como última fase, foi criado um navegador web que permite explorar a informação das ontologias armazenadas, onde é possível questionar as ontologias através de sparql e guardar essas mesmas interrogações para posterior uso. o repositório das ontologias foi desenvolvido segundo a norma oais, e tem por base uma aplicação web com várias interfaces, para assim proporcionar a ingestão da informação e a sua administração, preservação e disseminação.",
    0.0
  ],
  [
    [
      "a grande deslocalização de pessoas de áreas rurais para zonas urbanas obrigou à criação de novas e melhores infraestruturas nos ambientes citadinos, de forma a se poder assegurar padrões de qualidade de vida adequados. a criação de cidades inteligentes tem como objetivo melhorar a qualidade de vida das pessoas e do meio que as rodeia, garantido uma melhor resposta dos serviços com que interagem em ambientes citadinos. os habitantes das cidades costumam tecer opiniões concretas sobre a sua cidade. a possibilidade de recolher e conciliar essas opiniões é muito interessante para quem tem que reger tais ambientes, pois estas podem permitir a identificação de alguns pontos fortes e fracos dos vários aspetos de uma cidade. com esse conhecimento, um gestor poderá, de forma mais suportada, com informação proveniente dos habitantes da cidade, saber qual o efeito das suas decisões através do recurso a um conjunto de dashboards que incluam índices de bem-estar, conjugando os vários elementos recolhidos e refletindo a apreciação das pessoas sobre a cidade, em tempo real. os dashboards citadinos, quando dimensionados e implementados de forma adequada, constituem um instrumento importante para avaliação da qualidade de vida numa cidade. neste trabalho de dissertação, apresentar-se-á a forma como tais dashboards podem ser projetados e implementados, suportados por um caso de aplicação concreta que discutiremos com detalhe.",
      "the large relocation of people from rural to urban areas obligated the creation of new and better infrastructures in the urban environments, in order to ensure adequate standards of quality of life. the creation of smart cities aims to improve the quality of life of people and the environment that surrounds them, ensuring a better response to the services they interact with in urban environments. the citizens of the cities tend to have concrete opinions about their city. the possibility of collecting and conciliating these opinions is very interesting for those who have to govern such environments, as these can allow the identification of some strengths and weaknesses of the various aspects of a city. with this knowledge, a manager can, in a more supported way, with information from the inhabitants of the city, know the effect of their decisions through the use of a set of dashboards that include well-being indexes, combining the various elements collected and reflecting people's appreciation of the city in real time. the city dashboards, when properly dimensioned and implemented, are an important tool for evaluating the quality of life in a city. in this dissertation, it will be presented the way in which such dashboards can be designed and implemented, supported by a concrete application case that we will discuss in detail."
    ],
    [
      "diabetic retinopathy (dr) and diabetic macular edema (dme) are the damages caused to the retina and are complications that can affect the diabetic population. diabetic retinopathy (dr), is the most common disease due to the presence of exudates and has three levels of severity, such as mild, moderate and severe, depending on the exudates distribution in the retina. for screening of diabetic retinopathy or a population-based clinical study, a large number of digital fundus images are captured and to be possible to recognize the signs of dr and dme, it is necessary that the images have quality, because low-quality images may force the patient to return for a second examination, wasting time and possibly delaying treatment. these images are evaluated by trained human experts, which can be a time-consuming and expensive task due to the number of images that need to be examined. therefore, this is a field that would be hugely benefited with the development of an automated eye fundus quality assessment and analysis systems. it can potentially facilitate health care in remote regions and in developing countries where reading skills are scarce. deep learning is a kind of machine learning method that involves learning multi-level representations that begin with raw data entry and gradually moves to more abstract levels through non-linear transformations. with enough training data and sufficiently deep architectures, neural networks, such as convolutional neural networks (cnn), can learn very complex functions and discover complex structures in the data. thus, deep learning emerges as a powerful tool for medical image analysis and evaluation of retinal image quality using computer-aided diagnosis. therefore, the aim of this study is to automatically assess all the three quality parameters alone (focus, illumination and color), and then an overall quality of fundus images assessment, classifying the images into the classes “accept” or “reject with a deep learning approach using convolutional neural networks (cnn). for the overall classification, the following results were obtained: test accuracy=97.89%, sn=97.9%, auc=0.98 and 𝐹1-score=97.91%.",
      "a retinopatia diabética (rd) e o edema macular diabético (emd) são patologias da retina e são uma complicação que pode afetar a população diabética. a retinopatia diabética é a doença mais comum devido à presença de exsudatos e possui três níveis de gravidade, como leve, moderado e grave, dependendo da distribuição dos exsudatos na retina. para triagem da retinopatia diabética ou estudo clínico de base populacional, um grande número de imagens digitais de fundo do olho são capturadas e para ser possível reconhecer os sinais da rd e emd, é necessário que as imagens tenham qualidade, pois imagens de baixa qualidade podem forçar o paciente a retornar para um segundo exame, perdendo tempo e, possivelmente, retardando o tratamento. essas imagens são avaliadas por especialistas humanos treinados, o que pode ser uma tarefa demorada e cara devido ao número de imagens que precisam de ser examinadas. portanto, este é um campo que seria enormemente beneficiado com o desenvolvimento de sistemas automatizados de avaliação e análise da qualidade da imagem do fundo de olho. pode potencialmente facilitar a assistência médica em regiões remotas e em países em desenvolvimento, onde as habilidades de leitura são escassas. deep learning é um tipo de método de machine learning que envolve a aprendizagem de representações em vários níveis que começam com a entrada de dados brutos e gradualmente se transformam para níveis mais abstratos através de transformações não lineares, para se obterem as previsões. com dados de treino suficientes e arquiteturas suficientemente profundas, as redes neuronais, como as convolutional neural networks (cnn), podem aprender funções muito complexas e descobrir estruturas complexas nos dados. assim, o deep learning surge como uma ferramenta poderosa para analisar imagens médicas para avaliação da qualidade da retina, usando diagnóstico auxiliado por computador a partir do fundo do olho. portanto, o objetivo deste estudo é avaliar automaticamente a qualidade geral das imagens do fundo, classificando as imagens em “aceites” ou “rejeitadas”, com base em três parâmetros principais, como o foco, a iluminação e cor com abordagem de deep learning usando convolutional neural networks (cnn). para a classificação geral da qualidade das imagens, obtiveram-se os seguintes resultados: acurácia do teste = 97,89%, sn = 97,9%, auc = 0,98 e 𝐹1-score=97.91%."
    ],
    0.0857142857142857
  ],
  [
    [
      "the traditional role of a personal computer is dramatically changing with the shift towards cloud-based services. cloud computing and storage provides end-users with universal access to their data across various devices, coherent application and service experience, and substantially decreases hardware requirements for end-user clients (personal computers). however, this cloud-oriented paradigm requires the redesign of applications and services, as well as a serious analysis of the current situation and proper scaling of access networks since this new paradigm changes everyday habits of end-users. this work is focused on the impact of cloud-based applications (using the paradigm of software as a service (saas)) on access networks, analyzes the network and the application behavior, while also addressing application usability and quality of experience (qoe) in different scenarios. a detailed study of the impact on access networks imposed by such cloud-based services (and vice versa) is currently missing, especially in the case of bandwidth-constrained, high-latency mobile access networks. furthermore, this work involves analysis of various cloud-based applications, namely office tasks (text, presentation, and spreadsheet editing), in different combinations and executed on different hardware and software platforms with different levels of integration with cloud-based services. network traffic analysis will be executed, including collecting wireshark traces of the generated and received traffic, correlated with specific executed tasks. the impact of network congestion and latency is also examined in the qoe focused section. the work discussion is broken down into individual hypotheses, reflecting expectations regarding behavior of saas applications, data volume of the network, and qoe of the end user. different end-user experience metrics are used in combination with network-based monitoring (including peak and average bandwidth measurements, latency, packet loss, etc.).",
      "o conceito tradicional de computador pessoal está a mudar drasticamente com a mudança de paradigma para a utilização de serviços disponíveis através da nuvem (cloud-based services). a computação e armazenamento na nuvem possibilita aos utilizadores finais o acesso universal aos seus dados através de diversos dispositivos, um acesso coerente às aplicações e, respetiva, qualidade de serviço, reduzindo substancialmente os requisitos de hardware dos utilizadores finais (nos computadores pessoais). no entanto, a mudança para o paradigma da computação em nuvem exige que se repense não só as aplicações e serviços, mas também exige um estudo sério sobre o panorama atual e a adequada escalabilidade das redes de acesso, uma vez que existe alteração nos hábitos diários dos utilizadores. este trabalho foca-se no estudo do impacto da utilização das aplicações da nuvem (usando o paradigma de software-as-a-service (saas)) nas redes de acesso, na usabilidade das aplicações e na qualidade da experiência (qoe) em vários cenários. atualmente não existe um estudo detalhado sobre o impacto que estes serviços disponibilizados pela nuvem têm nas redes de acesso, especialmente nos casos das redes de acesso móveis que apresentam por si só restrições consideráveis de largura de banda e elevada latência. o estudo contempla a análise da utilização de várias aplicações da nuvem, nomeadamente, tarefas de escritório (edição de texto, edição de apresentações multimédia, edição folhas de cálculo), combinadas e executadas em diferentes plataformas de hardware e software com diferentes níveis de integração com os serviços disponíveis na nuvem. será efetuada uma análise do tráfego da rede, com recolha de traces do wireshark do tráfego gerado e recebido, correlacionando-se com tarefas específicas. o impacto da congestão da rede e a latência são também examinadas na seção focada na qoe. a discussão do trabalho encontra-se distribuída individualmente pelas várias hipóteses formuladas e que refletem as expectativas relativamente ao comportamento das aplicações saas, volume de dados na rede e qoe por parte do utilizador. diferentes métricas para a qoe são usadas, combinadas com a monitorização de parâmetros relevantes da rede (medições da largura de banda média e valores de pico, latência e perda de pacotes, etc.)."
    ],
    [
      "ao longo dos anos tem-se observado um aumento progressivo na frequência e sofisticação dos ataques informáticos. esta tendência obriga ao melhoramento constante do software de apoio às equipas de segurança e administração de sistemas. os sistemas de gestão e correlação de eventos de segurança (em inglês, security information and event managing - siem) fornecem a análise em tempo real de alertas de segurança gerados por aplicações e hardware de rede. o princípio base de todos os sistemas siem é agregar dados relevantes (logs) provenientes de múltiplas fontes de forma centralizada, identificar desvios da norma e tomar as ações necessárias. espera-se com este trabalho de dissertação implementar um siem eficaz, on-premises (i.e, que corra localmente usando os recursos computacionais da organização). a solução deverá permitir efetuar correlação de eventos, alertas e gestão de incidentes.",
      "over time, there has been a noticeable increase in the frequency and sophistication of cyber-attacks. this trend forces the constant improvement of software that supports system administration and security teams. security information and event managing systems provide a real-time analysis of security alerts gen erated by applications and network hardware. the basic principle of all siems is to aggregate relevant data (i.e, logs) originated from multiple sources in a centralized basis, identify deviations from the norm and take the appropriate actions. the aim of this dissertation is to implement an effective siem solution that is on-premises (i.e, that runs locally in the organization’s own computing resources). the solution should be able to perform incident management, event correlation and generate alerts."
    ],
    0.0
  ],
  [
    [
      "a resolução alternativa de conflitos, visa, por um lado, promover o acesso à justiça, sendo um meio alternativo à resolução de conflitos judiciais, neste caso recorre-se aos tribunais arbitrais e julgados de paz. por outro lado, visa, também, apoiar a criação e o funcionamento de meios extrajudiciais para resolução alternativa de conflitos, incluindo a mediação, negociação e arbitragem. no de direito, especificamente na área do direito do trabalho, a utilização de métodos de resolução alternativa de conflitos é muito vantajosa. é conhecido o estado actual da justiça em portugal, grande parte dos casos em litígio arrastam-se ao longo de anos nos tribunais, sem existir previsão de resolução. o recurso à resolução de conflitos em linha, visa retirar dos tribunais vários destes casos, tornando assim a resolução do litígio mais célere, bem como menos dispendiosa para ambas as partes. estes métodos de resolução online de conflitos são uma abordagem bastante recente, que se serve da internet e de ferramentas de suporte à decisão. neste tipo de ambientes as partes interagem e expõe os seus pontos de vista, em qualquer momento e em qualquer local, uma vez que estes sistemas estão disponíveis em linha. nesta dissertação procurou-se definir qual a melhor metodologia para o desenvolvimento de uma ontologia base, nesta área do conhecimento. nesse sentido desenvolveu-se uma ontologia e um motor de inferência de conhecimento que actua sobre ela de forma a disponibilizar o conhecimento obtido ao sistema. com a utilização das ontologias aliadas aos sistemas de resolução de conflitos em linha existiram enormes ganhos para a justiça, na medida em que estes processos tendem a ser mais transparentes, mais rápidos e mais justos.",
      "alternative dispute resolution aims to promote access to justice, as an alternative mean to litigation in court. on the other hand, alternative dispute resolution also aims at supporting the establishment and operation of non-judicial means for alternative dispute resolution, including mediation, negotiation and arbitration. in the field of the law, specifically in labor law, alternative dispute resolution is very advantageous. in fact, the current state of the legal field in portugal is well known, in which in most of the cases a dispute drags on for years in court, without prediction on when to end. the use of alternative methods, aims to remove many of the cases from the courts, thus making the resolution of the dispute faster and less expensive for both parties. these methods of online conflict resolution are a fairly recent approach, which uses web services and decision support tools. in such environments the parts interact and expose their points of view, at any time and any place, since these systems are available online. in this work we tried to define the best methodology for the development of an ontology base in this area of knowledge. in this sense developed an ontology and an inference engine of knowledge that acts on it in order to provide the knowledge gained to the system. with the combined use of ontologies to online systems of conflict resolution there are huge gains to justice, to the extent that these processes tend to be more transparent, faster and fairer"
    ],
    [
      "o estudo do fenómeno de cross-talking entre p. aeruginosa e c. albicans tem sido focado essencialmente nos processos de quorum sensing e formação de biofilmes, identificando-se os genes e proteínas envolvidas. contudo, mesmo já existindo um grande conhecimento dos genes e proteínas envolvidas, ainda existe uma lacuna na integração dos mesmos nas redes biológicas dos respectivos microorganismos. o número e qualidade das redes biológicas existentes (transcriptional regulatory network - trn and protein-protein interactions - ppi) para os fenómenos chave tais como, quorum sensing, formação de biofilmes, resistência a antibióticos e patogenecidade, não evidenciam a importância de alguns destes genes no fenómeno de crosstalking. esta tese apresenta-se como a primeira tentativa de colocar em evidência os genes e proteínas envolvidos em cross-talking, associando-os aos parceiros de interacção nas respectivas redes biológicas de cada microorganismo. primeiramente, utilizou-se um processo de integração de redes para os dois microrganismos, levando ao aumento do conhecimento geral sobre os processos de patogenecidade dos dois microorganismos, e por fim os genes envolvidos em cross-talking, identificados na literatura, foram evidenciados nestas redes integradas. com esta tese pretende-se dar algumas pistas sobre como é que estes genes de cross-talking estão envolvidos em importantes processos biológicos e também de algum modo apontar novos potenciais drug targets.",
      "the study of the cross-talking phenomenon between p. aeruginosa and c. albicans has been focus on the processes of quorum sensing and biofilm formation, identifying the genes and proteins involved in this relationship. although, there is a present knowledge on the genes and proteins involved, there is still a lack in the understanding on how these genes are integrated into biological regulatory networks of the respective microorganisms. the number and quality of the existing biological networks (i.e. transcriptional regulatory network - trn and protein-protein interactions - ppi) for key phenomena, such as quorum sensing, biofilm formation, antibiotic resistance and pathogenesis does not highlights the importance of some genes and proteins in the cross-talking phenomenon. in this thesis, constitutes the first attempt to put in evidence the genes involved in cross-talking, associating them to the interacting partners within the biological networks of the two microorganisms. first, the two microorganisms passed through a process of network integration, leading to an augmentation in the general knowledge on pathogenic processes, and then over those networks the cross-talking genes identified in literature were highlighted. with this thesis we expect to give some new perspectives on how these cross-talking genes are interconnected to important biological processes of the two microorganisms and to point some new potential drug targets."
    ],
    0.0
  ],
  [
    [
      "em contexto hospitalar, elaborar soluções interoperáveis remete para o conceito de sistemas de informação (si) e a forma como estes são capazes de cooperar na partilha de informação. aqueles que apresentam maior afinidade com a informática médica são naturalmente os sistemas de informação em saúde (sis), que constituem um dos pontos mais importantes e mais sensíveis no que à prestação de cuidados de saúde diz respeito. a evolução desde o armazenamento de dados no formato de papel (paper-based) até à utilização de sistemas informáticos apresenta inúmeras vantagens. a maior rapidez no acesso a dados, a redução de erros (consulta, processamento e apresentação) e o diminuição de perdas de informação encabeçam sem dúvida uma lista bastante extensa de pontos positivos. em contraponto, na altura em que muitos dos sistemas existentes foram implementados, o conceito da partilha de informação não era equacionado, resultando na criação de “ilhas de informação” sem qualquer comunicação com o exterior. com a evolução da internet e da capacidade de processamento dos sistemas informáticos, a partilha de dados entre sistemas tornou-se uma constante, sendo vital a questão da interoperabilidade. existem no entanto barreiras à implementação completa dessa ideologia motivado por divergências em formato de dados ou compatibilidade entre sistemas. embora muito já tenha sido estudado e implementado, existem ainda no serviço nacional de saúde (sns) lacunas que só soluções propostas de raiz em prol da interoperabilidade semântica podem corrigir. o setor da saúde nestes tempos de mudança deve aproveitar exemplos provenientes de outras áreas da economia, como por exemplo, a produção em larga escala onde uma nova revolução está em marcha, a revolução industrial 4.0. através da introdução do conceito de indústria 4.0 desenvolveram-se cyber-physical system (cps), onde dados relativos ao estado de uma máquina ou um processo são transmitidos pela rede com destino a unidades centrais de processamento. estas, através de processos de extract, load, transform (etl), transformam dados em informação útil acerca do estado da máquina, de uma linha de produção ou de toda ama unidade fabril, fazendo assim a ponte entre o mundo real e o virtual. com esses dados recolhidos em tempo real, as ditas unidades centrais criam as bases para sistemas de monitorização, geralmente sob a forma de aplicações web, permitem uma tomada de decisão mais informada por parte dos utilizadores envolvidos. os dispositivos móveis são hoje em dia um filão de negócio a ser explorado pelas unidades de saúde numa tentativa de aproximação ao utente e ao mesmo tempo baixando custos de comunicação, fomentando assim uma relação de confiança entre ambos. no desenvolvimento de uma aplicação móvel é percetível a forma de como a interoperabilidade é vital em sistemas distribuídos. a interoperabilidade física é garantida com utilização de internet e pedidos hypertext transfer protocol (http), a interoperabilidade sintática tira partido de estruturas como o javascript object notation (json) ou extensible markup language (xml) para envio de dados na resposta aos diversos pedidos. por último, a interoperabilidade semântica faz com que diferentes layouts sejam apresentados pela aplicação consoante os dados recebidos. ao longo do presente projeto, desenvolvido no âmbito da dissertação de mestrado em engenharia biomédica, ramo de informática médica foi primeiro realizado um levantamento de requisitos com base no estado atual dos si no centro hospitalar do porto (chp) e mediante os resultados, foram desenvolvidos casos de estudo de forma a perceber qual o possível impacto das soluções encontradas. quatro possíveis áreas de intervenção foram identificadas, a troca de informação seguindo a estrutura proposta pela health level seven internation na versão 2 relativa à troca de mensagens em contexto hospitalar, sistema de monitorização de agentes implementados na unidade de saúde (hl7 e aida), formulário de codificação international classification diseases 9 revision clinical modification (icd-9-cm) para posterior integração num sistema grupo de diagnósticos homogêneos (gdh) e por fim a comunicação com o utente quando este está fora da unidade de saúde como forma de melhorar a imagem da unidade de saúde. como referido, utilizando as diretrizes health level seven international para troca de mensagens na versão 2, foi desenvolvida uma interface do tipo interface engine na linguagem python, que sustenta um “multi-agent system” (mas) para envio e receção de mensagens. aproveitando as diretrizes da indústria 4.0, ´e fundamental que estes agentes inteligentes comuniquem a cada instante o seu estado de forma a que os responsáveis conseguirem um feedback do estado atual de todo o sistema. no segundo caso de estudo foi desenvolvida uma plataforma de monitorização com vista a rastrear o estado de agentes hl7 e aida j´a implementados no chp e ainda daqueles desenvolvidos no caso de estudo anterior. primeiro foi implementado um processo de etl que alimenta um datawarehouse (dw) onde a aplicação web faz queries para recolher dados. toda a plataforma foi desenvolvida, com base no conceito mvc com a framework angularjs. o terceiro caso de estudo apresenta uma plataforma de codificação icd-9-cm numa abordagem à interoperabilidade semântica relacionada com representação de diagnósticos e procedimentos clínicos. utilizando também o conceito mvc em angularjs, a plataforma está inserida num sistema de gdh com objetivo de facilitar o trabalho do codificador, aumentar a sua produtividade, servir de barómetro ao trabalho desenvolvido e ainda categorizar os resumo de altas hospitalares. com o gdh pode num futuro ser definido todo o financiamento hospitalar ou até compras realizadas por cada unidade. no último caso de estudo, desenvolveu-se uma aplicação android do enquadrada no conceito de electronic health (ehealth) do tipo appointment reminder. o objetivo é aproximar o paciente da unidade de saúde e evitando que este falhe o seu agendamento poupando em última instância dinheiro e recursos às unidades de saúde. utilizou-se o design proposto pela google para o envio de notificações o serviço firebase cloud message também ele proposto pela google. mais que um lembrete a aplicação regista consultas passadas, futuras e permite fazer a confirmação da consulta. este caso ainda se trata de um protótipo inicial para aceitação por parte da unidade de saúde. como métodologia de desenvolvimento em todos os casos de estudo, utiliza-se o ciclodesign research. primeiro é definido um requisito/problema que se pretende ver resolvido e todas as fases que se sucedem visam encontrar uma possível solução para tal. como prova de conceito, escolheu-se a análise swot, situando-se esta, na fase de conclusão do ciclo ds. todos os casos de estudo foram submetidos a essa análise apresentado resultados considerados positivos. fica assim vincada a importância da interoperabilidade no desenvolvimento de sistemas interoperáveis, sendo um conceito transversal a todas as soluções propostas, quer na fomentação direta da interoperabilidade ou no seu aproveitamento para a disseminação de informação. no primeiro caso, um sistema que favorece diretamente a partilha de informação utilizando standards de comunicação. no segundo, o controlo de agentes que trabalham diretamente na partilha de informação garantindo a sua segurança e rápida atuação em caso de erro. no terceiro caso, uma solução diretamente ligada à interoperabilidade semântica e por último uma aplicação que necessita de interoperabilidade para funcionar na sua plenitude.",
      "in order to develop interoperable solutions, the information systems and information exchange among them are two of the basic concepts. information systems and for the purpose of this work health information systems are without questions one of the key factors in the healthcare nowadays. the evolution from paper-based information storage to a computerized automated process presents numerous advantages like fastest information retrieval and less errors but there is a downside. the development of department or unit focused information system led to the creation and isolation of “islands of information”. at the time the concept of information charing was not on the agenda. the evolution of internet and technology motivated the data charing across information systems and organizations. the interoperability is then a key factor under this new scenario. although a large number of research and implementation has already occurred, there are still gaps. one the portuguese scenario the semantic interoperability presents a tricky question that only specialized solution can solve. the health care should look at examples from other fields, like manufacturing to seek for solutions. the industry 4.0 implements “cyber-physical systems” to get an live feed of the machine and production status. the data is provided by sensors or intelligent agent systems that extract the current status of the machine and transmit to a central unit. the mobile industry presents nowadays a business opportunity for the healthcare sector through development of dedicated applications dedicated to approximate the patient and health care unit and reducing the costs in communication. even at this point interoperability is vital. physical interoperability through the establishment of communications using internet and hypertext transfer protocol (http) requests. the syntactical interoperability with javascript object notation (json) format for information exchange, and at last the semantic interoperability for different displays with basis on different information. the present work was developed under the scenario of the master thesis in biomedical engineering medical informatics branch. intends then to present solutions in the format of study cases. were purposed 4 solutions. the first related with the information sharing standards through the usage of health level seven message standards on the v2. format. the implementation an hl7 interface engine mixed with intelligence agents is a powerful tool to accomplish reliability and fast information exchange. the agents were implemented using python 3.4 programming language. the second case study, presents a monitoring system for hl7 and aida agents already working at centro hospitalar do porto (chp) and the new agents from the system presented at case study number one. the main function of this platform is present to the user the information processed in a extract, load, transform (etl) process implement in python 3.4 programming language. information like errors, agents beginning date and number of exchanged messages are present in a mvc model web application, developed using angularjs framework. the third case study was a specific need of the chp. the related diseases groups have been working fundamental four hospital ranking and money distribution for many years. this systems require the specific usage of international classification diseases 9 revision clinical modification (icd-9-cm) codification of medical release resume. so a mvc single page application was developed using angularjs framework. the purpose is increase the number of codifications, evaluate the performance of the encoder and the health unit. the last case study, presents a android mobile application introduced to save money to chp in communication with patients, increase the health facility public image and prevents patients from not attend schedules. the application was developed using google guidelines and to send notification the firebase cloud message systems was adopted. the application meets the standards of electronic health (ehealth) mainly appointment remainders. as development methodology, the design research cycle presents a good method to solve the requisites or problems identified previously. the last step settles if another dr is needed or if the right solution has been found. as proof-of-concept the swot analysis is a good and cheap solution for the purposed of the project. all the purposed solutions have a common point, the interoperability. the first case enables directly the syntactical interoperability. the second, supports and helps identifying errors in agents working directly with information exchange. the third case represents a direct approach to the semantic interoperability. the fourth, repesents a information system that depends directly of interoperability to work properly."
    ],
    [
      "o consumo crescente do petróleo constitui um grave problema ambiental e económico para a sociedade atual. para solucionar este problema, o uso de biocombustíveis apresenta-se como uma viável alternativa. através do aproveitamento da cana-de-açúcar e do milho, utilizando microrganismos, é possível obter etanol, o biocombustível mais utilizado atualmente. porém, nem todos os açúcares provenientes destas matérias-primas são fermentados de forma economicamente eficiente, sendo que a d-xilose, o segundo açúcar mais abundante, continua a ter um aproveitamento ineficiente por parte de microrganismos como a saccharomyce cerevisiae, apesar das alternativas já exploradas. a via de fermentação da xilose é constituída por diversas enzimas, sendo que o problema da fraca taxa de conversão de xilose em etanol se deva essencialmente às primeiras duas: a xilose redutase e a xilitol desidrogenase. muitas das abordagens para solucionar o ineficiente consumo da d-xilose têm por base a engenharia metabólica, ou como tornar a xilose redutase ou a xilitol desidrogenase específica para o mesmo cofator. este trabalho tem o objetivo de descobrir caraterísticas estruturais, na xilose redutase e na xilitol desidrogenase, que possam ter influência na afinidade da d-xilose e no seu modo de ligação. para tal foram usadas e testadas abordagens in silico tendo em consideração a estrutura das proteínas e os açúcares que estas utilizam como substrato, deixando para segundo plano os cofatores e o metabolismo. foram recolhidas diversas sequências de xilose redutases e xilitol desidrogenases de diversos organismos e as suas estruturas tridimensionais modeladas. para esses modelos foi medido o volume dos seus centros ativos e realizado o docking molecular da d-xilose, xilitol e outros substratos. os resultados do docking e dos volumes foram comparados com os km dos diferentes substratos. praticamente todos os volumes da xilose redutase foram corretamente medidos, o que não se verificou no caso da xilitol desidrogenase. relativamente ao docking molecular, foram analisados os resíduos envolvidos nos processos catalíticos destas duas enzimas bem como os scores resultantes. a candida tenuis e candida boidiini foram as que apresentaram uma maior afinidade no docking da xilose com um valor de 5,1. no docking do xilitol nas xilitol desidrogenases a rhizomucor pusillu foi a que teve melhor score com 5,1. este trabalho evidenciou que os volumes do centro ativo e o km aparentam não estar diretamente relacionados. os resíduos n e h do centro catalítico da xilose redutase estão conservados em todas as xilose redutases e estão envolvidos nas interações polares com a xilose e outros açúcares.",
      "the growing consumption of oil is a serious environmental and economic problem for today's society. to solve this problem, the use of biofuels presents itself as a viable alternative. through the use of sugar cane and corn, using microorganisms, it is possible to obtain ethanol, the most commonly used biofuel. however, not all sugars from these raw materials are fermented in an economically efficient manner, and d-xylose, the second most abundant sugar, continues to be inefficiently utilized by microorganisms such as saccharomyces cerevisiae, despite of the alternatives already available and explored. the xylose fermentation path consists of several enzymes, and the problem of the low conversion rate of xylose into ethanol is due to the first two: xylose reductase and xylitol dehydrogenase. the approaches to solving the inefficient consumption of d-xylose are mainly based on metabolic engineering, such asturning xylose reductase or xylitol dehydrogenase specific for the same cofactor. in this work , in order to discover structural features in xylose reductase and xylitol dehydrogenase that may influence the affinity of d-xylose and its mode of binding, in silico approaches were used and tested taking into account the structure of proteins and sugars which they use as a substrate, leaving the cofactors and the metabolism behind. several xylose reductase and xylitol dehydrogenase sequences from various organisms were collected and their tridimensional structure modelled. for these models the volume of its active centers was measured and the molecular docking of d-xylose, xylitol and other substrates was performed. the results of the docking and the volumes were compared with the km of the different substrates. almost all volumes of xylose reductase were correctly measured, which was not observed in the case of xylitol dehydrogenase. regarding molecular docking, the residues involved in the catalytic processes of these two enzymes as well as the resulting scores were analyzed. candida tenuis and candida boidiini had the highest affinity for xylose docking with a value of 5.1. in the docking of xylitol in xylitol dehydrogenases rhizomucor pusillus was the one that had the best score with. this work evidenced that the volumes of the active center and the km are not directly related. the n and h residues of the catalytic center of xylose reductase are conserved in all xylose reductases and are involved in polar interactions with xylose and other sugars."
    ],
    0.03
  ],
  [
    [
      "information systems are continuously evolving in nature and complexity. infrastructure concerns such as availability, efficiency, and disaster recovery have been some of the most important drivers regarding how infrastructure is planned and executed. as these mechanisms evolved, so did the underlying foundation for their functioning — information technology (it) infrastructure monitoring. inside the healthcare environment, it is important to discuss it infrastructure monitoring and disaster prevention and recovery since availability and communication are vital for the proper functioning of healthcare units, whether acting in isolation or on a network. when acting on a network, it is especially important to be able to easily monitor and observe each unit from a single point of access so that actions can be swiftly taken when there is a problem. considering the wide range of available solutions and heterogeneous nature of it infrastructure, even within the healthcare industry, the majority of the solutions either focus too much on a particular problem of some industry, healthcare or not, or are too generic and can’t fulfill the needs of an increasingly connected and interdependent healthcare industry. this dissertation proposes a web and microservices-based it infrastructure monitoring backend solution with a multi-site and multi-organization scheme at its core that is designed to be scalable, easily deployed and integrated with existing tools, and simple to further extend and improve. this solution has two main components, one server which is the central point of the solution, the guardian server, and the other one, which is the local client to be installed on each organization’s infrastructure. the produced backend solution was tested and validated in two healthcare organizations which provided useful feedback and a positive answer to the usefulness of a monitoring solution, such as the one developed in this dissertation, in improving the efficiency and reliability of the organizations’ it infrastructure and, therefore, their healthcare services. a formal evaluation of the solution was also carried out with a combination of a strengths, weaknesses, opportunities and threats (swot) analysis and a risk assessment report, both mechanisms providing useful insights on the strengths and limitations of the solution, as well as possible improvement points.",
      "os sistemas de informação estão em constante evolução tanto em índole, como complexidade. questões como disponibilidade, eficiência e recuperação de falhas têm sido alguns dos fatores mais importantes no que diz respeito ao planeamento e execução de infraestruturas. à medida que esses mecanismos evoluíram, o mesmo aconteceu com a base subjacente para o seu funcionamento — a monitorização de infraestruturas de tecnologia de informação (ti). o ambiente hospitalar é particularmente relevante quando se discute monitorização de infraestruturas de ti e a prevenção e recuperação de desastres, uma vez que a disponibilidade e a comunicação com terceiros são vitais para o bom funcionamento das unidades de cuidados de saúde, quer estas atuem isoladamente ou em rede. ao atuar em rede, é especialmente importante ser capaz de facilmente monitorizar e observar cada unidade de saúde a partir de um único ponto de acesso, de modo a que, quando houver um problema, se possa agir de forma rápida e integrada. tendo em conta a vasta gama de soluções de monitorização disponíveis e a heterogeneidade das infraestruturas de ti, a maioria das soluções ou se concentra demasiado num problema específico de uma dada indústria ou setor, de cuidados de saúde ou não, ou é demasiado genérica e não consegue satisfazer as necessidades de uma indústria de cuidados de saúde cada vez mais conexa e interdependente. esta dissertação propõe uma solução backend de monitorização de infraestruturas de ti baseada em microserviços web, com um esquema multi-local e multi-organização na sua fundação, que foi concebida para ser escalável, facilmente instalada e integrada com ferramentas já existentes, e simples de expandir e melhorar. esta solução tem dois principais componentes, um servidor que é o ponto central da solução, o guardian server, e o outro que é o cliente local a ser instalado na infraestrutura de cada organização. a solução backend produzida foi testada e validada em duas organizações de cuidados de saúde que forneceram opiniões úteis e construtivas, bem como uma resposta positiva à utilidade de uma solução de monitorização, como a desenvolvida nesta dissertação, para melhorar a eficiência e fiabilidade da infraestrutura de ti das organizações e, consequentemente, dos cuidados de saúde que estas prestam. foi também realizada uma avaliação formal da solução através da combinação de uma análise swot e de um relatório de avaliação de risco, em que ambos forneceram informação útil sobre os pontos fortes e limitações da solução, bem como possíveis pontos de melhoria."
    ],
    [
      "the process of automatic speech recognition (asr) opens doors to a vast amount of possible improvements in customer experience. the use of this type of technology has increased significantly in recent years, this change being the result of the recent evolution in asr systems. the opportunities to use asr are vast, covering several areas, such as medical, industrial, business, among others. we must emphasize the use of these voice recognition systems in telecommunications companies, namely, in the automation of consumer assistance operators, allowing the service to be routed to specialized operators automatically through the detection of matters to be dealt with through recognition of the spoken utterances. in recent years, we have seen big technological breakthrough in asr, achieving unprecedented accuracy results that are comparable to humans. we are also seeing a move from what is known as the traditional approach of asr systems, based on hidden markov models (hmm), to the newer end-to-end asr systems that obtain benefits from the use of deep neural networks (dnns), large amounts of data and process parallelization. the literature review showed us that the focus of this previous work was almost exclusively for the english and chinese languages, with little effort being made in the development of other languages, as it is the case with portuguese. in the research carried out, we did not find a model for the european portuguese (ep) dialect that is freely available for general use. focused on this problem, this work describes the development of a end-to-end asr system for ep. to achieve this goal, a set of procedures was followed that allowed us to present the concepts, characteristics and all the steps inherent to the construction of these types of systems. furthermore, since the transcribed speech needed to accomplish our goal is very limited for ep, we also describe the process of collecting and formatting data from a variety of different sources, most of them freely available to the public. to further try and improve our results, a variety of different data augmentation techniques were implemented and tested. the obtained models are based on a pytorch implementation of the deep speech 2 model. our best model achieved an word error rate (wer) of 40.5%, in our main test corpus, achieving slightly better results to those obtained by commercial systems on the same data. around 150 hours of transcribed ep was collected, so that it can be used to train other asr systems or models in different areas of investigation. we gathered a series of interesting results on the use of different batch size values as well as the improvements provided by the use of a large variety of data augmentation techniques. nevertheless, the asr theme is vast and there is still a variety of different methods and interesting concepts that we could research in order to seek an improvement of the achieved results.",
      "o processo de reconhecimento automático de fala (asr) abre portas para uma grande quantidade de melhorias possíveis na experiência do cliente. a utilização deste tipo de tecnologia tem aumentado significativamente nos últimos anos, sendo esta alteração o resultado da evolução recente dos sistemas asr. as oportunidades de utilização do asr são vastas, abrangendo diversas áreas, como médica, industrial, empresarial, entre outras. é de realçar que a utilização destes sistemas de reconhecimento de voz nas empresas de telecomunicações, nomeadamente, na automatização dos operadores de atendimento ao consumidor, permite o encaminhamento automático do serviço para operadores especializados através da detecção de assuntos a tratar através do reconhecimento de voz. nos últimos anos, vimos um grande avanço tecnológico em asr, alcançando resultados de precisão sem precedentes que são comparáveis aos atingidos por humanos. por outro lado, vemos também uma mudança do que é conhecido como a abordagem tradicional, baseados em modelos de markov ocultos (hmm), para sistemas mais recentes ponta-a-ponta que reúnem benefícios do uso de redes neurais profundas, em grandes quantidades de dados e da paralelização de processos. a revisão da literatura efetuada mostra que o foco do trabalho anterior foi quase que exclusivamente para as línguas inglesa e chinesa, com pouco esforço no desenvolvimento de outras línguas, como é o caso do português. na pesquisa realizada, não encontramos um modelo para o dialeto português europeu (pe) que se encontre disponível gratuitamente para uso geral. focado neste problema, este trabalho descreve o desenvolvimento de um sistema de asr ponta-a-ponta para o pe. para atingir este objetivo, foi seguido um conjunto de procedimentos que nos permitiram apresentar os conceitos, características e todas as etapas inerentes à construção destes tipos de sistemas. além disso, como a fala transcrita necessária para cumprir o nosso objetivo é muito limitada para pe, também descrevemos o processo de coleta e formatação desses dados em uma variedade de fontes diferentes, a maioria delas disponíveis gratuitamente ao público. para tentar melhorar os nossos resultados, uma variedade de diferentes técnicas de aumento de dados foram implementadas e testadas. os modelos obtidos são baseados numa implementação pytorch do modelo deep speech 2. o nosso melhor modelo obteve uma taxa de erro de palavras (wer) de 40,5% no nosso corpus de teste principal, obtendo resultados ligeiramente melhores do que aqueles obtidos por sistemas comerciais sobre os mesmos dados. foram coletadas cerca de 150 horas de pe transcritas, que podem ser utilizadas para treinar outros sistemas ou modelos de asr em diferentes áreas de investigação. reunimos uma série de resultados interessantes sobre o uso de diferentes valores de batch size, bem como as melhorias fornecidas pelo uso de uma grande variedade de técnicas de data augmentation. o tema asr é vasto e ainda existe uma grande variedade de métodos diferentes e conceitos interessantes que podemos investigar para melhorar os resultados alcançados."
    ],
    0.3
  ],
  [
    [
      "there has been an increasing investment in cancer research that generated an enormous amount of biological and clinical data, especially after the advent of the next-generation sequencing technologies. to analyze the large datasets provided by omics data of cancer samples, scientists have successfully been recurring to machine learning algorithms, identifying patterns and developing models by using statistical techniques to make accurate predictions. deep learning is a branch of machine learning, best known by its applications in artificial intelligence (computer vision, speech recognition, natural language processing and robotics). in general, deep learning models differ from machine learning “shallow” methods (single hidden layer) because they recur to multiple layers of abstraction. in this way, it is possible to learn high level features and complex relations in the given data. given the context specified above, the main target of this work is the development and evaluation of deep learning methods for the analysis of cancer omics datasets, covering both unsupervised methods for feature generation from different types of data, and supervised methods to address cancer diagnostics and prognostic predictions. we worked with a neuroblastoma (nb) dataset from two different platforms (rna-seq and microarrays) and developed both supervised (deep neural networks (dnn), multi-task deep neural network (mt-dnn)) and unsupervised (stacked denoising autoencoders (sda)) deep architectures, and compared them with shallow traditional algorithms. overall we achieved promising results with deep learning on both platforms, meaning that it is possible to retrieve the advantages of deep learning models on cancer omics data. at the same time we faced some difficulties related to the complexity and computational power requirements, as well as the lack of samples to truly benefit from the deep architectures. there was generated code that can be applied to other datasets, wich is available in a github repository https://github.com/lmpeixoto/deepl_learning [49].",
      "nos últimos anos tem havido um investimento significativo na pesquisa de cancro, o que gerou uma quantidade enorme de dados biológicos e clínicos, especialmente após o aparecimento das tecnologias de sequenciação denominadas de “próxima-geração”. para analisar estes dados, a comunidade científica tem recorrido, e com sucesso, a algoritmos de aprendizado de máquina, identificando padrões e desenvolvendo modelos com recurso a métodos estatísticos. com estes modelos é possível fazer previsão de resultados. o aprendizado profundo, um ramo do aprendizado de máquina, tem sido mais notório pelas suas aplicações em inteligência artificial (reconhecimento de imagens e voz, processamento de linguagem natural e robótica). de um modo geral, os modelos de aprendizado profundo diferem dos métodos clássicos do aprendizado de máquina por recorrerem a várias camadas de abstração. desta forma, é possível “aprender” as representações complexas e não lineares, com vários graus de liberdade dos dados analisados. neste contexto, o objetivo principal deste trabalho é desenvolver e avaliar métodos de aprendizado profundo para analisar dados ómicos do cancro. pretendem-se desenvolver tanto métodos supervisionados como não-supervisionados e utilizar diferentes tipos de dados, construindo soluções para diagnóstico e prognóstico do cancro. para isso trabalhámos com uma matriz de dados de neuroblastoma, proveniente de duas plataformas diferentes (rna-seq e microarrays), nos quais aplicámos algumas arquiteturas de aprendizado profundo, tanto como métodos supervisionados e não-supervisionados, e com as quais comparámos com algoritmos tradicionais de aprendizado de máquina. no geral conseguimos obter resultados promissores nas duas plataformas, o que significou ser possível beneficiar das vantagens dos modelos do aprendizado profundo nos dados ómicos de cancro. ao mesmo tempo encontrámos algumas dificuldades, de modo especial relacionadas com a complexidade dos modelos e o poder computacional exigido, bem como o baixo número de amostras disponíveis. na sequencia deste trabalho foi gerado código que pode ser aplicado a outros dados e está disponível num repositório do github https://github.com/lmpeixoto/deepl_learning [49]."
    ],
    [
      "in the last years, the number of machine learning algorithms and their parameters has increased significantly. this allows for more accurate models to be found, but it also increases the complexity of the task of training a model, as the search space expands significantly. as datasets keep growing in size, traditional approaches based on extensive search start to become costly in terms of computational resources and time, especially in data streaming scenarios. with this growth, new challenges in machine learning started to appear. the speed at which data arrives and different ways of storing data are forcing organizations to address and explore new ways of adapting fast enough so their ml models don’t become obsolete. this dissertation aims to develop an approach based on meta-learning that tackles two main challenges: predict ing the performance metrics of a future model and recommending the best algorithm/configuration for training a model for a specific machine learning problem. throughout this dissertation, all the study objectives and questions, along with the relevant contextualization will be exposed. the proposed solution, when compared to an automl approach is up to 130x faster and only 2% worse in terms of average model quality, showing it is a good solution for scenarios in which models need to be updated regularly, such as in streaming scenarios with big data, in which some accuracy can be traded for a much shorter model training time.",
      "nos últimos anos, o número de algoritmos de machine learning e seus parâmetros aumentou significativamente. isso permite que modelos mais precisos sejam encontrados, mas também aumenta a complexidade da tarefa de treinar um modelo, pois o espaço de busca expande-se significativamente. à medida que os conjuntos de dados continuam a crescer em tamanho, abordagens tradicionais baseadas em uma pesquisa extensiva começam a se tornar caras em termos de recursos computacionais e tempo, especialmente em cenários de streaming de dados. com esse crescimento, novos desafios no machine learning começaram a aparecer. a velocidade com que os dados chegam e as diferentes maneiras de armazenar dados estão a forçar as organizações a abordar e explorar novas formas de se adaptar rápido o suficiente para que os seus modelos de ml não se tornem obsoletos. esta dissertação visa desenvolver uma abordagem baseada em meta-learning que aborda dois desafios principais: prever as métricas de desempenho de um modelo futuro e recomendar o melhor algoritmo/configuração para treinar um modelo para um problema específico de machine learning. ao longo desta dissertação, serão expostos todos os objetivos e questões do estudo, juntamente com a contextualização relevante. a solução proposta, quando comparada a uma abordagem automl é até 130x mais rápida e apenas 2% pior em termos de qualidade média do modelo, mostrando que é uma boa solução para cenários em que os modelos precisam ser atualizados regularmente, como em cenários de streaming com big data, em que alguma precisão pode ser negociada por um tempo de treino de modelo muito menor."
    ],
    0.3
  ],
  [
    [
      "this document describes the work done to understand the impact of implementing a type annotation algorithm plugged into the static application security testing (sast) engine used by checkmarx. the main goal is to upgrade the checkmarx’s sast tool allowing the execution of vulnerability detection taking into account expression types. this means that every result of an expression needs to have a specific type assigned accordingly to the kind of operation and to the different operand types. along with the assigned type, information on the size of the variable and signedness should be available. as a case study, the c/c++ programming language was used. then an extension to deal with expansion to the c# language was developed and a study of the implications of expanding the system to multiple languages was carried out. a proof of concept was implemented, using a language agnostic approach, that allows for expressions type annotation previous to the analysis of code vulnerabilities. that tool developed supports languages c, c++ and c# and the annotation of their base types. it was possible to re implement some of the vulnerability analysis queries previously created and in use at checkmax so that they use the implemented expression types. this led to a significant increase in readability of the queries, an increase in the amount of vulnerability results found by those queries as well as a minimum efficiency increase.",
      "este documento descreve o trabalho realizado para compreender o impacto da implemen tação de um algoritmo de anotação de tipos integrado ao mecanismo de teste de segurança de aplicativos estáticos (sast) utilizado pela checkmarx. o principal objetivo é aprimorar a ferramenta sast da checkmarx, permitindo a detecção de vulnerabilidades levando em consideração os tipos de expressões. isso significa que cada resultado de uma expressão deve ter um tipo específico atribuído de acordo com o tipo de operação e os diferentes tipos de operandos. juntamente com o tipo atribuído, informações sobre o tamanho da variável e a sinalização devem estar disponíveis. como caso de estudo, foi utilizada a linguagem de programação c/c++. em seguida, foi desenvolvida uma extensão para lidar com a expansão para a linguagem c# e foi realizado um estudo das implicações da expansão do sistema para várias linguagens. foi implementada uma prova de conceito, usando uma abordagem agnóstica à linguagem, que permite a anotação de tipos de expressões antes da análise das vulnerabilidades do código. a ferramenta desenvolvida suporta as linguagens c, c++ e c# e a anotação dos seus tipos base. foi possível reimplementar algumas das queries de análise de vulnerabilidades previamente criadas e usadas na checkmarx, para que utilizem os tipos de expressões implementados. isso resultou num aumento significativo na legibilidade das queries, um aumento na quantidade de resultados de vulnerabilidades encontrados por essas queries, bem como um aumento mínimo na eficiência."
    ],
    [
      "one of the reasons for the increased number of visits to emergency departments is the primary health care inability to handle urgent needs and provide all the health services needed to assess complex conditions. a significant amount of these visits are due to the abnormal flow of patients whose clinical condition is of low severity and could ideally be resolved with self-care and primary health care. the crowding in emergency departments causes operational and logistical problems and has undesirable consequences for patients, health professionals and hospitals. delays in treatment interventions and increased mortality, medical errors and waiting times are just a phew examples of critical consequences that can occur, resulting in a significant barrier to the quality of health care delivery. with the advances in technology, several institutions have found in self-service an alternative for the patient’s collection of health information autonomously. these devices can be used by low clinical severity patients (with the blue, green or yellow bracelets from manchester triage) to reduce waiting time in the emergency departments. this dissertation proposes a technological solution to improve both the time and quality of the anamnesis procedure performed by medical staff in the emergency department. the introduction of a self-service kiosk in the emergency department waiting room will make it possible to quickly and intuitively collect the patient’s past medical history, usual medication, main complaint symptoms and vital signs. subsequently, this data will be made available to the physician before each clinical observation. the hypothesis considered is that by providing a selective, structured and uniform anamnesis information’s presentation of each patient, medical staff observation can proceed much faster and accurately, focusing on the confirmation of the most relevant aspects. the primary purpose of this solution is to reduce the period of clinical observation and thus improve the response capacity of the emergency department with the same resources.",
      "uma das razões para o aumento do número de visitas ao serviço de urgência é a incapacidade dos cuidados de saúde primários de lidar com necessidades urgentes e de fornecer todos os serviços de saúde necessários para avaliar condições complexas. a maioria destas visitas deve-se ao fluxo anormal de doentes cuja condição clínica é de baixa gravidade que poderiam, idealmente, ser resolvidos com recurso ao auto-cuidado e aos cuidados de saúde primários. a lotação nos serviços de urgência provoca problemas operacionais e logísticos, apresentando consequências indesejáveis para os doentes, profissionais de saúde e hospitais. atrasos nas intervenções de tratamento e o aumento da mortalidade, dos erros médicos e dos tempos de espera são apenas alguns exemplos de consequências críticas que podem ocorrer, resultando numa barreira significativa a qualidade da prestação de cuidados de saúde. com os avanços da tecnologia, diversas instituições, encontraram nos serviços de auto-atendimento uma alternativa para a recolha autónoma de informações de saúde do doente. estes dispositivos, poderão ser usados por doentes de baixa gravidade clínica (com pulseira azul, verde ou amarela da triagem de manchester) nos serviços de urgência com vista à redução do tempo de espera. esta dissertação propõe uma solução tecnológica para melhorar tanto o tempo como a qualidade do procedimento de anamnese realizado pelos médicos no serviço de urgência. a introdução de um quiosque de auto-atendimento na sala de espera do serviço de urgência permitirá recolher de forma rápida e intuitiva a história clínica, medicação habitual, sintomas da queixa principal e sinais vitais do doente. posteriormente estes dados serão colocados à disposição do médico antes de cada observação clínica. a hipótese considerada é que ao fornecer uma apresentação seletiva, estruturada e uniforme da informação de anamnese de cada doente, a observação dos médicos possa proceder de forma muito mais rápida e precisa, concentrando-se na confirmação dos aspectos mais relevantes. o principal objectivo desta solução é reduzir o período de observação clínica e assim melhorar a capacidade de resposta do serviço de urgência com os mesmos recursos."
    ],
    0.3
  ],
  [
    [
      "the effects of the rapid technological revolution occurring in our society are undeniable. in the health area, the quick growth of information technologies has had a particular and striking impact as it has led to an urgent need to improve the health care provided to the population. it is imperative that care delivery becomes an increasingly computerized process in order to facilitate not only the work of all health professionals, but the lives of all users. however, it is necessary that this phenomenon of clinical informatization is evaluated, in order to make it possible to determine the current state of health institutions, as monitored, so that a path of gradual progression can be defined and followed, and internal flows, processes and systems can be improved. in portugal, many initiatives have been implemented, such as the national strategy for the health information ecosystem 2020, in particular the sns sem papel. the objective is to improve access to the national health system and to expedite the sharing of clinical information by eliminating paper in hospital institutions. obstacles and resistance to change can naturally occur as these initiatives are implemented. thus, the emergence of entities such as himss analytics, capable of creating maturity models that provide a clear and concise method, capable of helping institutions to achieve their goals, becomes crucial. within the scope of this dissertation, two maturity models created by this entity, emram and amam, were studied in order to understand their dynamics and scrutinize how they possibilitate the gradual improvement of the analytics of the institutions and the progressive dematerialization of their systems, flows and processes.",
      "os efeitos da célere revolução tecnológica que ocorre na nossa sociedade são inegáveis. na área da saúde, o crescimento acelerado das tecnologias de informação teve um particular e marcante impacto, uma vez que levou a uma necessidade urgente de melhorar os cuidados de saúde fornecidos à população. é imperativo que a prestação de cuidados se tome um processo cada vez mais informatizado, de modo a facilitar não só o trabalho de todos os profissionais de saúde, como a vida de todos os utentes. no entanto, é necessário que este fenómeno de informatização clínica seja, por um lado, avaliado, para que se tome possível a determinação do estado atual das instituições de saúde, como monitorizado, para que se possa definir e percorrer um caminho de gradual progressão e melhoria de fluxos internos, processos e sistemas. em portugal, têm sido implementadas inúmeras iniciativas, como a estratégia nacional para o ecossistema de informação de saúde 2020, em particular o sns sem papel. o objetivo consiste em melhorar o acesso ao sistema nacional de saúde e aperfeiçoar e agilizar a partilha de informação clínica, através da eliminação do papel nas instituições hospitalares. é natural a existência de obstáculos e resistência à mudança no decorrer da implementação destas iniciativas. assim, torna-se crucial o surgimento de entidades como a himss analyties, capazes de criar modelos de maturidade que providenciem um método claro e conciso, apto a auxiliar as instituições a atingir os seus objetivos. no âmbito deste projeto de dissertação, foram estudados dois modelos de maturidade disponibilizados por esta entidade, o emram e o amam, de modo a perceber a sua dinâmica e a escrutinar como tomam possível a gradual melhoria da analítica dos sistemas das instituições e a progressiva desmaterialização dos seus fluxos e processos."
    ],
    [
      "a tecnologia da informação tem avançado significativamente nas últimas décadas; desde o desenvolvimento de hardware, cada vez mais compacto e eficiente, até ao desenvolvimento de aplicações e serviços com impacto mundial. como consequência deste avanço, novos sistemas, mais complexos e sofisticados, tem surgido com o intuito de melhorar serviços já existentes ou criar novos serviços. os sistemas iot são um exemplo deste avanço tecnológico. estes sistemas permitem a monitorização, análise e controlo dos mais diversos ambientes através da recolha e processamento de dados e consequente interação com o meio, por via de sensores e atuadores. a maturação destas tecnologias e a evolução industrial atraiu a atenção de vários países e empresas para o potencial da sua integração nos processos industriais. como tal, várias iniciativas relacionadas com o desenvolvimento e aplicação de sistemas iot e cps surgiram, como por exemplo a iniciativa i4.0. sendo que, uma das áreas mais influenciadas por esta evolução é a indústrias de manufactura. nos últimos anos esta indústria têm vindo a adotar cada vez mais as tecnologias de iot devido às suas vantagens, especialmente na monitorização e controlo das linhas de produção. isto é através da recolha e posterior análise dos dados provenientes das linhas de montagem é possível identificar ineficiências, reduzir os custos de produção, aumentar o nível de controlo, entre outros. neste caso de estudo, a medição do tempo dos estágios da linha de produção é essencial para a avaliação da produtividade da empresa. no entanto, o processo de medição é efetuado manualmente quando necessário, o que torna o processo mais demorado e mais propenso a erros. assim sendo, o objetivo deste trabalho é o desenho, desenvolvimento e análise de um sistema iot que complemente os métodos de monitorização utilizados, a fim de melhorar a qualidade dos dados obtidos desta. para a elaboração da solução, foram, em primeiro lugar, estudadas e comparadas algumas plataformas de desenvolvimento de soluções iot e aplicações de visualização de dados. seguidamente procedeu-se com o desenho e desenvolvimento da solução proposta, tendo em conta a recolha de dados, tanto dos sensores como da infraestrutura preexistente, e posterior análise e apresentação dos dados. por fim, delineou-se um cenário de testes em que se simula o comportamento da linha de produção através de dados históricos do caso de estudo, a fim de testar e validar a solução desenvolvida.",
      "information technology has advanced significantly in recent decades; from the development of hardware, increasingly compact and efficient, to the development of applications and services with worldwide impact. as a consequence of this advance, new systems, more complex and sophisticated, have appeared with the aim of improving existing services or creating new ones. the iot systems are an example of this technological advance. these systems allow the monitoring, analysis and control of the most diverse environments through the collection and processing of data and consequent interaction with the environment, via sensors and actuators. the maturation of these technologies and the industrial evolution attracted the attention of several countries and companies to the potential of their integration in industrial processes. as such, several initiatives related to the development and application of iot and cps systems have emerged, such as the i4.0 initiative. one of the areas most influenced by this evolution is the manufacturing industries. in the last decade, this industry has been increasingly adopting iot technologies due to its advantages, especially in the monitoring and control of production lines. that is, through the collection and subsequent analysis of data from the assembly lines, it is possible to identify inefficiencies, reduce production costs, increase the level of control, among others. in the present study, the measurement of time of the production line stage is essential for the evaluation of the company’s productivity. however, the measurement process is carried out manually when necessary, which makes the process more time consuming and more prone to errors. therefore, the objective of this work is the design, development and analysis of an iot system that complements the monitoring methods used, in order to improve the quality of the data obtained from it. for the elaboration of the solution, first, some platforms for the development of iot solutions and data visualization applications were studied and compared. then, the proposed solution was designed and developed, taking into account the data collection, both from the sensors and from the pre-existing infrastructure, and subsequent analysis and presentation of the data. finally, a test scenario was outlined in which the behavior of the production line is simulated through historical data from the case study, in order to test and validate the developed solution."
    ],
    0.3
  ],
  [
    [
      "desde as últimas décadas que a população mundial tem vindo a aumentar de uma forma exponencial, originando e potenciando vários problemas de difícil resolução como, por exemplo, problemas de trânsito relacionados com o elevado fluxo de veículos, problemas de poluição, de alojamento, de acesso à saúde, de gestão de recursos, entre outros. neste enquadramento surgiu a necessidade de se “virtualizar” as próprias cidades, levando ao conceito de cidades inteligentes (smart cities) com o principal objetivo de criar condições de sustentabilidade nas próprias cidades e disponibilizar um acesso mais flexível a informação útil para os seus cidadãos. o conceito de smart cities é conhecido pelo uso de várias tecnologias para melhorar as infraestruturas urbanas e tornar os centros urbanos mais eficientes tendo em conta as necessidades das populações. estes projetos recorrem geralmente a redes de sensores distribuídas de forma estratégica para a recolha de dados do meio. geralmente, estes projetos passam por uma avaliação experimental do problema em estudo, recorrendo na maioria das vezes a ferramentas de simulação, de que são exemplos o cup- carbon, o interscsimulator, o urbansim, entre outros. neste contexto, neste projeto de mestrado pretende-se estudar um problema concreto no âmbito das smart cities, sendo este o problema da gestão inteligente de estacionamento em ambiente urbano tendo em vista uma maior eficiência da ocupação dos espaços disponíveis e uma melhor experiência dos utilizadores no uso e partilha deste recurso muitas vezes escasso. o ambiente de simulação cupcarbon será utilizado para simulação e teste dos cenários de gestão de estacionamento propostos.",
      "in the last decades the world population has been increasing exponentially, giving rise to potential problems of difficult resolution, such as traffic issues due to the high flow of vehicles, pollution levels, housing, health access, resource management, among others. these problems tend to be stressed in metropolitan areas. in this context, the need to “virtualize” the cities themselves is being pursued, leading to the concept of smart cities. the main objective is creating sustainable conditions in the cities and providing more flexible access to useful information for the citizens, while achieving better management of resources. the concept of smart cities is known for using of various technologies to improve urban infrastructures and make urban centers more efficient taking into account the needs of the population. these projects usually rely on a series of sensor networks strategically distributed for the collection of data from the environment. generally, these projects undergo a trial evaluation of the problem under study, resorting mostly to simulation tools, such as cupcarbon, interscsimulator, urbansim, among others. in this context, this master thesis’s project aims to study a specific problem in the scope of the smart cities, more specifically, the problem of intelligent management of parking in an urban environment. the objective is to improve the efficiency in the management of available vehicle spaces, providing a better experience to the users in the use and sharing of this often scarce resource. the cupcarbon simulation environment will be used for simulating and testing the proposed parking management scenarios."
    ],
    [
      "os corpora textuais são um recurso importante no processamento de linguagem natural e em áreas relacionadas, tais como a mineração de textos biomédicos, a linguística de corpus, aprendizagem máquina e recuperação de informação. a preparação de documentos para inclusão num corpus envolve vários passos distintos e uma rede complexa de dependências e condições, que resulta num fluxo difícil de gerir manualmente. esta dissertação foca-se nos diversos desafios encontrados no processo de construção de corpora, e propõe métodos para ultrapassar essas questões. o primeiro problema abordado foi a limpeza de documentos de texto –remoção de resíduos estruturais, normalização de formatos e notações e deteção de delimitadores de secção– tornando os documentos passíveis de serem processados. outra questão abordada foi a deteção de documentos duplicados e de pares de documentos candidatos a alinhamento, tendo sido introduzido e implementado um método para medição da similaridade entre documentos. posteriormente, introduziu-se o conceito de sincronização de documentos, seguido da descrição de uma implementação baseada nos delimitadores de secção. dois casos de estudo reais foram utilizados para guiar a implementação das ferramentas desenvolvidas: alinhamento multi-língua de documentos para inclusão em corpora paralelos alinhados e a construção de corpora de textos biomédicos para mineração de texto. um protótipo de um sistema de gestão da construção de corpora foi desenvolvido – um sistema de corpora-flow. este sistema incorpora mecanismos que facilitam a implementação do fluxo necessário para a construção de um corpus. uma avaliação comparativa do conjunto de ferramentas desenvolvido foi realizada através do alinhamento de documentos com e sem a intervenção das ferramentas desenvolvidas. um pequeno conjunto de ferramentas foi desenvolvido para avaliar os resultados de alinhamentos.",
      "text corpora are important resources on natural language processing and related areas such as biomedical text mining, corpus linguistics, machine learning and information extraction. preparing documents to be included in a corpus involves several different steps and a complex network of dependencies and conditions, which results in a workflow difficult to manage manually. this dissertation focuses on different challenges which can be found when building corpora, and proposed methods to overcome such questions. cleaning of text documents – removing structural residues, normalizing encodings and notations and finding section delimiters – to make the documents suitable for further processing. another question addressed was the detection of duplicated documents and candidate document pairs for alignment. a method for measuring the similarity between documents was introduced and implemented. then, the concept of document synchronization was introduced, followed by the description of an implementation based on section delimiters. two real-world scenarios were used to guide the implementation of the tools developed: multi-language document alignment for inclusion in parallel aligned corpora and building corpora of biomedical texts for text mining. a prototype of a corpora building management system was developed – a corpora-flow system. this system includes mechanisms which facilitate the implementation of the workflow needed to build a corpus. a comparative evaluation of the set of tools developed was performed by aligning documents with and without using the tools developed. a small set of auxiliary tools was created to evaluate the results of alignments."
    ],
    0.0
  ],
  [
    [
      "yawl é uma linguagem gráfica para a especificação de processos, com uma semântica bem definida, que permite o desenho, especificação, simulação e validação de sistemas, cujos processos a modelar exijam características específicas de comunicação, concorrência e sincronização entre si. a yawl é baseada, por um lado, em padrões bem definidos de fluxo de trabalho e, por outro, nas conhecidas redes de petri coloridas. devido às suas características, vários estudos utilizaram esta linguagem na modelação de sistemas de etl (extract-transformation-load), tentando facilitar e agilizar todo este processo. este trabalho de dissertação teve como objetivo desenvolver e implementar um sistema de geração de “esqueletos” para sistemas de etl a partir de um conjunto de especificações yawl. nesse sentido, foi idealizado e desenvolvido um sistema capaz de representar e produzir de forma semi-automática a configuração de processos etl para várias ferramentas de implementação de processos etl, nomeadamente kettle, informatica e talend, todas elas ferramentas conceituadas no mercado dos sistemas de povoamento de data warehouses. nesta dissertação apresentamos e descrevemos tal sistema, desde as suas fases de fundamentação e conceptualização até às suas fases de teste e exploração.",
      "yawl is a graphical language with a well-formed semantics, that allows the design, specification, simulation, and validation of systems where the processes to model have specific characteristics such as, communication, concurrency and synchronization. it is based, on one hand, on well-formed workflow patterns and on the other standards known as the colored petri nets. due to its characteristics several studies have used this language in the modeling of etl (extract transformation-load) systems, trying to facilitate and improve uppon this process. the main goal of this dissertation was to develop and implement a system for the generation of skeletons for etl systems from yawl. a system was designed and developed to represent the semi-automatic generation of etl process for some etl tools, namely kettle, informatica and talend, all of them renowned tools to populate data warehouses. in this dissertation we present and describe such a system, from its founding and conceptualization phases to its testing and exploration phases."
    ],
    [
      "o uso de painéis de digital signage ou displays (ou, em português, painéis digitais) está cada vez mais a ser implementado nos locais públicos e semipúblicos, uma vez que é uma forma actual de apresentar, de um modo dinâmico, informação, publicidade e conteúdos de entretenimento. desta forma, as pessoas expostas a esta tecnologia passam a ter um acesso mais rápido e actualizado à informação sobre tudo o que as rodeia. os ecrãs dos displays têm evoluído no sentido da melhoria da sua qualidade. além disso, a descida dos preços torna esta tecnologia bastante mais acessível para ser aplicada em vários sectores, nomeadamente na indústria, no comércio, na educação, na saúde, etc., substituindo os meios tradicionais publicitários e informativos, baseados em papel. uma rede de painéis digitais permite a actualização dos seus conteúdos de forma remota, com base num servidor central que controla toda a informação apresentada na rede, enquanto que nos meios tradicionais a actualização de conteúdos é muito mais cara, demorada e de difícil gestão. como as pessoas olham pouco tempo para os painéis digitais, exige-se um esforço muito grande no estudo do local onde estes são instalados e na modelação dos conteúdos a serem apresentados, de forma a que o ambiente se torne o mais atractivo possível e, deste modo, se promova a atenção de quem passa pelos painéis. também existem displays que usam tecnologias mais sofisticadas, permitindo a adaptação automática dos conteúdos apresentados em função do contexto envolvente, fazendo com que a informação seja mais direccionada ao público, sem a necessidade de controlo humano. dado o crescente sucesso, à escala planetária, da digital signage, têm surgindo cada vez mais soluções de software aplicadas nesta vertente. por conseguinte, o tema principal desta dissertação incidirá sobre o desenvolvimento de uma aplicação web para painéis de digital signage, sugerida pela empresa ubisign, com o objectivo de permitir configurar visualizações com informação sobre eventos provenientes do google calendar. uma vez que a promoção de eventos geralmente exige custos elevados de design e produção, é necessário minimizá-los no contexto das redes de digital signage. para tal, existem determinadas ferramentas on-line de gestão de eventos, como o google calendar, com a função de permitir que o utilizador especifique eventos que vão ocorrer e efectue o seu escalonamento com base em calendários. do ponto de vista de um developer, estas ferramentas evitam a necessidade de implementar outros softwares de gestão de eventos, permitindo também desenvolver outras aplicações que comuniquem com estas ferramentas, através de apis apropriadas e bem documentadas. para tirar partido disto, a aplicação a desenvolver terá de recorrer à api google calendar para disponibilizar, de forma personalizada, informação sobre eventos que estejam planeados para um dado sítio com uma rede de digital signage instalada e, para esse efeito, terá de ser integrada no serviço ubisign.com.",
      "the usage of digital signage displays in public and semi-public places is increasing, because they are a modern way of dynamically displaying information, advertisements and entertainment content. in this way, those exposed to digital signage have a quick and up to date access to information around them. display screens have been improving in quality. also prices have been falling, making this technology more affordable in order to be applied in various areas (particularly in industry, commerce, education, health, etc.), replacing the paper-based traditional means of advertising and information [müller, 2009]. a digital signage network allows the remote update of its contents, based on a central server that manages all the information presented on the network, while in traditional means the contents’ updating process is much more costly, time consuming and difficult to manage. because people glance briefly into the displays, their content and locus must be carefully studied, so that the environment becomes more attractive. thus it is a way to promote the attention of the passersby [huang et al., 2008]. some displays use more sophisticated technologies, allowing a contextbased automatic adaptation of their content. so the information is targeted to the public, without the need for human control [müller, 2007, payne et al., 2006]. ever more software solutions applied to digital signage have emerged due to its increasing success in a worldwide scale. therefore the development of a digital signage web application that allows the set up of views with event information coming from google calendar (proposed by ubisign company) will be the main theme of this dissertation. since event promotion generally requires high costs of design and produc tion, they must be minimized in digital signage networks. to this end, some online tools for managing events, such as google calendar, allow calendarbased event scheduling and its specification. from a developer’s perspective, these tools avoid the need to implement other event management software, allowing one to develop other applications that communicate with these tools through appropriate and well documented apis. to take advantage of this, the application that will be developed will need to use the google calendar api to provide information in a personalized way about events that are planned for a given place with an installed digital signage network. for this purpose, the application must be integrated in the ubisign.com service. area of application: information interfaces and presentation. keywords: digital signage displays; google calendar; event information."
    ],
    0.0
  ],
  [
    [
      "as bases de dados relacionais não foram desenhadas para tratar de dados interligados, em oposição às tecnologias de grafos. a modelação de recursos humanos trabalha com estruturas altamente relacionadas entre si, pelo que a substituição de bases de dados relacionais por bases de dados de grafos neste contexto poderia melhorar a robustez e desempenho das aplicações. na presente dissertação, pretendeu-se comparar as tecnologias existentes (relacionais, não relacionais e grafos) e, dentro da tecnologia de grafos, avaliar qual a mais adequada em contexto de recursos humanos. a revisão de literatura revelou que as bases de dados de grafos são mais eficientes para dados interligados do que as bases de dados relacionais e não relacionais. de todos os modelos de grafos analisados, o neo4j foi o sistema de gestão de bases de dados que reuniu, num âmbito geral, as melhores características e, por este motivo, foi utilizado como prova de conceito. foram realizadas três interrogações (duas com obtenção de dados de diferentes relações e uma com obtenção de dados de uma única relação), tendo-se obtido tempos de resposta de 41.48, 18.58 e 62.14ms vs. 804.68, 103.08 e 318.42ms entre bases de dados de grafos e relacionais, respetivamente. os resultados obtidos revelaram melhor desempenho do neo4j na maioria das situações avaliadas. em situações sem junções entre diferentes relações num ambiente relacional, o desempenho do sql foi superior ao do neo4j. adicionalmente, verificou-se quebra significativa de desempenho do neo4j quando foi analisado mais de metade do grafo. as bases de dados de grafos apresentaram um melhor desempenho no tratamento de bases de dados altamente relacionadas.",
      "relational databases were not designed to handle linked data, as opposed to graph technologies. human resource modelling works with highly interrelated structures. therefore, replacing relational databases with graph databases in this context could improve applications' robustness and performance. the aim of this dissertation was to 1) compare existing technologies (relational, non-relational and graphs) and 2) evaluate which is the most appropriate graph technology in a human resources context. the literature review showed that graph databases are more efficient for interconnected data than relational and non-relational databases. of all graph models analysed, neo4j was the database management system that gathered, overall, the best features and therefore was used as proof of concept. three queries were performed two with data obtained from different relations and one with data obtained from a single relation), having obtained response times of 41.48, 18.58 and 62,14rns vs. 804.68, 103.08 and 318,42ms between graph and relational databases, respectively. results showed better performance of neo4j in most of the evaluated situations. in a relational environment without joins between different relations, sql outperformed nleo4i. additionally, there was a significant drop in neo4cs performance when more than half of the graph was analysed, in conclusion, graph databases performed better in processing highly related databases."
    ],
    [
      "deep learning (dl) has become fundamental to the advancement of several areas, such as computer vision, natural language processing and expert systems. utilizing dl techniques demands vast amounts of data and processing power, which raises challenges to the training performance of dl models. high performance computing (hpc) systems are becoming increasingly popular to support dl training, by offering extensive computing capabilities, however, due to convenience and usability, many dl jobs running on these infrastructures resort to the shared parallel file system (pfs) for storing and accessing training data. under such scenario, where multiple input/output (i/o)-intensive applications operate concurrently, the pfs can quickly get saturated with simultaneous storage requests and become a critical performance bottleneck, leading to throughput variability and performance loss. to solve these issues, this dissertation presents a storage middleware agnostic to any dl solution, monarch, that deploys storage tiering to accelerate dl models’ training performance and decrease the i/o pressure imposed over the pfs. it leverages from existing storage tiers of supercomputers (e.g., compute node’s local storage, shared pfs), as well as the i/o patterns of dl solutions to improve data placement across storage tiers. furthermore, this middleware is non-intrusive and easily installed in hpc centers, thus enabling its wide adoption and applicability. the performance and applicability of monarch are validated with the tensorflow and pytorch dl frameworks. results show that, when the training dataset can only be partially stored at the local storage tier, monarch decreases tensorflow’s and pytorch’s training time by up to 28% and 37% for i/o-intensive models, respectively. furthermore, monarch can reduce the number of i/o operations submitted to the pfs by up to 56%.",
      "aprendizagem profunda (ap) tornou-se fundamental para o avanço de diversas áreas, como visão por computadores, processamento de linguagem natural e sistemas especializados. a utilização de técnicas de ap requer vastas quantidades de dados e de poder de processamento, o que impõe desafios ao de sempenho do treino de modelos de ap. os sistemas de computação de alto desempenho (cad) estão a tornar-se cada vez mais populares para suportar treino de ap, uma vez que oferecem extensos recursos de computação, contudo, por razões de conveniência e usabilidade, muitas tarefas de ap que correm nestas infraestruturas recorrem a sistema de ficheiros paralelos (sfp) para armazenar e aceder a dados de treino. neste cenário, onde múltiplas aplicações intensivas em entrada/saída (e/s) operam concor rentemente, o sfp pode ficar saturado com os pedidos de armazenamento simultâneos e tornar-se um gargalo de desempenho crítico, levando à variabilidade do débito e perda de performance. para resolver estes problemas, esta dissertação propõe um middleware de armazenamento agnóstico a qualquer solução de ap, monarch, que implementa armazenamento por camadas, para acelerar o desempenho do treino de ap e diminuir a pressão de e/s imposta sobre o sfp. este sistema aproveita camadas de armazenamento existentes em supercomputadores (p.ex., armazenamento local do nó de computação, sfp partilhado), assim como o padrão de e/s das soluções de ap para melhorar a colocação dos dados ao longo das camadas de armazenamento. para além disso, este middleware é não-intrusivo e facilmente instalado em centros de cad, permitindo, deste modo, a sua ampla adoção e aplicabilidade. o desempenho e aplicabilidade do monarch são validados recorrendo às soluções de ap tensorflow e pytorch. os resultados mostram que, quando o conjunto de dados de treino apenas pode ser parcialmente armazenado na camada de armazenamento local, o monarch diminui o tempo de treino com tensorflow e pytorch entre 28% e 37%, para modelos intensivos em e/s, respetivamente. para além disso, o monarch consegue reduzir o número de operações de e/s submetidas para o sfp até 56%"
    ],
    0.3
  ],
  [
    [
      "peer-to-peer broadcasting algorithms are a scalable and cheap way of disseminating information to a large number of participants. however, most of these algorithms do not consider the possibility of some members acting in an unintended way, with malicious or selfish motives. in order to be useful in a real world scenario, these algorithms must be secure, robust and efficient, even in the presence of adversaries. this thesis presents an overview of the challenges that peer-to-peer broadcasting algorithms face, as well as some of the security mechanisms that can be employed to mitigate them. thus, we present bycast, a secure and efficient peer-topeer broadcasting algorithm that is able to tolerate up to 45% of malicious nodes in the system. in order to achieve a high level of security, bycast relies on strong membership integrity guarantees that make it harder for attackers to successfully compromise other nodes. in order to force nodes to cooperate, and contribute to the good performance of the system, bycast employs an innovative auditing scheme that is able to detect nodes that are not cooperating with their resources, and evict them from the system.",
      "os algoritmos de transmissão “peer-to-peer” são uma maneira escalável e barata de disseminar informação para um grande número de participantes. no entanto, a maioria desses algoritmos não considera a possibilidade de alguns participantes agirem de forma imprevista face à especificação do algoritmo, quer por motivos maliciosos quer egoístas. para serem úteis num cenário do mundo real, esses algoritmos devem ser seguros, robustos e eficientes, mesmo na presença de adversários. esta tese proporciona uma visão geral dos desafios que os algoritmos de disseminação peer-to-peer enfrentam, bem como alguns dos mecanismos de segurança que podem ser utilizados para mitigá-los. assim, apresentamos o bycast, um algoritmo de transmissão peer-to-peer seguro e eficiente, capaz de tolerar até 45% de nodos maliciosos no sistema. para alcançar um alto nível de segurança, o bycast conta com fortes garantias de integridade no sistema de membership que torna mais difícil para os atacantes comprometerem outros nodos com sucesso. para forçar os nodos a cooperarem e contribuir para o bom desempenho do sistema, o bycast emprega um esquema de auditoria inovador que é capaz de detectar nodos que não estão a cooperar com os seus recursos de forma a removê-los do sistema."
    ],
    [
      "this document is a report for the final project of the master’s in informatics engineering degree, accomplished at universidade do minho in braga, portugal. the project consists, in a first phase, on the study of many snapshots of programs archived by nuno fonseca for his doctoral thesis \"contributos para a monitorização do desempenho de estudantes de programação\" at the universidade de coimbra. this study’s main goal is to analyse the code snapshots and explore as much as possible the knowledge that can be extracted from that repository in order to understand the students’ behavior while solving problems by computer. the research begins with the gathering and analysis of different types of tools whose purpose is to analyze code, both to assist students in learning and to help teachers in evaluation. after this research, the data provided by professor nuno fonseca is deeply analysed and adapted to fit this project’s requirements and fulfill the proposed goals. finally, a web application that allows the teacher to conduct a comprehensive and systematic analysis of the learning progression of their students was developed. all documentation associated with this application was also produced. the outcomes of this thesis are expected to contribute to the field of code teaching tools, with the research made in this field and the resulting web application.",
      "este documento é um relatório do projeto final do mestrado em engenharia informática, realizado na universidade do minho em braga, portugal. o projeto consistiu, numa primeira fase, no estudo de vários snapshots de programas arquivados por nuno fonseca para a sua tese de doutoramento \"contributos para a monitorização do desempenho de estudantes de programação\" na universidade de coimbra. o objetivo principal deste estudo é analisar os snapshots de código e explorar ao máximo o conhecimento que pode ser extraído daquele repositório para entender o comportamento dos alunos durante a resolução de problemas por computação. a pesquisa começa então pela recolha e análise de diferentes tipos de ferramentas cujo objetivo é analisar código quer para ajudar os estudantes a aprender quer para ajudar os professores a avaliar. após esta pesquisa, os dados fornecidos pelo professor nuno fonseca são profundamente analisados e adaptados para atender aos requisitos deste projeto e cumprir os objetivos propostos. finalmente foi desenvolvida uma aplicação web que permite ao professor fazer uma análise exaustiva, sistemática da evolução de aprendizagem dos seus alunos. foi também produzida toda a documentação associada a esta aplicação. os resultados desta tese têm como expectativa contribuir para o campo de ferramentas de ensino de programação, através da pesquisa realizada nesta área e a aplicação web resultante."
    ],
    0.0
  ],
  [
    [
      "as estações de tratamento de águas residuais que utilizam o sistema de lamas ativadas caraterizam-se por serem um complexo ecossistema, responsável pelo tratamento de águas residuais, através de processos biológicos que metabolizam substâncias orgânicas e inorgânicas em produtos mais toleráveis a nível ambiental. a comunidade de microrganismos desse sistema requer uma apertada monitorização e controlo, recorrendo a inspecções regulares por microscopia para evitar perdas económicas e danos ambientais provocados por populações bacterianas que não estejam balanceadas. a inovação tecnológica das últimas décadas permitiu que a monitorização que era realizada através de um operador humano, passasse a ser desempenhada por computadores, através por exemplo de programas como o ”flocos e filamentos” que consegue analisar morfologicamente as populações bacterianas com o tratamento de uma ou mais imagens digitais grayscale 8 bits, adquiridas de reatores laboratoriais ou estações de tratamento de águas residuais. nesta dissertação de mestrado, foi construída uma plataforma online, que integrou o programa ”flocos e filamentos” otimizado e compilado. a plataforma permite facilitar a monitorização deste processo biológico, sendo acessível através de dispositivos moveis e computadores.",
      "wastewater treatment plants that use the activated sludge system are characterized by being a complex ecosystem, responsible for the treatment of waste water, through biological processes that metabolize organic and inorganic substances into products that are more environmentally tolerable . the microorganism0s community of this system requires close monitoring and control, using regular microscopic inspections to avoid economic losses and environmental damage caused by unbalanced bacterial populations. the technological innovation of the last decades has allowed the monitoring by computers that was previously carried out by a human operator. ”flocs and filaments” is a software that can morphologically analyse the bacterial populations with the treatment of one or more digital grayscale images, acquired from laboratory reactors or wastewater treatment plants. in this master’s dissertation, an online platform was built, which integrated the ”flocs and filaments” software, previously optimized and compiled. the platform allows the monitoring of this biological process, being accessible through mobile devices and computers."
    ],
    [
      "a história pode ser resumida em um punhado de eventos que influenciaram a evolução humana: a nossa capacidade de controlar o fogo, a invenção da roda ou a implementação da linha de montagem. todas estas descobertas tiveram um enorme impacto no futuro da nossa raça e do próprio mundo. agora encontramo-nos novamente noutra revolução: a revolução dos dados. facilmente impercetível, esta nova perspetiva está a mudar de todas as formas possíveis como interagimos com a internet e, pela primeira vez na história, como a internet interage connosco. este novo tipo de interações é definido por conexões entre utilizadores e bens consumíveis (produtos, artigos, filmes, etc.). através destas conexões, conhecimento pode ser encontrado. esta é a definição de mineração de dados. com o aumento da oferta que as plataformas de comércio virtual oferecem hoje, é necessária uma ferramenta que auxilie a conexão entre clientes e produtos. os sistemas de recomendação surgem como o fator dominante na criação de novas conexões entre oferta e procura. esta dissertação segue a investigação e implementação de um motor de recomendações adaptável a várias plataformas de comércio virtual de diversas áreas de negócio. consequentemente, trata-se de um sistema escalável, genérico e customizável, com o objetivo de aumentar a interação dos clientes, através de uma experiência personalizada com múltiplos tipos de recomendações espalhados pelas plataformas. este projeto irá seguir uma metodologia crisp-dm e o motor deverá implementar algoritmos aceites mas adaptados aos dados e à estrutura das plataformas de comércio virtual. as recomendações serão também avaliadas de acordo com métricas apropriadas ao projeto, e os seus resultados discutidos.",
      "history could be epitomised to a handful of events that changed the course of human evolution: our ability to control fire, the invention of the wheel, or the implementation of the assembly line. all these discoveries made an enormous impact on the future of our race and the world itself. now we might find ourselves right across another revolution: the data revolution. easily unnotice-able, this new outlook is shifting in every single and possible way how we interact with the internet and, for the first time in history, how the internet interacts with us. this new kind of interactions is defined by connections between users and consumable goods (products, articles, movies, etc.). and through these connections, knowledge can be found. this is the definition of data mining. with the increase of supply that e-commerce platforms offer today, a tool is needed to connect clients to products. recommender systems emerge as the dominant factor in creating new connections between supply and demand. this dissertation follows the research and implementation of a recommender engine adaptable to various e-commerce platforms from very different business areas. consequently, it is a scalable, generic and customisable system, intending to increase customer engagement, by providing a personalised experience with multiple types of recommendations across the platforms. the project will follow a crisp-dm methodology, and the engine should use accepted algorithms but adapted to the data and the structure of the e-commerce platforms. the recommendations will also be evaluated according to metrics appropriate to the project, and its results discussed."
    ],
    0.06666666666666667
  ],
  [
    [
      "with the growing computational power that we have at our disposal and the ever-increasing amount of data available the field of machine learning has given rise to deep learning, a subset of machine learning algorithms that have shown extraordinary results in a variety of applications from natural language processing to computer vision. in the field of computer vision, these algorithms have greatly improved the state-of-the-art accuracy in tasks associated with object recognition such as detection. this thesis makes use of one of these algorithms, specifically the yolo algorithm, as a basis in the development of a system capable of detecting objects laying inside a car cockpit. to this end a dataset is collected for the purpose of training the yolo algorithm on this task. a comparative analysis of the detection performance of the yolov2 and yolov3 architectures is performed.several experiments are performed by modifying the yolov3 architecture to attempt to improve its accuracy. specifically tests are performed in regards to network size, and the multiple outputs present in this network. explorative experiments are done in order to test the effect that parallel network might have on detection performance. lastly tests are done to try to find an optimal learning rate and batch size for our dataset on the new architectures.",
      "com o crescente poder computacional que temos à nossa disposição e o aumento da quantidade dados a que temos acesso o campo de machine learning deu origem ao deep learning um subconjunto de algoritmos de machine learning que têm demonstrado resultados extraordinários numa variedade de aplicações desde processamento de linguagens naturais a visão por computador. no campo de visão por computador estes algoritmos têm levado a enormes progressos na correção de sistemas de deteção de objetos. nesta tese usamos um destes algoritmos, especificament o yolo, como base para desenvolver um sistema capaz de detetar objetos dentro de um carro. dado isto um dataset é recolhido com o propósito de treinar o algoritmo yolo nesta tarefa. uma analise comparativa da correção dos algoritmos yolov2 e yolov3 ´e realizada. várias técnicas relacionadas com a modificação da arquitetura yolov3 são exploradas para otimizar o sistema para o problema especifico de deteção a bordo de veículos. especificamente testes são realizados no contexto de tamanho da rede e dos múltiplos outputs presentes nesta rede. experiencias exploratórias são realizadas de forma a testar o efeito que redes parallelas podem ter na correção dos algoritmos. por fim testes são feitos para tentar encontrar learning rates e batch sizes apropriados para o nosso dataset nas novas arquiteturas."
    ],
    [
      "proteins are one of the more important biological structures in living organisms, since they perform multiple biological functions. each protein has different characteristics and properties, which can be employed in many industries, such as industrial biotechnology, clinical applications, among others, demonstrating a positive impact. modern high-throughput methods allow protein sequencing, which provides the protein sequence data. machine learning methodologies are applied to characterize proteins using information of the protein sequence. however, a major problem associated with this method is how to properly encode the protein sequences without losing the biological relationship between the amino acid residues. the transformation of the protein sequence into a numeric representation is done by encoder methods. in this sense, the main objective of this project is to study different encoders and identify the methods which yield the best biological representation of the protein sequences, when used in machine learning (ml) models to predict different labels related to their function. the methods were analyzed in two study cases. the first is related to enzymes, since they are a well-established case in the literature. the second used transporter sequences, a lesser studied case in the literature. in both cases, the data was collected from the curated database swiss-prot. the encoders that were tested include: calculated protein descriptors; matrix substitution methods; position-specific scoring matrices; and encoding by pre-trained transformer methods. the use of state-of-the-art pretrained transformers to encode protein sequences proved to be a good biological representation for subsequent application in state-of-the-art ml methods. namely, the esm-1b transformer achieved a mathews correlation coefficient above 0.9 for any multiclassification task of the transporter classification system.",
      "as proteínas são estruturas biológicas importantes dos organismos vivos, uma vez que estas desempenham múltiplas funções biológicas. cada proteína tem características e propriedades diferentes, que podem ser aplicadas em diversas indústrias, tais como a biotecnologia industrial, aplicações clínicas, entre outras, demonstrando um impacto positivo. os métodos modernos de alto rendimento permitem a sequenciação de proteínas, fornecendo dados da sequência proteica. metodologias de aprendizagem de máquinas tem sido aplicada para caracterizar as proteínas utilizando informação da sua sequência. um problema associado a este método e como representar adequadamente as sequências proteicas sem perder a relação biológica entre os resíduos de aminoácidos. a transformação da sequência de proteínas numa representação numérica é feita por codificadores. neste sentido, o principal objetivo deste projeto é estudar diferentes codificadores e identificar os métodos que produzem a melhor representação biológica das sequências proteicas, quando utilizados em modelos de aprendizagem mecânica para prever a classificação associada à sua função a sua função. os métodos foram analisados em dois casos de estudo. o primeiro caso foi baseado em enzimas, uma vez que são um caso bem estabelecido na literatura. o segundo, na utilização de proteínas de transportadores, um caso menos estudado na literatura. em ambos os casos, os dados foram recolhidos a partir da base de dados curada swiss-prot. os codificadores testados incluem: descritores de proteínas calculados; métodos de substituição por matrizes; matrizes de pontuação específicas da posição; e codificação por modelos de transformadores pré-treinados. a utilização de transformadores de última geração para codificar sequências de proteínas demonstrou ser uma boa representação biológica para aplicação subsequente em métodos ml de última geração. nomeadamente, o transformador esm-1b atingiu um coeficiente de correlação de matthews acima de 0,9 para multiclassificação do sistema de classificação de proteínas transportadoras."
    ],
    0.06666666666666667
  ],
  [
    [
      "a substituição do papel pelo formato digital nas instituições e empresas tornou-se uma prática comum, sendo que algumas já extinguiram a utilização de formatos analógicos. as políticas europeias incentivam a que sejam adotadas medidas para a redução de papel. desta forma, a administração pública tem abandonado a utilização de suportes analógicos, substituindo-os pelo formato digital, tendo as entidades públicas passado a prestar os seus serviços e a disponibilizar a documentação de forma eletrónica. a plataforma “clav - classificação e avaliação da informação pública” desenvolvida pela dglab pretende disponibilizar instrumentos, tal como a lista consolidada; a mediação des materializada da produção de tabelas de seleção; e a prestação de um serviço automatizado de controlo da eliminação da informação pública. a adoção de esquemas de metainformação para a interoperabilidade permite a disponibilização de uma linguagem comum aos vários organismos da administração existente na lista consolidada, através da plataforma clav, permitindo ainda a integração com sistemas de informação organizacionais e a troca de informação entre entidades. em concordância com os objetivos delineados para o desenvolvimento do projeto, realizou se um estudo sobre abordagens idênticas ao clav noutros países, onde foram identificadas as formas de classificação desenvolvidas e as tecnologias utilizadas para essa implemen tação, fazendo uma comparação com a abordagem de portugal. o plano de trabalho do projeto dividiu-se então em três fases. inicialmente realizou-se um estudo teórico acerca da abordagem de andaluzia (espanha) e a de portugal. seguiu-se com o desenvolvimento de um modelo gráfico acompanhando as especificações do business process model and notation e, por fim, a implementação e o desenvolvimento desse modelo na plataforma clav. verificou-se que, após a implementação do workflow para a gestão dos pedidos, este tornou todo o processo mais simples e rápido para os responsáveis pelo tratamento destes, uma vez que todo o processo é realizado através da plataforma, não havendo a necessidade de transitar documentos físicos.",
      "the substitution of paper for digital format in institutions and companies has become a common practice, and some have already extinguished the use of analog formats. european policies encourage measures to reduce paper. in this way, the public administration has abandoned the use of analogue media, replacing them with the digital format, with public entities now providing their services and making documentation available electronically. the platform “clav - classification and evaluation of public information” developed by dglab intends to provide instruments, such as the consolidated list; the dematerialized mediation of the production of selection tables; and the provision of an automated service to control the elimination of public information. the adoption of metadata schemes for inte roperability allows a common language to be made available to the various administration bodies in the consolidated list, through the clav platform, also allowing integration with organizational information systems and the exchange of information between entities. in accordance with the objectives outlined for the development of the project, a study was carried out on approaches similar to clav in other countries, where the forms of classification developed and the technologies used for this implementation were identified, making a comparison with the approach of portugal. the project’s work plan was then divided into three phases. initially, a theoretical study was carried out on the approach of andalusia (spain) and that of portugal. this was followed by the development of a graphic model following the specifications of the business process model and notation and, finally, the implementation and development of this model on the clav platform. it was found that, after the implementation of the workflow for order management, it made the whole process simpler and faster for those responsible for handling these orders, since the whole process is carried out through the platform, with no need to transit physical documents."
    ],
    [
      "o clav é um projeto nacional que tem como objetivo a classificação e avaliação de todos os documentos da administração pública portuguesa, com base em normas e orientações delineadas pela direção geral do livro, dos arquivos e das bibliotecas (dglab). esta iniciativa surgiu como solução à grande perda de informação causada pelo o uso de papel como método principal de conservação, bem como à incapacidade deste método de processar a enorme quantidade de informação digital produzida atualmente. uma parte essencial para o bom funcionamento da plataforma inclui a eliminação documental de informação arquivística, que permite uma gestão mais eficiente, assim como uma redução de custos de armazenamentos, melhorando a eficiência e eficácia na recuperação de informação. com isto, para proceder à eliminação de um documento, é necessário que as entidades que o desejem fazer enviem através da plataforma do classificação e avaliação de processos da administração pública (clav) um pedido de autorização para essa eliminação. este pedido tem o nome de auto de eliminação (ae). para ser autorizado, terá de ser validado e analisado tendo em conta prazos de conservação administrativa, destinos finais, quem são os donos do processo a quem respeita a documentação, etc. neste sentido, esta dissertação tem como desígnio a continuação do desenvolvimento do tratamento dos ae, a serem incluídos no clav, e do seu armazenamento, para que seja possível garantir transparência da ação administrativa, bem como da capacidade do estado no cumprimento da sua missão (lourenço and gomes, 2019).",
      "clav is a national project that aims to classify and evaluate all documents regarding the portuguese public administration, based on standards and guidelines outlined by dglab. this initiative arose as a solution to the great loss of information caused by the use of paper as the main method of conservation, as well as the inability of this method to process the enormous amount of digital information produced nowadays. an essential part for the good functioning of the platform includes the elimination of archived documentation, which allows a more efficient management, as well as a reduction in storage costs, which improves efficiency in retrieving information. therefore, to proceed with the elimination of a document, it’s necessary that the entities wishing to do so send an authorisation request for that elimination through the clav platform. this request is called an ae. to be authorised, it will have to be validated and analysed taking into account administrative retention periods, final destinations, who the owners of the file are to whom the documentation pertains, etc. in this sense, this dissertation has as its main objective the further development of the treatment of ae, to be included in clav, and their storage, so that it is possible to guarantee transparency of administrative action, as well as the state’s capacity to fulfil its mission (lourenço and gomes, 2019)."
    ],
    0.0
  ],
  [
    [
      "ao longo dos últimos anos, tem aumentado exponencialmente a utilização de tecnologias de informação (tis) e de ferramentas computacionais em vários setores económicos, incluindo o setor da saúde, por serem defendidas como tecnologias que podem transformar e melhorar radicalmente a prestação de cuidados de saúde. o principal objetivo das instituições de saúde é prestar os melhores cuidados de saúde aos seus utentes, garantindo, assim, a prestação de serviços de qualidade e a consequente satisfação dos utentes, bem como reduzir os custos e desperdícios desnecessários associados. portanto, as decisões devem ser tomadas rapidamente mas, igualmente, eficazmente. acredita-se, assim, que o futuro realmente bem sucedido das tis no setor da saúde passa então pelo desenho e pela implementação de sistemas user-friendly, incluindo sistemas de apoio à decisão clínica (sadcs), personalizados e focados no paciente, bem como a receptibilidade dos profissionais de saúde aos mesmos. abrange, igualmente, o uso de tecnologias emergentes na sua conceção, incluindo business intelligence (bi), de modo a tirar real partido da informação disponível. deste modo, no âmbito deste projeto de dissertação, foram desenhadas, desenvolvidas e exploradas uma nova geração de ferramentas web de business intelligence para o apoio à decisão e a prática clínica em unidades hospitalares. englobou, em particular, o desenvolvimento de uma plataforma de bi versátil, incluindo a sua aplicação a dois casos práticos diferentes, notadamente no apoio à decisão nas listas de espera de consultas e de cirurgias, assim como nos cuidados de ginecologia e obstetrícia (go), e de uma ferramenta de codificação clínica icd-9-cm (international classification of diseases, ninth revision, clinical modification). assim, as ferramentas foram projetadas de modo a auxiliar os profissionais de saúde do centro hospitalar do porto (chp) no seu trabalho diário, incluindo a lidar com pacientes em condições delicadas e determinadas situações que requerem uma tomada de decisão eficiente, bem como a codificação clínica de episódios de altas hospitalares.",
      "over the past few years, the use of information technology (it) and computer applications has been widespread exponentially amongst various economic sectors, including healthcare, since they are defended as technologies that can transform and improve radically the delivery of health services. the main goal of health institutions is to provide the best healthcare services to their patients and, thereby, ensuring the delivery of quality services and the consequent patient satisfaction, but also to reduce associated costs and unnecessary waste. thus, decisions have to be made quickly, but also effectively. therefore, it is believed that the truly successful future of it in the health sector could be achieved by the design and implementation of user-friendly systems, including clinical decision support systems (cdsss), personalized and focused on the patient, and the physicians’ positive acceptance to these it tools. it also includes the use of emerging technologies in their design, including business intelligence (bi), in order to take real advantage of the information available. hereupon, in the context of this master’s dissertation, the aim of this present work was to design, develop and explore the next generation of business intelligence web tools to support the decisionmaking process and clinical practice in healthcare units. thus, it included the development of a versatile bi platform, including its application to two different case studies, namely in order to support the decision-making process in appointments and surgeries waiting lists, as well as in obstetrics and gynaecology services, and a clinical coding tool for icd-9-cm (international classification of diseases, ninth revision, clinical modification). thereby, the tools were designed in order to help health professionals of centro hospitalar do porto (chp) in their daily work, including dealing with patients in delicate conditions and certain situations that require an efficient decision-making process, as well as the clinical coding of the episodes of hospital discharges."
    ],
    [
      "a doença de alzheimer é o tipo mais predominante de demência e, apesar de não existir cura para a mesma, o seu diagnóstico prematuro é fundamental para um tratamento efetivo e que permita retardar o progresso dos sintomas. desta forma, nos últimos anos, tem surgido um grande interesse em estudar e desenvolver sistemas automáticos de diagnóstico que usam como fonte de dados os exames médicos realizados pelos pacientes. esta dissertação enquadra-se na temática da utilização de aprendizagem profunda para diagnosticar a doença de alzheimer. pretende-se que seja avaliado o desempenho de redes neuronais profundas existentes da forma mais real possível, e que seja proposta uma arquitetura com bom desempenho, que possa ser usada num sistema de diagnóstico assistido por computador. a capacidade da aprendizagem profunda em encontrar padrões ocultos em imagens médicas permite reduzir o erro de diagnóstico humano e auxiliar num diagnóstico muito mais preciso. é pretendido que, com base numa única imagem por paciente a rede neuronal proposta seja capaz de fazer um diagnóstico sobre a doença de alzheimer. para isso, serão utilizadas imagens do cérebro obtidas pela técnica de ressonância magnética estrutural, adquiridas a partir do conjunto de dados da iniciativa de neuroimagem da doença de alzheimer. para a concretização do objetivo proposto, foram estudados e implementados, num cenário experimental recorrendo a ferramentas simples, os métodos mais indicados para a realização desta tarefa. os resultados mostram que as redes neuronais convolucionais (cnns) permitem construir modelos com enorme potencial, contudo, a sua utilização em ambientes reais ainda não é muito viável nos dias de hoje.",
      "alzheimer’s disease is the most prevalent type of dementia and, although there is no cure for it, its early diagnosis is fundamental for an effective treatment to slow the progression of symptoms. therefore, in the last few years, there has been a great interest in studying and developing automatic diagnostic systems that use medical examinations performed by patients as a source of data. this dissertation addresses the use of deep learning models to diagnose the alzheimer’s disease. it is intended to evaluate the performance of existing deep neural networks as realistically as possible, and to propose an architecture with good performance that can be used in a computer-assisted diagnostic system. the ability of deep learning to find hidden patterns in medical images makes it possible to reduce human diagnostic error and enables a much more accurate diagnosis. it was intended that based on a single image per patient the proposed neural network was able to make a diagnosis about alzheimer’s disease. the present work was carried out with brain images obtained by the structural magnetic resonance imaging technique, acquired from the alzheimer’s disease neuroimaging initiative (adni) dataset. to achieve the proposed goal, the most suitable methods for performing the mentioned task were studied and implemented in an experimental scenario using simple tools. the results show that convolutional neural networks allow the construction of very powerful models, however, their use in real environments is still not very feasible nowadays."
    ],
    0.3
  ],
  [
    [
      "the main goal of software security testing is to assess the security risks of an application so that programmers can eliminate all vulnerabilities, as early as possible, before they are exploited by attackers. there are several tools on the market that allow to perform these tests during the software development life cycle to ensure that there are no security flaws in the final product. however, like all tools, these can also have imperfections, one of them being unable to detect weaknesses in vulnerable software. the project of this dissertation aims to tackle this problem, so that it is possible to find and correct flaws in security tests in order to, consequently, increase the effectiveness of the tools that intend to certify the security of applications. for this, the solution studied in this document is to apply syntactic transformations in vulnerable code samples without interfering in the presence of the vulnerabilities that should later be detected. this process is based on: 𝑖) code refactoring techniques that allow improving the internal quality of the software; 𝑖𝑖) the mutation testing system used to evaluate the quality of software testing. to implement this idea, a tool called vsg was developed with the functionality of producing new code samples with security flaws. this document describes the whole development process, from the architecture to the implementation of the tool. in the end, there is an analysis with the results obtained when trying to detect the vulnerabilities present in the samples produced through the cxsast application of the company checkmarx, from which this dissertation emerged.",
      "o objetivo principal de testes de segurança de software consiste em avaliar os riscos de segurança de uma aplicação para que os programadores possam eliminar todas as vulnerabilidades o mais cedo possível, antes que sejam exploradas por atacantes. existem várias ferramentas no mercado que permitem realizar estes testes durante o processo de desenvolvimento de software para garantir que não existam falhas de segurança no produto final. porém, tal como todas as ferramentas, estas também podem apresentar imperfeições, sendo uma delas não conseguir detetar fraquezas em software vulnerável. o projeto desta dissertação pretende combater este problema, de modo a que seja possível encontrar e corrigir falhas nos testes de segurança para, consequentemente, aumentar a eficácia das ferramentas que pretendem certificar a segurança das aplicações. para isto, a solução estudada neste documento passa por aplicar transformações sintáticas em amostras de código vulneráveis sem interferir na presença das vulnerabilidades que deverão, posteriormente, ser detetadas. este processo baseia-se: 𝑖) nas técnicas de refatoração de código que permitem melhorar a qualidade interna do software; 𝑖𝑖) no sistema de testes de mutação usado para avaliar a qualidade de testes de software. para implementar esta ideia, uma ferramenta chamada vsg foi desenvolvida com a funcionalidade de produzir novas amostras de código com falhas de segurança. neste documento é descrito todo o processo de desenvolvimento, desde a arquitetura até à implementação da ferramenta. no final, existe uma análise com os resultados obtidos ao tentar detetar as vulnerabilidades presentes nas amostras produzidas através da aplicação cxsast da empresa checkmarx, da qual esta dissertação surgiu."
    ],
    [
      "conseguir satisfazer os clientes em mercados altamente competitivos depende diretamente da qualidade e desempenho das aplicações que lhes são direcionadas. alguns segundos de atraso podem fazer a diferença entre o sucesso e o fracasso de uma empresa. a incapacidade de processar, aceder, analisar e integrar dados rapidamente num dado sistema é bastante problemática para organizações que têm de processar uma grande quantidade e variedade de dados. os sistemas in memory data grids (imdg) operam essencialmente com os seus dados em memória, podendo, porém, ser suportados por vários servidores incorporados num sistema distribuído. estes sistemas são recomendados para aplicações que exijam a manipulação de grandes volumes de dados, uma vez que são facilmente escaláveis e de fácil implementação. além disso, em termos técnicos, os sistemas imdg são claramente vantajosos em processos que requeiram rápidas tomadas de decisão, exijam elevados níveis de produtividade e solicitem um atendimento de alta qualidade aos seus sistemas e utilizadores clientes. neste trabalho de dissertação foram estudas, de forma detalhada, várias alternativas imdg open source existentes na atualidade, tendo como base de trabalho um conjunto de condições funcionais e estruturais definidas por uma empresa de telecomunicações, com o objetivo de viabilizar a utilização de uma solução imdg open source em substituição de uma solução dita comercial. adicionalmente, idealizou-se um pequeno conjunto de casos de estudo que foram utilizados como base para o processo de criação de duas aplicações práticas reais utilizando duas soluções imdg open source distintas, nomeadamente, o hazelcast e o infinispan. no processo de elaboração destes casos de estudo tomou-se em consideração alguns cenários de aplicação bastante típicos em sistemas de telecomunicações, bem como, nas fases de implementação das aplicações, as funcionalidades mais relevantes que se podem encontrar em sistemas distribuídos deste género, em particular a execução local de dados em ambiente distribuído, a afinidade de dados em casos de particionamento, a capacidade de replicação de cache em cenários topológicos com mais de um cluster e, por fim, a integração de java persistence api (jpa) e java transaction api (jta) como mecanismos para controlo e gestão de persistência e das transações distribuídas.",
      "being able to satisfy clients in highly competitive markets depends directly on the quality and performance of applications directed to them. seconds of delay can make the difference between success and failure of a new company. the inability to process, access, analyze and integrate data quickly is more problematic for organizations as they have to process a greater quantity and variety of data. in memory data grids (imdgs) systems operate with its data in memory, possibly supported by multiple servers embedded in a distributed system. these systems are especially geared to handling large data volumes, featuring a remarkable performance, easily scalable and easy to implement. furthermore, in technical terms, these systems are clearly advantageous in processes that require quick decision-making, require high levels of productivity and request a high quality service to its customers systems or users. in this dissertation work were studied in detail the existing imdg open source alternatives taking into account a set of functional and structural conditions defined by a telecommunications company, with the aim of enabling open source alternatives as paid products substitutes. additionally was envisioned a small set of case studies as basis for the process of creating two applications with two different imdgs open source, in particular, hazelcast and infinispan, demonstrating the versatility of these systems as well as its applicability. in the creation process of the case studies were taken into consideration rather typical application scenarios of telecommunication systems, as well, in the implementation of applications were considered the features relevant to distributed systems of this kind, in particular, local execution of data in a distributed environment, data affinity in partitioning cases, cache replication capacity in topological scenarios with more than one cluster, and integration of java persistence api (jpa) and java transaction api (jta) as mechanisms for persistence and distributed transactions management and control."
    ],
    0.3
  ],
  [
    [
      "internet of things concept has achieved a huge popularity in the last years due to the quality of life its use is able to provide its users. among the several areas where this concept is applied, smart homes are one of the most popular, which are increasingly a reality in our daily lives. in a smart home, in addition to the iot devices and a hub that connects them to each other, it’s indispensable the use of an application that allows the management of their equipment, regardless of the location around the house. in this context, whenever a company decides to invest in this business sector, among which several decisions are made in the initial phase of the project, stands out the choice of the type of application to be developed to control the different equipment. this choice may prove difficult due to the little information available regarding the behavior of application types changes in this context. the goal of this dissertation is to study one of the existing types of applications, hybrid applications, and to bridge the analysis results in the development of one that allows the management of a smart home. this application will allow the analysis of the behavior of hybrid applications in the context of smart homes, providing study tools for the evolution of this business area.",
      "o conceito de internet of things alcançou uma enorme popularidade ao longo dos últimos anos graças à qualidade de vida que a sua utilização consegue proporcionar aos seus utilizadores. entre as diversas áreas onde este conceito é aplicado, destacam-se as smart homes, ou casas inteligentes, que são cada vez mais uma realidade no nosso quotidiano. numa casa inteligente, para além dos dispositivos de iot e de um hub que os conecta entre si, é indispensável a existência de uma aplicação que possibilite ao utilizador a gestão integrada dos seus equipamentos, independentemente da localização destes pela casa. nesse sentido, sempre que uma empresa decide investir neste setor de negócio, entre as diversas decisões que são tomadas na fase inicial do projeto, destaca-se a escolha do tipo de aplicação a desenvolver para controlar os diferentes equipamentos. esta escolha pode revelar-se difícil devido à pouca informação existente no que diz respeito ao comportamento das respetivas tipologias neste contexto. o objetivo desta dissertação passa por estudar uma das tipologias de aplicações existentes, as híbridas, e colmatar os resultados da análise no desenvolvimento de uma aplicação que possibilite a gestão de uma casa inteligente. a sua conceção irá permitir a análise do comportamento das aplicações híbridas no contexto das smart homes, fornecendo assim ferramentas de estudo para o desenvolvimento deste setor."
    ],
    [
      "nowadays, with the existence of several chronic health conditions, diabetes mellitus (dm) being one of the main ones, there is a great burden that patients must have in order to be able to take care of themselves. thus, in addition to seeking to resolve their needs by obtaining information from health professionals, they increasingly seek information and advices in forums, communities and groups. the use of dialogue systems has become essential in people’s lives. the development of conversational agents is still an unresolved research problem that poses many challenges in the artificial intelligence (ai) community. this work aims to build an ai chatbot that is based on the principles and techniques of ai directed to natural language processing (nlp) and deep learning to help people newly diagnosed with dm in the self management of the disease. a literature search of dm education and information for people newly diagnosed with dm was con ducted. additionally, the main topics in which patients ask for support were retrieved from a search of several online support groups of dm, as well as questionnaires with 8 patients and interviews with 3 health professionals. the application were developed through the back-end side in python and the front-end side in react native and its communication was made through websockets. furthermore, an interaction and interface design are developed in this work using human-centered design (hcd) methodology. for that purpose, iterative usability test sessions were conducted with 12 users using think aloud methods and the system usability scale (sus). the chatbot developed is information retrieval (ir) type and answered questions asked by users in a human-like way. the result of the usability tests of the final version of the application was satisfactory (with a system usability scale (sus) score of 88) and users found the application quite intuitive and robust. further studies should concentrate on customizing the chatbot to each user by collecting information from prior interactions and verifying the impact of using this chatbot for newly diagnosed portuguese users with dm.",
      "atualmente, com a existência de várias condições crónicas de saúde sendo uma das principais a diabetes mellitus (dm), há um grande fardo que os pacientes devem ter para poderem cuidar de si mesmos. assim, para além dos pacientes buscarem procurar resolver as suas necessidades por meio da obtenção de informações junto aos profissionais de saúde, cada vez mais buscam informações e conselhos em fóruns, comunidades e grupos. o uso de sistemas de diálogo tornou-se essencial na vida das pessoas. o desenvolvimento de agentes conversacionais é ainda um problema de pesquisa não resolvido que apresenta muitos desafios na comunidade da inteligência artificial (ia). este trabalho visa construir um ia chatbot que é baseado nos princípios e técnicas de inteligência artificial (ia) direcionado a processamento de linguagem natural (pln) e aprendizagem profunda para ajudar pessoas recém-diagnosticadas com dm no autocuidado desta doença. neste trabalho, foi acompanhada uma pesquisa bibliográfica sobre a educação e informações da dm para pessoas recém-diagnosticadas com dm. além disso, foram abordados em vários grupos de apoio online relacionados com a dm os principais tópicos que os pacientes pedem apoio, a utilização de um questionário com 8 pacientes e entrevistas com 3 profissionais de saúde. a aplicação foi desenvolvida através do back-end em python e front-end em react native e a sua comunicação foi feita através de websockets. foi também desenvolvido um design de interação e interface através da metodologia human-centered design (hcd). para tal, foram realizadas sessões de testes iterativos de usabilidade com 12 participantes seguindo os métodos think aloud e system usability scale (sus). o chatbot desenvolvido é do tipo information retrieval (ir) e responde às perguntas feitas pelos utilizadores de forma humana. o resultado dos testes de usabilidade da versão final da aplicação foram satisfatórios (sus de 88) e os utilizadores acharam a aplicação bastante intuitiva e robusta. os estudos futuros devem concentrar-se na personalização do chatbot para cada utilizador, com a coleção de informações e de interações anteriores e na verificação do impacto da utilização deste chatbot para utilizadores portugueses recém-diagnosticados com dm."
    ],
    0.3
  ],
  [
    "software applications evolve over the years at a cost: their architecture modularity tends to be degraded. this happens mainly because software application maintenance often leads to architectural degradation. in this context, software architects need to elaborate strategies for detecting architectural degradation symptoms and thus maintaining the software architectural quality. the elaborations of these strategies often rely on tools with domain-specific languages (dsls), which help them to specify software architecture rules. these tools also enforce the adherence of these rules in the evolving program. however, their adoption in mainstream software development is largely dependent on the usability of the language. unfortunately, it is also often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. usability is a multi-faceted quality characteristic, which is challenging to quantify before a dsl is actually used by its stakeholders. there is even less support and experience on how to quantitatively evaluate the usability of dsls used in software maintenance tasks. to this end in this dissertation, a usability measurement framework was developed based on the cognitive dimensions of notations (cdn). the framework was evaluated both qualitatively and quantitatively using two textual dsls for architecture rules in the context of two evolving object-oriented systems. the results suggested that the proposed metrics were useful: (1) to early identify the dsl usability limitations to be addressed, (2) to reveal specific features of the dsls favoring software maintenance tasks, and (3) to successfully analyze eight usability dimensions that are critical in many dsls. however, along with these results this evaluation also revealed that this kind of tools lack support for communication among the stakeholders, creating a gap in the software development. to solve this problem we proposed heuristics for tools that use dsls for detecting architecture degradation symptoms. these heuristics will permit the exchange of information between the stakeholders, thereby, also increasing the tool usability. finally, we chose tamdera as the tool to implement these heuristics in our study domain. therefore, we implemented in the new version of tamdera the communication support for the stakeholders by using a new architecture and a new environment with the developed heuristics.",
    [
      "o reconhecimento automático de expressões faciais tem sido um tópico bastante ativo desde o início dos anos noventa. nos últimos anos, têm ocorrido bastantes avanços nas temáticas do tracking e da deteção de faces, nos mecanismos de extração de características faciais e nas técnicas de classificação das emoções. neste último campo, existem algumas propostas de jogos em que, através da deteção de faces, é possível detetar quem está a jogar e desse modo carregar automaticamente o perfil do jogador. igualmente, o reconhecimento das expressões faciais dos jogadores tem sido usado para alterar a expressão dos seus avatars. com a realização desta dissertação, pretenderam-se estudar várias formas de usar o reconhecimento da expressão facial de um jogador para alterar alguns parâmetros de um jogo, como a diminuição da sua dificuldade, por exemplo, quando se detetar que o jogador está stressado ou, em casos extremos, até sugerir a suspensão do jogo. o maior desafio que se encontra, neste caso, é o da deteção e classificação da expressão facial do jogador em tempo real, assim como a inclusão desta funcionalidade dentro do jogo. durante o período de desenvolvimento, foram estudados diversos métodos de deteção e classificação das emoções de um indivíduo e procurou-se a melhor forma de as conciliar com a construção do jogo, que foi desenvolvido para uma plataforma desktop. a abordagem tomada para a classificação das expressões faciais de um jogador passou pelo treino de modelos de aprendizagem, com pontos que representam as características da face. isso ocorreu com a ajuda de uma base de dados de fotografias tiradas a determinados sujeitos de teste, tendo sido determinados os pontos faciais para cada imagem e, de seguida, alimentados aos modelos de aprendizagem. após este processo, passa a ser possível a classificação em tempo real de uma expressão facial, bastando para tal fornecer-se ao modelo pré-treinado o conjunto dos pontos faciais do jogador, obtidos a cada frame. o passo seguinte consistiu na construção de um jogo que implementasse estas funcionalidades. com o modelo de classificação de expressões faciais a correr em paralelo ao módulo do jogo, foi possível determinar qual o estado emocional sentido pelo jogador durante certos intervalos de tempo. com esta informação, foram alterados determinados parâmetros do jogo, nomeadamente a dificuldade do mesmo. no que toca à classificação de expressões faciais, verificou-se uma eficácia elevada dos modelos de aprendizagem, através da utilização deste método. contudo, é preciso ter em conta que mesmo que seja possível definir um determinado conjunto de emoções, o que é certo é que a grande variedade de estados emocionais e a sua instabilidade ao longo do tempo acarretam um grande número de dificuldades. os parâmetros do jogo foram alterados de forma bastante satisfatória, embora ainda se possa proceder a uma exploração mais profunda sobre este aspeto, na medida em que as emoções expressas por um jogador dependem sempre de um grande número de fatores, nomeadamente do género do jogo.",
      "the automatic recognition of facial expressions has been a very active topic since the early nineties. in recent years, there have been many advances in the thematics of tracking and face detection, in the mechanisms of the extraction of facial features and in the techniques of the classification of emotions. in this field, there are some game proposals, in which by detecting faces, it is possible to detect who is playing and automatically load the player’s profile. likewise, the recognition of the player’s facial expressions has been used to alter the expression of their avatars. with the realization of this dissertation, we intended to study several ways to use the recognition of a player’s facial expression to change some parameters of a game, such as reducing it’s difficulty, for example, when it detects that the player is stressed or, in extreme cases, even suggesting the suspension of the game. the biggest challenge in this case is the detection and classification of the player’s facial expression in real time, as well as the inclusion of this functionality within the game. during the development period, we intended to study several methods of detecting and classifying the emotions of an individual and to seek the best way to reconcile them with the construction of the game, which was developed for a desktop platform. the approach adopted for the classification of the player’s facial expressions was been through the training of learning models with points that represent the characteristics of the face. this was done with the help of a database of photographs taken from certain test subjects, and the facial points were determined for each image and then fed to the learning models. after this process, it becomes possible to classify a facial expression in real time by simply providing the pre-trained model with a set of the player’s facial points, obtained from each frame. the next step was to build a game that implemented this functionality. with the facial expressions classification model running parallel to the game module, it was possible to determine which emotional state the player felt during certain time intervals. with this information, certain parameters of the game have been changed, as the difficulty of the same. regarding the classification of facial expressions, there was a high efficacy of the learning models through the use of this method. however, it must be known that even if it is possible to define a particular set of emotions, what is certain is that the great variety of emotional states and their instability over the time lead to a great number of difficulties. the parameters of the game have been changed quite satisfactorily, although a deeper exploration on this aspect can still be made, since the emotions expressed by a player always depend on a large number of factors, as the genre of the game."
    ],
    0.2571428571428571
  ],
  [
    [
      "roc (receiver operating characteristic) curve is a statistic tool that allows the evaluation of the accuracy of a classification system. these curves are drawn on a two-dimensional graph, with the ordinate representing the true positive fraction or sensitivity and the abscissa representing the false positive fraction or 1-specificity. the index that evaluates the accuracy of these graphs is represented by the area under the curve (auc) where the larger that area is the bigger the test performance is. its first appearance dates to the year of 1950. nevertheless, computationally , the first software only appeared around 1993 and since then several tools have been made available for its analysis. regarding the theoretical part of the subject, there is a vast bibliography existing which introduces all the necessary concepts to analyze a roc curve visually and statistically. however, only a few of those documents discuss the evaluation and the comparison of software that attain these same curves, consisting of old works in which the vast majority corresponds to software that when compared to the current scenario are outdated or fell out of use. the r software environment with a programming language mainly for statistical use is currently one of the best tools to perform the roc analysis. the variety of packages in this work environment make it an interesting study product, which allows us to take advantage of the different features in different the packages or enjoy the same features but by different means and formats. like r there are several tools that can perform this same analysis, as is the case of stata software, which receives regular updates that have been improving this tool recurrently. with the versatility of allowing us to work from a command line or through menus predefined by the software itself, it makes it a very accessible and convenient tool to explore. the r language is also related to the package called shiny, which can create browser applications through its own commands, making it possible to transpose the different commands of packages r into a single application. due to the wide variety of roc packages in r, it is interesting to link them to shiny. therefore, a library in the application format was designed to group the different packages on the same browser page. the result of this is rosy application available on https://pquintasbcl.shinyapps.io/rosy/. due to the increasing use of roc analysis in different systems, it is essential to explore the best computational methods to process it in a correct way. therefore, in this work the research and selection of different software/tools to perform this type of analysis is done, based on the different existing bibliographic documents in order to compare them and create a checklist, which will allow us to visualize the fundamental characteristics present in each software analyzed.",
      "a curva roc (receiver operating characteristic) é uma ferramenta estatística que permite avaliar o desempenho de um sistema de classificação. estas curvas são representadas num gráfico bidimensional, com a ordenada a representar a fração de verdadeiro positivos ou sensibilidade e a abcissa a representar a fração de falsos positivos ou 1-especificidade. o índice que avalia a medida de exatidão destes gráficos é a área abaixo da curva (auc) e quanto maior for a área maior é o desempenho do sistema em causa. a sua primeira aparição remete para o ano de 1950, contudo, computacionalmente o primeiro software terá surgido por volta de 1993 e, desde então, que têm sido disponibilizadas diversas ferramentas para a sua análise. relativamente à parte teórica do tema abordado, é disponibilizada uma vasta bibliografia capaz de introduzir todos os conceitos necessários para se conseguir analisar visualmente e estatisticamente uma curva roc. contudo, há poucos registos relativos à avaliação e comparação de software que produzem estas mesmas curvas, sendo trabalhos demasiado antigos, ou seja, na sua grande maioria foi utilizado software que comparativamente ao cenário real está desatualizado ou caiu em desuso. o r é um ambiente de trabalho com uma linguagem de programação destinada essen cialmente à estatística, sendo por isso atualmente, uma das melhores fontes para realizar análise roc. a variedade de packages existentes neste ambiente de trabalho torna-o num interessante produto de estudo, sendo possível tirar partido de diferentes funcionalidades em diferentes packages ou então usufruir das mesmas funcionalidades mas por meios e formatos distintos. em paralelo com o r existem diversas ferramentas capazes de realizar esta mesma análise, como é o caso do software stata, que sofre recorrentemente atualizações que têm aprimorado esta ferramenta. com a versatilidade de se poder trabalhar através de uma linha de comandos ou através de menus pré-definidos pelo próprio software torna-o bastante acessível e prático de explorar. o ambiente de trabalho r está ainda relacionado com um package denominado shiny, que possui a capacidade de criar aplicações browser através de comandos próprios, sendo assim possível transpor os diferentes comandos de packages r numa aplicação. devido à grande variedade de packages roc existentes neste ambiente de trabalho, torna-se interessante criar uma ligação dos mesmos com o shiny. sendo por isso, idealizada uma library no formato de aplicação para agrupar os diferentes packages numa mesma página web. o resultado é a aplicação rosy disponível em https://pquintasbcl.shinyapps.io/rosy/. devido à crescente utilização deste tipo de análise nos diferentes sistemas, torna-se fundamental explorar os melhores métodos computacionais para processar uma correta análise roc, pelo que, neste trabalho é efetuada a pesquisa e seleção de diferentes soft ware/ferramentas capazes de realizar este tipo de análise, tendo como base os diferentes registos bibliográficos já existentes e posteriormente a sua comparação e criação de uma checklist, que permitirá visualizar as características fundamentais presentes em cada software analisado."
    ],
    [
      "smartphones increasingly play an important role in everyday life, whether to communicate or perform tasks that require more computing power. this has become an indispensable object in the lives of many people so their use increases more and more. this growth is associated with an increase in network traffic due to the existing applications and internet services. this increase in traffic is also due to the development and growth of 3g and 4g mobile networks, which allow internet access when out of wi-fi networks. such access requires a mobile data plan that is normally limited to a certain level. expiring this plan, access to the internet from mobile networks is prohibited or limited to a certain rate which is often inadequate for the requirements of the applications used. this disturbance is commonly associated with the consumption of bandwidth in order to reproduce contents downloaded from the network. these contents are often associated with the normal operation of the applications, i.e., expected contents, however, there are contents that were not requested by the user, for instance advertisements contributing to data plan exhaustion. this justifies the need for studying and understanding the traffic involved between the user device and the network in order to assist the end user in identifying how much data has been consumed and distinguishing by the type of traffic involved. to answer this need, a systematic methodology is developed and proposed in this work considering as inputs popular user applications - youtube, facebook and instagram - with potencial impact on the user data plan. therefore, the present dissertation is a contribution in the field of traffic analysis and characterization, shedding light in the process of identifying and measuring traffic not requested by the user.",
      "os smartphones cada vez mais desempenham um papel importante no quotidiano, seja para apenas comunicar ou realizar tarefas que exijam mais poder computacional. este tornou-se um objeto indispensável na vida de muitas pessoas pelo que a sua utilização cada vez cresce mais. a este crescimento está associado um aumento de tráfego de rede devido às aplicações e serviços de internet atualmente existentes. este aumento de tráfego, deve-se também ao desenvolvimento e crescimento das redes móveis do tipo 3g e 4g, que permitem o acesso à internet quando se está fora de alcance de redes wi-fi. para tal acesso é necessário um plano de dados móveis que normalmente está limitado até um certo patamar. esgotando este plano, o acesso à internet a partir de redes móveis fica interdito ou é limitado a um certo débito muitas vezes bastante diminuto para as exigências das aplicações utilizadas. esta perturbação está associada ao consumo de largura de banda a fim de reproduzir conteúdos que venham da rede. estes conteúdos muitas vezes estão associados ao funciona-mento das aplicações, ou seja, tratam-se de conteúdo desejável, contudo, existem conteúdos aos quais não foram solicitados ao utilizador acabando por consumir o plano de dados. surge daqui o pretexto para a necessidade de um estudo e compreensão de todo o tráfego envolvido entre o dispositivo do utilizador e a rede que, vise mostrar ao utilizador final a identificar quantos dados consumiu e distinguir por tipo de tráfego envolvido. em resposta a esta necessidade, uma metodologia sistemática será desenvolvida e proposta no decorrer deste trabalho considerando como objetos de estudo aplicações populares - youtube, facebook e instagram - com potencial impacto no plano de dados do usufruidor. assim sendo, a presente dissertação é uma contribuição para o campo de análise e caracterização de tráfego, abrindo caminhos no processo de identificação e medição de tráfego não solicitado pelo utilizador."
    ],
    0.3
  ],
  [
    [
      "atualmente tem-se verificado um aumento significativo da quantidade de informação armazenada digitalmente, devido a cada vez mais as infraestruturas de saúde recorrerem a sistemas digitais para armazenar a informação relativa aos intervenientes no seu universo. é, portanto, fulcral que essa informação seja regida por um conjunto de normas, de modo a permitir que seja compreendida sem se perderem dados importantes. com o aumento do uso de ferramentas digitais para armazenamento e troca de informação também se torna importante a monitorização dos dados trocados entre os sistemas, de modo a garantir a privacidade e segurança dos intervenientes, bem como garantir o bom funcionamento do sistema. nesse sentido, o trabalho consistiu na implementação de uma ferramenta para a monitorização de mensagens trocadas na plataforma aida - agência de interoperação, difusão e arquivo - de maneira a contribuir de forma ativa para resolver as questões abordadas no rgpd. esta ferramenta é baseada em tecnologias como containers docker, a base de dados elasticsearch e a interface de monitorização kibana.",
      "there has been a significant increase in the quantity of information that is stored digitally, as health infrastructures increasingly use digital systems to store information about the actors in their universe. it is therefore crucial that this information is governed by a set of rules in order to allow it to be understood without losing important data. with the increasing use of digital tools for storing and exchanging information, it is also important to monitor the information that is exchanged between systems to ensure the privacy and security of stakeholders, as well as ensuring the proper functioning of the system. therefore, the purpose of this work was to implement a tool to monitor messages exchanged on the agência de interoperação, difusão e arquivo (aida) platform, in order to actively contribute to solve the issues addressed in the regulamento geral de proteção de dados (rgpd). this tool is based on technologies like docker containers, the elasticsearch database and kibana, as a monitor interface."
    ],
    [
      "the movement of people from dispersed living to concentration in urban environments is a large change both for human civilization and for the environment. urbanization is the process of changing from natural habitats to dense grey space made up primarily of buildings, roads, and accessory infrastructure accompanied by dense human populations. while many cities are well established, humans continue to build new cities or expand cities outward in a network of suburban environments. and urbanization is not simply about a transition from green to grey space, other abiotic changes such as changes in light regimens due to artificial lighting, increased pollution, and increased impervious surfaces leading to runoff are found in urban areas. the study of urban evolution of fafe in the xix and xx centuries is an interesting theme not only because of the lack of works in this area but also because of the possibility of understanding the organization of the current city. the main problem that we faced it was that as the years go, the mapping and buildings of cities change. and the information of these changes is stored in texts, records, maps, etc. this fact made the study of urban evolution difficult because the information is widespread and hard to gather. so, in order to study fafe urban evolution we needed to recover and gather information of the changes and new buildings in the city during the xix and xx centuries. given the inexistence of an exhaustive investigation of an urban history we had to seek to interpret from the present formation the successive processes of urbanization and respective extensions, juxtapositions and overlaps. more important is the diverse set of sources that allowed to characterize the urbanism of the city of fafe. with that said it was important to create an integrated repository in digital format to enable its analysis and search of information, and visual exploration through a map. for that purpose, it was necessary to create ontologies related to urban evolution that allowed us to develop web-supported tools derived from these ontologies for the acquisition of the state and the location of the buildings and in order to analyze the changes as the years go by. the web-supported tools are available in http://www4.di.uminho.pt/∼gepl/uef/.",
      "o movimento de pessoas de zonas rurais para zonas de concentração urbana é uma grande mudança tanto para a civilização humana quanto para o meio ambiente. a urbanização é o processo de transformação de zonas naturais em zonas densas e cinzas, compostas principalmente por edifícios, estradas e infraestruturas, acompanhadas por densas populações humanas. embora muitas cidades estejam bem estabelecidas, os humanos continuam a construir novas cidades ou a expandi-ias para fora em uma rede de ambientes suburbanos. e a urbanização não é simplesmente sobre a transição do espaço verde para o cinza, outras mudanças abióticas, como mudanças na luz devido à iluminação artificial, aumento da poluição e aumento das superfícies impermeáveis que levam ao escoamento são encontradas nas áreas urbanas. o estudo da evolução urbana de fafe nos séculos xix e xx é um tema interessante não só pela falta de trabalhos nesta área, mas também pela possibilidade de compreender a organização atual da cidade. o principal problema que nos enfrentamos foi que com o passar dos anos, o mapeamento e os edifícios das cidades mudam. e as informações dessas mudanças são armazenadas em textos, registos, mapas, etc. esse fato dificultou o estudo do desenvolvimento urbano, pois a informação está difundida e é difícil de reunir. para estudar a evolução urbana de fafe, foi necessário recuperar e coletar informações sobre as mudanças e novos edifícios da cidade durante os séculos xix e xx. dada a inexistência de uma investigação exaustiva da história urbana, tivemos que procurar interpretar da formação atual os sucessivos processos de urbanização e respetivas extensões, justaposições e sobreposições. mais importante é o conjunto diversificado de fontes que permitem caracterizar o urbanismo da cidade de fafe. com isso dito, foi importante criar um repositório integrado em formato digital para permitir sua análise, a pesquisa de informação e a exploração visual através de um mapa. para esse efeito, foi necessário criar ontologias relacionadas com a evolução urbana que nos permitiram desenvolver ferramentas suportadas pela web derivadas destas mesma ontologias, para a aquisições do estado e a localização dos edifícios e para analisar as mudanças á medida que os anos passam. as ferramentas suportadas pela web estão disponíveis em http:www4.di.uminho.p/~gepl/uef/."
    ],
    0.06666666666666667
  ],
  [
    [
      "o acidente vascular cerebral (avc) foi, em 2020, a segunda principal causa de morte no mundo e primeira no que toca a incapacidade. com a motivação de contribuir para ajudar a reduzir os números que são alarmantes e continuam a crescer, surge este projeto, do qual se pretende que resultem modelos que possam tentar prever se um indivíduo irá, ou não, ser vítima deste problema e descobrir quais as suas características ou dados clínicos que mais influenciam esta previsão, pois, segundo a sociedade portuguesa de medicina interna (spmi), 80% dos casos podem ser prevenidos[1]. para o efeito, o projeto a desenvolver incluirá uma recolha e tratamento de datasets que organizem dados clínicos de vários pacientes e a incidência desta problemática, um estudo acerca das técnicas e algoritmos de machine learning mais adequados aos modelos a desenvolver, sendo depois aplicados através de modelos de data mining (dm), dando uso a ferramentas como weka e rapidminer, para indução dos modelos de previsão, assim como algoritmos em linguagens como python e r, conjugando, assim, os factos de que \"o setor da saúde é rico em informação, e o data mining está a tornar-se uma necessidade\"[2]. finalmente, estes modelos serão testados, validados e comparados, do qual resulta esta dissertação.",
      "stroke was, in 2020, the second leading cause of death in the world and the first in terms of disability. with the motivation of contributing to help reduce these alarming numbers, which continue to grow, this project arose, which aims to produce models that can try to predict whether or not an individual will be a victim of this problem and discover which characteristics or clinical data most influence this prediction, since, according to the portuguese society of internal medicine (spmi), 80% of cases can be prevented[1]. to this end, the project to be developed will include the collection and processing of datasets that organize clinical data from several patients and the incidence of this problem, a study on the techniques and algorithms of machine learning more suitable for the models to be developed, these will then be applied through data mining (dm) models, using tools such as weka and rapidminer to induce the prediction models, as well as algorithms in languages such as python, thus combining the facts that \" because healthcare sector is rich with information, and data mining is becoming a necessity”[2]. finally, these models will be tested, validated and compared, resulting in this dissertation."
    ],
    [
      "as instituições de saúde vivem atualmente num ambiente de crescente densidade de informação, sendo esta uma área de intensa transferência eletrónica de dados. como resultado, tem-se recorrido cada vez mais às interfaces de utilizador como forma de auxilio no processamento de dados. tratando-se de uma questão inovadora, o surgimento de novas tecnologias de informação e comunicação tem sido um motor para o desenvolvimento de novos tipos de interfaces de utilizador. com o aumento do volume de informação, tem surgido problemas no que diz respeito ao seu processamento. cada vez mais são exigidos que os serviços sejam prestados com maior eficiência, implicando maior rapidez na obtenção de dados. é neste contexto que surge a necessidade de direcionar os novos avanços no campo da saúde, tendo como linha orientadora a busca de um serviço mais eficiente e com maior qualidade. neste sentido surge a necessidade de optimização dos processos de acesso e utilização dessa informação, alterando a forma como a informação sobre os utentes é obtida. é, essencialmente, nestes pontos que se foca esta dissertação. adicionalmente, e do ponto de vista funcional, pode dizer-se que estes desenvolvimentos apresentam características favoráveis à prevenção e ao controlo de infeções hospitalares, reduzindo a necessidade de contato direto entre os objetos, o que leva, à diminuição da propagação das ditas infeções. com a concretização deste estudo procura-se avaliar as potencialidades do reconhecimento gestual aplicado às interfaces de utilizador, na sua implementação na área específica da saúde. paralelamente desenvolveu-se um protótipo destinado aos utentes que frequentem as instituições de saúde. este protótipo teve como objetivo a validação das interfaces de utilizador analisadas, considerando a utilização das tecnologias recentemente introduzidas no mercado. foi dada especial atenção a interfaces de utilizador sem contato entre dispositivos e utilizadores (e.g. utilizando a tecnologia do kinect da microsoft). no final é apresentado um estudo estatístico relativo à avaliação por parte dos utilizadores da interface do protótipo desenvolvido, onde se conclui a funcionalidade da utilização de gestos se revelou intuitiva e de fácil execução.",
      "health institutions are currently living in an environment of increasing information density, being considered an area of intense electronic transfer of data. as a result, there has been an increasing use of the user interfaces in order to aid data processing. since this is a matter innovative, the emergence of new information and communication technologies has been a catalyst for the development of new types of user interfaces. with the increasing volume of information, problems have arisen with regard to their processing. increasingly it is being required the services to be delivered with more effectiveness resulting in faster data retrieval. it’s in this context that arises the need to drive new advances in healthcare, having as a guideline the search for a service with more efficient and higher quality. in this way occurs the need to optimize the process of access and use of this information, changing the way that the patient’s information is obtained. this dissertation focuses essentially on these points. additionally it can be said that these developments have characteristics favourable to the prevention and control of hospital infections, reducing the need for direct contact with the objects, which leads to decreased spread of infections said. this study aim to evaluate the potential of gesture recognition applied to the user interfaces implemented specifically in the healthcare area. alongside it was developed a prototype intended to the patients that usually attend to healthcare institutions. the goal of this prototype is to validate the user interfaces proposals, considering the use of technology recently introduced on the market. it was given special attention to user interfaces without contact between users and devices (e.g. using the technology of the microsoft kinect). at the end it is presented a statistical study on the evaluation of the user interface prototype by users, in which it is proved that the functionality of using gestures is intuitive and easy to perform."
    ],
    0.0
  ],
  [
    [
      "a capacidade de agregar dados é uma característica fundamental na conceção de sistemas de informação escaláveis, que permite a determinação de propriedades globais importantes de forma descentralizada, para a coordenação de aplicações distribuídas, ou para fins de monitorização. agregados simples como mínimos/ máximos, contagens, somas e médias foram já extensivamente estudados no passado. no entanto, este tipo de agregados pode não ser suficiente para caracterizar distribuições de dados enviesadas e na presença de valores atípicos (outliers), tornando-se então relevante a determinação de uma estimativa dos valores na rede (e.g. histograma, função de distribuição cumulativa), dado que métricas como médias ou desvio padrão escondem em muitos casos alterações na propriedade monitorizada que são relevantes para decisão de controlo. são ainda relativamente escassos os trabalhos que se focam sobre a agregação de métricas mais expressivas. uma proposta recente nesse domínio [snsp10] refere atingir uma precisão nas estimativas superior à atingida em abordagens anteriores. trata-se de um algoritmo para a determinação de funções cumulativas de distribuições. apesar do contributo, essa proposta mostra limitações na tolerância a faltas e no suporte à monitorização contínua de propriedades, dado que para acompanhar alterações dos valores amostrados, a estratégia usada exige que o protocolo seja reiniciado periodicamente. para além disso, os pressupostos dessa abordagem não admitem a perda de mensagens nem a sua duplicação. assim, e tomando como ponto de partida o actual estado da arte, é apresentado nesta tese um algoritmo distribuído para a determinação de funções cumulativas de probabilidade em redes de larga escala. as suas principais vantagens são a imunidade à perda de mensagens, a velocidade de convergência e a precisão que se obtém na aproximação à distribuição original. é simultaneamente adaptável a alterações no valor amostrado e resiliente a dinamismo no número de nodos na rede. usa também um mecanismo de quiesciência dos nodos assim que a variação local da estimativa é inferior a um determinado limiar. nessa circunstância, o nodo deixa de transmitir. isto leva à diminuição do número de mensagens trocadas entre nodos. as distribuições determinadas em todos os nodos permitem a tomada de decisões que tirem partido do facto de se estar a agregar uma função probabilística. assim o nodo pode excluir outliers ou observar determinados quantis da propriedade. para além disso, cada nodo da rede possui uma estimativa global sobre o estado geral da propriedade distribuída, o que lhe permite também a tomada de decisões com base em conhecimento local. são apresentados nesta tese resultados de simulação que confirmam a validade da abordagem seguida. é também apresentada uma revisão da literatura relacionada cujo âmbito incluiu as técnicas mais representativas da agregação de dados para métricas escalares e as técnicas de agregação de dados para métricas complexas.",
      "the ability to aggregate data is a fundamental feature in the design of scalable information systems, which allows the estimation of relevant global properties in a decentralized way in order to coordinate distributed applications, or for monitoring purposes. simple aggregates such as minima/ maxima, counts, sums and averages have been thoroughly studied in the past. nonetheless, this kind of aggregates may not be comprehensive enough to characterize biased data distributions and in presence of outliers, making the case for richer estimates of the values on the network (e.g. histograms, cumulative distributed functions), since scalar metrics like average or standard deviation hide in many cases changes in the property that are relevant to the control decision. the amount of scienti c work is relatively scarce in what concerns more expressive aggregation metrics. a recent proposal within this domain [snsp10] claims to obtain estimates with a better precision than in previous approaches. it is an algorithm for the estimation of cumulative distribution functions. despite the contribution, the proposal mentioned above is not fault tolerant and is also not sensible to the continuous variation of the sampled properties, for it demands the protocol to be restarted frequently in order to achieve quasi-continuous monitoring. besides, the approach does also not admit loss or duplication of messages. having this scenario as a starting point, this work presents a distributed algorithm for the estimation of cumulative distribution functions over large scale networks of which the main advantages are immunity to message loss, convergence speed and precision of the estimate. it can also cope with changes of the sampled property and is resilient to churn. it has also a quiescence mechanism that allows nodes to minimize communication cost by not exchanging redundant messages, whenever local variations of the estimate fall below a speci ed threshold. the estimated cumulative distribution function allows nodes to take advantage of having a broader view of the properties on the network: they may exclude outliers or monitor particular quantiles of a property. also, each and every node of the network has a local vision of the global state of the property, thus allowing nodes to make decisions based on local knowledge. this thesis presents simulation results that support and validate the proposed approach. it also presents a state of the art that includes both representative techniques for scalar aggregates and representative techniques for complex aggregates."
    ],
    [
      "reinforcement learning has had many recent achievements and is becoming increasingly more relevant in the scientific community. as such, this work uses quantum computing to find potential advantages over classical reinforcement learning algorithms, using bayesian networks to model the considered decision making environments. for this purpose, this work makes use of quantum rejection sampling, a quantum approximate inference algorithm for bayesian networks proposed by low et al. [2014] with a quadratic speedup over its classical counterpart for sparse networks. it is shown that this algorithm can only provide quantum speedups for partially observable environments, and a quantum-classical hybrid lookahead al gorithm is presented to solve these kinds of problems. moreover, this work also includes both sample and computational complexity analysis of both this quantum lookahead algorithm and its classical alternative. while the sample complexity is shown to be identical for both algorithms, the quantum approach provides up to a quadratic speedup in computational complexity. finally, the potential advantages of this new algo rithm are experimentally tested in different small experiments. the results show that this speedup can be leveraged either to improve the rational decision-making skills of agents or to reduce their decision-making time due to the reduction in computational complexity.",
      "a aprendizagem por reforço tem recentemente alcançado muito sucesso e a tornar-se cada vez mais relevante na comunidade científica. este trabalho tira proveito da computação quântica para encontrar potenciais vantagens do seu uso comparativamente a algoritmos clássicos de aprendizagem de reforço. nesta procura por vantagens, são utilizadas redes bayesianas para modelar os ambientes de tomada de decisão considerados. para este propósito, é utilizado o algoritmo de quantum rejection sampling, um algoritmo para inferência aproximada em redes bayesianas proposto por low et al. [2014] com um speedup quadrático comparativamente ao equivalente clássico para redes esparsas. é mostrado que este algoritmo quântico de inferência apenas tem vantagem na sua aplicação a ambientes parcialmente ob serváveis, e é apresentado um algoritmo híbrido clássico-quântico de lookahead para resolver este tipo de problemas. para além disto, é também incluída uma análise da complexidade de amostragem e complex idade computacional de ambos os algoritmos. enquanto a complexidade de amostragem é idêntica para as duas abordagens, o algoritmo quântico apresenta um speedup na complexidade computacional que é quadrático no melhor dos casos. por fim, as potenciais vantagens deste novo algoritmo são testadas em experiências de pequena dimensão. os resultados mostram que este speedup pode ser utilizado tanto para melhorar a capacidade de tomada de decisão de agentes como para diminuir o tempo de tomada de decisão dos mesmos devido à redução da complexidade computacional."
    ],
    0.0
  ],
  [
    [
      "one of the biggest problems for business owners, of both big and small companies, is the correct forecast of its financial evolution. this means that the owners in question need to spend a lot of time studying the economic issues that involve these situations and/or pay specialized workers to do a thorough study on the area. in addition, it becomes a constant, chronic and seasonal problem that demands to be solved. if there was a prediction system capable of forecasting and advising the owner of the company with enough precision to be reliable, one could save time, money and, above all, test or simulate the future with different parameters. this project then aims to complete a framework capable of encapsulating all the data-science work and knowledge needed to create good enough machine learning models that provide these predictions even to those without data-science knowledge and to any generic business. this framework goes by the name generic predictive machine (gpm). its performance, both in terms of accuracy of results and execution time in various different situations, as well as the process leading to its development and the development itself are thoroughly documented. the problem of having to create a generic system arises as every business’ data is inherently different and assumptions cannot be made on the developer’s end. therefore the final application allows for the solving of a vast array of problems even if not related to economics and requires little knowledge about machine learning. the problem as a whole serves too as a vehicle for a deeper study on machine learning itself. touching upon important aspects like, how far can such a system learn beyond the capabilities of the human who programmed it, how faster can it do it and what can it teach us about the inherent patterns in data that sometimes remain unnoticed to the human eye. in the process of answering these questions many works on the area are referenced that touch specific points of the theory behind machine learning, a final novel conclusion is then derived from the knowledge found among all related works as to take a step forward in the theory of machine learning as a whole.",
      "um dos maiores problemas para empresários, tanto de grandes como de pequenas empresas, e a previsão correta da sua evolução financeira. isso significa que os empresários em questão precisam de despender bastante tempo a estudar as questões económicas que envolvem essas situações e/ou de pagar a trabalhadores especializados para fazerem um estudo completo e profundo sobre a área. além disso, torna-se um problema constante, crónico e sazonal que exige ser resolvido. se houvesse um sistema preditivo capaz de aconselhar o proprietário da empresa com precisão suficiente para ser confiável, poder-se-ia economizar tempo, dinheiro e, acima de tudo, testar e simular o futuro com diferentes parâmetros. este projeto tem então o objetivo de construir uma ferramenta informática, de nome generic predictive machine (gpm), capaz de encapsular todo o trabalho e conhecimento necessário para construir modelos de machine learning bons o suficiente para que utilizadores sem conhecimento profundo na área possam realizar previsões sobre o futuro financeiro de uma empresa genérica. o desempenho desta ferramenta, tanto em termos de acurácia dos resultados como de tempo de execução, assim como o processo que levou ao seu desenvolvimento estão então completamente documentados ao longo desta dissertação. este problema levanta ainda a necessidade de criar um sistema preditivo genérico, já que os dados de cada empresa são inerentemente diferentes e, por isso, não podem ser feitas nenhum tipo de assunções por parte do programador. desta forma, a aplicação final permite aos seus utilizadores solucionar uma vasta gama de problemas mesmo que estes não estejam relacionados com economia e não exige grande conhecimento prévio de machine learning. o problema como um todo serve também como um veículo para um estudo mais aprofundado sobre machine learning, tocando em aspetos importantes como, até onde pode a maquina aprender para além das capacidades do humano que a programou, quão mais rápido a máquina pode aprender e o que a máquina nos pode ensinar sobre os padrões inerentes aos dados que recebe e que por vezes permanecem despercebidos ao olhar humano. ao responder a estas questões, muitos trabalhos que abordam pontos específicos da teoria por trás de machine learning são referenciados, chegando-se assim a uma conclusão final derivada do conhecimento encontrado entre todos estes estudos relacionados com o intuito de dar um passo em frente na teoria de machine learning como um todo."
    ],
    [
      "the continuous joint growth of the communication networks is causing a simultaneous increase in the energy consumption of these infrastructures. to help fight the consequent environmental impact is required that new traffic engineering techniques are developed to help mitigate this energy consumption. this work proposes the flexcomm simulator 1 , a simulation environment with the intent of creating a platform that helps to develop new routing algorithms, that combine software defined networking (sdn) and energy flexibility techniques to optimize the energy consumption of large scale wired communication networks. the environment allows simulating real infrastructures conditions, so policies can be developed close to real scenarios, facilitating their deployment in real environments. the tool’s abilities have been demonstrated, by generating data that allows the evaluation of different routing strategies. moreover, the flexcomm simulator has made possible the developments on early work of new algorithms that demonstrate the ability to achieve a more balanced energy consumption. these new techniques are capable of adapting to changes in energy availability and relocating network flows across different regions of a network, respecting flexibility imposed by electrical grids while maintaining a minimum quality of service (qos).",
      "o contínuo crescimento conjunto das redes de comunicação está a causar simultaneamente um aumento no consumo energético destas infraestruturas. para ajudar a combater o consequente impacto ambiental, é necessário que novas técnicas de engenharia de tráfego sejam desenvolvidas para ajudar a mitigar este consumo energético. este trabalho propõe o flexcomm simulator, um ambiente de simulação com o intuito de criar uma plataforma que ajude no desenvolvimento de novos algoritmos de routing, que combinem técnicas de sdn e flexibilidade energética para otimizar consumo energético em redes de comunicação de larga escala. o ambiente permite simular condições de infraestruturas reais, de forma a que políticas possam ser desenvolvidas o mais perto possível da realidade, facilitando a sua adaptação em ambientes reais. as capacidades da ferramenta foram demonstradas, gerando informação que permite avaliar diferentes estratégias de routing. para além disso, o flexcomm simulator tornou possível o desenvolvimento de trabalho inicial em novos algoritmos que demonstram a capacidade de atingir um consumo energético mais balanceado. estas novas técnicas demonstram a capacidade de se adaptarem a mudanças na disponibilidade energética e de realocarem fluxos de comunicação ao longo de vários pontos de uma rede, respeitando a flexibilidade imposta pela rede elétrica enquanto mantendo níveis mínimos de qualidade de serviço."
    ],
    0.3
  ],
  [
    [
      "in wireless sensor networks (wsns), typically composed of nodes with resource constraints, leveraging efficient processes is crucial to enhance the network longevity and consequently the sustainability in ultra-dense and heterogeneous environments, such as smart cities. epidemic algorithms are usually efficient in delivering packets to a sink or to all it’s peers but have poor energy efficiency due to the amount of packet redundancy. directional algorithms, such as minimum cost forward algorithm (mcfa) or directed diffusion, yield high energy efficiency but fail to handle mobile environments, and have poor network coverage. this work proposes a new epidemic algorithm that uses the current energy state of the network to create a topology that is cyclically updated, fault tolerant, whilst being able to handle the challenges of a static or mobile heterogeneous network. depending on the application, tuning in the protocol settings can be made to prioritise desired characteristics. the proposed protocol has a small computational footprint and the required memory is proportional not to the size of the network, but to the number of neighbours of a node, enabling high scalability. the proposed protocol was tested, using a esp8266 as an energy model reference, in a simulated environment with ad-hoc wireless nodes. it was implemented at the application level with udp sockets, and resulted in a highly energy efficient protocol, capable of leveraging extended network longevity with different static or mobile topologies, with results comparable to a static directional algorithm in delivery efficiency.",
      "em redes de sensores sem fios (rsf), tipicamente compostas por nós com recursos lim-itados, alavancar processos eficientes é crucial para aumentar o tempo de vida da rede e consequentemente a sustentabilidade em ambientes heterogéneos e ultra densos, como cidades inteligentes por exemplo. algoritmos epidêmicos são geralmente eficientes em en-tregar pacotes para um sink ou para todos os nós da rede, no entanto têm baixa eficiência energética devido a alta taxa de duplicação de pacotes. algoritmos direcionais, como o mcfa ou de difusão direta, rendem alta eficiência energética mas não conseguem lidar com ambientes móveis, e alcançam baixa cobertura da rede. este trabalho propõe um novo protocolo epidêmico que faz uso do estado energético atual da rede para criar uma topologia que por sua vez atualizada ciclicamente, tolerante a falhas, ao mesmo tempo que é capaz de lidar com os desafios de uma rede heterogênea estática ou móvel. a depender da aplicação, ajustes podem ser feitos às configurações do protocolo para que o mesmo priorize determinadas características. o protocolo proposto tem um pequeno impacto computacional e a memória requerida é proporcional somente à quantidade de vizinhos do nó, não ao tamanho da rede inteira, permitindo assim alta escalabilidade. o algoritmo proposto foi testado fazendo uso do modelo energético de uma esp8266, em um ambiente simulado com uma rede sem fios ad-hoc. foi implementado à nível aplicacional com sockets udp, e resultou em um protocol energeticamente eficiente, capaz de disponibilizar alta longevidade da rede mesmo com diferentes topologias estáticas ou móveis com resultados comparáveis à um protocolo direcional em termos de eficiência na entrega de pacotes."
    ],
    [
      "o objectivo do trabalho proposto passa por analisar a viabilidade de esquemas de assinatura digital “low-algebra” derivados de um dos algoritmos sphincs+ ou picnic recorrendo cada um a primitivas lwc das famílias ascon e skinny. para esse efeito comerçar-se-á por implementar cada um desses algoritmos em c seguindo-se uma seleção de quais as variáveis que melhor se adequam a cada esquema. posteriormente realizar-se-á uma análise de complexidade computacional das eventuais implementações em “software”, “hardware” ou hibrida.",
      "the objective of this thesis is the viability analysis of the “low-algebra” digital signature schemes sphincs+ and picnic using a lwc (lightweight cryptography) primitive of the ascon or skinny families. to achieve this objective a c implementation will be coded for each combination of the algorithms and afterwards a selection will be made regarding the variables that best suit each of the schemes. after selecting the best variables for each algorithm they will be analysed for their computational complexity regarding either a software, hardware or hybrid implementation."
    ],
    0.3
  ],
  [
    [
      "this thesis intends to describe our approach towards developing a framework for the interactive creation of music driven animations. we aim to create an integrated environment where real-time musical information is easily accessible and is able to be flexibly used for manipulating different aspects of a reactive simulation. such modifications are specified through the use of a scripting language and include, for instance, geometrical transformations and geometry synthesis, gradual colour changes as well as the application of arbitrary forces. our framework thus represents a proof-of-concept for converting musical information into arbitrary modifications to a dynamic simulation, producing a variety of animations. this is possible due to a bargaining between control and automation, where control is present by allowing the user to program these modifications with a scripting language and automation is present by using physics and interpolation to estimate the visual effects resulting from those modifications. the particular test case for our system was the animation/simulation of a growing tree reacting to wind. in order to control or influence both the tree growth and wind field, as well as other visual parameters, the system accepts two different but complementary representations of music: a midi event stream and raw audio data. different musical features are obtainable from each of these representations. on one hand, by using midi, we are able to discretely synchronise visual effects with the basic elements of music, such as the sounding of notes or chords. on the other, using audio, we are able to produce continuous changes by obtaining numerical data from basic spectral analysis. our framework provides a common interface for the combined application of these different sources of musical information to the generation of visual imagery, under the form of procedural animations. we will describe algorithms presented in multiple research papers, namely for tree generation, wind field generation and tree reaction to wind, briefly detailing our implementation and architecture. we also describe why each of these particular methods was chosen, how they are organised in our platform and how their parameters may be modified from our scripting environment leading to what we regard as the procedural generation of animations. by allowing the user to access musical information and give them control of what we have come to refer to as animation primitives, such as wind and tree growth, we believe to have taken a first step towards exploring a novel concept with a seemingly endless expressive potential.",
      "esta dissertação descreve o desenvolvimento de uma plataforma para a criação interativa de animações dirigidas por música. focamo-nos em desenvolver um ambiente integrado onde vários aspetos de uma animação podem ser controlados pelo processamento em tempo real de informação musical, com recurso a uma linguagem de script. o caso de teste específico da nossa aplicação consiste na animação de uma árvore em crescimento capaz de reagir a um campo de vento dinâmico. de forma a controlar ou influenciar quer o crescimento da árvore, quer o campo de vento, o sistema aceita como input duas representações diferentes, mas complementares, de música, uma sequência contínua de eventos midi e áudio. realçamos a distinção entre estas duas representações visto que apesar de serem ambas referentes a música, são fundamentalmente diferentes em termos da informação que contêm. eventos midi contêm informação simbólica relativa à interpretação da música, nomeadamente os tempos de começo e final de notas. por outro lado, informação áudio consiste num sinal contínuo, que resulta da gravação de um instrumento ou de uma atuação musical. com midi, a nossa plataforma é capaz de sincronizar alterações discretas à simulação, com base nos elementos fundamentais da teoria musical, como o soar de notas ou acordes. com informação áudio, é possível produzir alterações contínuas com base nos dados numéricos obtidos por análise espectral elementar do sinal de áudio. neste documento serão descritos vários algoritmos apresentados em artigos de investigação, nomeadamente para a geração de árvores, geração de campos de vento e reação da árvore ao vento. iremos descrever os motivos que levaram à sua escolha, a sua organização na nossa plataforma e os vários parâmetros que podemos modificar a partir do nosso ambiente de scripting. em suma, a nossa plataforma pode ser descrita como um sistema que converte informação musical em alterações arbitrárias a um ambiente, que por sua vez influencia uma simulação reativa, produzindo animações. foi estabelecido um compromisso entre controlo e automação de forma a tornar esta abordagem possível. o controlo provém da capacidade de programar as modificações que ocorrem no sistema, sendo que é utilizada automação de forma a estimar o movimento resultante de tais modificações. ao fornecer ao utilizador informação musical em tempo real e oferecer-lhe controlo sobre o que nos referimos como \"primitivas de animação\", como o controlo sobre vento e o crescimento da árvore, consideramos que demos um primeiro passo no que toca à exploração de um novo conceito, com um potencial expressivo aparentemente infinito."
    ],
    [
      "with the fast evolution of the internet over the last years, the top priority on software development has shifted from what? to when?. reduced time-to-market is now the competitive edge that all companies strive for. the usage of container-based virtualization technologies keep the multiple environments where a development team works similar enough, that their work is made easier when devel oping and testing new features, which in turn results in a significantly faster delivery. the nature of this tecnhology also brings numerous advantages when it comes to management, monitoring and maintaining resources, allowing for an ease of adjustment, based on the client needs. throughout this dissertation is presented an extended base of knowledge about container technologies, especially docker, as well as what are the basic techniques to use when building an application inside such infrastructure, from the writing of the dockerfile to the adaptation of the multiple pipelines responsible to deploy the application.",
      "com a rápida evolução da internet nos últimos anos, a prioridade máxima no desenvolvi-mento de software transformou-se de \"o quê?\" para \"quando?\". disponibilizar rapidamente uma aplicação no mercado é agora a vantagem competitiva que todas as empresas ambi-cionam ter. a utilização de tecnologias de virtualização através de containers uniformizam os vários ambientes em que a equipa de desenvolvimento opera, facilitando assim o seu trabalho no que diz respeito à adição e teste de novas funcionalidades, o que resulta numa entrega significativamente mais rápida. a natureza desta tecnologia trás inúmeras vantagens à gestão, monitorização e manutenção de recursos, permitindo facilmente aumentar ou reduzir os mesmos baseado nas necessidades dos seus clientes. nesta dissertação é apresentada uma extensa base de conhecimento sobre as tecnologias de containerização, em especial o docker, bem como quais as técnicas base a utilizar quando se pretende construir uma aplicação com uma infraestrutura deste tipo, desde a escrita do dockerfile, até à adaptação das várias pipelines responsáveis por disponibilizar a aplicação em ambiente de produção."
    ],
    0.0
  ],
  [
    [
      "sistemas de eventos discretos (des) são uma importante subclasse de sistemas (à luz da teoria dos sistemas). estes têm sido usados, particularmente na indústria para analisar e modelar um vasto conjunto de sistemas reais, tais como, sistemas de produção, sistemas de computador, sistemas de controlo de tráfego e sistemas híbridos. o nosso trabalho explora uma extensão de des com ênfase nos processos estocásticos, comummente chamado como sistemas de eventos discretos estocásticos (sdes). existe assim a necessidade de estabelecer uma abstração estocástica através do uso de processos semi-markovianos generalizados (gsmp) para sdes. assim, o objetivo do nosso trabalho é propor uma metodologia e um conjunto de algoritmos para aprendizagem de gsmp, usar técnicas de model-checking estatístico para a verificação e propor duas novas abordagens para teste de des e sdes (respetivamente, não estocasticamente e estocasticamente). este trabalho também introduz uma noção de modelação, analise e verificação de sistemas contínuos e modelos de perturbação no contexto da verificação por model-checking estatístico.",
      "discrete event systems (des) are an important subclass of systems (in systems theory). they have been used, particularly in industry, to analyze and model a wide variety of real systems, such as production systems, computer systems, traffic systems, and hybrid systems. our work explores an extension of des with an emphasis on stochastic processes, commonly called stochastic discrete event systems (sdes). there was a need to establish a stochastic abstraction for sdes through generalized semi-markov processes (gsmp). thus, the aim of our work is to propose a methodology and a set of algorithms for gsmp learning, using model checking techniques for verification, and to propose two new approaches for testing des and sdes (non-stochastically and stochastically). this work also introduces a notion of modeling, analysis, and verification of continuous systems and disturbance models in the context of verifiable statistical model checking."
    ],
    [
      "biomedical text mining (btm) seeks to derive high-quality information from literature in the biomedical domain, by creating tools/methodologies that can automate time-consuming tasks when searching for new information. this encompasses both information retrieval, the discovery and recovery of relevant documents, and information extraction, the capability to extract knowledge from text. in the last years, silicolife, with the collaboration of the university of minho, has been developing @note2, an open-source java-based multiplatform btm workbench, including libraries to perform the main btm tasks, also provid ing user-friendly interfaces through a stand-alone application. this work addressed the development of a web-based software platform that is able to address some of the main tasks within btm, supported by the existing core libraries from the @note project. this included the improvement of the available restful server, providing some new methods and apis, and improving others, while also developing a web-based application through calls to the api provided by the server and providing a functional user-friendly web-based interface. this work focused on the development of tasks related with information retrieval, addressing the efficient search of relevant documents through an integrated interface. also, at this stage the aim was to have interfaces to visualize and explore the main entities involved in btm: queries, documents, corpora, annotation processes entities and resources.",
      "a mineração de literatura biomédica (biolm) pretende extrair informação de alta qualidade da área biomédica, através da criação de ferramentas/metodologias que consigam automatizar tarefas com elevado dispêndio de tempo. as tarefas subjacentes vão desde recuperação de informação, descoberta e recuperação de documentos relevantes para a extração de informação pertinente e a capacidade de extrair conhecimento de texto. nos últimos anos a silicolife tem vindo a desenvolver uma ferramenta, o @note2, uma biolm workbench multiplataforma baseada em java, que executa as principais tarefas inerentes a biolm. também possui uma versão autónoma com uma interface amigável para o utilizador. esta tese desenvolveu uma plataforma de software baseada na web, que é capaz de executar algumas das tarefas de biolm, com suporte num núcleo de bibliotecas do projeto @note. para tal foi necessário melhorar o servidor restfid atual, criando novos métodos e apis, como também desenvolver a aplicação baseada na web, com uma interface amigável para o utilizador, que comunicará com o servidor através de chamadas à sua apl este trabalho focou o seu desenvolvimento em tarefas relacionadas com recuperação de informação, focando na pesquisa eficiente de documentos de interesse através de uma interface integrada. nesta fase, o objetivo foi também ter um conjunto de interfaces capazes de visualizar e explorar as principais entidades envolvidas em biolm: pesquisas, documentos, corpora, entidades relacionadas com processos de anotações e recursos."
    ],
    0.0
  ],
  [
    [
      "with the overwhelming increase of demand of computational power made by fields as big data, deep machine learning and image processing the graphics processing units (gpus) has been seen as a valuable tool to compute the main workload involved. nonetheless, these solutions have limited support for object-oriented languages that often require manual memory handling which is an obstacle to bringing together the large community of object oriented programmers and the high-performance computing field. in this master thesis, different memory optimizations and their impacts were studied in a gpu java context using aparapi. these include solutions for different identifiable bottlenecks of commonly used kernels exploiting its full capabilities by studying the gpu hardware and current techniques available. these results were set against common used c/opencl benchmarks and respective optimizations proving, that high-level languages can be a solution to high-performance software demand.",
      "com o aumento de poder computacional requisitado por campos como big data, deep machine learning e processamento de imagens, as unidades de processamento gráfico (gpus) tem sido vistas como uma ferramenta valiosa para executar a principal carga de trabalho envolvida. no entanto, esta solução tem suporte limitado para linguagens orientadas a objetos. frequentemente estas requerem manipulação manual de memória, o que é um obstáculo para reunir a grande comunidade de programadores orientados a objetos e o campo da computação de alto desempenho. nesta dissertação de mestrado, diferentes otimizações de memória e os seus impactos foram estudados utilizando aparapi. as otimizações estudadas pretendem solucionar bottle-necks identificáveis em kernels frequentemente utilizados. os resultados obtidos foram comparados com benchmarks c / opencl populares e as suas respectivas otimizações, provando que as linguagens de alto nível podem ser uma solução para programas que requerem computação de alto desempenho."
    ],
    [
      "conflict-free replicated data types (crdts) provide deterministic outcomes from concurrent executions. the conflict resolution mechanism uses information on the ordering of the last operations performed, which indicates if a given operation is known by a replica, typically using some variant of version vectors. this thesis will explore the construction of crdts that use a novel stochastic mechanism that can track with high accuracy knowledge of the occurrence of recently performed operations and with less accuracy for older operations. the aim is to obtain better scaling properties and avoid the use of metadata that is linear on the number of replicas.",
      "conflict-free replicated data types (crdts) oferecem resultados determinísticos de execuções concorrentes. o mecanismo de resolução de conflitos usa informação sobre a ordenação das últimas operações realizadas, que indica se uma dada operação é conhecida por uma réplica, geralmente usando alguma variante de version vectors. esta tese explorara a construção de crdts que utilizam um novo mecanismo estocástico que pode identificar com alta precisão o conhecimento sobre a ocorrência de operações realizadas recentemente e com menor precisão para operações mais antigas. o objetivo é a obtenção de melhores propriedades de escalabilidade e evitar o uso de metadados em quantidade linear em relação ao número de réplicas."
    ],
    0.3
  ],
  [
    [
      "a gestão do ciclo de vida de produtos informáticos é um processo contínuo que, nos casos de maior sucesso, se prolonga por períodos que ultrapassam duas décadas e se transforma num aspeto crítico à atividade das empresas que desenvolvem e comercializam esses produtos. a evolução natural de um produto deste tipo resulta de estímulos previsíveis associados à inovação tecnológica nas plataformas computacionais, como a cada vez maior utilização de plataformas móveis, à melhoria de funcionalidades existentes e à introdução de características novas. incorpora também estímulos menos previsíveis, tais como alterações nos modelos de negócio resultantes de mudanças nos paradigmas de prestação de serviços, como a migração para o modelo cloud. neste contexto, as implicações para a gestão da segurança da informação das alterações gradualmente introduzidas nem sempre são claras, e é comum as equipas de desenvolvimento não estarem preparadas para lidar com a diversidade de novos riscos que surgem com estas mudanças. deste modo, e na perspetiva de combater as possíveis falhas supra referidas, pretende-se com este projeto efetuar uma análise de segurança de produtos de software para a cloud da primavera bss , identificando potenciais riscos e ameaças de segurança e apresentando soluções viáveis para os mitigar.",
      "application life cycle management is a continuous process that, in the most successful cases, is extended for periods exceeding two decades and becomes a critical aspect to the activity of companies that develop and sell these products. the natural evolution of a product of this type results in predictable stimulus associated with technological innovation in computing platforms, such as the increasing use of mobile platforms, with the improvement of existing features and introduction of new characteristics. it also incorporates less predictable stimuli, like changes in business models resulting from changes in service provision paradigms, such as migrating to cloud model. in this context, the implications for information security management of the gradually introduced changes are not always clear, and it’s common that development teams are not prepared to deal with the diversity of new risks that come with these changes. accordingly, and in view of combating the possible failures mentioned above, the aim of this project is to make a software security analysis for primavera bss ’s cloud products, identifying potential risks and security threats and presenting viable solutions to mitigate them."
    ],
    [
      "nos dias de hoje, os sistemas de informação hospitalar assumem-se como uma ferramenta indispensável para a prestação de cuidados de saúde, uma vez que permitem o aumento da qualidade e da eficiência quer na prática clínica, quer na gestão hospitalar. a interoperabilidade emerge, assim, como uma necessidade, uma vez que a enorme diversidade de sistemas torna difícil a troca e a partilha de informação clínica. além disso, a elevada quantidade de sistemas não articulados faz com que seja muito mais provável a existência de dados repetidos e contraditórios, razão pela qual se torna ainda mais imperativa a utilização de normas e terminologias que conduzam à uniformização do registo clínico. neste contexto, a presente dissertação descreve a criação de um modelo relacional de dados, que serve de base a uma aplicação de classificação de termos médicos segundo o systematized nomenclature of medicine clinical terms (snomed ct), no âmbito da anatomia patológica. a base de dados construída assenta na estrutura original do snomed, e foi obtida através da criação de subsets que contêm os conceitos de interesse para o referido setor.",
      "nowadays, health information systems present themselves as an imperative tool regarding health care, as they allow an increase of quality and efficiency in both medical practice and hospital management. interoperability rises as a necessity, since the enormous diversity of systems make the trade and share of clinical information difficult. besides, the high quantity of non-articulated systems make the occurrence of repeated and contradictory data more likely, making the use of standards and terminologies even more necessary, as they are a mean to achieve the standardization of clinical records. that said, this dissertation describes the creation of a data relational model for further use as the background of an application that aims to classify medical terms according to the systematized nomenclature of medicine clinical terms (snomed ct), in the area of pathological anatomy. the database built uses snomed’s original structure and was obtained through the creation of subsets that contain the concepts of interest."
    ],
    0.0857142857142857
  ],
  [
    [
      "flexible molecular alignment is a complex and challenging problem in the area of medic inal chemistry. the current approach to this problem does not test all possible alignments, but makes a previous analysis of all the variables and chooses the ones with potentially greater impact in the posterior alignment. this procedure can lead to wrong ”best align ments” since not every data is considered. quantum computation, due to its natural parallelism, may improve algorithmic solutions for this kind of problems because it may test and/or simulate all possible solutions in an execution cycle. as a case study proposed by bial and in collaboration with ibm, the main goal of this dissertation was to study and create quantum algorithms able to refactor the problem of molecular alignment in the new setting of quantum computation. additionally, the comparison between both classical and quantum solutions was defined as a subsequent goal. during this dissertation and due to its complexity, in order to produce a practical solu tion to this problem, we resorted to a manageable number of conformations per molecule, revisited the classical solution and elaborated a corresponding quantum algorithm. such algorithm was then tested in both a quantum simulator and a real device. despite the privileged collaboration with ibm, the quantum simulations were not pro duced in viable time, making them impractical for industry applications. nonetheless, tak ing in consideration the current point of development of quantum hardware, the suggested solutions still has potential for the future.",
      "o alinhamento de moléculas flexíveis é um problema complexo na área de química medicinal, onde, mesmo hoje em dia, é um desafio encontrar uma solução. a atual abordagem para este problema não testa todos os possíveis alinhamentos. em vez disso, realiza uma análise prévia de todas as variáveis e escolhe aquelas com maior potencial de impacto no posterior alinhamento. este procedimento pode levar a falsos “melhores alinhamentos” visto que nem todos os dados são considerados. a computação quântica, devido ao seu natural paralelismo, pode melhorar as soluções algorítmicas deste tipo de problemas visto que poderá testar e/ou simular todas as possíveis soluções num ciclo de execução. partindo de um caso de estudo proposto pela bial, e em colaboração com a ibm, o objetivo principal desta dissertação foi estudar e criar algoritmos quânticos capazes reformular no contexto de computação quântica o problema de alinhamento de moléculas. adicionalmente, e como objetivo subsequente, foi prevista a comparação entre os algoritmos clássicos e quânticos. durante esta dissertação e devido à sua complexidade, de modo a produzir uma solução prática para este problema, foi utilizado um número tratável de conformações por molécula, revisitada a solução clássica e desenvolvido um algoritmo quântico correspondente. tal algoritmo foi depois testado tanto num simulador quântico como num dispositivo real. apesar da colaboração privilegiada com a ibm, as simulações quânticas não foram produzidas em tempo viável, tornando-as impraticáveis para aplicações industriais. não obstante, tendo em consideração o ponto atual de desenvolvimento dos dispositivos quânticos, as soluções propostas terão potencial para o futuro."
    ],
    [
      "in recent years, due to constant social and economic crises, there has been some concern regarding the quality of life and satisfaction of the population. then, a need to create measures or criteria that would allow assessing the population's well-being, arose. usually, these criteria are quite complex, because any one of them can be analyzed through different perspectives or dimensions, since the quality of life of a human being depends on several factors, such as health and education. the analysis of such criteria can be done through indexes. to deal with the complexity of these indexes and to create conditions that facilitate decision making, multidimensional systems can be used. these allow for a broader analysis of well-being indexes and their underlying dimensions. in this dissertation work, we will explore this area by creating a well-being analysis system based on indexes that will be calculated through the analysis of feelings expressed in texts.",
      "nos últimos anos, devido às constantes crises sociais e económicas, gerou-se alguma preocupação relativamente àquilo que diz respeito à qualidade de vida e da satisfação da população. surgiu, então, a necessidade de criar medidas ou critérios que permitissem avaliar o seu bem-estar. usualmente, estes critérios são bastante complexos, isto porque qualquer um deles pode ser analisado através de diferentes perspetivas ou dimensões, já que a qualidade de vida de um ser humano depende de vários fatores, como por exemplo a saúde e a educação. a análise de tais critérios pode ser realizada através de índices. para lidar com a complexidade destes índices e para criar condições que facilitem a tomada de decisões, podem ser utilizados sistemas multidimensionais. estes permitem uma análise mais ampla dos índices de bem-estar e das suas dimensões subjacentes. neste trabalho de dissertação iremos explorar esta área através da criação de um sistema de análise de bem-estar baseado em índices que serão calculados através da análise de sentimentos expressa em textos."
    ],
    0.06666666666666667
  ],
  [
    "wireless sensors networks consist of large numbers of small, battery-powered, self-organizing computing motes. nowadays, these networks are considered ideal candidates for a wide range of applications such as environmental monitoring, military operations and other application fields where it is hard to maintain a continuous presence of human beings. online remote reprogramming is usually carried out to update the code running on nodes due to factors such as changes in the environment or application. remote reprogramming might be applied to the whole network or just to a subset of nodes (selective reprogramming), either way it is crucial to provide reliability for such procedure. therefore, most of the approaches oriented to remote reprogramming resort to flooding the whole network, leading to a major waste of energy in network nodes. when dealing with selective reprogramming, the waste of energy increases steeply even when just a small number of nodes need to get the update messages. these messages may be received and retransmitted from all nodes in the network resulting in a waste of resources. this research identifies multiple scenarios for selective reprogramming and proposes a different energy-aware approach for each one trying to reduce energy consumption in the network by taking advantage of multiple and complementary solutions such as wise routing, clustering and the ability to manage nodes sleeping time instead of using the typical flooding approach. these approaches were tested and compared with typical flooding and deluge solutions. the results show a significant reduction of the power consumption, thus, making the selective remote reprogramming more energy-efficient.",
    [
      "com o sucessivo aumento da informatização e robotização de processos, existe uma crescente neces sidade de produção de código que tenha como objetivo a melhoria da escalabilidade, rapidez e eficiência de uma aplicação com a finalidade de garantir que a plataforma a trabalhar seja mais rápida, escalável e mais fácil de testar e modificar. assim, a presente dissertação aborda a transição da arquitetura de uma aplicação da empresa em venci, com o objetivo de melhorar a sua eficiência e desempenho. a motivação para este estudo surgiu da necessidade da empresa aplicar melhorias à arquitetura atual, visando aprimorar a compreensão do código, identificar problemas com maior rapidez e obter ganhos significativos de desempenho. desta forma, é realizada uma exposição da arquitetura inicial, identificando pontos críticos e anali sando potenciais melhorias, como repetidos acessos á base de dados, código pouco comentado e mistura de lógica de negócio com a camada de acesso a dados. com base nessa análise, foi proposto um novo design arquitetural, que foi cuidadosamente planeado e fundamentado no uso de clean code archi tecture. apresentado posteriormente o processo de implementação deste design sempre exemplificado com recurso a um caso de estudo que incorpora o projeto, nomeadamente um dos relatórios de phishing. por fim, foram conduzidos testes de funcionalidade e desempenho para garantir a saúde da aplica ção e realizar uma análise comparativa com a arquitetura anterior. os resultados obtidos demonstraram claramente o sucesso da nova arquitetura, com melhorias significativas no desempenho, na compreen são da estrutura e no tempo de resolução de problemas, destacando-se pela facilidade com que novos membros da equipa, após uma breve formação na nova arquitetura, conseguem navegar e compreender o código da plataforma em comparação com a arquitetura antiga. os resultados obtidos evidenciam o impacto positivo dessa transição no contexto da empresa, beneficiando tanto os colaboradores quanto os clientes. o trabalho futuro envolverá a continuidade da transição dos use cases para a nova arquitetura, a fim de consolidar ainda mais os ganhos alcançados e manter a emvenci na vanguarda tecnológica do seu segmento.",
      "with the successive increase in computerization and robotization of processes, there is a growing need to produce code that aims to improve the scalability, speed, and efficiency of an application or program. in order to ensure that the platform is faster, scalable, and easier to test and modify. thus, this dissertation addresses the architecture transition of an application from the company em venci to improve its efficiency and performance. the motivation for this study arose from the company’s need to apply improvements to the current architecture, aiming to improve code comprehension, identify problems more quickly and obtain significant performance gains. in this way, the initial architecture is exposed, identifying critical points and analyzing potential impro vements, such as repeated database access, little commented code, and a mix of business logic with the data access layer. based on this analysis, a new architectural design was proposed, which was carefully planned and based on the use of clean code architecture. the implementation process of this design is presented later, always exemplified using a case study that incorporates the project, namely one of the phishing reports. finally, functionality and performance tests were conducted to ensure the application’s health and perform a comparative analysis with the previous architecture. the results obtained clearly demonstrated the success of the new architecture, with significant performance improvements, in the understanding of the structure and in the time to solve problems, standing out for the ease with which new team members, after a brief training in the new architecture, are able to navigate and understand platform code compared to the old architecture. the results show this transition’s positive impact on the company’s context, benefiting both employees and customers. future work will involve continuing the transition from use cases to the new architecture to further consolidate the gains achieved and keep emvenci at the technological forefront of its segment."
    ],
    0.06666666666666667
  ],
  [
    [
      "nos últimos anos a identificação e sequenciação de proteínas transportadoras tem crescido, uma vez que estas são de extrema importância no corpo humano e em todos os seres vivos, sendo responsáveis pela absorção e movimentação de moléculas essenciais às células e ainda pela excreção de produtos do metabolismo celular. a identificação de genes que codificam proteínas transportadoras é muito importante em várias áreas, como farmacocinética e reconstrução de modelos metabólicos em escala genómica que permitem perceber a relação entre genótipos-fenótipos. de forma a tentar diferenciar proteínas transportadoras de não transportadoras duas abordagens foram realizadas, treinando e testando modelos de machine learning e de deep learning. os dados utilizados provêm da base de dados tcdb, que contém proteínas transportadoras, e da base de dados swiss-prot, onde as proteínas foram filtradas para serem obtidas proteínas não transportadoras, obtendo no final um conjunto de dados equilibrado. de seguida, através desses dados foram obtidas características das proteínas através das suas sequências, sendo assim utilizado para treinar diferentes modelos de machine learning e deep neural networks. nesta abordagem os modelos apresentaram um bom desempenho global, atingindo 89% de acerto na identificação de proteínas transportadoras. todos os modelos treinados apresentam um elevado número de falsos negativos em comparação com o número de falsos positivos, indicando que a maior falha nos modelos prende-se na identificação de proteínas transportadoras como não transportadoras. o principal objetivo deste projeto prendia-se com a utilização de métodos de deep learning para identificar proteínas transportadoras, apenas utilizando as suas sequências de aminoácidos como entrada, comparando assim as duas abordagens realizadas. desta forma, utilizando apenas as sequencias das proteínas, diferentes redes neuronais foram treinadas e testadas, desde redes neuronais recorrentes a convolucionais, obtendo um desempenho global muito semelhante ao da abordagem anterior, atingindo também um valor de 89% de acerto na identificação de proteínas transportadoras. assim, foram alcançados modelos de desempenho preditivo semelhante sem a necessidade de calcular características.",
      "in the last years, the identification and sequencing of transport proteins has grown, once they are extremely important in the human body and in all living beings, being responsible for the absorption and movement of molecules essential to cells and also for the excretion of cellular metabolism products. identification of genes that encode transport proteins is very important in areas,such as pharmacokinetics and genome-scale metabolic models reconstruction, which allow us to understand the relationship between genotypes and phenotypes. in order to try to differentiate transport proteins from non-transport ones, two approaches were taken, training and testing machine learning and deep learning models. the data used came from the tcdb database, which contains transport proteins, and from the swiss-prot database, where the proteins were filtered to obtain non-transport proteins, obtaining at the end a balanced dataset. next, using this dataset, features were created from the protein sequences and used to train different machine learning models and deep neural networks. in this approach the models presented a good overall performance, reaching 89% accuracy in the identification of transport proteins. all trained models have a high number of false negatives compared to the number of false positives, indicating that the major failure in the models is the identification of transport proteins as non-transport proteins. the main objective of this project was to use deep learning methods to identify transport proteins, only using their aminoacid sequences as inputs, thus comparing the two approaches. thus, using only the protein sequences, different neural networks were trained and tested, from recurrent to convolutional neural networks, obtaining an overall performance very similar to that of the previous approach, reaching once more 89% accuracy in the identification of transport proteins. thus, we have attained models of similar predictive performance without the need to compute features."
    ],
    [
      "nowadays, the ability to predict protein functions directly from amino-acid sequences alone remains a major biological challenge. the understanding of protein properties and functions is extremely important and can have a wide range of biotechnological and medical applications. technological advances have led to an exponential growth of biological data challenging conventional analysis strategies. high-level representations from the field of deep learning can provide new alternatives to address these problems, particularly nlp methods, such as word embeddings, have shown particular success when applied for protein sequence analysis. here, a module that eases the implementation of word embedding models toward protein representation and classification is presented. furthermore, this module was integrated in the propythia framework, allowing to straightforwardly integrate we representations with the training and testing of ml and dl models. this module was validated using two protein classification problems namely, identification of plant ubiquitylation sites and lysine crotonylation site prediction. this module was further used to explore enzyme functional annotation. several we were tested and fed to different ml and dl networks. overall, we achieved good results being even competitive with state-of-the-art models, reinforcing the idea that language based methods can be applied with success to a wide range of protein classification problems. this work presents a freely available tool to perform word embedding techniques for protein classification. the case studies presented reinforce the usability and importance of using nlp and ml in protein classification problems.",
      "hoje em dia, a habilidade de prever a função de proteínas a partir apenas da sequências de amino-ácidos permanece um dos grandes desafios biológicos. a compreensão das propriedades e das funções das proteinas é de extrema importância e pode ter uma grande variedade de aplicações médicas e biotecnológicas. os avanços nas tecnologia levaram a um crescimento exponencial de dados biológicos, desafiando as estratégias convencionais de análise. o campo do deep learning pode providenciar novas alternativas para atender à resolução destes problemas, em particular, os métodos de processamento de linguagem, como por exemplo word embeddings, mostraram especial sucesso quando aplicados para análise de sequências proteicas. aqui, é apresentado um módulo que facilita a implementação de modelos de “word embedding” para representação e classificação de proteínas. além disso, este módulo foi integrado na framework propythia, permitindo integrar diretamente as representações we com o treino e teste de modelos ml e dl. este módulo foi validado usando dois problemas de classificação de proteínas, identificação de locais de ubiquitilação de plantas e previsão de locais de crotonilação de lisinas. este módulo foi usado também para explorar a anotação funcional de enzimas. vários we foram testados e utilizados em diferentes redes ml e dl. no geral, as técnicas de we obtiveram bons resultados sendo competitivas, mesmo com modelos descritos no estado da arte, reforçando a ideia de que métodos baseados em linguagem podem ser aplicados com sucesso a uma ampla gama de problemas de classificação de proteínas. este trabalho apresenta uma ferramenta para realizar técnicas de word embedding para classificação de proteínas. os caso de estudo apresentados reforçam a usabilidade e importância do uso de nlp e ml em problemas de classificação de proteínas."
    ],
    0.02727272727272727
  ],
  [
    [
      "in the past 30 years, accumulated evidence has been supporting viral infection as one factor responsible for 15-20% of human malignancies worldwide (w. s. liang et al. 2014; mclaughlin-drubin and munger 2008). studies on oncogenic viruses have proved their importance on cellular malfunction along the carcinogenic process, and showed that their association with cancer can amount from 15% to 100% (mclaughlin-drubin and munger 2008), depending on the type of tumour. with the large amount of genomic and metagenomic information available on public international consortia, such as tcga database, it is nowadays possible to indirectly infer viral infections from the human centred omics studies, as a portion of the reads will align in viruses and bacteria. taking as starting point the research made by tang et al. 2013, we focused on cervical (cesc), hepatocellular (lihc) and head and neck squamous cell (hnsc) carcinomas, which are known to show a high proportion of viral-positive cases (tang et al. 2013). we downloaded rnaseq data from 309, 424 and 566 samples, respectively, and run the unmapped reads against a reference database of viruses (downloaded from ncbi) by using the tools batch, samtools, bowtie and printseq. quantification of each virus was performed using parts per million reads (ppm) and only viruses with ppm above 10 were considered as positively infecting the sample. we confirmed that around 94% of cesc samples were infected, mostly by hpv (human papillomavirus) and specifically by the hpv16 strain. nearly 32% of lihc were infected by hbv (hepatitis b virus). almost 17% of hnsc samples were infected, and the hpv16 was the most common present virus. the evaluation of differential enrichment of metabolic pathways between infected and noninfected groups, for each cancer type, was performed in gsea. signs of enrichment for infection and immune related pathways were evident in cesc infected group, while in lihc and hnsc infected groups the enrichment was mostly related with dna replication and repair. this seems to indicate that infection is especially active in cesc, contradicting previous claims that tumorigenesis in cervix was not directly linked with infection. for the three cancer types, the viruses integrate their genome in the host genome, affecting dna replication, maintenance and repair. in our investigation of integration of hpv16 genome in one hnsc tumor sample, we confirmed integration in the human rad51b gene that codes a protein involved in dna repair by homologous recombination. we thus confirmed that hpv16 can act both as indirect and direct carcinogen. the infection, most probably through the integration of the viral genome in the host genome, increased the amount of somatic mutations in the infected group in lihc, but not in hnsc where tobacco consumption is also an important carcinogen. the low number of non-infected samples in cesc did not allow a reliable evaluation of changes in the amount of somatic mutations. even so, in both lihc and hnsc infected groups, some somatic mutations occurred in the context of immune-related pathways, showing that they can contribute to render these individuals susceptible to infection. also, when checking expression of hpv16 genes in five samples each from cesc and hnsc, we confirmed that e6 and e7 genes are amongst the ones more expressed in many samples, while e2 is not expressed. e6 and e7 have been said to be preferentially integrated in the host genome, while e2, which controls their expression, is not integrated or it is disrupted. it is believed that the overexpression of e6 and e7 initiates carcinogenesis. the viral infection rates inferred here from mining the omics databases are very similar to the ones evaluated by standard methods (tang et al. 2013), showing that public international consortia can indirectly provide interesting insights into the involvement of viral infection in tumorigenesis. the high number of samples per tumor, the wide geographic origin of the samples, and the high-throughput characterisation for different omics platforms allows multilayer comparisons and evaluations, in a scale not affordable before.",
      "nos últimos 30 anos foram-se acumulando evidências que têm vindo a apoiar a infecção viral como um factor responsável por 15-20% dos tumores malignos em humanos a nível mundial (w. s. liang et al. 2014; mclaughlin-drubin and munger 2008). estudos sobre os vírus oncogénicos demonstraram a sua importância no mau funcionamento celular ao longo do processo carcinogénico e demonstraram que a sua associação com o cancro varia entre 15% e 100% (mclaughlin-drubin and munger 2008), dependendo do tipo de tumor. com a grande quantidade de informação genómica e metagenómica acessível nos consórcios internacionais públicos, tais como a base de dados tcga, hoje em dia é possível inferir indiretamente infecções virais a partir de estudos genómicos centrados em humanos, uma vez que parte das reads irá alinhar com vírus e bactérias. tomando como ponto de partida a pesquisa feita por tang et al. 2013, concentramo-nos nos cancros cervical (cesc), hepatocelular (lihc) e da cabeça e pescoço (hnsc), que são conhecidos por apresentar uma alta proporção de casos virais-positivos (tang et al. 2013). fizemos download de dados rna-seq de 309, 424 e 566 amostras, respectivamente, e comparamos unmapped reads contra uma base de dados viral de referência (retirada da base de dados do ncbi) usando as ferramentas batch, samtools, bowtie e printseq. a quantificação de cada vírus foi feita usando partes por milhão (ppm) e apenas vírus com ppm acima de 10 foram considerados como estando a infectar positivamente uma amostra. confirmamos que cerca de 94% das amostras de cesc foram infectadas, principalmente por hpv (papilomavírus humano) e, especificamente, pela estirpe hpv16. quase 32% das amostras lihc foram infectadas por hbv (vírus da hepatite b). e por volta de 17% de amostras hnsc foram infectadas e o hpv16 foi o vírus mais comum. a avaliação de enriquecimento diferencial de vias metabólicas entre grupos infectados e não infectados, para cada tipo de cancro, foi realizada por gsea. os sinais de enriquecimento para infecção e vias relacionadas com sistema imune eram evidentes no grupo infectado cesc, enquanto nos grupos infectados de lihc e hnsc o enriquecimento era principalmente relacionado com replicação e reparação de dna. este facto parece indicar que a infecção é especialmente ativa no cesc, contradizendo alegações anteriores de que a tumorigenese no colo do útero não estava diretamente ligada à infecção. nos três tipos de cancro, os vírus integraram os seus genomas no genoma do hospedeiro, afetando a replicação, manutenção e reparação do dna. no nosso estudo sobre a integração do genoma de hpv16 numa amostra de tumor hnsc, foi confirmada a integração viral no gene humano rad51b que codifica uma proteína implicada na reparação de dna por recombinação homóloga. desta forma, conseguimos confirmar que hpv16 pode atuar tanto como agente cancerígeno directo e indirecto. provavelmente através da integração do genoma viral no genoma do hospedeiro, a infecção aumentou a quantidade de mutações somáticas no grupo de amostras infectadas em lihc, mas não em hnsc onde o consumo de tabaco é também um importante agente cancerígeno. o reduzido número de amostras não-infectadas em cesc não permitiu uma comparação fiável da quantidade de mutações somáticas entre grupos de infectados e não-infectados. ainda assim, nos grupos infectados de lihc e hnsc, algumas mutações somáticas ocorreram no contexto de vias relacionadas com o sistema imunológico, mostrando que podem contribuir para tornar estes indivíduos susceptíveis à infecção. além disso, ao verificar a expressão dos genes de hpv16 em cinco amostras de cesc e de hnsc, confirmou-se que os genes e6 e e7 estão entre os mais expressos em muitas das amostras, enquanto que o e2 não é expresso. os genes e6 e e7 são conhecidos por serem preferencialmente integrados no genoma do hospedeiro, ao contrário do gene e2, o qual controla a expressão daqueles, que não é integrado ou é fragmentado. acredita-se que é a sobreexpressão de e6 e e7 que inicia a carcinogénese. as taxas de infecção viral inferidas neste trabalho por mining de bases de dados omicos são muito semelhantes aos obtidos pelos métodos tradicionais (tang et al. 2013), mostrando que a informação disponível nos consórcios internacionais públicos pode elucidar, indiretamente, sobre o envolvimento da infecção viral na tumorigénese. o elevado número de amostras por tumor, a grande variedade de origem geográfica das amostras e a caracterização de alto rendimento para diferentes plataformas omicas permitem comparações e avaliações múltiplas, numa escala não acessível anteriormente."
    ],
    [
      "finite automata are valuable models for various types of software, such as system verifica-tion software and text search software, etc. a widely acclaimed result in computer science is the proof that finite automata can be represented through a notation structure called a regular expression. this means that regular expressions succeed in describing the same patterns that can be represented by a finite automaton. we know that it is difficult to prove that two finite automata are equivalent. still, by converting them to a regular expression, you can determine relatively easily whether they represent the same language. timed automata are an extension of finite automata with a finite set of clocks. timed automata are widely used in model checking and also in real-time systems such as commu-nication and security protocols. in recent years there is an increasing demand for this type of software. as in classical automata, timed automata can also be represented by regular expressions. however, so far, there is no conversion software developed. there are two main methods for converting: the state elimination method; and the brzozowski's method. these methods refer to the classic automaton model. the goal of this work is the study and development of an algorithm that converts a timed automaton into a timed regular expression. for this, i) we developed a conversion al-gorithm based on brzozowski's method for timed automata adding to the classical case several changes, in particular, the incorporation of clocks and transition restrictions, among other, 2) we developed a software tool that converts a timed regular expression into a timed automaton, and depicting both of them.",
      "autómatos finitos são modelos úteis para vários tipos de software, tais como, software de verificação de sistemas e software de pesquisa de texto, etc. um resultado amplamente aclamado na ciência da computação é a prova de que autómatos finitos podem ser repre-sentados através de uma estrutura de notação chamada expressão regular. estas expressões regulares conseguem descrever os mesmos padrões que podem ser representados por um autómato finito. sabemos que é difícil provar que dois autómatos finitos são equivalentes, mas ao converte-los para uma expressão regular consegue-se determinar com relativa facil-idade se eles representam a mesma linguagem. os autómatos temporais são uma extensão de autómatos finitos com um conjunto finito de relógios. os autómatos temporais são muito usados em model cheking e também em sis-temas de tempo-real, como protocolos de comunicação e segurança. nos últimos anos tem havido procura crescente para esse tipo de software. tal como nos autómatos clássicos, os autómatos temporais podem ser representados por expressões regulares. mas até agora não existe software de conversão desenvolvido. existem dois métodos principais para efetuar a conversão: método de eliminação de estados; e o método de brzozowski. estes métodos funcionam para o modelo clássico de autómato. o objetivo deste trabalho é o estudo e desenvolvimento de um algoritmo que converta um autómato temporal numa expressão regular. para isso, i) desenvolvemos um algoritmo de conversão baseado no método de brzozowski, incorporando diversos elementos adiciona-dos, nomeadamente, relógios e restrições nas transições, entre outras 2) desenvolvemos uma ferramenta que converte uma expressão regular temporal num autómato temporal, e no final estes dois elementos são representados numa única imagem."
    ],
    0.3
  ],
  [
    [
      "o desenvolvimento de todo o código necessário para uma loja online é um trabalho demorado e complexo. todas as lojas online têm diferentes exigências e condições, no entanto, existem pontos comuns entre vários tipos de loja. supondo a possibilidade de extrair esses aspetos comuns, seria possível a criação de um esqueleto de código com todos os componentes básicos, sobre os quais um programador poderia acrescentar e moldar a loja online, reduzindo assim o tempo necessário para a desenvolver. a elaboração desta dissertação começou pela criação de uma loja online que suportasse um negócio fictício de venda de bicicletas e produtos relacionados. este passo serviu para identificar e avaliar quais seriam os parâmetros necessários especificar, e em que sentido fariam diferença na construção de um website de vendas bem como, distinguir os componentes comuns e as suas relações. a presente dissertação tem como propósito principal o desenvolvimento de uma ferramenta para a criação de lojas online. esta ferramenta pretende ser usada por programadores para produzir o código padrão que é comum a qualquer implementação de uma solução de loja online. através da recolha de parâmetros sobre a loja online a ser criada, são construídos os ficheiros necessários para a implementação do site. a aplicação concebida focou-se na utilização de apenas alguns dos componentes básicos, nomeadamente, “utilizadores” e “categorias” no back-end, “registo e login” e “perfil” no front-end, mostrando assim ser possível usar a framework desenvolvida para uma implementação parcial de uma loja online. a abordagem modular permite expandir a aplicação, criando e adicionando outros componentes à estrutura parcial existente.",
      "developing all the code necessary for an online store is a complex and time-consuming task. every online store has different demands and conditions, however, there are common aspects to various types of stores. assuming the possibility of extracting those common aspects, it would be possible to create a code skeleton with all the basic components, over which a programmer could add or mold an online store, reducing the development time. the elaboration of this dissertation began with the creation of an online store that would support a fictitious business selling bicycles and related products. this step served to identify and evaluate which parameters needed to be specified, and in what sense they would make a difference in the construction of a sales website, as well as to distinguish common components and their relationships. the main purpose of this dissertation is to develop a tool for creating online stores. this tool is intended to be used by programmers to produce the “boilerplate” code that is common to any implementation of an online store solution. by collecting parameters about the online store to be created, the necessary files for the “deploying” of the site are generated. the conceived application focused on the use of only some of the basic components, namely, “users” and “categories” in the back-end, “registration and login” and “profile” in the front-end, thus proving that a partial implementation of an online store is possible. the modular approach allows expanding the application by creating and adding other components to the existing partial structure."
    ],
    [
      "com o aumento constante da quantidade e complexidade dos dados, vem a necessidade de serem estudadas novas tecnologias de forma a acompanhar este crescimento sem pôr em causa o desempenho. a presente dissertação foca-se numa destas soluções, os in-memory data grid (imdg). um imdg define-se como sendo uma base de dados com armazenamento primário em memória volátil colocada numa camada imediatamente a cima de uma base de dados tradicional (armazenamento persistente). além disso, permite a distribuição de recursos e computações por diversos nodos. assim, no decorrer da dissertação, foram estudados os principais conceitos desta solução, bem como algumas das mais prestigiadas tecnologias na área. após estabelecido que o apache ignite seria a tecnologia que poderia trazer mais vantagens, esta foi aplicada a um caso real. assim foi executado o cálculo de uma matriz de segurança, que permite atribuir as permissões de visualização de dados entre membros de uma empresa, bem como a execução de uma simples interrogação de forma a efetuar uma análise de desempenho entre o imdg e a solução previamente existente baseada em sqlserver. no que diz respeito ao cálculo da matriz de segurança, os valores de desempenho foram limitados, maioritariamente devido às limitações de structured query language (sql) da ferramenta, obtendo um desempenho três vezes inferior comparativamente com a solução anterior. por outro lado, com a interrogação selecionada, o ignite mostrou um melhor desempenho (na utilização de uma grid com 4 nodos, esta interrogação apresentou, para uma thread, uma média de 2266.92ms vs. 8099.64ms no sqlserver).",
      "with the constant increase in the amount and complexity of data, it becomes necessary to study new technologies in order to keep up with this growth without compromising performance. this dissertation focuses on one of these solutions, the imdg. an imdg is defined as a database having its primary storage in volatile memory placed in a layer immediately above a traditional database (persistent storage). furthermore, it allows the distribution of resources and computations across multiple nodes. thus, in this project, the main concepts of this solution were studied, as well as some of the most prestigious technologies in the area. after establishing that apache ignite was the technology that could bring the most advantages, this technology was applied to a real case. thus, the security matrix computation was executed, which allows assigning data visualization permissions between members of a company, as well as the execution of a simple query in order to carry out a performance analysis between imdg and the previously existing solution based on sqlserver. the performance values for the security calculation were limited, mainly due to the limitations of the tool’s sql, obtaining a performance three times lower compared to the previous solution. on the other hand, with the selected query, ignite has shown a better performance (using a 4-node grid, this query presented, for a thread, an average of 2266.92ms vs. 8099.64ms in sqlserver)."
    ],
    0.3
  ],
  [
    [
      "automatic text annotation systems are mechanisms that aim to provide assistance to users who need to extract and annotate relevant information in a given text. usually, this type of system is developed for very specific application domains, in order to facilitate research processes on text content. the works of this dissertation will be developed based on the tombo da mitra, a codex that contains the inventory of the properties of the archbishop’s table of braga, in the 17th century. the quantity and diversity of the elements referred to in the book are impressive, as it contains all the names and surnames, settlements, professions, types of land and buildings, among many other elements, which are very important for the study and learning of geography, culture, economy, architecture, religion and portuguese language of the 17th century. the annotation of these elements expressively shows their location in time and space, as well as their potential relationships, facilitating the study of the book and providing linguistic researchers, teachers and students with a valuable instrument to reach and reinforce knowledge about the book. in this dissertation, we present a tool specially designed for the annotation of documents in the livro das propriedades, allowing the management and listing of annotation tags and providing a clearer view of the content of the manuscript.",
      "os sistemas de anotação automática de textos são mecanismos que visam prestar auxílio a utilizadores que necessitem de extrair e anotar informação relevante num dado texto. usualmente, este tipo de sistema e desenvolvido para domínios de aplicação bastante específicos, com vista a facilitar processos de pesquisa sobre conteúdos de textos. os trabalhos da presente dissertação foram desenvolvidos com base no tombo da mitra, um códice que contém o inventário das propriedades da mesa arcebispal de braga, no século xvii. a quantidade e diversidade dos elementos referidos no livro são impressionantes, uma vez que este contém nomes, apelidos, povoações, profissões, tipos de terrenos e edificações, entre tantos outros elementos, que são muito importantes para o estudo e aprendizagem da geografia, cultura, economia, arquitetura, religião e língua portuguesa até ao século xvii. a anotação destes elementos evidencia de forma expressiva a sua localização no tempo e no espaço, bem como as suas potenciais relações, facilitando o estudo do livro e proporcionando aos investigadores, linguistas, professores e alunos, um valioso instrumento para alcançar e reforçar o conhecimento sobre o manuscrito. nesta dissertação, apresentamos uma ferramenta que foi concebida especialmente para a anotação dos documentos do livro de propriedades, que permite gerir e relacionar as etiquetas de anotação e proporcionar uma visão mais clara do conteúdo do referido manuscrito."
    ],
    [
      "tuberculosis, caused by the intracellular pathogen mycobacterium tuberculosis is an infectious disease that remains a global public health problem where approximately one-third of the world population have been at least in contact and is latently infected with. whole genome sequencing has revolutionized the investigation of mycobacterial genomes. the application of this technology has provided innovative understandings into the evolution of the mycobacterium tuberculosis due to recent studies reporting conflicting findings on its genomic stability, particularly during the evolution of drug resistance in modern lineages. to address this question we focused on understanding the genotypic and epidemiological factors that influence the spread and fitness of this bacterium by analyzing deep –sequencing data of 85 patient samples from central asia. samples were part of a larger study of 399 clinical isolates of newly diagnosed patients with pulmonary tb collected between 2012 and 2013 at the nctld in tbilisi, georgia. all the samples were mapped against h37rv strain. we focused on single-nucleotide polymorphisms to reconstruct models for molecular evolution, using maximum likelihood and bayesian inference methods. 84% of our population belongs to the beijing lineage, associated with the massive spread of multidrug-resistant strains. relationship between mutations on rpob and rpoc were associated with drug resistance to rifampicin and mutations on pnca region also demonstrated to be related with drug resistance to pyrazinamide. furthermore we found that the amount of variation accumulated within a patient can be as high as that observed between patients along, what we assume to be, a chain of transmission. intrapatient diversity was found in all of the follow up patients. our study adds new data to the understandings of the variability among mycobacterium tuberculosis strains in an intra and interpatient microevolution scenario.",
      "a tuberculose provocada pelo agente patogénico intracelular mycobacterium tuberculosis é uma doença infeciosa que continua a ser um dos maiores problemas de saúde global, estimando-se que aproximadamente um terço da população tenha estado em contacto e esteja infetada de forma latente. whole genome sequencing surgiu como um método revolucionário da investigação de genomas de micobactérias. a sua aplicação têm proporcionado conhecimentos inovadores relativamente à evolução da mycobacterium tuberculosis devido a estudos recentes que reportam resultados contraditórios sobre a sua estabilidade genómica, particularmente durante a evolução da sua resistência a antibióticos em linhagens consideradas modernas. para abordar esta questão, focámo-nos na análise e compreensão dos fatores genotípicos e epidemiológicos que influenciam a capacidade de disseminação e o fitness desta bactéria através da análise de dados deep-sequencing provenientes de amostras de 85 pacientes provenientes da ásia central. as amostras pertencem a um estudo maior composto por 399 isolados clínicos de pacientes recentemente diagnosticados com tuberculose pulmonar recolhidas entre 2012 e 2013 no national center of tuberculosis and lung diseases (nctld) em tbilisi, geórgia. todas as amostras foram mapeadas contra a estirpe h37rv. para a reconstrução de modelos de evolução molecular, focámo-nos apenas em single-nucleotide polymorphisms e utilizámos dois métodos distintos, maximum likelihood e bayesian inference. cerca de 84% da nossa população pertence à linhagem beijing, associada com a propagação em massa de estirpes resistentes a múltiplos antibióticos. além disso, as mutações no rpob e rpoc foram associadas à resistência a rifampicina e mutações na região pnca também demonstraram estar relacionadas com a resistência à pirazinamida. verificou-se ainda que a quantidade de variabilidade genética acumulada dentro de um paciente pode ser tão alta quanto a observada entre pacientes ao longo, do que supomos ser, uma cadeia de transmissão. todos os pacientes que foram acompanhados durante tratamento apresentaram variabilidade genética. o nosso estudo acrescenta novos dados relativamente à variabilidade entre diferentes estirpes de mycobacterium tuberculosis tendo em conta um panorama de microevolução intra e inter paciente."
    ],
    0.06666666666666667
  ],
  [
    [
      "personalized medicine is a constantly growing area. important goals of this field are early diagnosis and the discovery of new personalized treatments. gene expression data play a key role at this level, as variations in these data can often offer explanations for some phenotypes. to this end, machine learning (ml) models capable of predicting biologically relevant information, have been widely used. deep learning (dl) is a branch of ml that has become popular over the past few years. the increasing amounts of data that have been generated, and the growing use of this type of models in biomedical areas, have been accelerating the analysis of biological processes associated with cancer and other complex diseases. in this work, we focused on developing a framework that allows to create and evaluate distinct work-flows for the application of a variety of machine and deep learning models, working over gene expression data, including different options regarding data preprocessing pipelines, distinct ml and dl models, including traditional ml models, dense neural networks, convolutional neural networks and variational autoencoders. the framework has been validated using different case studies, where the data sources were two of the main repositories of gene expression data (tcga and gtex). the goal of each case study was to predict important variables for clinical application. a variety of models were developed and evaluated for each case study, generally with competitive performance. for the first case study, the task was to predict the type of cancer from tcga data, and the best performing dl model was a dense neural network, being outperformed by a logistic regression model. in the second case, where the task was to predict the hypoxia score, the best dl model was a two dimensional convolutional neural network (2d cnn), being outperformed by the lightgbm model. as for the third case study, where the objective was to predict the aneuploidy score, the best model was an one dimensional convolutional neural network (1d cnn). for the fourth case, where the task was to predict body mass index, the best model was again a 1d cnn. finally, in the fifth case study, where the main goal was to predict gene expression for a set of genes based on landmark genes, the best dl model was found by an 1d cnn, still slightly outperformed by linear regression. some of the dl models developed in this work show promising results. however, these need to be improved in the future as they are not clinically applicable at this time. this framework can be reused for new problems and can be easily expanded.",
      "a medicina personalizada é uma área em constante crescimento. um dos objectivos importantes deste campo são o diagnóstico precoce e a descoberta de novos tratamentos personalizados. os dados de expressão genética desempenham um papel fundamental a este nível, pois variações nestes dados podem muitas vezes oferecer explicações para alguns fenótipos. para este fim, modelos machine learning (ml) capazes de prever informação biologicamente relevante, tem sido amplamente utilizados. o deep learning (dl) é um ramo do ml que se tornou popular ao longo dos últimos anos. a quantidade crescente de dados que tem sido gerada, e a crescente utilização deste tipo de modelos em áreas biomédicas, têm vindo a acelerar a análise dos processos biológicos associados ao cancro e a outras doenças complexas. neste trabalho, concentrámo-nos em desenvolver uma framework que permita criar e avaliar workflows distintos para a aplicação de uma variedade de modelos de machine e deep learning, trabalhando sobre dados de expressão genética, incluindo diferentes opções relativas a pipelines de pré-processamento de dados, modelos distintos ml e dl, incluindo modelos tradicionais ml, dense neural networks, convolutional neural networks e variational autoencoders. a framework foi validada utilizando diferentes casos de estudo, em que as fontes de dados foram dois dos principais repositórios de dados de expressão genética (tcga e gtex). o objectivo de cada caso de estudo foi a previsão de variáveis relevantes para aplicação clínica. foi desenvolvida e avaliada uma variedade de modelos para cada caso de estudo, geralmente com desempenho competitivo. para o primeiro caso de estudo, a tarefa era prever o tipo de cancro a partir dos dados tcga, e o modelo com melhor desempenho dl foi uma dense neural network, sendo superado por um modelo de regressão logística. no segundo case, onde a tarefa era prever a pontuação de hipoxia, o melhor modelo dl era uma convolutional neural network bidimensional (2d cnn), sendo superado pelo modelo lightgbm. quanto ao terceiro caso de estudo, em que o objectivo era prever a pontuação de aneuploidia, o melhor modelo era uma convolutional neural network unidimensional (10 cnn). no quarto caso, onde a tarefa era prever o índice de massa corporal, o melhor modelo foi novamente uma 1d cnn. finalmente, no quinto caso de estudo, onde o objectivo principal era prever a expressão genética para um conjunto de genes baseados em genes de referência, o melhor modelo dl foi encontrado por uma 1d cnn, ainda que ligeiramente ultrapassado por um modelo de regressão linear. alguns dos modelos dl desenvolvidos neste trabalho mostram resultados promissores. no entanto, estes precisam de ser melhorados no futuro, uma vez que não são clinicamente aplicáveis neste momento. esta framework pode ser reutilizado para novos problemas e pode facilmente ser expandida."
    ],
    [
      "dcss is a roguelike game in which the player must explore and find artifacts. in every step of the game, there are decisions to make, and the complexity of the game resides in the vast amount of options available to the player at any given time. the commands can be divided into classes such as movement, combat, inventory management and usage, spell casting, and divine abilities. we aim to implement an intelligent bot that will be able to play the game. to do so, we will use drl. drl is where deep learning and rl meets. it uses the same principles of rl, to learn to perform a task, receiving rewards for every action made. the difference is that the action we will perform is chosen by a dnn, and therefore, we call it drl pytorch[11] will be as the framework used to implement a nn that will be able to find the solution for every small decision that can be made during gameplay. if all of those decisions are merged, an intelligent bot should be able to play with some degree of success. the aim of the intelligent bot is to learn a task, which in this case, is playing the game, without programming any real behavior.",
      "dcss é um jogo roguelike no qual o jogador deve explorar e encontrar artefactos. a cada passo do jogo existem decisões a tomar e a complexidade do jogo reside na grande quanti-dade de opções disponíveis para o jogar a cada momento do jogo. os comandos podem ser divididos em desses como: movimento, combate, gestão e utilização de inventário, magia e abilidades divinas. o objetivo é implementar um bot inteligente capaz de jogar o jogo. para tal usar-se-á drl. drl é a área onde deep learning e rl se encontram. usa os mesmos princípios de rl, para aprender a realizar uma tarefa, recebendo uma recompensa por cada ação efetuada. a diferença é que a ação vai ser escolhida por uma dnn. durante o projeto utilizar-se-à pytorch[i i] como a framework para implementar uma nn que irá ser capaz de encontrar uma solução para cada pequena decisão que pode ser efetuada ao longo do jogo. tomadas todas essas decisões, o bot deve ser capaz de jogar o jogo com algum grau de sucesso. o objetivo de um bot inteligente é aprender uma tarefa, neste caso, jogar o jogo, sem programar qualquer comportamento."
    ],
    0.3
  ],
  [
    [
      "sociedade encontra-se a gerar um volume de dados sem precedentes na história. apesar disso, a desinformação tem vindo a crescer, provocando preocupações no meio jornalístico e democrático. este problema se tornou ainda mais evidente com os avanços tecnológicos, como a capacidade de manipular o significado semântico de imagens, vídeos ou áudios. os resultados destas manipulações impõem uma dificuldade em distinguir entre conteúdo falso e original. desta forma, os conteúdos multimédia manipulados são chamados de deepfake. esta nomenclatura é resultado da combinação dos termos deep learning (dl) e fake. para solucionar tal problemática trabalhos no estado da arte apresentam diferentes algoritmos para classificação de vídeos deepfake, dos quais utilizam-se de arquiteturas neuronais baseadas em convolutional neural network (cnn), long short-term memory (lstm) ou redes transformers. há trabalhos dos quais apresentam resultados positivos ao classificar dados deepfake criados a partir de uma determinada técnica para geração deepfake. no entanto, estes em sua maioria não apresentam resultados quando seus modelos são confrontados com dados gerados a partir de outra técnica deepfake distinta dos dados utilizados no treino do modelo. consequentemente, há um lacuna nos trabalhos para uma classificação mais generalista independente do método utilizado para criação do conteúdo manipulado. desta forma, as experiências desenvolvidas nesse trabalho utilizaram redes neuronais em diferentes estratégias. resultando na proposta de uma solução para tal lacuna encontrada no estado da arte, a capacidade de um modelo ao classificar um vídeo falso independente da técnica de criação. após as experiências escolheu-se o algoritmo de multi-classificação para a utilização no detetor. para uma maior precisão na análise dos vídeos, o detetor possui níveis de confiança (não confiável, nada confiável, muito confiável) para cada classificação realizada. ao validar o detetor, com vídeos das quatro técnicas deepfake e vídeos originais, foi possível saber que o nível de confiança ”muito confiável”a precisão média é de 91% quando a label era deepfake, independente da técnica (deepfakes, face2face, faceswap e neuraltextures) utilizada para criá-lo, enquanto que ao rotular dados como reais com o mesmo nível de confiança obtém um precisão de 60%.",
      "a society generates a volume of data unprecedented in history. despite this, misinformation is growing, causing concerns that are not the best of journalism and democracy. this problem becomes even more evident with technological advances, such as the ability to manipulate or semantic meaning of images, videos, or audio. the results of these manipulations, imply difficulty in distinguishing between fake and original content. in this way, manipulated multimedia content is called deepfake. this nomenclature is the result of the combination of two terms deep learning (dl) and fake. to solve this problem, non state-of-the-art works present different algorithms for classifying deepfake videos, two of which use neural architectures based on convolutional neural network (cnn), long short-term memory (lstm) or transformers networks. some studies show positive results for classifying deepfake data generated from a given technique for generating deepfake. however, these mostly show no positive results when the models classified data generated from another deepfake technique other than two data used rather than three models. consequently, work is lacking for a more general classification regardless of the method used to create the manipulated content. thus, experiments developed used neural networks in different strategies. consequently, propose a solution to a gap found in the state of the art, the ability of a model to classify a fake video independent of the creation technique. after the experiments, the algorithm chosen was multi-classification used in the detector. for greater accuracy when analyzing the videos, the classifier has confidence levels (not confident, unreliable, very confident) for each classification performed. thus, the detector validated with videos from the four text techniques and original videos. it was possible to know when the classifier labeled a video as deepfake with a ”very confident”confidence level. this classification has an average 91% chance of being correct, the unmanipulated videos it has a 60% chance."
    ],
    [
      "a contratação eletrónica é apresentada como uma obrigação a ser cumprida pelos diversos estados, no contexto da união europeia. até ao ano 2016, todas as contratações públicas deverão ser executadas com total transparência, segurança e fiabilidade, através da utilização de plataformas web de contratação devidamente certificadas. a vortalnext> posiciona-se como um dos líderes desta área em portugal, oferecendo aos seus clientes, uma plataforma transversal e poderosa de contratação. apesar da qualidade existente na plataforma vortalnext>, existem diversas lacunas de usabilidade que são objeto de estudo nesta dissertação. a usabilidade apresenta-se como um ponto crítico para a aceitação da plataforma por parte dos seus clientes. entrevistas, heurísticas, questionários e testes foram as ferramentas utilizadas para análise dos reais problemas dos clientes, tendo sido concluído que o dashboard é um dos fatores de aceitação mais relevantes. tendo como base os resultados obtidos e como foco o dashboard, foram desenhadas diversas aproximações, tanto da arquitetura de informação, como posicionamento e organização dos elementos no ecrã. utilizaram-se cores para evidenciar as ações mais importantes, números para representar volume de trabalho/negócio, alertas visuais como aviso/lembrete, listas priorizadas e “one-click actions”, conseguindo assim minimizar as falhas de usabilidade detetadas através deste trabalho. através desta estratégia, um novo dashboard foi desenhado para apresentar apenas o necessário, quando necessário e de uma forma verdadeiramente útil e agradável para o utilizador.",
      "e-procurement is presented today, as an obligation to be fulfilled by the various states in the context of the european union. by 2016, all public procurement should be carried out with full transparency, security and reliability through the use of electronic procurement web platforms. the vortalnext> platform (leader in this area in portugal), offers its customers a powerful cross platform of electronic procurement. despite the quality of the existing vortalnext> platform, there are several gaps in usability that are the object of study in this dissertation. usability presents itself as a critical point for the acceptance of the platform by its customers. interviews, heuristics, questionnaires and tests were used for the analysis of real customer problems and it was concluded that the dashboard is one of the most relevant acceptance factors. based on the results, and with the focus on the dashboard, there were used several tools, such as information architecture and positioning and organization of items on the screen. colors were used to highlight the most important actions, numbers to represent workload/business opportunities, visual alerts for warning/reminder, prioritized lists and one-click actions, thus minimizing the usability problems detected through this work. through this strategy, a new dashboard was designed to present only the necessary when it is necessary in a lovely and useful way for the final user.",
      "l’e-procurement est présenté comme une obligation à être accompli, par les divers états, dans le contexte de l’union européenne. jusqu’en 2015, tous les recrutements publiques devront être exécutes dans une total transparence, sécurité et fiabilité, à travers l’utilisation des plateformes web de recrutement correctement certifiés. l’entreprise vortalnext> est positionné comme une des leaders, dans ce domaine, sur le marché portugais, offrant à ses clients une plateforme transversal et puissante au niveau des recrutements. malgré la qualité existante dans la plateforme de vortalnext>, il existe plusieurs lacunes d’utilités qui sont objet d´une étude sur cette thèse. l’ergonomie se présente comme un point critique pour l’acceptation de la plateforme de la part de ses clients. entretiens, heuristiques, questionnaires et tests sont les outils utilisés pour l’analyse des réels problèmes des clientes, concluant que le dashboard est un des facteurs d’acceptation le plus pertinents. ayant comme base les résultats obtenus et comme point principal le dashboard, plusieurs approximations ont été dessinées, tant que l’architecture d’information comme le positionnement et l’organisation des éléments de l’écran. sont utilisées des couleurs pour mettre en évidence les actions les plus importantes, des numéros pour représenter le volume de travail/négociation, des messages visuels comme signal d’avertissement, listes prioritaire et one-click actions, réussissant comme ça la diminution des erreurs dans l’ergonomie détectées à travers le travail réalisé. à travers cette stratégie, un nouveau dashboard a été dessiné pour présenter à peine le nécessaire, quand le nécessaire et d’une forme utile et agréable pour l’utilisateur."
    ],
    0.0
  ],
  [
    [
      "the brain functional connectivity extracted from rs-fmri has been used as a powerful tool to study the different networks in the brain. this neuronal network, found in normal condition, can be associated to different cognitive processes. the applicability of these networks in the future is promising, since is a greater technique to study the effects of several diseases or even treatments on normal brain functional connectivity. firstly, this question should be addressed: are these networks possible to be described and to be used as features to classify a group or a particular subject?. in order to answer this question, it was settled the use of a machine learning method, which has been developed great advances in the recent years, due the good performances in the deep learning (dl) method. therefore, it was created a workflow since the beginning, started with data acquisition until the application of dl methods and the process of creation and fine-tune of these models. in the end, several studies using the functional connectivity were done, namely the assessment of the brain functional connectivity to be used as a “fingerprint”. additionally, it were performed some tests regarding the groups’ classification. after settled the correct approach and validate the dl framework, the “fingerprint” study showed a great improvement on impairment classification, even for simple models. we proved that rs-fmri can be use in research field to identify singular brain patterns as well as the differences between the subjects, which could be applied as group differentiator in a population.",
      "a conectividade funcional cerebral extraída de imagens de rs-fmri demonstrou um potencial adquirido no estudo das diferentes redes existentes no cérebro. relativamente a estas redes, estas são associadas a diferentes processos cognitivos e sensoriais, ou até simplesmente ao funcionamento normal do cérebro. o uso promissor destas redes é acrescido quando aplicado ao estudo de efeitos de diversas doenças, ou até tratamentos que afetam a normal conectividade funcional cerebral. mas, primeiramente, surge a questão: são estas redes possíveis de serem descritas e usadas como características para classificar um determinado grupo ou um indivíduo em particular?. assim, para responder à questão proposta foi definido o uso de um método de machine learning com grandes avanços nos últimos anos devido ao bom desempenho – o método de deep learning (dl). foi então criado um fluxo de trabalho desde a aquisição de dados à aplicação de métodos de dl, seguido de uma framework que lida com o processo de criação e ajuste dos parâmetros dos modelos dl. posteriormente foram feitos diversos estudos usando a conectividade funcional, estática e dinâmica, para nomeadamente estudar se a conectividade funcional cerebral pode ser usada como “impressão digital” e se tem melhor desempenho que outros métodos já aplicados neste tipo de estudo. adicionalmente, foram realizados alguns testes relativamente à classificação e regressão de grupos. em conclusão, foi constatado e validado que esta abordagem e esta framework de dl são boas escolhas. no estudo da “impressão digital”, os resultados melhoraram imenso, mesmo usando modelos simples. consequentemente, foi provado que a rs-fmri pode ser usada para estudos de padrões singulares do cérebro e nas diferenças entre sujeitos, o mesmo aplicado à classificação em grupo na distinção dos mesmos."
    ],
    [
      "em contexto educativo e numa sociedade em permanente mudança, há uma necessidade premente de adaptar as metodologias pedagógicas, no sentido de adequar os processos de ensino e de aprendizagem às características dos alunos, implementando diferentes abordagens, que permitam captar a evolução do aluno e incrementar a sua atenção, motivação e empenho nas matérias a estudar, como forma de promover o sucesso da aprendizagem. durante os últimos anos várias iniciativas de investigação e de aplicação têm sido desenvolvidas com o objetivo de integrar técnicas e modelos de gamificação no domínio dos sistemas de ensino e de aprendizagem, com vista ao desenvolvimento da motivação e desenvolvimento dos alunos nas mais variadas áreas do conhecimento. nesta dissertação demonstramos a aplicação de técnicas de gamificação em sistemas educacionais, através da incorporação de elementos de jogos nas suas várias vertentes, convencidos que estas permitem contribuir positivamente para o desenvolvimento dos processos de aprendizagem e, em particular, para o aumento da concentração e interesse dos alunos nesses sistemas. para esse fim, utilizámos um sistema específico de avaliação de conhecimento, com o objetivo de combater a diminuição da motivação e potenciar o uso e a exploração desse sistema. tendo isso presente, começamos por analisar os benefícios que a gamificação pode ter em contextos educativos, analisando aspetos que permitam aumentar a motivação dos alunos, melhorar o seu processo de aquisição de conhecimento e, consequentemente, o seu sucesso académico. posteriormente, concebemos um modelo de gamificação específico e fizemos a sua implementação no sistema de avaliação referido recorrendo a linguagens como a python, a javascript e a html, entre outras.",
      "in an educational context and in a constant change society, there is an urgent need to adapt the pedagogical methodologies, in order to adjust the teaching and learning processes to the students’ characteristics, implementing different approaches, which allow to capture the student’s evolution and increase their attention, motivation and commitment to the subjects to be studied, as a way to promote learning success. during the last years, several research and application initiatives have been developed with the aim of integrating gamification techniques and models in the domain of teaching and learning systems, with a view to developing students’ motivation and development in the most varied knowledge areas. in this dissertation, we demonstrate the application of gamification techniques in educational systems, through the incorporation of game elements in its various aspects, convinced that these allow a positive contribution to the development of learning processes and, in particular, the increasing of concentration and students interest in these systems. to that end, we used a specific knowledge assessment system, with the aim of combating the decrease in motivation and enhancing the use and exploitation of that system. bearing this in mind, we began by analysing the benefits that gamification can have in educational contexts, evaluating aspects that allow to increase student’s motivation, improve their knowledge acquisition process and, consequently, their academic success. subsequently, we designed a specific gamification model and implemented it in the referred evaluation system, using languages such as python, javascript and html, among others."
    ],
    0.12857142857142856
  ],
  [
    [
      "nos primórdios a internet era usada apenas por algumas pessoas. nessa altura muitas das grandes empresas de tecnologia ainda não tinham aparecido. porém tudo mudou quando a internet entrou nos circuitos comerciais, provocando o aparecimento de estruturas para fazer a ligação das pessoas ao seu mundo. desde aí que as grandes empresas têm adotado novas formas de encarar o mercado, aumentando gradualmente a sofisticação da forma de o fazerem. as empresas começaram a monitorizar a atividade dos seus clientes para que com isso melhorar a oferta dos seus bens e serviços ao público em geral. todavia o conhecimento acerca daquilo que o cliente gosta não é suficiente para o atrair. uma empresa também precisa que a informação apresentada ao cliente seja feita da maneira mais rápida possível. por exemplo, se um cliente esperar mais do que “três” segundos para que o site seja carregado, o cliente irá abandoná-lo e, provavelmente, procurar um outro site de uma empresa concorrente. a google avalia a rapidez dos sites e com isso dá-lhes uma pontuação. os sites com piores pontuações são apresentados em últimos, o que tem, como sabemos, um grande impacto na escolha dos clientes. mas não é só com os clientes que as empresas se tem de preocupar. internamente os serviços dos funcionários de uma empresa podem ser afetados por uma internet lenta, o que conduz a uma perda de performance e ao aumento da frustração do próprio funcionário no local de trabalho. por estas razões é importante que as empresas estejam constantemente a monitorizar o tráfego passado pelos seus servidores, para serem capazes de verificar se os motivos da lentidão dos seus serviços de rede são internos ou não. neste trabalho de dissertação desenvolvemos um trabalho baseado em process mining, que através de uma ferramenta de monitorização de rede, wireshark, permite avaliar a qualidade de serviço da rede através da observação e análise das logs produzidas por alguns dos seus equipamentos, em particular, dos seus routers. como são geradas várias logs para cada um tipo de router foi necessário fazer a sua conciliação, para que, a partir daí, se pudesse obter o percurso que os vários pacotes realizaram na sua movimentação pela rede. desta forma, é possível criar um modelo matemático capaz de determinar um índice de bem-estar relativo à qualidade de serviço da rede de uma empresa. basicamente, este índice permitirá avaliar o desempenho da rede e permitir aos seus gestores identificar, por exemplo, quais os pontos da rede que apresentam menor desempenho (ou estrangulamentos de serviço) e prevenir futuras quebras no serviço geral da rede em análise.",
      "in the early days the internet was only used by some people. by then many of the big technology companies had not yet appeared. but everything changed when the internet entered the market, causing the appearance of structures to connect people to the world. since then, large companies have adopted new ways of looking at the market. companies have begun to monitor the activity of their customers so that they can improve the supply of their goods and services to the public. however, knowledge about what the client likes is not enough to attract him. a company also needs the information presented to the customer is made as quickly as possible. for example, if a client waits more than \"three seconds\" for the site to load, the client will abandon it and probably look for another site from a competing company. google evaluates the speed of websites and gives them a score. the sites with the worst scores are presented in the last, which has, as we know, a great impact on the choice of customers. but it is not just with customers that companies must worry. internally the services of a company's employees can be affected by a slow internet, which leads to a loss of performance and to the frustration of the employee himself in the workplace. for these reasons it is important that companies are constantly monitoring traffic passed by their servers to be able to verify that the reasons for the slowness of their network services are internal or not. in this dissertation we developed a work based on process mining, which through a network monitoring tool, wireshark, allows to evaluate the quality of service of the network through the observation and analysis of logs produced by some of its equipment the routers. as several logs are generated for each type of router, it was necessary to reconcile it, so we could obtain the route that the various packages made through the network. in this way, it is possible to create a mathematical model capable of determining an index of well-being related to the quality of service in a company's network. basically, this index will allow assessing the performance of the network and allow its managers to identify, for example, which network points have the lowest performance (or service bottlenecks) and prevent future outages in the network."
    ],
    [
      "as the years go by, the interaction between humans and machines seems to gain more and more importance for many different reasons, whether it's taken into consideration personal or commercial use. on a time where technology is reaching many parts of our lives, it's important to keep thriving for a healthy progress and help not only to improve but also to maintain the benefits that everyone gets from it. this relationship can be tackled through many points, but here the focus will be on the mind. emotions are still a mystery. the concept itself brings up serious questions because of its complex nature. till the date, scientists still struggle to understand it, so it's crucial to pave the right path for the growth on technology on the aid of such topic. there is some consensus on a few indicators that provide important insights on mental state, like words used, facial expressions, voice. the context of this work is on the use of voice and, based on the field of automatic speech emotion recognition, it is proposed a full pipeline of work with a wide scope by resorting to sound capture and signal processing software, to learning and classifying through algorithms belonging on the semi supervised learning paradigm and visualization techniques for interpretation of results. for the classification of the samples,using a semi-supervised approach with neural networks represents an important setting to try alleviating the dependency of human labelling of emotions, a task that has proven to be challenging and, in many cases, highly subjective, not to mention expensive. it is intended to rely mostly on empiric results more than theoretical concepts due to the complexity of the human emotions concept and its inherent uncertainty, but never to disregard prior knowledge on the matter.",
      "à medida que os anos passam, a interacção entre indivíduos e máquinas tem vindo a ganhar maior importância por várias razões, quer seja para uso pessoal ou comercial. numa altura onde a tecnologia está a chegar a várias partes das nossas vidas, é importante continuar a perseguir um progresso saudável e ajudar não só a melhorar mas também manter os benefícios que todos recebem. esta relação pode ser abordada por vários pontos, neste trabalho o foco está na mente. emoções são um mistério. o próprio conceito levanta questões sobre a sua natureza complexa. até aos dias de hoje, muitos cientistas debatem-se para a compreender, e é crucial que um caminho apropriado seja criado para o crescimento de tecnologia na ajuda da compreensão deste assunto. existe algum consenso sobre indicadores que demonstram pistas importantes sobre o estado mental de um sujeito, como palavras, expressões faciais, voz. o conteúdo deste trabalho foca-se na voz e, com base no campo de automatic speech emotion recognition, é proposto uma sequência de procedimentos diversificados, ao optar por software de captura de som e processamento de sinais, aprendizagem e classificação através de algoritmos de aprendizagem semi supervisionada e técnicas de visualização para interpretar resultados. para a classificação de amostras, o uso de uma abordagem semi supervisionada com redes neuronais representam um procedimentos importante para tentar combater a alta dependência da anotação de amostras de emoções humanas, uma tarefa que se demonstra ser árdua e, em muitos casos, altamente subjectiva, para não dizer cara. a intenção é estabelecer raciocínios baseados em factores experimentais, mais que teóricos, devido à complexidade do conceito de emoções humanas e à sua incerteza associada, mas tendo sempre em conta conhecimento já estabelecido no assunto."
    ],
    0.3
  ],
  [
    [
      "in today’s world the utilization of deep learning (dl) is intrinsically integrated in the activity of several enterprises and industries. it allows us to extract knowledge from data, detect patterns and make pre dictions, increasing the competitivity and quality of the services provided. however, the dl frameworks (e.g., tensorflow, pytorch, apache mxnet) require not only considerable of computational power, but also efficient data storage, since they need to deal with large amounts of data. in particular, in each iteration of the dl model train different batches of the training dataset are accessed to be processed and incorporated in the model. the retrieval of this data can be a bottleneck to the performance of the system, since the datasets are getting increasingly bigger, reaching sizes in the order of tbs. in the case of multi-node dl this becomes increasingly critical since there are many compute nodes training models, possibly with the same dataset, resulting in more requests directed to the shared file system competing with each other. if data could be stored nearer to the computational nodes and those nodes shared the data with one another, it would reduce the i/o pressure in the shared storage system and potentially reduce the time taken by these accesses and, consequently, the training time. this thesis presents distmonarch, a dl framework agnostic system that takes advantage of the storage system hierarchy by copying data to levels closer to each compute node and allows the nodes to share data with each other, in a transparent manner. results show that using this system reduces accesses to the shared file system by up to 90% and training time of some models and configurations by up to 48%.",
      "no mundo atual a utilização de aprendizagem profunda (ap) está intrinsecamente integrada na ativi dade de diversas empresas e instituições. esta permite extrair conhecimento de dados, detetar padrões e efetuar previsões, aumentando assim a competitividade e qualidade dos serviços prestados. no en tanto, as ferramentas de ap (p. ex., tensorflow, pytorch, apache mxnet) requerem não só um grande poder de computação, mas também de armazenamento, uma vez que necessitam de lidar com grandes quantidades de dados. nomeadamente, em cada iteração do treino de um modelo de ap são acedidas diferentes amostras de dados do conjunto de dados de treino para serem processadas e incorporadas no modelo. a obtenção desses dados pode ser um gargalo no desempenho do sistema, uma vez que os conjuntos de dados são cada vez maiores, chegando a tamanhos na ordem dos tbs. no caso de abordagens ap multi-nodo isto é ainda mais crítico, uma vez que temos vários nodos de computação a treinar possivelmente com o mesmo conjunto de dados, resultando em mais pedidos feitos ao sistema de ficheiros partilhado a concorrer entre si. se os dados pudessem ser guardados mais perto dos nodos computacionais e os dados fossem partilhados entre eles isso permitiria reduzir a pressão de e/s no sistema de armazenamento partilhado e potencialmente reduzir os tempos dos acessos e, por conseguinte, do treino. esta tese apresenta o distmonarch, um sistema agnóstico à ferramenta de ap que aproveita a hie rarquia do sistema de armazenamento existente em infraestruturas de computação avançada, copiando dados para níveis mais próximos de cada nodo de computação e permite que os vários nodos partilhem esses dados entre si de forma transparente. os resultados mostram que a utilização deste sistema per mite reduzir os acessos ao sistema de ficheiros partilhado até 90% e tempo de treino de alguns modelos e configurações até 48%."
    ],
    [
      "the controller area network (can) is the backbone of automotive networking, connecting many electronic controlunits (ecus) that control virtually every vehicle function from fuel injection to parking sensors. it possesses,however, no security functionality such as message encryption or authentication by default. attackers can easily inject or modify packets in the network, causing vehicle malfunction and endangering the driver and passengers. there is an increasing number of ecus in modern vehicles, primarily driven by the consumer’s expectation of more features and comfort in their vehicles as well as ever-stricter government regulations on efficiency and emissions. combined with vehicle connectivity to the exterior via bluetooth, wi-fi, or cellular, this raises the risk of attacks. traditional networks, such as internet protocol (ip), typically have an intrusion detection system (ids) analysing traffic and signalling when an attack occurs. the system here proposed is an adaptation of the traditional ids into the can bus using a one class support vector machine (ocsvm) trained with live, attack-free traffic. the system is capable of reliably detecting a variety of attacks, both known and unknown, without needing to understand payload syntax, which is largely proprietary and vehicle/model dependent. this allows it to be installed in any vehicle in a plug-and-play fashion while maintaining a large degree of accuracy with very few false positives.",
      "a controller area network (can) é a principal tecnologia de comunicação interna automóvel, ligando muitas electronic control units (ecus) que controlam virtualmente todas as funções do veículo desde injeção de combustível até aos sensores de estacionamento. no entanto, não possui por defeito funcionalidades de segurança como cifragem ou autenticação. é possível aos atacantes facilmente injetarem ou modificarem pacotes na rede causando estragos e colocando em perigo tanto o condutor como os passageiros. existe um número cada vez maior de ecus nos veículos modernos, impulsionado principalmente pelas expectativas do consumidores quanto ao aumento do conforto nos seus veículos, e pelos cada vez mais exigentes regulamentos de eficiência e emissões. isto, associada à conexão ao exterior através de tecnologias como o bluetooth, wi-fi, ou redes móveis, aumenta o risco de ataques. redes tradicionais, como a rede internet protocol (ip), tipicamente possuem um intrusion detection systems (idss) que analiza o tráfego e assinala a presença de um ataque. o sistema aqui proposto é uma adaptação do ids tradicional à rede can utilizando uma one class support vector machine (ocsvm) treinada com tráfego real e livre de ataques. o sistema é capaz de detetar com fiabilidade uma variedade de ataques, tanto conhecidos como desconhecidos, sem a necessidade de entender a sintaxe do campo de dados das mensagens, que é maioritariamente proprietária. isto permite ao sistema ser instalado em qualquer veículo num modo plug-and-play enquanto mantém um elevado nível de desempenho com muito poucos falsos positivos."
    ],
    0.3
  ],
  [
    [
      "nos últimos anos, a procura por soluções que tirem partido de um computação segura na cloud é um conceito em expansão e de grande interesse. a atratividade deste tema tem motivado a apresentação de inúmeras propostas de protocolos que tiram partido dessas características. contudo, a grande maioria desses protocolos requerem que as funcionalidades a executar se apresentem descritos a um nível de abstração muito baixo, concretamente na forma de circuitos lógicos booleanos. obviamente que não é simples nem produtivo trabalhar a esse nível de abstração, pelo que surge uma necessidade de converter descrições de programas realizado numa linguagem de alto-nível nesses circuitos. este projeto baseia-se no estudo dessa transformação, assegurando que a mesma é correta garantindo a preservação da semântica do código fonte. para a realização desta transformação será proposto um compilador certificado, que terá a intenção de gerar descrições de circuitos booleanos a partir de programas c. para a produção destas descrições será tido em conta a sua eficiência de forma a melhorar a sua performance mantendo a fiabilidade do mesmo.",
      "over the last years, the demand of a secure computation in the cloud has been a growing concept in which people are taking interest in. the attractiveness of his theme has been driving the arise of protocols proposals that take advantages in cloud computing characteristics. however, to solve this need the majority of these solutions require that their features to be implemented in a very low-level of abstraction, more precisely in the format of logical boolean circuits. clearly it is not simple, neither work productive, to implement these specifications in such a low-level of abstraction. so there is a need to transform the description of the functionality implemented in a higher level language into those circuits. this project is based on the study of this transformation, ensuring its correction and the semantic preservation of the source code. in order to perform this transformation we propose a certified compiler which will be able to generate descriptions of boolean circuits from a c programs with certain constraints. it will be also take into account the degree of eficiency of these descriptions, keeping its correctness."
    ],
    [
      "the colour of the sky has always fascinated people throughout the ages. the blue sky, the colour of the sun, the orange of the sunsets, the clouds, etc... for centuries, physicists and mathematicians tried to explain with formulae what artists like leonardo da vinci reproduced in their paintings, like the colour of the sky, the bluish colour of distant mountains, fog, and several other nature effects. in the area of computer graphics there is a great interest in recreating these natural effects, observable everyday, as real as possible. the reproduction of these effects is essential for certain applications such as flight simulators, video-games and other scientific applications, hence, the relevance of rendering these effects in real time. this project aims to present a state of the art on atmospheric scattering and so consolidate knowledge on the subject. much work has been done with the purpose of recreating these effects digitally. in result many algorithms, implemented using different techniques, are now available. when evaluating atmospheric scattering there are several aspects that are considered such as the light source, the density and size of particles in the atmosphere, among others. in this thesis, only daylight models will be considered and in these models, the sun is considered the only light source. the particles on the atmosphere can be divided in 2 main entities: air molecules and aerosol particles. the phenomenon can be explained as the result of the interaction between the rays that come from the sun and the particles in the atmosphere. the most relevant interaction, regarding colour, is scattering. the atmospheric scattering models present methods and techniques to evaluate and recreate this phenomenon. these models are very diverse but they base their techniques with the physics behind the phenomenon. their evolution showed a path that went from quality to efficiency. part of this evolutionary trait came with the evolution on both hardware and software. the early as models had a greater concern in recreating with the most possible accuracy the atmospheric scattering phenomenon. that concern passed, over the years, to recreate this phenomenon the fastest way possible. when talking about rendering the colours of the sky, is natural to want to render the other natural effects such as clouds and light shafts. therefore it is also valuable to present information of some algorithms that can recreate these effects.",
      "a cor do céu sempre fascinou as pessoas ao longo dos tempos. o azul do céu, a cor do sol, os pores-do-sol alaranjados, as nuvens, etc... durante séculos, físicos e matemáticos tentaram explicar com fórmulas o que artistas como leonardo da vinci reproduziram nas suas pinturas, como a cor do céu, a cor azulada de montanhas distantes, névoa, entre outros. na área de computação gráfica há um grande interesse em reproduzir com o maior realismo possivel estes efeitos naturais observados no quotidiano. a reprodução destes efeitos é fundamental para certas aplicações como simuladores de vôo, jogos e outro tipo de aplicações mais científicas, daí, a importância em reproduzir estes efeitos em tempo real. este projeto tem como objetivo apresentar um estado da arte em dispersão atmosférica e assim consolidar o conhecimento sobre o assunto. muito trabalho já foi feito com o propósito de recriar estes efeitos digitalmente. para isso muitos algoritmos, implementados usando diferentes técnicas, foram apresentados. ao avaliar o fenómeno de dispersão atmosférica existem vários aspectos que são considerados, tais como a fonte de luz, a densidade e tamanho de partículas na atmosfera, entre outros. nesta tese, apenas os modelos diurnos serão considerados e nestes modelos, o sol é considerada a única fonte de luz. as particulas existentes na atmosfera podem ser divididas em dois tipos principais: moléculas de ar e aerosóis. o fenómeno pode ser visto como o resultado da interacção entre a luz que vem do sol e as partículas na atmosfera. a interação mais relevante, em relação a cor, é a dispersão. os modelos de dispersão atmosférica apresentaram métodos e técnicas que conseguem avaliar e recriar este fenómeno. estes modelos são diferentes entre eles, mas todos têm como base a física por de trás do fenómeno. a evolução destes modelos mostra um caminho que passou de maior qualidade para uma maior efi- ciência. parte desta característica evolutiva veio com a evolução tanto do hardware como do software. os primeiros modelos, tinham como preocupação principla recriar o fenómeno de dispersão atmosférica com a maior precisão possível. essa preocupação passou, ao longo dos anos, para recriar este fenómeno da maneira mais rápida possível. ao falar sobre como renderizar as cores do céu, é natural falar também sobre outros efeitos naturais tais como nuvens e feixes de luz. por isso, também é importante apresentar alguns algoritmos que conseguem recriar estes efeitos."
    ],
    0.01875
  ],
  [
    [
      "hoje em dia as zonas urbanas sofrem de sobrepopulação, por isso, é essencial que os seus recursos sejam geridos da melhor forma com o intuito de proporcionar uma melhor qualidade de vida aos seus habitantes bem como, um desenvolvimento sustentável. neste contexto, surge o conceito de cidades inteligentes que é focado no uso de novas tecnologias quer ao nível dos transportes, da saúde ou mesmo relacionadas com o meio ambiente, dando a oportunidade de melhorar a qualidade de vida dos habitantes das zonas urbanas. com este trabalho, pretende-se desenvolver um sistema de recomendação de rotas para auxiliar cidadãos com base numa função objetivo específica, como a definição de um percurso de atividade desportiva. um protótipo do sistema foi construído e testado usando uma ferramenta de simulação chamada cupcarbon. assim, como caso de estudo, considerase uma zona urbana onde é definida uma rede de sensores sem fios para monitorizar parâmetros ambientais como luminosidade ou cobertura wi-fi, com o intuito de auxiliar o planeamento de rotas de acordo com os parâmetros pretendidos pelo utilizador.",
      "nowadays urban areas suffer from overpopulation so it is essential that their resources are managed in the best way, with the purpose of providing quality of life for the population and providing a sustainable development. in this context, the concept of smart cities emerges, focusing on the use of new technologies, whether in transport, health or even in the environment domain, opens the opportunity for improving the quality of life of urban dwellers. with this work we intend to develop a route recommendation system oriented to assist citizens based on a specific objective function, such as the definition of a sports activity route. a prototype of the system was built and tested using a simulation tool called cupcarbon. thus, as a case study, it is considered an urban zone where a network of wireless sensors are defined for monitoring environmental parameters such as luminosity or wi-fi coverage, in order to assist the planning of routes according to user preferred parameters."
    ],
    [
      "a modularidade é um conceito importante na implementação de sistemas suportados por software. a linguagem java é uma das linguagens utilizadas para implementar este tipo de sistemas. esta dissertação apresenta um estudo sobre os conceitos de modularidade que o projeto jigsaw propõe para a linguagem java, demonstrando como se comparam com o estado de arte de modularidade em ambientes de desenvolvimento java, as melhorias para a linguagem java e para os sistemas de software desenvolvidos em java, nomeadamente sistemas baseados em servidores aplicacionais. o projeto, através do conceito de modularidade proposto, introduz alterações importantes na linguagem e plataforma java, na forma de desenvolvimento e distribuição de aplicações e esta dissertação pretende, através de análise e demonstração, mostrar a importância da metodologia apresentada e de que forma pode melhorar e substituir as várias metodologias de modularidade em java atualmente existentes. no âmbito desta dissertação, é apresentada uma aplicação informática, na forma de prova de conceito, desenvolvida utilizando a linguagem java, que procura automatizar processos associados à aplicação da metodologia jigsaw no desenvolvimento de aplicações. as conclusões deste estudo permitem perceber que o jigsaw apresenta melhorias significativas que devem ser incorporadas no java mas, permitem também perceber a existência de limitações que devem ser corrigidas por forma a tornar o conceito mais abrangente para ser utilizado nos mais variados cenários, nomeadamente na implementação de aplicações complexas, como é o caso de servidores aplicacionais. a plataforma java encontra-se numa fase de evolução sensível, onde decisões que estão a ser tomadas pelas várias entidades que determinam o futuro da plataforma podem implicar o sucesso ou fracasso da plataforma, sendo o jigsaw um ponto em aberto nesses processos de decisão.",
      "modularity is an important concept in the implementation of systems supported by software. java is one of the languages used to implement such systems. this thesis presents a study on the concepts of modularity presented in jigsaw project for the java language, showing how they compare with the state of the art of modularity in java development environments, improvements to the java language and the software systems developed in java, in particular, on application servers. the project, through the concept of modularity proposed, brings significant changes in the java language and platform, to the form of developing and distributing applications and this thesis seeks, through analysis and demonstration, to show the importance of the methodology presented for the future of the platform and how it can improve and replace the various methodologies of modularity in java that currently exists. under this thesis, we present a computer application (proof of concept), developed using the java language, which seeks to automate processes associated with implementing the jigsaw method in application development. the findings of this study allow us to realize that jigsaw has significant improvements that should be incorporated in java but also allows to realize that there are limitations that should be corrected in order to make the concept more broadly to be used in various scenarios, including in the implementation of complex applications, such as the application servers. the java platform is at a sensitive stage of development, where decisions are being taken by the various entities that determine the future of the platform that can lead to success or failure of the platform, with the jigsaw as an open point in these decision processes."
    ],
    0.0
  ],
  [
    [
      "information storage is a paramount challenge in the current century driven by the need to scale down memory cells and lower their operating voltages while ensuring high-speed and non-volatile characteristics. over the last century, ferroelectric materials have emerged as promising candidates for the development of non-volatile memories where two electrical switching states can be written and retained for a long time. the use of conventional ferroelectrics (perovskite oxides) for memory applications has been intensively studied for decades. however, they are not cmos compatible and are limited by the high growth temperature. recently, the discovery of ferroelectricity in binary oxides, such as zirconium oxide, zro2, or hafnium oxide, hfo2, added new advantages and functionalities in ferroelectrics-based memory devices. this thesis explores the potential of using ferroelectric hfo2- and zro2-based materials for non volatile memory applications. in the first stage, it was investigated the impact of annealing temperature on the ferroelectric properties of (hfxzr1-x)o2 films x = 0, 0.3 and 0.5 in a pt/(hfxzr1-x)o2/w capacitor structure. it was found that an annealing at 680 ºc is the optimal choice for improving ferroelectric properties in terms of thermal budget resulting in a remanent polarization (𝑃𝑟 ) of 9.2 µc/cm2 for the (hf0.3zr0.7)o2 composition. moreover, the device showed a stable performance up to 1.8x106 cycles. in a second research study, a la0.7sr0.3mno3 (lsmo)/hfo2/w stack was grown on a nb:srtio3 substrate. the 3 nm-thick epitaxially grown hfo2 layer was found to crystallize in the polar rhombohedral phase. although no evidence of ferroelectric properties was found, a bipolar interfacial resistive switching behaviour was reported in the fabricated device. it is suggested that this rs behaviour is explained by phase transitions at the lsmo/hfo2 interface caused by a reversible migration of oxygen vacancies. the device showed a memory window of almost 10 and a non-volatility retention of at least 100 seconds.",
      "o armazenamento de informação é um dos grandes desafios do século atual, impulsionado pela necessidade de reduzir a dimensão das unidades de memória e das suas tensões de operação, garantindo simultaneamente uma alta velocidade e características não voláteis. durante o último século, os materiais ferroelétricos surgiram como candidatos promissores para o desenvolvimento de memórias não voláteis, onde dois estados elétricos podem ser registados e retidos durante muito tempo. a utilização de materiais ferroelétricos convencionais (óxidos tipo perovskita) para aplicações de memória tem sido intensamente estudada. todavia, estes materiais são incompatíveis com a tecnologia cmos e são caracterizados por uma elevada temperatura de crescimento. recentemente, a descoberta da ferroeletricidade em óxidos binários, como o óxido de zircónia, zro2, ou o óxido de háfnio, hfo2, introduziu novas vantagens e funcionalidades nos dispositivos de memória baseados em materiais ferroelétricos. esta tese explora o potencial da utilização de materiais ferroelétricos à base de zro2 e hfo2 para aplicações de memória não volátil. numa primeira fase, foi investigado o impacto da temperatura de recozimento nas propriedades ferroelétricas de filmes de (hfxzr1-x)o2 com x = 0, 0.3 e 0.5 numa estrutura pt/(hfxzr1-x)o2/w. verificou-se que um recozimento a 680 ºc é a escolha ideal para a otimização das propriedades ferroelétricas em termos de relação custo-benefício, resultando numa polarização remanescente de 9.2 µc/cm2 para a composição (hf0.3zr0.7)o2. o dispositivo mostrou estabilidade até 1.8x106 ciclos. num segundo estudo, uma estrutura de la0.7sr0.3mno3 (lsmo)/hfo2/w foi depositada num substrato de nb:srtio3. verificou-se que a camada de hfo2 com 3 nm de espessura cristalizou epitaxialmente na fase polar romboédrica. apesar da inexistência de propriedades ferroelétricas, foi registado um comportamento de comutação resistiva (rs) interfacial bipolar no dispositivo fabricado. sugere-se que este rs seja explicado por transições de fase na interface lsmo/hfo2 causadas por uma migração reversível de lacunas de oxigénio. o dispositivo apresentou uma janela de memória de quase 10 e uma retenção de pelo menos 100 segundos"
    ],
    [
      "a usurpação de contas e o roubo de identidade são problemas muito frequentes nos atuais sistemas informáticos. a facilidade de acesso à internet e a exposição das pessoas a este meio, torna muito frequente a utilização indevida e a usurpação de contas (tais como: e-mail, redes sociais, contas bancárias) por outras pessoas que não as suas legítimas proprietárias. atualmente o método de autenticação dominante é o da combinação nome de utilizador e palavra-chave. no entanto, este método pode não ser fiável, pois estas credenciais podem ser partilhadas, roubadas ou até esquecidas. por outro lado podem-se combinar várias técnicas para reforçar a segurança dos sistemas. cartões de acesso (tokens), certificados digitais e biometrias são algumas delas. os cartões de acesso, por exemplo os das caixas multibanco, podem ser roubados ou duplicados, como é frequentemente noticiado em fraudes bancárias. os certificados seguem o mesmo caminho dos tokens uma vez que estes podem ser distribuídos por correio eletrónico ou em dispositivos usb. as biometrias físicas (impressão digital, íris, retina ou geometria da mão por exemplo), para além de serem um pouco intrusivas, requerem a aquisição de equipamento caro. uma possível solução para os problemas inumerados são as biometrias comportamentais. a forma como nos comportamos e agimos num computador pode ser usada como informação biométrica. esta informação pode ser utilizada à posteriori, geralmente complementada com mais dados, para identificar, inequivocamente, (ou pelo menos com um determinado grau de confiança) um indivíduo. a informação recolhida pode variar desde o tipo de escrita no teclado, habilidade com o rato, hábitos, cliques, número de páginas abertas, origem do acesso, etc., que depois será sujeita à utilização de algoritmos comportamentais para autenticar, de forma inequívoca, um utilizador. neste trabalho pretende-se implementar como reforço aos atuais sistemas de autenticação e de deteção de intrusões, a verificação de perfis comportamentais do proprietário da conta. este sistema não irá apresentar grandes custos, já que só serão usados equipamentos básicos, e será completamente invisível para o utilizador, ou seja este será continuamente autenticado de forma silenciosa e não intrusiva.",
      "session hijacking and identity theft are a problem increasingly common in computer systems nowadays. with the growing usage of online services, people become more exposed to different techniques, technological or social, that can be used to easy to their personal accounts, from services such as emails, facebook, bank accounts, among others. currently, the dominant method of authentication is the combination of username and password. this method can be unreliable, because these credentials can be shared, forgotten or stolen. to offer better authentication mechanisms, other techniques are used; among then are the tokens or digital certificates and biometrics. none of them completely solve the problem once they can be duplicated or stolen. moreover the physiological biometrics (fingerprint, iris, retina, hand geometry, etc.) are intrusive, require the purchase of expensive equipment and may not work in all the scenarios. the way we behave and act in a computer can be used as biometric information. this information supplemented with more data (i.e. contextual data) can be used to identify unequivocally (or at least with a certain degree of confidence) an individual. the information collected may vary from the way of typing on a keyboard (keystroke dynamics), skill with the mouse (mouse dynamics), habits, clicks, number of pages open, source access, etc., which will then be subject to the use of behavioral algorithms to identify and authenticate, unequivocally, the user. in this work we present the implementation of a system that strengthens existing authentication and intrusion detection systems, helping them by checking behavioral profiles of the account owner. this system will not be costly, since it only uses basic hardware. additionally, it will be completely invisible to the user, i.e., it will be working in an unobtrusive way, collecting data in background mode. the aim of this paper is to present a system capable of recognizing biometric patterns and, through behavioral algorithms and complex event processing, create user profiles that are used as identification and continuously authentication to services."
    ],
    0.0
  ],
  [
    [
      "in the last few years, the automotive industry has relied heavily on deep learning applications for perception solutions. with data-heavy sensors, such as lidar, becoming a standard, the task of developing low-power and real-time applications has become increasingly more challenging. to obtain the maximum computational efficiency, no longer can one focus solely on the software aspect of such applications, while disregarding the underlying hardware. in this thesis, a hardware-software co-design approach is used to implement an inference application leveraging the squeezesegv3, a lidar-based convolutional neural network, on the versal acap vck190 fpga. automotive requirements carefully drive the development of the proposed solution, with real-time performance and low power consumption being the target metrics. a first experiment validates the suitability of xilinx’s vitis-ai tool for the deployment of deep convolutional neural networks on fpgas. both the resnet-18 and squeezenet neural networks are deployed to the zynq ultrascale+ mpsoc zcu104 and versal acap vck190 fpgas. the results show that both networks achieve far more than the real-time requirements while consuming low power. compared to an nvidia rtx 3090 gpu, the performance per watt during both network’s inference is 12x and 47.8x higher and 15.1x and 26.6x higher respectively for the zynq ultrascale+ mpsoc zcu104 and the versal acap vck190 fpga. these results are obtained with no drop in accuracy in the quantization step. a second experiment builds upon the results of the first by deploying a real-time application containing the squeezesegv3 model using the semantic-kitti dataset. a framerate of 11 hz is achieved with a peak power consumption of 78 watts. the quantization step results in a minimal accuracy and iou degradation of 0.7 and 1.5 points respectively. a smaller version of the same model is also deployed achieving a framerate of 19 hz and a peak power consumption of 76 watts. the application performs semantic segmentation over all the point cloud with a field of view of 360°.",
      "nos últimos anos a indústria automóvel tem cada vez mais aplicado deep learning para solucionar problemas de perceção. dado que os sensores que produzem grandes quantidades de dados, como o lidar, se têm tornado standard, a tarefa de desenvolver aplicações de baixo consumo energético e com capacidades de reagir em tempo real tem-se tornado cada vez mais desafiante. para obter a máxima eficiência computacional, deixou de ser possível focar-se apenas no software aquando do desenvolvimento de uma aplicação deixando de lado o hardware subjacente. nesta tese, uma abordagem de desenvolvimento simultâneo de hardware e software é usada para implementar uma aplicação de inferência usando o squeezesegv3, uma rede neuronal convolucional profunda, na fpga versal acap vck190. são os requisitos automotive que guiam o desenvolvimento da solução proposta, sendo a performance em tempo real e o baixo consumo energético, as métricas alvo principais. uma primeira experiência valida a aptidão da ferramenta vitis-ai para a implantação de redes neuronais convolucionais profundas em fpgas. as redes resnet-18 e squeezenet são ambas implantadas nas fpgas zynq ultrascale+ mpsoc zcu104 e versal acap vck190. os resultados mostram que ambas as redes ultrapassam os requisitos de tempo real consumindo pouca energia. comparado com a gpu nvidia rtx 3090, a performance por watt durante a inferência de ambas as redes é superior em 12x e 47.8x e 15.1x e 26.6x respetivamente na zynq ultrascale+ mpsoc zcu104 e na versal acap vck190. estes resultados foram obtidos sem qualquer perda de accuracy na etapa de quantização. uma segunda experiência é feita no seguimento dos resultados da primeira, implantando uma aplicação de inferência em tempo real contendo o modelo squeezesegv3 e usando o conjunto de dados semantic-kitti. um framerate de 11 hz é atingido com um pico de consumo energético de 78 watts. o processo de quantização resulta numa perda mínima de accuracy e iou com valores de 0.7 e 1.5 pontos respetivamente. uma versão mais pequena do mesmo modelo é também implantada, atingindo uma framerate de 19 hz e um pico de consumo energético de 76 watts. a aplicação desenvolvida executa segmentação semântica sobre a totalidade das nuvens de pontos lidar, com um campo de visão de 360°."
    ],
    [
      "model checking é uma técnica comum de verificação; garante a consistência e integridade de qualquer sistema fazendo uma exploração exaustiva de todos os possíveis estados. devido à grande quantidade de intercalações possíveis entre eventos, modelos de sistemas distribuídos muitas vezes acabam por gerar um número de estados muito grande. nesta dissertação vamos explorar os efeitos de partial order reduction — uma técnica para mitigar os efeitos da explosão de estados — implementando uma linguagem semelhante ao electrum com ltsmin. vamos também propor um event layer por cima do electrum e uma análise sintática para extrair informação necessária para que esta técnica possa ser implementada.",
      "model checking is a common verification technique to guarantee the consistency and integrity of any system by an exhaustive exploration of all possible states. due to the large amount of interleavings, models on distributed systems often end up with a huge state-space. in this dissertation we will explore the effects of partial order reduction — a technique to mitigate the effects of this state-explosion problem — by implementing an electrum-like language with ltsmin. we will also propose an event layer over electrum and a syntactic analysis to extract valuable information for this technique to be implemented."
    ],
    0.3
  ],
  [
    [
      "xylella fastidiosa is a phytopathogenic bacteria that causes disease in hundreds of differ ent plant species. increased reports of plants infected by these xylem-limited bacteria are alarming as this pathogen continues to attack crops of relevant economic power such as citrus, grapes, olives and almonds, with considerable economical losses for the producers. the current employed strategy to contain this epidemic is radical in action as it destroys the infected plant and surrounding area. for this reason, it became urgent to develop new ways to eliminate these bacteria with therapeutics that are more pathogen oriented. genome-scale metabolic (gsm) models contain genomic and metabolic information of a given organism and can be used to discover new potential drug targets. thus, a gsm model of x. fastidiosa may unveil new ways to control these bacteria. in this work, we developed a high-quality gsm model for x. fastidiosa subsp. pauca de donno, using the user-friendly software metabolic models reconstruction using genome scale information (merlin). this strain was chosen for its importance in the national econ omy, as it causes olive quick decline syndrome. the reconstructed model of x. fastidiosa comprises a set of 1280 reactions and 524 genes. the genome of x. fastidiosa subsp. pauca de donno was functionally annotated in order to identify the metabolic potential of the phytopathogenic organism. metabolic functions identified in the genome were used to assemble the initial draft metabolic network. manual curation procedures were made in order to correctly represent organism’s capabilities and biomass related reactions, based on literature and experimental data, were added to model. the reconstructed model was then validated using experimental data, regarding the aerobic metabolism, carbon flux pattern, carbon usage, amino acid auxotrophies and growth in several media developed for x. fastidiosa. in silico simulations revealed interesting metabolic properties of x. fastidiosa. the usage of carbon through the entner-doudoroff pathway seems to be a way of generating redox potential for defensive mechanism against the host plant. an absence of auxotrophies and the presence of catabolic routes for amino acids shows the metabolic and adaptive potential of the organism, as expected since it grows on nutrient-limited environments. this reconstructed model can be used to explore the metabolism and provide information for potential drug targets for the agricultural-destructive phytopathogen x. fastidiosa.",
      "xylella fastidiosa é uma bactéria fitopatogénica que causa doenças em centenas de espécies de plantas. o aumento de relatos de plantas infetadas por esta bactéria limitada ao xilema é alarmante, uma vez que o patogéneo ataca cultivações de significante valor económico, como o caso de citrinos, uvas, azeitonas e amêndoas, resultando em perdas económicas extensivas para produtores. a atual estratégia de contenção da epidemia é radical, uma vez que resulta na destruição da planta infetada e da área circundante. desta forma, é urgente desenvolver novas formas de eliminar esta bactéria com terapias direcionadas ao patogéneo. modelos metabólicos a escala-genómica contém informação genómica e metabólica de um organismo e podem ser utilizados para descobrir novos potenciais alvos terapêuticos. assim, um modelo metabólico à escala-genómica de x. fastidiosa poderá desvendar novas formas de controlar a bactéria. nesta tese, foi desenvolvido um modelo de alta qualidade para x. fastidiosa subsp. pauca de donno, recorrendo a ferramenta merlin. esta estirpe foi escolhida pela sua importância na economia nacional, visto que causa síndrome de declínio rápido da oliveira. o modelo reconstruído de x. fastidiosa contém um conjunto de 1280 reações e 524 genes. o genoma de x. fastidiosa subsp. pauca de donno foi funcionalmente anotado de forma a identificar o potencial metabólico do organismo fitopatogénico. funções metabólicas identificadas no genoma foram utilizadas para agregar uma rede metabólica. posteriormente, curação manual da rede metabólica foi feita para representar corretamente as capacidades do organismo, e reações associadas a biomassa, baseadas em literatura e dados experimentais, foram adicionadas ao modelo. o modelo reconstruído foi de seguida validado usando dados experimentais em relação ao metabolismo aeróbico, padrão do fluxo de carbono, utilização de carbono, auxotrofias de aminoácidos e crescimento em diversos meios criados para o crescimento de x. fastidiosa. simulações in sílico revelaram propriedades metabólicas interessantes de x. fastidiosa. a utilização de carbono pela via entner-doudoroff parece ser uma estratégia para gerar potencial redutor para mecanismos defensivos contra a planta hospedeira. a ausência de auxotrofias e a presença de vias catabólicas de aminoácidos mostram o potencial metabólico e adaptativo do organismo, como seria de esperar face ao seu crescimento natural em ambientes com restrições nutritivas. este modelo pode ser usado para explorar o metabolismo e fornecer informação de potenciais alvos terapêuticos para o fitopatogénico destrutivo para a agricultura, x. fastidiosa."
    ],
    [
      "com a enorme quantidade de aplicações e sistemas web que nos são apresentados existe uma constante preocupação com a nossa segurança e privacidade como utilizadores dos mesmos. todos nós temos o direito à privacidade dos nossos dados e quando fazemos um registo num novo produto de software queremos acreditar que estaremos protegidos de ataques alheios e que, a não ser que a nossa password seja descoberta, nenhuma informação nossa vai ser vazada. para tal também nos é exigido, consumidores de tecnologia e aplicações, que tomemos uma atitude no sentido de nos protegermos. uma dessas formas é usar passwords seguras e diferentes para cada conta criada. como isto facilmente se toma impraticável devido à enorme quantidade de contas e, consequentemente passwords que é necessário decorar, surgiram os gestores de passwords. estes servem para guardar as nossas passwords de forma segura e confiável para que sempre que precisemos de uma password a irmos buscar de forma simples e rápida. assim este projecto visa re-implementar a componente criptográfica do gestor de passwords keepass de forma a garantir os mais altos níveis de confiabilidade e segurança. para isso, dever-se-á tirar partido das soluções tecnológicas mais recentes para assegurar os referidos níveis de confiabilidade e segurança, como sejam o uso de linguagens de domínio específico para codificação de técnicas criptográficas e sistemas de provas que possam assegurar a respectiva correcção. para o efeito fazer-se-á uso da linguagem jasmin e do sistema de provas easycrypt.",
      "with the huge amount of applications and web systems that are presented to us there is a constant concern about our security and privacy as users of them. we all have the right to the privacy of our data and when we register for a new software product we want to believe that we will be protected from outside attacks and that unless our password is discovered, no information about us will be leaked. this also requires us, consumers of technology and applications, to take action to protect ourselves. one of these ways is to use secure and different passwords for each account created. as this easily becomes impractical due to the huge amount of accounts and consequently passwords that need to be memorized, the passwords managers appeared. these serve to store our passwords safely and reliably so that whenever we need a password we get it in a simple and fast way. so this project aims to re-implement the cryptographic component of keepass passwords manager in order to ensure the highest levels of reliability and security. to this end, the latest technological solutions should be used to ensure these levels of reliability and security, such as the use of domain-specific languages for coding cryptographic techniques and security proof system that can ensure their correctness. for this purpose will be used the jasmin language and easycrypt as security proof system."
    ],
    0.03
  ],
  [
    [
      "today’s computer networks face demanding challenges with the proliferation of services and applications requiring constant access, low latency and high throughput from network infrastructures. the increase in the demand for this type of services requires continuous analysis and a network topology capable of adapting to the dynamic nature of applications, in order to overcome challenges such as performance, security and flexibility. software-defined networking (sdn) emerge as a solution to meet these challenges by using a network control plane, dissociated from the data plane, able to have a global view of the topology and act when required, depending on the variation in infrastructure congestion. decisions involving different activities, such as network management and performance evaluation, rely on information about the state of the network that in traditional networks involves a substantial amount of data. traffic sampling is essential in order to provide valuable statistical data to applications and enable appropriate control and monitoring decisions to be made. in this context, this work proposes the application of time-based sampling techniques in a sdn environment to provide network statistics at the controller level, taking into account the underlying need to establish a balance between the reliability of the data collected and the computational burden involved in the sampling process. the results obtained emphasize that it is possible to apply these sampling techniques by using openflow group mod messages, although packet losses can occur on the switch during periods of network congestion.",
      "as redes de computadores atuais enfrentam desafios exigentes, com a proliferação de serviços e aplicações que exigem acesso constante, baixa latência e elevado fluxo de dados. o aumento na procura deste tipo de serviços exige análise contínua e uma topologia de rede capaz de se adaptar à natureza dinâmica das aplicações, de forma a superar desafios como desempenho, segurança e flexibilidade. as redes definidas por software (software-defined networking - sdn) surgem como uma solução para corresponder a este desafio, através da utilização de uma estrutura de controlo na rede, separada do plano de dados, capaz de ter uma visão global da arquitetura e agir adequadamente consoante as necessidades, dependendo da variação na congestão da infraestrutura. as decisões que envolvem diversas atividades, tais como gestão da rede e avaliação de desempenho, dependem de informação sobre o estado da rede que, em redes tradicionais, envolve uma quantidade substancial de dados. a amostragem de tráfego é essencial para fornecer dados estatísticos valiosos a aplicações e permitir definir decisões adequadas de controlo e monitorização. neste contexto, este trabalho propõe a implementação de técnicas de amostragem baseadas em tempo num ambiente sdn, para disponibilizar estatísticas da rede ao nível do controlador, tendo em conta a necessidade subjacente de estabelecer um balanço entre a fiabilidade dos dados recolhidos e o peso computacional envolvido no processo de amostragem. os resultados obtidos enfatizam que é possível aplicar essas técnicas de amostragem utilizando mensagens openflow group mod, embora perdas de pacotes possam ocorrer no switch em períodos de congestionamento da rede."
    ],
    [
      "atualmente com a massificação da internet ao redor do mundo é inegável que a implantação das tecno logias de suporte à internet para os seus consumidores tem de passar por um processo de planeamento antes da sua implantação, de modo a garantir máxima eficiência económica e máxima qualidade nos serviços de comunicação. a tecnologia mais recente apta para a construção de novas redes fixas é a fibra ótica, capaz de transmitir dados recorrendo a sinais luminosos ao invés de sinais elétricos, garantindo elevadíssimas taxas de transferências de dados assim como confiabilidade e segurança acrescida. o planeamento destas redes é de elevada importância para o sucesso do projeto de rede. com este criam-se diversos resultados (um dos quais, a lista de materiais (bom)) que permitem minimizar os custos envolvidos, tanto na implantação como na futura manutenção da rede, resultando em economias de dinheiro significativas a longo prazo. apesar das vantagens do planeamento destas redes, nas empresas, geralmente este é efetuado super ficialmente, utilizando ferramentas genéricas que não automatizam a maioria das fases de planeamento e obrigam à intervenção manual que, devido à sua natureza minuciosa e ao erro humano, obriga a um prazo de execução prolongado que acaba por causar impactos negativos altamente significativos. para mitigar este problema a proef, empresa promotora desta dissertação, decidiu projetar quatro sistemas capazes de automatizar ao máximo cada uma das fases do planeamento, aspirando tornar este mais rápido e fiável. a presente aplicação, fttx bom, incorpora-se na fase de construção da lista com todos os materiais necessários (bom) e, visa automatizar a sua construção recorrendo apenas a informação dos documen tos resultantes da fase de survey, juntamente de regras de construção pré-definidas pela operadora financiadora que, tipicamente é um provedor de serviço de internet (isp). o bom desempenha um papel relevante na aprovação final por parte da operadora que financia o projeto e, atualmente, como a sua elaboração é feita manualmente, implica um período significativo frequentemente estendido devido a erros e inconsistências encontradas em fases avançadas, acarretando prejuízos reputacionais e financeiros.",
      "nowadays, with the massification of the internet around the world, it is undeniable that the implementation of internet support technologies for consumers must go through a planning process prior to deployment, in order to guarantee maximum economic efficiency and maximum quality in communication services. the latest technology suitable for building new fixed networks is optical fiber, which is capable of transmitting data using light signals rather than electrical signals, guaranteeing extremely high data transfer rates as well as increased reliability and security. the planning of these networks is of great importance to the success of the network project. it creates a number of deliverables (one of which is the bill of materials (bom)) that make it possible to minimize the costs involved, both in the deployment and future maintenance of the network, resulting in significant savings in the long term. despite the advantages of planning these networks, in companies it is generally carried out superfici ally, using generic tools that don’t automate most of the planning phases and require manual intervention which, due to its meticulous nature and human error, results in a long lead time that ends up causing highly significant negative impacts. to mitigate this problem, proef, the company behind this dissertation, decided to design four systems capable of automating each of the planning phases as much as possible, with the aim of making planning faster and more reliable. the present application, fttx bom, is incorporated into the construction phase of the list with all the necessary materials (bom) and aims to automate its construction using only information from the documents resulting from the survey phase, together with construction rules predefined by the financing operator, which is typically an internet service provider (isp). the bom plays an important role in the final approval by the operator financing the project and, currently, as it is drawn up manually, it involves a significant period, often extended due to errors and inconsistencies found at later stages, leading to reputational and financial damage."
    ],
    0.3
  ],
  [
    [
      "the time of doing all the work manually is passing by as the influence of developing technology is increasing. most of the tasks done in a business can be automated by software. because of that, the growing demand for this kind of technology is making it companies develop any software that is required. this report - the dissertation that describes a thesis in informatics engineering - covers some of these technologies, focusing on the clinic area. the development of a clinic web application was proposed by wintouch to help clinic businesses boost their productivity and organization. this master’s work, herein reported, began with the research of the state of the art, studying what the market is like, and analyzing what are the drawbacks of the existing similar applications. the lessons learned at that stage were relevant to design a new web application that can stand out above competitors. the design of the application’s architecture is discussed below along with the technologies used to best fit the application to reach the objectives proposed and meet the desired requirements. the report presents a detailed account of the outcomes of the development process, encompassing both backend and frontend implementations. notable features and functionalities are thoroughly documented, alongside a reflection of the challenges encountered during the development journey.",
      "o tempo de realizar todo o trabalho manualmente está a passar à medida que aumenta a influência do desenvolvimento tecnológico. a maioria das tarefas realizadas num negócio pode ser automatizada por software. devido a isso, a crescente procura por este tipo de tecnologia está a levar as empresas de ti a desenvolverem qualquer software necessário. este relatório - a dissertação que descreve uma tese em engenharia informática - aborda algumas dessas tecnologias, com foco na área clínica. o desenvolvimento de uma aplicação web para clínicas foi proposto pela wintouch para ajudar a impulsionar a produtividade e a organização dos negócios clínicos. este trabalho de mestrado, aqui relatado, começou com a pesquisa do estado da arte, estudando como está o mercado e analisando as limitações das aplicações similares existentes. as lições aprendidas nessa fase foram relevantes para projetar uma nova aplicação web que se destacasse dos concorrentes. o design da arquitetura da aplicação é discutido abaixo, juntamente com as tecnologias usadas para melhor adequar a aplicação aos objetivos propostos e satisfazer os requisitos desejados. o relatório apresenta um relato detalhado dos resultados do processo de desenvolvimento, abrangendo tanto as implementações do backend quanto do frontend. recursos e funcionalidades notáveis são minuciosamente documentados, juntamente com uma discussão dos desafios encontrados durante a jornada de desenvolvimento."
    ],
    [
      "this document presents the topic “applying attribute grammars to teach linguistic rules”, at universidade do minho in braga, portugal. this thesis is focused on using the formalisms of attribute grammars in order to create a tool to help linguistic students learn the different rules of a natural language. the system developed, named lyntax, consists in a processor for a domain specific language which intends to enable the user to specify different kinds of sentence structures, and afterwards, test various phrases against said structures. the processor validates and evaluates the input given, generating a grammar which is specific to a previously chosen sentence. lastly, using antlr, a parser is generated for that specific grammar referred above. the processor built by antlr also creates a syntax tree that is presented to the user for analysis purposes. an interface that supports the specification of the language (written in lyntax dsl) was built, also allowing the use of the processor and the generation of the specific grammar, exempting the user from knowing the details of the process. within this document, the focus will be primarly dedicated to the analysis of the system and how each block was built. different examples of the processor in action will be shown and explained.",
      "este documento refere-se a uma dissertação sobre o tópico “aplicar gramáticas de atribu tos no ensino de regras de linguística”, e será concluída na universidade do minho em braga, portugal. esta dissertação pretende focar-se no uso dos formalismos das gramáticas de atributos de maneira a criar uma ferramenta que ajude os alunos de linguística a aprender as diversas regras da língua natural. o sistema desenvolvido, denominado de lyntax, consiste em um processor para uma linguagem de domínio específico cujo objetivo é o de permitir ao seu utilizador a possibili dade de especificar diversas estruturas de frases, e posteriormente, testar frases contra essas mesmas estruturas. o processador valida e avalia o input recebido, gerando uma gramática específica à frase previamente escolhida. por fim, usando uma ferramenta como o antlr, um parser é gerado para a gramática específica acima referida. o processador construído pelo antlr também gera a árvore de syntax que é apresentada ao utilizador com o intuito de ser analisada. foi também criada uma interface que suporta a especificação da linguagem, permitindo também o uso do processador e a geração da gramática específica, abstraindo assim o utilizador de quaisquer tipo de cálculos. neste documento, o focus primário será dedicado à análise do sistema e como cada bloco foi construído. diferentes exemplos de uso do processador serão apresentados e explicados."
    ],
    0.3
  ],
  [
    [
      "a realidade aumentada (ra) caracteriza-se pela mistura de elementos virtuais no mundo real de forma interativa e em tempo real. o conceito de ra levanta uma ampla variedade de questões quanto à coerência visual entre os objetos reais e virtuais num ambiente. de forma a melhorar o processo de inclusão destes elementos no meio físico foram criadas várias técnicas e algoritmos de visão por computador que através do mapeamento de espaços físicos, extração de características e marcadores fiduciais de objetos, verificação, deteção, identificação, classificação, entre outros, permitem analisar e estruturar o conteúdo de uma cena. o maior desafio que se coloca com a realização desta proposta de dissertação encontra-se associado à forma como é extraída e processada a informação que conseguimos obter a partir dos sensores que complementam os dispositivos de ra hoje em dia, a fim de representar e compreender, da melhor forma possível, os ambientes que nos rodeiam e preparar um espaço apto para a introdução e apresentação de conteúdo virtual com a maior harmonia. neste documento é possível encontrar o estado da arte relativo aos temas previamente citados a fim de explorar, melhorar e desenvolver novas técnicas e paradigmas para, a partir da informação dos sensores mais genéricos encontrados em muitas das tecnologias móveis e óculos de realidade aumentada mais atuais, extrair várias características do cenário e objetos envolventes em tempo real. o processamento e tratamento desta informação tem como objetivo final realizar o reconhecimento e compreensão da cena e objetos que se encontram no espaço que rodeia estes sensores. em paralelo à realização desta proposta de dissertação, foi desenvolvida uma framework denominada “tangible environments in augmented reality systems (tears)” com o objetivo de demonstrar tudo o que é discutido neste documento não só como algo para fins de investigação científica, mas também para utilização e apoio num projeto e protótipo realizado no âmbito da unidade curricular do 5ºano do mestrado integrado em engenharia informática (miei) de projeto em engenharia informática (pei) e que apresenta o título: “assistência remota com realidade mista (arrm)”.",
      "augmented reality (ar) is described as the mixing of virtual elements in the real world in an interactive way and in real time. the concept of ar raises many questions about the visual coherence between real and virtual objects in an environment. in order to improve the process of inclusion of these elements in the physical environment, a number of techniques and algorithms of computer vision have been created, which, through spatial mapping, extraction of characteristics and fiducial markers of objects, verification, detection, identification, classification, among others, allow us to analyse and structure the content of a scene. the greatest challenge with this dissertation proposal is associated to how information, that we can get from the sensors that complement the ar devices today, is extracted and processed to better represent and understand our surroundings and prepare a suitable space that allows the introduction and presentation of virtual content with the greatest harmony. in this document it is possible to find the state of art related to the before mentioned themes in order to explore, improve and develop new techniques and paradigms in a way that, from the information of the most generic sensors found in many of the most current mobile technologies and augmented reality glasses, we can extract various features of the scene and surrounding objects in real time. the stage of processing and treat this information has as its final goal the recognition and understanding of the scene and objects that are in the space that surrounds these sensors. in parallel to this dissertation proposal, a framework called \"tangible environments in augmented reality systems (tears)\" was developed with the intention of demonstrating everything that is discussed in this document not only for scientific research purposes, but also for use and support in a project and prototype carried out within the scope of the curricular unit of the 5th year of the integrated master’s in informatics engineering (imie) named informatics engineering project (iep) and is titled: \"remote assistance with mixed reality (ramr)\"."
    ],
    [
      "mobile devices have evolved in a continuous and steady way both in terms of computing capacity and communication capabilities. this evolution is also reflected in the level of operating systems and apps developed for the most diverse areas of activity and interest. however, the mobile data plan negotiated with network providers as well as the capacity of the batteries continue to be limitations, implying an efficient management of these resour ces. although monitoring of data usage and computational resources already exists in most mobile devices, it is not clear what are the differences between the available platforms, web and application, when these offer the same service. thus, this study intends to develop a process that details the differences of a service in both platforms, mainly focusing on assessing the data involved in its use and the necessary computational resources. this comparative study contributes to assist the end user to elect the most convenient platform for saving resources.",
      "os dispositivos móveis têm evoluído de forma acentuada e contínua, quer em termos de capacidade de computação, quer em termos de recursos de comunicação. esta evolução reflete-se também ao nível dos sistemas operativos e apps desenvolvidas para as mais diver sas áreas de atividade e interesse. apesar disto, o plano de dados móveis negociados com fornecedores de serviço assim como a capacidade das baterias continuam a ser limitações, exigindo uma gestão eficiente destes recursos. apesar da monitorização de dados móveis e recursos computacionais já existir na maior parte dos dispositivos móveis, não são claras as diferenças entre as plataformas disponíveis, web e aplicacional, mesmo quando oferecem o mesmo serviço. assim, neste projeto propõe se o desenvolvimento de um processo que detalhe as diferenças de um dado serviço nas duas plataformas, focando-se principalmente nas diferenças de tráfego necessárias para a sua utilização. este estudo comparativo ajuda o utilizador final na escolha da plataforma que menores recursos consome."
    ],
    0.3
  ],
  [
    [
      "currently, the university of minho owns a driving simulator, from now on referred to as driving simulator mockup 2-wheeler (dsm-2w), which mimics a real driving environment for motorcycles. this simulator can reproduce diverse driving scenarios, like driving on different roads, traffic, and weather conditions, and is mostly used to test how the driver reacts to stimulus from subsystems under test in a particular scenario. the simulator has several components, namely, the mock-up, which represents the motorcycle physically, the software responsible for the simulation environment, that is also projected on a screen, called silab [1] as well as several other subsystems and respective software, which all together form a complex distributed system. silab creates realistic graphic environments, has different models to control the behavior of other drivers and pedestrians, generates 3d sounds, and facilitates the personalization of the simulation scenario. robot operating system 2 (ros2) [2] provides a set of tools and software libraries that facilitate the develop ment of robot systems and applications. with the increasing reliance on software, sensors, and actuators in the automotive domain, it makes sense to view cars [3] and motorcycles as robots. therefore, it also makes sense to use ros2 in the simulation domain to solve the problems at hand. this dissertation describes how ros2, a well-known and accepted middleware for robotic applications, can also play a role in these contexts acting as a universal interface between motorcycle simulators and external subsystems and thereby significantly improving the system’s expansibility and those subsystems’ portability and reusability.",
      "a universidade do minho possui um simulador de motas, denominado driving simulator mockup 2-wheeler (dsm-2w), que imita um ambiente real de condução de motas. esta ferramenta consegue reproduzir diversos cenários de condução, como conduzir em diferentes condições de estrada, tráfego, bem como em diferentes condições meteorológicas. esta ferramenta é sobretudo usada para testar como o condutor reage a estímulos de vários sub-sistemas em teste em cenários particulares. o simulador possui diversos componentes, o mock-up, que representa a mota fisicamente, o software responsável pela projeção do ambiente de simulação no ecrã, chamado silab [1], mais um conjunto de sub-sistemas e o respetivo software, que no conjunto formam um complexo sistema distribuído. o silab cria ambientes de simulação realistas, tem diferentes modelos para controlar o comportamento dos outros condutores e dos pedestres, gera sons 3d e facilita a personalização do cenário da simulação. o robot operating system 2 (ros2) possui um conjunto de ferramentas e bibliotecas para desenvolver aplicações para robôs [2]. com o aumento do uso de software, sensores, e atuadores no contexto automóvel, faz sentido equiparar veículos automóveis [3] e motas a robôs portanto, também faz sentido usar o ros2 para resolver problemas neste contexto. o objetivo desta dissertação passa por mostrar como o ros2, um middleware bastante utilizado em aplicações para robôs, pode ter um papel importante em contextos de simulação ao atuar como uma interface universal entre sub-sistemas a testar e um simulador de motas e consequentemente melhorar a extensibilidade do simulador e a portabilidade e reusabilidade desses sub-sistemas."
    ],
    [
      "forest fires are a major problem that affects the entire world, causing tragic loss of life and serious injuries, which have been worsening due to global warming, making it essential to minimize the serious consequences of these phenomena. in this sense, this project addresses the problem of positioning resources to combat forest fires. as uncertainty is an important aspect in fire propagation modeling, stochastic approaches are used, such as the equivalent deterministic model and the sample average approximation. the purpose of these approaches is to determine the best locations to deploy a limited number of combat assets, for example fire crews. another important point is to study how fire spreads in a forest given the region's topography, wind and other factors to incorporate fire propagation modeling with the management and planning of fire prevention and firefighting resources (optimization). although there are several fire propagation simulation software, their integration with optimization problems is still very limited. in this work, this integration is achieved through the minimum travel time (mtt) principle that, when representing the forest by a network in which the transmission times between adjacent homogeneous forest zones are known, states the fire takes the quickest paths. this principle is used in mixed integer programming models to optimize the positioning of the available resources, both in a deterministic and in a stochastic setting. computational experiments are conducted to validate the approach.",
      "os incêndios florestais são um problema grave que afeta todo o mundo, causando trágicas perdas de vidas e ferimentos graves, que se têm vindo a agravar devido ao aquecimento global, tornando-se essencial minimizar as graves consequências destes fenómenos. neste sentido, este projeto aborda o problema do posicionamento de meios de combate a incêndios florestais. como a incerteza é um aspeto importante na modelação da propagação do fogo, são utilizadas abordagens estocásticas, tais como deterministic equivalent model e sample average aproximation. o objetivo destas abordagens é determinar os melhores locais para colocar um número limitado de meios de combate, por exemplo, equipas de bombeiros. outro ponto importante é estudar a forma como o fogo se propaga numa floresta, tendo em conta a topografia da região, o vento e outros fatores, para incorporar a modelação da propagação do fogo na gestão e planeamento dos recursos de prevenção e combate a incêndios (otimização). embora existam vários softwares de simulação de propagação de incêndios, a sua integração com problemas de otimização é ainda muito limitada. neste trabalho, esta integração é conseguida através do princípio do tempo mínimo de viagem (mtt) que, ao representar a floresta por uma rede em que se conhecem os tempos de transmissão entre zonas florestais homogéneas adjacentes, estabelece que o fogo toma os caminhos mais rápidos. este princípio é utilizado em modelos de programação inteira mista para otimizar o posicionamento dos recursos disponíveis, tanto num contexto determinístico como num contexto estocástico. são efetuados testes computacionais para validar a abordagem."
    ],
    0.3
  ],
  [
    [
      "with the advancement of environmental dna (edna) monitoring methods, there is a growing interest in developing new, simple, and cost-effective devices to collect edna from seawater, particularly from the deep sea. this thesis provides a theoretical introduction to the environmental dna concept and existing methods for sample collection, followed by a state-of-the-art review highlighting the key features of each analysed device. in the scope of this work, a novel device was designed and constructed, capable of filtering water at different depths and preserving the filters for subsequent analysis, by the addition of ethanol. the device can collect up to four independent filters. to clean the equipment, the user simply needs to fill a container a decontaminating liquid (10% sodium hypochlorite (naclo) (bleach), per example) and place the device in cleaning mode. the constructed equipment consists of two aquarium pumps, one of them serving for the intake of external liquid, while the other one directs the alcohol from a reservoir to the system; and a valve system electronically controlled by a microcontroller that determines the state of each component based on the depth measured by a pressure sensor during sample collection. in cleaning mode, the microcontroller opens all valves, except the alcohol valve, sequentially and activates the intake pump for ten minutes. once the ten minutes have elapsed, the pump is turned off, and the valves are closed sequentially. finally, conclusions are presented, weaknesses of the equipment are revealed. additionally, indications for possible future projects, aiming to optimize the device, are presented.",
      "com o avanço dos métodos de monitorização baseados na análise de dna ambiental (edna), há um crescente interesse no desenvolvimento de dispositivos simples e acessíveis para recolha de dna ambiental da água do mar, em particular do mar profundo. esta tese realiza uma introdução teórica do conceito de dna ambiental e dos métodos existentes para a recolha de amostras, seguida de uma revisão do estado da arte, destacando os pontos mais relevantes de cada dispositivo analisado. no âmbito deste trabalho, foi projetado e construído um novo dispositivo com a capacidade de filtrar água em diferentes profundidades, preservando os filtros para análise posterior, através da adição de etanol. esse dispositivo é capaz de recolher até quatro filtros independentes. para a limpeza do equipamento, o utilizador deve apenas encher um recipiente com uma solução descontaminante (hipoclorito de sódio (naclo) (lixívia) 10%, por exemplo) e nele inserir o dispositivo em modo de limpeza. o equipamento desenvolvido é composto por duas bombas, em que uma delas serve para a introdução de líquido do exterior, enquanto a outra dirige o álcool de um reservatório para o sistema; e um sistema de válvulas eletronicamente controladas por um microcontrolador, que ajusta o estado de cada componente com base na profundidade medida por um sensor de pressão durante a recolha de amostras. no modo de limpeza, o microcontrolador abre todas as válvulas, exceto a válvula do álcool, sequencialmente, e aciona a bomba de alimentação por um período de dez minutos. após o término desses dez minutos, a bomba é desligada e as válvulas são fechadas sequencialmente. finalmente, as conclusões obtidas a partir deste estudo são apresentadas, juntamente com uma análise das limitações do equipamento. adicionalmente, são apresentadas orientações para potenciais projetos futuros, com o propósito de aperfeiçoar o dispositivo."
    ],
    [
      "microservices are a modern architecture style that divides a single application into small, indepen dently deployable services, each running in its own process and communicating through lightweight mechanisms. however, there is still a lack of research on the design and development of microservices applications. the development of applications using microservice-based architectures requires a variety of es sential factors that must be kept in mind to achieve good and future proof results. given the growing demand for scaling applications and the growth of cloud infrastructures, mi croservices emerged as one of the most prominent architectural advancements in recent years. they are still in their early stages of integration, and for that reason this architecture style has yet to be widely studied. with that in mind, this dissertation aims to close this gap by providing the key elements that should be considered when designing and building solutions based in microservices. it begins by researching and studying these architectures and finishes with a implementation of microservices based on a case study.",
      "os microsserviços emergiram recentemente como um estilo arquitetural moderno que divide uma única aplicação em vários serviços de forma independente, cada um executando o seu próprio processo e comunicando através de mecanismos simples. no entanto, existem ainda falhas sobre o estudo e desenvolvimento de aplicações baseadas em microsserviços. o desenvolvimento destas aplicações requer uma variedade de fatores essenciais que devem ser tidos em conta para que seja possível obter bons resultados a longo termo. com a necessidade de escalar aplicações e com o crescimento de infraestruturas na cloud, os microsserviços surgem como um dos avanços arquiteturais mais importantes nos últimos anos. ainda se encontram nas fases inicias de integração e, por essa razão, este estilo arquitetural necessita de ser amplamente estudado. neste sentido, o objectivo desta dissertação é colmatar esta lacuna, através do estudo dos elementos chave que devem ser considerados durante a concepção e construção de soluções baseadas em microsserviços. inicalmente procede-se à pesquisa e estudo destas arquiteturas e no fim efetua-se a implementação de uma arquitetura de microsserviços baseada num caso de estudo."
    ],
    0.0857142857142857
  ],
  [
    [
      "o boletim de saúde da grávida é um livro de registos para grávidas. este permite o registo de informações alusivas à história clínica da grávida, bem como o registo diário do peso, da pressão arterial, dos movimentos fetais, e entre outros valores que serão relevantes para a sua monitorização e o seu acompanhamento por parte do médico. desta forma, considerando a importância clínica associada, é prioritário proceder-se à melhoria desse sistema. além disso, as utentes quando encorajadas a serem mais independentes no ato de registo, considerando-o mais relevante, contribuem para a minimização da perda de informação relevante. tudo isto favorece o diagnóstico antecipado de potenciais riscos à saúde da grávida ou à condição do seu feto. o cálculo da data provável de parto e da semana gestacional da gravidez é um processo extremamente importante no acompanhamento da grávida. contudo, os métodos comuns são demorados e, facilmente, induzem em erro. assim, uma ferramenta capaz de calcular esses valores é vantajosa quer para o médico, quer para a paciente. este trabalho visa o desenvolvimento de uma progressive web app capaz de proporcionar um suporte à gravida em termos de informação e monitorização, disponibilizando o boletim de saúde da grávida em formato digital, sendo também uma ferramenta que facilitará o cálculo da data provável de parto e da idade gestacional. o desenvolvimento do projeto foi auxiliado pelo centro materno infantil do norte (cmin), com o acompanhamento do doutor jorge braga – diretor geral de obstetrícia do cmin.",
      "the boletim de saúde da grávida is a record book for the pregnant woman. it includes a wide range of personal health information, such as the daily records of arterial blood pressure, body weight, and fetal movements, among other parameters relevant to properly follow up the gestation period. considering the clinical importance of the above, it is critical to improve this data collection system. hence, it is pivotal to develop a more user-friendly real-time process, that concomitantly provides the attending physician with more precise and relevant clinical information. in addition, when patients are encouraged to independently collect data about their health, whether when they feel it is relevant, or in pre-established time-points, loss of valuable information is minimized. together, this allows the early diagnosis of potentially dangerous conditions that may impair the health of the pregnant woman or their developing fetus. estimating the date of delivery and gestation age are arguably among the most important steps when following up the gestation period. current methods, though, are very time consuming and often inaccurate to a significant extent. therefore, a tool capable of calculating such values would be helpful both to the attending doctor and to the patient. this thesis project aims to develop a progressive web program that includes the following parameters: a boletim de saúde da grávida in digital format and a tool capable of estimating accurately the date of delivery and gestation age. with that, it is hoped that the follow up of the pregnancy will be significantly improved. this project was supported by centro materno infantil do norte (cmin) under the close supervision and mentorship of dr jorge braga, md – main director of obstetrics of cmin."
    ],
    [
      "metabolic network reconstructions provide the mathematical in-silico frame work for the study of metabolism through the simulation of generic or specific metabolic pathways. polyphenols are niche dietary compounds with a growing field of interest in the scientific community. modelling with these compounds is a step for ward in the completion of metabolic reconstructions. as polyphenols are metabolized by both the gut microbiome and the human host, it is essential to understand the organism-specific metabolic mechanism behind their degradation. since information is spread out through several information sources and their metabolism is highly complex, it is extremely challenging and time-consuming to manually annotate their metabolism. the focus of the work here developed was thus the creation of a tool that could speed up the data collection process for posterior manual curation, with a focus on the addition of polyphenol metabolism into the largest and most comprehensive reconstruction of human metabolism, ”recon”. this resulted in the creation of the database reaction automatic extraction (drax) tool, a biological database web scraper. drax was initially targeted at polyphenols but it also allows the collection of reactions for other metabolites and drugs. drax allows the comprehensive extraction of metabolite reactions through metabolic pathway-based iterative web scraping. it will provide researchers with a starting point for metabolism reconstruction, allowing a more efficient addition of novel metabolic pathways.",
      "reconstruções metabólicas fornecem um framework matemático in-silico para o estudo de metabolismo através de simulações genéricas ou específicas de pathways metabólicos. um modelo metabólico é a união de uma reconstrução metabólica com restrições fisiologicamente coerentes. polifenóis são substâncias dietéticas com um crescente interesse na comunidade científica. modelação com estes compostos é, portanto, um importante passo a dar na completação de reconstruções metabólicas. como polifenóis são metabolizados pelo microbioma intestinal e pelo hospedeiro humano, é essencial perceber o metabolismo, específico a cada organismo, por trás da sua degradação. como a informação para anotar o seu metabolismo está espalhada em várias fontes, este processo torna-se extremamente desafiante e moroso. o objetivo desta tese foi então o desenvolvimento de uma ferramenta que acelere a coleção desta mesma informação, para que esta possa depois ser curada manualmente por investigadores. isto levou à criação da ferramenta database reaction automatic extraction (drax). inicialmente esta ferramenta focava-se na adição de polifenóis à reconstrução metabólica mais compreensiva,” recon”, contudo também permite a adição de outros metabolitos e drogas. drax permite então a extração de reações metabólicas através de web scraping iterativo baseado em pathways metabólicos. irá então disponibilizar investigadores com um ponto de começo para reconstrução do metabolismo específico a diversos compostos previamente desconhecido."
    ],
    0.0
  ],
  [
    [
      "o processo de tomada de decisão revela-se um fator cada vez mais diferenciador nas empresas atuais. sejam elas empresas diretamente relacionadas com as tecnologias de informação, sejam empresas de outros sectores. nesse aspeto, a recolha e tratamento de informação crítica a cada atividade, pode ser bastante útil para ser usado no processo de tomada de decisão, sendo sempre necessário avaliar até que ponto pode a informação ser válida e qual a sua qualidade e importância no processo em que é necessária. recorrendo a um sistema de case-based reasoning tentar-se-á criar-se um sistema que de forma autónoma aprenda a melhor maneira de atribuir tarefas dentro de uma equipa multifacetada, aprendendo a discernir quais os colabores que melhor se adaptam a cada tarefa e cujos resultados podem ser melhores para a empresa. esse mesmo sistema também estará apto para lidar com equipas dinâmicas aproveitando os melhores recursos que tem em cada altura.",
      "the process of decision making proves to be an increasingly differentiating factor in today's enterprises. whether companies directly related to information technology, are other sectors. in this aspect, the collection and processing of critical information to each activity, can be quite useful to be used in the decision making process is always necessary to assess to what extent can the information is valid and what is its quality and importance in the process what is required. using a system of case-based reasoning will be trying to create a system that autonomously learns the best way to assign tasks within a multifaceted team, learning to discern which team members are best suited for each task and the results are better for the company. this same system will also be able to deal with dynamic teams taking advantage of the best features it has at each point."
    ],
    [
      "scalable database services combining multiple technologies, including sql and nosql, are increasingly in vogue. in this context, the coherentpaas research project aims at providing an integrated platform with multiple data management technologies, united by a common query language and global transactional coherence. for this integration to succeed, it must provide the same monitoring capabilities of tra ditional relational databases, namely, for database administrators to optimise its operation. however, achieving this in a distributed and heterogeneous system is in itself a challenge. this work proposes a solution to this problem with x-ray, that allows monitoring code to be added to a java-based distributed system by manipulating its bytecode at runtime. the resulting information is collected in a nosql database and then processed and visualised graphically. this system is evaluated experimentally by adding monitoring to apache derby and tested with the standard tpc-c benchmark workload.",
      "os serviços escaláveis de base de dados combinando diversas tecnologias, incluindo sql e nosql, estão cada vez mais em voga. neste contexto, o projeto de investigação coherent paas tem como objetivo oferecer uma plataforma integradora de múltiplas tecnologias de gestão de dados, unidas por uma linguagem de interrogação comum e por mecanismos de coerência transacional global. para que esta integração seja utilizável na prática, ´e necessário que ofereça as capacidades de monitorização que são comuns em bases de dados relacionais, por exemplo, para que o administrador de bases de dados seja capaz de otimizar a sua operação. no entanto, a concretização desta funcionalidade num sistema distribuído e heterogéneo é em si um desafio. este trabalho propõe uma solução para este problema com o sistema x-ray, que permite adicionar a capacidade de monitorização a um sistema distribuído em java através da manipulação do código-objeto em tempo de execução. a informação resultante é recolhida numa base de dados nosql e depois processada e visualizada graficamente. este sistema ´e avaliado experimentalmente aplicando-o ao apache derby e utilizando o padrão de testes tpc-c."
    ],
    0.0
  ],
  [
    [
      "o teste e o desenvolvimento de ambientes ubíquos podem tornar-se uma tarefa bastante dispendiosa. a necessidade de adquirir ou de construir espaços físicos, assim como de todos os dispositivos que o compõem, como sensores e similares, levanta questões de cariz económico a ter em consideração. em complemento, convém ter em consideração que nem todos os obstáculos são meramente monetários. se ponderarmos uma fase mais avançada de um projeto de computação ubíqua, onde poder a ser necessária a recolha e analise de informação de equipamento presente em espaços delicados ou críticos, como o caso de hospitais ou aeroportos, verificamos que quer a nível politico, quer a nível de segurança publica, estamos deveras limitados. deste modo, o desenvolvimento de protótipos de ambientes ubíquos utilizando mundos virtuais surge como uma potencial solução. neste âmbito, este trabalho descreve a modelação de dois ambientes virtuais, uma biblioteca com informação de lugares disponíveis e um lar residencial inteligente com componentes de prevenção e segurança implementados. a ferramenta utilizada neste trabalho, que permite esse desenvolvimento, e a apex [14], esta plataforma permite modelar um ambiente a imagem do espaço físico pretendido, com o acréscimo de o comportamento dos objetos poder ser modelado. assim torna-se possível replicar e explorar variados tipos de instalações tecnológicas de forma segura, bem como permitir a recolha generalizada da informação produzida pelos equipamentos. os protótipos desenvolvidos neste trabalho permitiram avaliar uma solução para uma biblioteca universitária e recolher a opinião dos residentes de um lar, possibilitando desta forma, refinar os requisitos dos sistemas.",
      "the development and test of ubiquitous environments may become quite expensive. the necessity of acquiring or building physical spaces, and the technologies needed for these environments, like sensors and similar equipment, are economical aspects that need to be considered. in addition, it is also important to understand that not all obstacles are entirely financial. in a latter phase of a ubiquitous project, when it might be necessary to collect and analyse information from equipment’s in critical spaces, like hospitals or airports, some limitations can emerge. the development of prototypes pf ubiquitous environments using virtual worlds can be a potential solution to this problem. thus, this work describes the modelling of two virtual environments, a university library with seats availability information, and a “smart” care home, with prevention and security features. the tool used to accomplish this development is the apex framework [14]. modelling an environment of this nature, based on the physical one, using virtual data or real data from real sensors, gave us the opportunity to explore the implications and effects of the design and technological decisions. the developed prototypes allowed to evaluate a solution for a library and gather information on several others solutions for the residents of a care home."
    ],
    [
      "the energy-split tool receives as input pieces of a very large molecular system and computes all intra and inter-molecular energies, separately calculating the energies of each fragment and then the total energy of the molecule. it takes into account the connectivity information among atoms in a molecule to compute (i) the energy of all terms involving atoms covalently bonded, namely bonds, angles, dihedral angles, and improper angles, and (ii) coulomb and the van der waals energies, that are independent of the atom’s connections, which have to be computed for every atom in the system. the required operations to obtain the total energy of a large molecule are computationally intensive, which require an efficient high-performance computing approach to obtain results in an acceptable time slot. the original energy-split tcl code was thoroughly analyzed to be ported to a parallel and more efficient c++ version. new data structures were defined with data locality features, to take advantage of the advanced features present in current laptop or server systems. these include the vector extensions to the scalar processors, an efficient on-chip memory hierarchy, and the inherent parallelism in multicore devices. to improve the energy-split’s sequential variant a parallel version was developed using auxiliary libraries. both implementations were tested on different multicore devices and optimized to take the most advantage of the features in high performance computing. significant results by applying professional performance engineering approaches, namely (i) by identifying the data values that can be represented as boolean variables (such as variables used in auxiliar data structures on the traversal algorithm that computes the euclidean distance between atoms), leading to significant performance improvements due to the reduced memory bottleneck (over 10 times faster), and (ii) using an adequate compress format (csr) to represent and operate on sparse matrices (namely matrices with euclidean distances between atoms pairs, since all distances further the cut-off distance (user defined) are considered as zero, and these are the majority of values). after the first code optimizations, the performance of the sequential version was improved by around 100 times when compared to the original version on a dual-socket server. the parallel version improved up to 24 times, depending on the molecules tested, on the same server. the overall picture shows that the energy-split code is highly scalable, obtaining better results with larger molecule files, even when the atom’s arrangement influences the algorithm’s performance.",
      "a ferramenta energy-split recebe como ficheiro de input a descrição de fragmentos de um sistema molecular de grandes dimensões, de maneira a calcular os valores de energia intramolecular. separadamente, também efetua o cálculo da energia de cada fragmento e a energia total de uma molécula. ao mesmo tempo, tem em conta a informação das ligações entre átomos de uma molécula para calcular (i) a energia que envolve todos os átomos ligados covalentemente, nomeadamente bonds, angles, dihedral angles and improper angles, e (ii) energias de coulomb e vand der waals, que são independentes das conexões dos átomos e têm de ser calculadas para cada átomo do sistema. para cada átomo, o energy-split calcula a energia de interação com todos os outros átomos do sistema, considerando a partição da molécula em fragmentos, feita num programa open source, visual molecular dynamics. as operações para o cálculo destas energias podem levar a tarefas muito intensivas, computacionalmente, fazendo com que seja necessário utilizar uma abordagem que tire proveito de computação de alto desempenho de modo a desenvolver código mais eficiente. o código fornecido, em tcl, foi profundamente analisado e convertido para uma versão paralela e, mais eficiente, em c++. ao mesmo tempo, foram definidas novas estruturas de dados, que aproveitam a boa localidade dos mesmos para tirar vantagem das extensões vetoriais presentes em qualquer computador e, também, para explorar o paralelismo inerente a máquinas multicore. assim, foi implementada uma versão paralela do código convertido numa fase anterior com recurso ao uso de bibliotecas auxiliares. ambas as versões foram testadas em diferentes ambientes multicore e otimizadas de maneira a ser possível tirar o máximo partido da computação de alto desempenho para obter os melhores resultados. após a aplicação de técnicas de engenharia de performance como (i) a identificação de dados que poderiam ser representados em formatos mais leves como variáveis booleanas (por exemplo, variáveis usadas em estruturas de dados auxiliares ao cálculo da distância euclideana entre átomos, utilizadas no algoritmo de travessia da molécula), o que levou a melhorias significativas na performance (cerca de 10 vezes) devido à redução de sobrecarga da memória. (ii) a utilização de um formato adequado para a representação de matirzes esparsas (nomeadamente a de representação das mesmas distâncias euclidianas do primeiro ponto, uma vez que todas as distâncias que ultrapassem a distância de cutoff (definida pelo utilizador) são consideradas como 0, representado a maioria dos valores). 3 4 depois das otimizações à versão sequencial, esta apresentou uma melhoria de cerca de 100 vezes em relação à versão original. a versão paralela foi melhorada até 24 vezes, dependendo das moléculas em questão. no geral, o código é escalável, uma vez que apresenta melhores resultados consoante o aumento do tamanho das moléculas testadas, apesar de se concluir que a disposição dos átomos também influencia a perfomance do algoritmo."
    ],
    0.3
  ],
  [
    [
      "the previous few decades have seen an enormous volume of articles from the scientific commu nity on the most diverse biomedical topics, making it extremely challenging for researchers to find relevant information. methods like machine learning (ml) and deep learning (dl) have been used to create tools that can speed up this process. in that context, this work focuses on examining the performance of different ml and dl techniques when classifying biomedical documents, mainly regarding their relevance to given topics. to evaluate the different techniques, the dataset from the biocreative vi track 4 challenge was used. the objective of the challenge was to identify documents related to protein-protein interactions altered by mutations, a topic extremely important in precision medicine. protein-protein interactions play a crucial role in the cellular mechanisms of all living organisms, and mutations in these interaction sites could be indicative of diseases. to handle the data to be used in training, some text processing methods were implemented in the omnia package from omniumai, the host company of this work. several preprocessing and feature extraction methods were implemented, such as removing stopwords and tf-idf, which may be used in other case studies. they can be used either with generic text or biomedical text. these methods, in conjunction with ml pipelines already developed by the omnia team, allowed the training of several traditional ml models. we were able to achieve a small improvement on performance, compared to the challenge baseline, when applying these traditional ml models on the same dataset. regarding dl, testing with a cnn model, it was clear that the biowordvec pre-trained embedding achieved the best performance of all pre-trained embeddings. additionally, we explored the application of more complex dl models. these models achieved a better performance than the best challenge submission. biolinkbert managed an improvement of 0.4 percent points on precision, 4.9 percent points on recall, and 2.2 percent points on f1.",
      "as décadas anteriores assistiram a um enorme aumento no volume de artigos da comunidade científica sobre os mais diversos tópicos biomédicos, tornando extremamente difícil para os investigadores encontrar informação relevante. métodos como aprendizagem máquina (am) e aprendizagem profunda (ap) tem sido utilizados para criar ferramentas que podem acelerar este processo. neste contexto, este trabalho centra-se na avaliação do desempenho de diferentes técnicas de am e ap na classificação de documentos biomédicos, principalmente no que diz respeito à sua relevância para determinados tópicos. para avaliar as diferentes técnicas, foi utilizado o conjunto de dados do desafio biocreative vi track 4. o objectivo do desafio era identificar documentos relacionados com as interações proteína-proteína alteradas por mutações, um tópico extremamente importante na medicina de precisão. as interacções proteína-proteína desempenham um papel crucial nos mecanismos celulares de todos os organismos vivos, e as mutações nestes locais de interacção podem ser indicativas de doenças. para tratar os dados a utilizar no treino, alguns métodos de processamento de texto foram implementados no pacote omnia da omniumai, a empresa anfitriã deste trabalho. foram implementados vários métodos de pré-processamento e extracção de características, tais como a remoção de palavras irrelevantes e tf-idf, que podem ser utilizados em outros casos de estudos, tanto com texto genérico quer com texto biomédico. estes métodos, em conjunto com as pipelines de am já desenvolvidas pela equipa da omnia, permitiram o treino de vários modelos tradicionais de am. conseguimos alcançar uma pequena melhoria no desempenho, em comparação com a linha de referência do desafio, ao aplicar estes modelos tradicionais de am no mesmo conjunto de dados. relativamente a ap, testando com um modelo cnn, ficou claro que o embedding pré-treinado biowordvec alcançou o melhor desempenho de todos os embeddings pré-treinados. adicionalmente, exploramos a aplicação de modelos de ap mais complexos. estes modelos alcançaram um melhor desempenho do que a melhor submissão do desafio. biolinkbert conseguiu uma melhoria de 0,4 pontos percentuais na precisão, 4,9 pontos percentuais no recall, e 2,2 pontos percentuais em f1."
    ],
    [
      "over the last years, one of the areas that have most evolved and extended its application to a multitude of possi bilities is artificial intelligence (ai). with the increasing complexity of the problems to be solved, human resolution becomes impossible, as the amount of information and patterns that can be detected is limited, while ai thrives on the dimension of the problem under analysis. furthermore, as nowadays more and more traditional devices are computerized, an increasing number of elements are producing data that has many potential applications. consequently, we find ourselves at the height of big data, where huge volumes of data are generated, being entirely unfeasible to process and analyze them manually. additionally, with the increasing complexity of network topologies, it is necessary to ensure the correct func tioning of all equipment, avoiding cascade failures among devices, which can lead to catastrophic consequences depending on their use. thus, root cause analysis (rca) tools become fundamental since these are developed to automatically, through rules established by its users, realize the underlying causes when some equipment mal functions. however, with the growing network complexity, the definition of rules becomes exponentially more complicated as the possible points of failure scale drastically. in this context, framed by the altice labs rca and network environment use case, the main objective of this research project is defined. the aim is to use machine learning (ml) techniques to extrapolate the relationship between different types of equipment alarms, gathered by the alarm manager tool, to have a better understanding of the impact of a failure on the entire system, thus easing and helping the process of manual implementation of rca rules. as this tool manages millions of daily alarms, it becomes unfeasible to process them manually, making the application of ml essential. furthermore, ml algorithms have tremendous capabilities to detect patterns that humans could not, ideally exposing which specific failure causes a series of malfunctions, thus allowing system administrators to only focus their attention on the source problem instead of the multiple consequences. the ml approach proposed in this project is based on the causality among alarms, instead of their features, and uses the cartesian product of a specific problem, the involved technology, and the manufacturer, to extrap olate the correlations among faults. the results achieved reveal the tremendous potential of this approach and open the road to automatizing the definition of rca rules, which represents a new vision on how to manage network failures efficiently.",
      "ao longo dos últimos anos, uma das áreas que mais tem evoluído e estendido a sua utilização para uma infinidade de possibilidades é a inteligência artificial (ia). com a crescente complexidade dos problemas, a resolução humana torna-se impossível, uma vez que a quantidade de informação e padrões que podem ser detectados é limitada, enquanto a ia prospera na dimensão do problema em análise. além disso, como hoje em dia cada vez mais dispositivos tradicionais são informatizados, um número crescente de elementos está a pro duzir dados com muitas potenciais aplicações. consequentemente, encontramo-nos no auge do big data, onde enormes volumes de dados são gerados, sendo totalmente inviável processá-los e analisá-los manualmente. esta é uma das razões que tem levado à prosperidade da ia. além disso, com a crescente complexidade das topologias de rede, é necessário assegurar o correcto fun cionamento de todos os equipamentos, evitando falhas em cascata entre dispositivos, o que pode levar a con sequências catastróficas dependendo da sua utilização. assim, as ferramentas de root cause analysis (rca) tornam-se fundamentais, uma vez que são desenvolvidas para, através de regras estabelecidas pelos seus utilizadores, se aperceberem automaticamente das causas subjacentes quando algum equipamento apresenta anomalias. no entanto, com a crescente complexidade da rede, a definição de regras torna-se exponencial mente mais complicada, uma vez que os pontos possíveis de falha escalam tremendamente. neste contexto, enquadrado pelo ambiente de rede e cenários de rca da altice labs, foi definido o principal objectivo deste projecto de investigação. este objectivo consiste na aplicação de técnicas de machine learning (ml) para extrapolar a relação entre os diferentes tipos de alarmes dos equipamentos, geridos pela ferramenta alarm manager, para ter uma melhor compreensão do impacto de uma falha em todo o sistema, facilitando e ajudando assim o processo de implementação manual das regras rca. como esta ferramenta gere milhões de alarmes diários, torna-se inviável processá-los manualmente, tornando essencial a aplicação do ml. além disso, os algoritmos ml têm uma enorme capacidade para detectar padrões que os humanos não conseguem detectar, idealmente expondo quais as falhas específicas que causam uma série de falhas, permitindo assim que os administradores do sistema apenas concentrem a sua atenção no problema de raiz em vez das suas múltiplas consequências. a abordagem ml proposta neste projecto baseia-se na causalidade entre os alarmes, em vez das suas car acterísticas, e utiliza o produto cartesiano de um problema específico, da tecnologia envolvida, e do fabricante, para extrapolar as correlações entre falhas. os resultados alcançados revelam o enorme potencial desta abor dagem e abrem o caminho para automatizar a definição de regras rca, o que representa uma nova visão sobre como gerir eficazmente as falhas da rede."
    ],
    0.0
  ],
  [
    [
      "both energy consumption analysis and energy-aware development have gained the attention of both developers and researchers over the past years. the interest is more notorious due to the proliferation of mobile devices, where energy is a key concern. there is a gap identified in terms of tools and information to detect and identify anomalous energy consumption in android applications. a large part of the existing tools are based on external hardware (costly solutions in terms of setup-time), through predictive models (requiring previous hardware calibration) or static code analysis methods. we could not identify so far a tool capable of monitor all relevant system resources and components that an application uses and appoint its energy consumption, while being easily integrated with the application and/or with its development environment. due to the lack of a tool capable of gathering all this information, a natural consequence is the lack of information about the energy consumption of applications and factors that can influence it. this dissertation aims to carry out a study on the energy consumption of applications and mobile devices in the android platform, having developed in this scope the greensource infrastructure, a repository containing the source code, representative metadata and metrics relatively to a large number of applications (and respective execution in physical devices). in order to gather the results, an auxiliary tool has been developed to automatize the process of testing and collect the respective results for each one of the applications. this tool is a software-based solution, allowing to obtain results in terms of consumption through executions made directly on a physical device running the android platform. the developed framework, the anadroid, has the capability to perform static and dynamic analysis of an application, being able to monitor power consumption and usage of resources for each application through tests execution. this is done following a whitebox testing approach, in order to test applications at source code level. it invokes calls to the trepnlib library at strategic locations of the application code (through instrumentation techniques) to gain control over relevant portions of the source code, like methods and unit tests. in this way the programmer can have results about the use, state and consumption of resources such as energy, cpu, gpu, memory, sensor usage and complexity of developed test cases. the information gathered through the use of the anadroid over a large set of applications was stored in greensource backend. with the collected results, we expect to be able to characterize and classify applications, as well the tests developed for it. it is intended that this will be made publicly available and serve as a reference for future works and studies.",
      "quer a análise do consumo de energia, quer o desenvolvimento de aplicações com consciência neste sentido têm vindo a cativar a atenção de desenvolvedores e investigadores nos últimos anos. o interesse é mais notório devido à proliferação de dispositivos móveis, onde a energia é uma preocupação fundamental mas ainda pouco explorada. como tal, existem lacunas identificadas em termos de ferramentas e informações para detectar e identificar o consumo anómalo de energia em aplicações android. grande parte das ferramentas existentes são baseadas em hardware externo (soluções dispendiosas em termos de tempo de setup), através de modelos preditivos (que exigem calibração prévia) ou métodos de análise estática de código. não conseguimos identificar até ao momento uma ferramenta capaz de monitorizar de forma precisa todos os recursos e componentes relevantes do sistema usados por uma aplicação, bem como de determinar o seu consumo energético. esta lacuna tem como consequência natural a falta de informação sobre o consumo de energia de aplicações e fatores que podem influenciá-lo. esta dissertação tem como objetivo realizar um estudo sobre o consumo de energia na plataforma android, tendo sido desenvolvido neste âmbito a infraestrutura greensource. esta contém um repositório que engloba o código fonte, resultados e métricas relativas a um grande número de aplicações. a fim de obter resultados ilustrativos para um grande número de aplicações, foi desenvolvida uma ferramenta para automatizar o processo de teste e reunir os respectivos resultados. a ferramenta desenvolvida é baseada em software, permitindo obter resultados em termos de consumo através de execuções realizadas diretamente num dispositivo físico android. esta framework, denominada anadroid, possui a capacidade de analizar aplicações de forma estática e dinâmica, bem como de monitorizar o consumo e uso de recursos durante a sua execução. para este efeito, são efetuadas invocações a uma biblioteca denominada trepnlib, em locais estratégicos do código da aplicação para obter controlo sobre partes relevantes deste. desta forma obtém-se resultados sobre o uso, estado e consumo de recursos, tais como consumo energético, cpu, gpu, memória, sensores. as informações reunidas através da execução do anadroid foram armazenadas na base de dados do greensource. com todos os resultados coletados, pretende-se caracterizar e classificar energeticamente aplicações e testes desenvolvidos para estas. pretende-se disponibilizar abertamente estes resultados, para que possam servir como referencia para futuros trabalhos, análises e estudos."
    ],
    [
      "a inteligência artificial surgiu nos anos 50, com a criação de um espaço de estudo com o objetivo principal de desenvolver máquinas inteligentes. a sua evolução foi abrupta e hoje existem mesmo programas que, ao escreverem um conjunto de palavras, conseguem gerar imagens surpreendentes com base no que foi escrito em segundos, algo que um pintor famoso faria, mas que demoraria horas ou mesmo dias a concluir. em vários setores empresariais, desde a educação aos cuidados de saúde, entre outros, tem havido um aumento notável do interesse e da utilização de chatbots. esta tendência crescente abriu caminho para a implementação de chatbots em diversas áreas. especificamente, estes chatbots servem como intermediários informativos em empresas como a retail consult, uma empresa de tecnologia especializada em desenvolvimento de software na área do retalho e na utilização de processamento em lote, que envolve o tratamento de grandes volumes de dados. além disso, o sistema de chatbot foi concebido para reconhecer e compreender uma multiplicidade de línguas, incluindo o inglês, que será a língua principal do utilizador, bem como o português e o espanhol, assegurando uma comunicação na língua preferida do utilizador. esta capacidade multilingue é particu larmente crucial para a retail consult, dada a sua presença global com escritórios espalhados por vários países. sem dúvida, esta versatilidade linguística acrescenta um valor imenso ao produto. para conseguir este reconhecimento linguístico, foram utilizadas técnicas de inteligência artificial e de processamento da linguagem natural, que foram implementadas através de um quadro bem estruturado concebido para a criação de chatbots e de software similar, incluindo assistentes virtuais. o desenvolvimento de um serviço com estas características está preparado para aumentar significa tivamente a produtividade interna da organização. por exemplo, um analista de sistemas pode perguntar diretamente ao chatbot o que pretende, poupando tempo que, de outra forma, teria sido gasto na procura e consulta de informações à base de dados. quer se trate de determinar o número de trabalhos em execução ou de identificar os que apresentam falhas, este sistema procura simplificar as operações e melhorar a eficiência dentro da empresa. com base no trabalho apresentado na introdução, onde discutimos os papéis cruciais do processa mento em lote e da integração do chatbot no ambiente em específico, agora aprofundamos a dinâmica do setor de retalho nesta análise minuciosa. o objetivo está centrado na criação de um chatbot versátil capaz de comunicar com os utilizadores em várias línguas, fornecer respostas precisas e, acima de tudo, auxiliar na tradução de idiomas. além disso, os resultados confirmam a competência do chatbot na identificação e tradução precisa de idiomas, com base em nossa análise avançada, onde investigamos a importância do processamento em lote e introduzimos a estrutura de ia conversacional rasa. esses elementos, contribuem para uma experiência mais envolvente e satisfatória para o utilizador. ou seja, este trabalho permitiu e ampliou o nosso conhecimento sobre a interação entre processamento em lote, tecnologia de chatbot e comunicação multilíngue no contexto do retalho.",
      "artificial intelligence emerged in the 1950s with the establishment of a research field aimed at developing intelligent machines as its main objective. its evolution was abrupt, and today there are even programs that, by writing a set of words, can generate astonishing images based on what has been written in seconds, something that a famous painter would do, but which would take hours or even days to complete. in various business sectors, from education to healthcare, among others, there has been a notable increase in interest in and use of chatbots. this growing trend has paved the way for the implementation of chatbots in various areas. specifically, these chatbots serve as information intermediaries in companies such as retail consult, a technology company specializing in software development in the area of shredding and the use of batch processing, which involves handling large volumes of data. in addition, the chatbot system has been designed to recognize and understand a multitude of lan guages, including english, which will be the user’s main language, as well as portuguese and spanish, ensuring communication in the user’s preferred language. this multilingual capability is particularly crucial for retail consult, given its global presence with offices in several countries. undoubtedly, this linguistic ver satility adds immense value to the product. to achieve this linguistic recognition, artificial intelligence, and natural language processing techniques will be used, which will be implemented through a well-structured framework designed for the creation of chatbots and similar software, including virtual assistants. the development of a service with these characteristics is set to significantly increase the organization’s internal productivity. for example, a systems analyst can ask the chatbot directly what they want, saving time that would otherwise have been spent searching for and consulting information in the database. whether it’s determining the number of jobs in progress or identifying those with faults, this system seeks to simplify operations and improve efficiency within the company. building on the work presented in the introduction, where we discussed the crucial roles of batch processing and chatbot integration in the specific environment, we now delve into the dynamics of the retail sector in this in-depth analysis. our goals are centered on creating a versatile chatbot capable of communicating with users in multiple languages, providing accurate responses, and, above all, assisting with language translation. in addition, our results confirm the chatbot’s competence in identifying and accurately translating languages, based on our advanced analysis, where we investigated the importance of batch processing and introduced the rasa conversational ai framework. these elements, together, contribute to a more engaging and satisfying user experience. so, the research has broadened our knowledge of the interaction between batch processing, chatbot technology, and multilingual communication in the retail context, which can guide future advances in this field, as emphasized throughout our study."
    ],
    0.3
  ],
  [
    [
      "with the increased use of applications that substitute physical elements of our daily lives, comes the necessity of sharing personal data with third parties so that the use of such applications is possible and viable. the access to these personal data from the applications should only be possible with explicit permission from the users, which he should always be able to manage, so the privacy of his personal data is respected and the user has control over the access and use of it. as a response to this situation, arises the idea of developing a system that allows users more control over the use of their data, as well as allowing the information of the permission to access to personal attributes to be stored in a secure and trustworthy manner. this system will use a dlt (distributed ledger technology) infrastructure, with which is intended to be possible to keep information of the access to personal attributes of each user, in a reliable, transparent manner, and assuring the possibility to audit over the information stored on the infrastructure.",
      "com a utilização de cada vez mais aplicações que substituem elementos físicos do dia-a-dia, como é o caso de documentos de identificação, surge também a necessidade de partilha de dados pessoais com partes terceiras para que seja possível e viável a utilização desse tipo de aplicações. o acesso a esses dados pessoais por parte das aplicações só deve ser possível com permissão explícita por parte de um utilizador, permissão esta que um utilizador deve sempre poder gerir, de modo a respeitar a privacidade dos seus dados e permitir ao utilizador controlo sobre o acesso e utilização destes. como resposta a esta situação, surge a ideia do desenvolvimento de um sistema que permita a utilizadores ter um maior controlo sobre a utilização dos seus dados e paralelamente permite que estas informações sobre a permissão dos acessos a atributos de identificação pessoal sejam mantidos de forma segura e confiável. este sistema recorrerá à tecnologia dlt (distributed ledger technology), com que se pretende que seja posteriormente possível manter informações sobre o acesso aos atributos pessoais de cada utilizador, de forma confiável, transparente e mantendo a possibilidade de auditar a informação mantida na infraestrutura."
    ],
    [
      "detecting and recognizing information contained in urban scenery such as informative boards or billboard advertising is increasingly becoming a trending topic in the machine vi sion community duo to it’s increased utility in automated applications such as, for example, assisted and autonomous software for self-driving vehicles and maintenance assessment of urban signage. compared to text recognition in documents, which is for the most part solved with state of-art optical character recognition (ocr) algorithms, text detection and recognition in urban scenes still presents several problems to the community. there is no single algorithm that can handle all the difficulties encountered in real-world scenery (scale, perspective, distortions, defocus, occlusion, etc.), making it an arduous task in real world scenes compared with text detection and recognition in documents. regarding image pre-processing in documents, a simple image translation, such as a rotation or scaling, in most cases is sufficient to overcome some recognition issues whilst in urban scene imagery, text can appear with different alignments, languages and fonts, requiring some sort of sequential pipeline to overcome the difficulties and increase the success rate. the main goal of this dissertation is to explore and reproduce end-to-end state-of-art techniques both in urban scene text detection and recognition, further comparing the top ranked algorithms in a testing environment through several challenging benchmarks. furthermore, we develop a pipeline combining computer vision and deep learning tech niques to assess the conditioning of informative placards in urban scenery by employing the models with the best results reported in our benchmarks. the pipeline is divided in 3 main components: placard extraction, text detection and text recognition. in the placard extraction step we crop placards of interest from the rest of the background, the text detection component detects text boxes in the placard and the text recognition component predicts character sequences in every text box detected. additionally, we develop an intuitive front-end prototype displaying some of the results attained throughout our pipeline, showcasing the potential and usability of our research in the assessment and management of street placards.",
      "a informação contida em cenários urbanos tal como placares informativos e publicitários é cada vez mais um tópico relevante na comunidade de processamento de imagem devido ao aumento da necessidade e utilidade em aplicações autónomas tais como no auxílio de veículos de condução autónoma e assistida. comparado com o problema de reconhecimento de texto em documentos, que está em grande parte resolvido com algoritmos de optical character recognition (ocr), a deteção e reconhecimento de texto em ambientes urbanos apresenta muitos problemas para a comunidade. apesar de nos últimos anos ter havido melhorias tanto no desempenho como exatidão dos algoritmos, ainda não existe um único algoritmo ou combinação de algoritmos que resolva todas as dificuldades encontradas em cenários do mundo real (escala, perspetiva, distorção, desfocagem, oclusão, etc.). em relação ao pre-processamento requerido quando tratamos de texto em documentos, uma simples translação, como uma rotação ou escala, é na maioria dos casos suficiente para ultrapassar alguns problemas de reconhecimento, enquanto que, em cenários urbanos, o texto pode aparecer com diferentes alinhamentos, tamanhos, fontes e até mesmo em diferentes línguas, requerendo algum tipo de pipeline para ultrapassar estas mesmas dificuldades. o objetivo desta dissertação é então explorar e replicar técnicas de estado-da-arte nos domínios de deteção e reconhecimento de texto em ambientes urbanos, comparando os \"melhores\" algoritmos através de um conjunto de testes em diferentes benchmarks. para além disso, foi desenvolvida uma pipeline combinando técnicas de visão por computador e deep learning para determinar a condição de placardes de informação em cenários urbanos, combinando os algoritmos com os melhores resultados nos nossos benchmarks. a pipeline é composta por 3 componentes: extração do placar, deteção do texto e reconhecimento do texto. na extração do placar recortamos o placar de interesse do resto da imagem, no componente de deteção de texto detetamos as caixas de texto do placar e no componente de reconhecimento de texto prevemos sequências de caracteres para cada caixa de texto detetada. adicionalmente, desenvolvemos um protótipo de front-end intuitivo, demonstrando alguns dos resultados obtidos na pipeline, exibindo o potencial e usabilidade do nosso projeto na avaliação e manutenção de placares urbanos."
    ],
    0.3
  ],
  [
    [
      "o controlo e a prevenção de infeções nosocomiais são essenciais para a redução de custos, bem como para a melhoria dos cuidados prestados numa instituição de saúde. por outro lado, o tratamento de dados que permitam compreender, caracterizar e monitorizar as infeções possibilita um controlo e uma prevenção mais eficaz das mesmas. sendo um método automatizado e eficiente para o tratamento de dados, a tecnologia de business intelligence permite a extração de informação importante para gerar conhecimento que pode auxiliar o processo de tomada de decisão dos profissionais de saúde. o principal objetivo deste trabalho é o desenvolvimento e implementação de uma plataforma de business intelligence que permita o estudo da incidência de infeção nosocomial nas unidades de medicina do centro hospitalar do porto. este estudo é feito através da apresentação de um conjunto de indicadores clínicos (informações importantes extraídas dos dados referentes a infeções nosocomiais) que ajudam a analisar e caracterizar estas infeções. por conseguinte, depois de identificados os indicadores relevantes, torna-se pertinente desenvolver um sistema que permita tratar os dados, extrair os indicadores destes e apresentá-los, de forma atrativa, na plataforma. por sua vez, a plataforma facilita a análise das informações que disponibiliza, apoiando a tomada de decisões, nomeadamente através da identificação dos principais fatores de risco. assim, o sistema atua como um sistema de apoio à decisão clínica, podendo auxiliar no controlo e prevenção destas infeções. pretende-se ainda estudar a aplicabilidade da tecnologia de data mining na criação de modelos de classificação capazes de prever a ocorrência de infeções nosocomiais, na presença de determinados fatores de risco. o conhecimento obtido com a análise dos indicadores e as previsões efetuadas pode possibilitar a diminuição da incidência de infeção nosocomial e, consequentemente, a redução dos custos associados à sua ocorrência, bem como o aumento da segurança e do bem-estar dos doentes, ao permitir a tomada de decisões mais fundamentadas. a aplicação de business intelligence na área da saúde contribui para melhorar não só o fluxo de trabalho diário nas unidades de saúde, como também a qualidade dos cuidados prestados.",
      "the control and prevention of nosocomial infections are essential to reduce costs and improve the care delivered in healthcare institutions. on the other hand, the treatment of data that allow to understand, characterize and monitor the infections enables a more effective control and prevention of them. being an automated and efficient method to treat data, the business intelligence technology allows to retrieve important information to create knowledge that can assist the decision making process of the healthcare professionals. the main goal of this work is to develop and implement a business intelligence platform for the study of nosocomial infection incidence in the medicine units of centro hospitalar do porto. this study is made through the presentation of a set of clinical indicators (important informations extracted from the nosocomial infection data) that help to analyse and characterize these infections. therefore, after identifying the relevant indicators for this study, it becomes pertinent to develop a system that treats data, extracts the indicators from data and presents the indicators, in an attractive way, on the platform. the platform makes the analysis of the information derived from processed data easier, assisting the decision making, namely in the identification of the main risk factors. thus, the system acts as a clinical decision support system, helping in the control and prevention of these infections. this work also intends to study the applicability of data mining technology in the creation of classification models, capable to predict the occurrence of nosocomial infections, in the presence of certain risk factors. the knowledge, achieved by the analysis of the indicators and the predictions accomplished, may allow the decrease of nosocomial infection incidence. consequently, it may allow the reduction of the costs associated with its occurrence, as well as the increase of patients' safety and well-being, by allowing a more reasoned decision making process. the application of business intelligence to the healthcare sector contributes to improve not only the daily work ow of healthcare units, but also the quality of the healthcare delivered."
    ],
    [
      "as result of the disruption caused by the appearance of digital companies with an agile delivery model, there is a need to review the delivery model in organisations. the devops philosophy aims to provide an answer to this problem, bringing all of the individuals responsible for the delivery to work collaboratively, with the support of a set of tools to automate and streamline all of the processes. the objective of this dissertation is to re-evaluate and, consequently, improve the delivery model within deloitte, proposing an automated process for code analysis, functional tests and deployment of software packages. to this end, a set of cutting-edge tools were analysed and a case study in the context of a real project was built, in order to put into practice the automation of the processes developed.",
      "resultado da disrupção causada pelo surgimento de empresas digitais com um modelo de entrega ágil, existe a necessidade de rever o modelo de entrega nas organizações. a filosofia devops pretende dar resposta a esta problemática, aproximando os intervenientes responsáveis pela entrega, com o apoio de um conjunto de ferramentas que permitem automatizar e otimizar todos os processos. o objetivo deste trabalho passa por reavaliar e, consequentemente, melhorar o modelo de entrega de projetos dentro da deloitte, propondo para isso uma automatização dos processos de análise automática de código, de testes funcionais e de criação automática de pacotes de software. para esse efeito, foram analisadas um conjunto de ferramentas de última geração e elaborado um caso de estudo no contexto de um projeto real, de modo a colocar em prática a automatização dos processos desenvolvidos."
    ],
    0.09
  ],
  [
    [
      "over the past few years, we have seen an exponential increase in the amount of data produced. this increase in data is due, in large part, to the massive use of sensors, as well as the immense amount of existing applications. due to this factor, and in order to obtain relevant information through the data, companies, institutions and the scientific community are constantly looking for new solutions to be able to respond to the challenges. one of the areas where evolution is most needed is the area of healthcare, an area on which we all depend as a society. every day, traditional healthcare information systems produce a large amount of data, making it complex to manage. much of this data is produced by iot devices, such as vital signs monitors, and in many cases can be critical to the patient’s health, as in the case of intensive care units. in this sense, the main objective of this dissertation is to expose the advantages and disadvantages of the applicability of microservices architectures and the use of apache kafka in the health area, more specifically in intensive care units where the information flow is critical. in order to support these objectives, a proof of concept was developed, based on a future real applicability, which will support the carrying out of analyzes and tests.",
      "durante os últimos anos, temos assistido a um aumento exponencial da quantidade de dados produzida. este aumento de dados deve-se, em grande parte, à utilização massiva de sensores, assim como à enorme quantidade de aplicações existentes. devido a esse fator, e de forma a conseguir obter informações relevantes através dos dados, empresas, instituições e comunidade científica, estão constantemente à procura de novas soluções para conseguir responder aos desafios. uma das áreas onde a evolução é mais necessária é a área da saúde, uma área da qual todos dependemos enquanto sociedade. todos os dias, os sistemas tradicionais de informação em saúde produzem uma grande quantidade de dados, tornando-os complexos de gerir. muitos destes dados são produzidos pelos dispositivos iot, como os monitores de sinais vitais, e em muitos dos casos podem ser fulcrais para a saúde do paciente, como é o caso das unidade de cuidados intensivos. neste sentido, a presente dissertação tem como objetivo principal expor as vantagens e desvantagens da aplicabilidade das arquiteturas de microservices e da utilização do apache kafka na área da saúde, mais concretamente nas unidades de cuidados intensivos onde o fluxo de informação é crítico. de forma a auxiliar estes objetivos, foi desenvolvido uma prova de conceito, tendo por base uma futura aplicabilidade real, que servirá de suporte à realização de análises e testes."
    ],
    [
      "em portugal existe o serviço chave móvel digital (scmd), que permite a qualquer cidadão efetuar a assinatura eletrónica qualificada remota de dados. atualmente é disponibilizada, publicamente a todos os cidadãos, a aplicação autenticação.gov, que oferece um conjunto de funcionalidades, sendo uma delas a assinatura de documentos pdf utilizando o scmd. mas esta é limitada às plataformas windows, linux e mac. desse modo, esta dissertação de mestrado tem como objectivo desenvolver uma aplicação que assine documentos pdf, com o scmd, em android e ios. para desenvolver a aplicação, é utilizada a framework de desenvolvimento de aplicações móveis android e ios, react native e a api nativa em conjunto com a ferramenta de geração, programação e manipulação de documentos pdf, itext 7, para desenvolver as operações de assinatura e a comunicação com o scmd, resultando numa aplicação móvel, que apesar das limitações enfrentadas, assina documentos pdf, com o scmd. com a aplicação desenvolvida, é possível assinar documentos pdf com o scmd, nos quais as assinaturas são claramente visíveis, válidas e estão em conformidade com a especificação técnica dos standards de assinaturas eletrónicas avançadas de pdfs. tendo isto em consideração, a aplicação desenvolvida oferece assinaturas mais robustas, é mais rápida a concluir todo o processo e produz assinaturas de menor tamanho em comparação com a aplicação existente.",
      "the serviço chave móvel digital (scmd), a service available in portugal, allows for any citizen to sign data remotely with a qualified eletronic signature. currently, the autenticação.gov application is publicly available to every citizen, that offers many features, one of these being the signing of a pdf document with the scmd. but this application is limited to windows, linux and mac. as such, the main objective of this dissertation is to develop a mobile application for android and ios, that signs pdf documents using the scmd. both the react native framework and the native api are used to develop the application and the pdf toolkit for pdf generation, programming, handling and manipulation itext 7 is used to develop the signature process and the process of communication with the scmd, resulting in a mobile application, that despite the limitations faced, signs pdf documents with the scmd. with the developed application, it is possible to sign pdf documents with the scmd, in which the signature are visible, valid and are in accordance with the technical specifications of the pdf advanced electronic signatures standards. having this in consideration, the developed application offers more robust signatures, signs faster and generates smaller signatures compared to the application currently available."
    ],
    0.3
  ],
  [
    [
      "mathematics learning in basic education is a much debated subject and the difficulties experienced by most students in this area are worrisome. a teacher is concerned with questions about how to attract the attention of students or try to provoke a greater taste in most students in this discipline. therefore, the use of technologies, namely platforms that capture the attention of students and help teachers in the teaching/learning process is current and preponderant. thus, the hypatiamat, which is a pedagogical web platform that helps basic mathematics teaching, it allows games and content applications depending on the school year and with levels of difficulty that will increase over time, allowing students to evolve in the discipline. furthermore, this platform allows teachers to observe the evolution of their students, resulting in a monitoring of their performance in different games and content applications. so, this dissertation intends to evolve the current hypatiamat architecture from a mo nolithic server to an architecture based on micro-services. accordingly, a data api will be specified and documented. in addition, an interface server, called the office, will be built, where new features related to project management will be added: more statistics about games and content applications; user management (administrator, teacher, group, municipality and student); global statistics for an academic year, among other features.",
      "a aprendizagem de matemática no ensino básico (eb) é um assunto muito debatido e as dificuldades sentidas pela generalidade dos alunos nesta área é preocupante. os professores questionam-se sobre a melhores estratégias, metodologias e recursos que devem ser utilizados para captar a atenção dos alunos e assim capitalizar o gosto natural que estes têm por ambientes digitais. portanto, a utilização de tecnologias, nomeadamente de plataformas que captem a atenção dos alunos e auxiliem os professores no processo de ensino/aprendizagem é atual e preponderante. deste modo, a plataforma hypatiamat, sendo uma plataforma digital interativa com centenas de recursos para auxiliar no ensino da matemática no ensino básico, permite, entre outros, a realização de jogos sérios e o trabalho em aplicações que trabalham as matérias do currículo de matemática consoante o ano de escolaridade e com níveis de dificuldade que aumentam, ao longo do tempo, permitindo que os alunos construam o conhecimento na disciplina de matemática com elevados índices de sucesso. além disso, a plataforma hypatiamat permite aos professores observarem a evolução dos seus alunos, acompanhando essa evolução nos diversos jogos e aplicações disponibilizadas. assim, nesta dissertação pretende-se fazer evoluir a arquitetura atual do hypatiamat de um servidor monolítico para uma arquitetura baseada em micro-serviços. nesse sentido, será especificada e documentada uma api de dados. além disso, será construído um servidor, de interface, designado por escritório, onde serão acrescentadas novas funcionalidades, relacionadas com a gestão do projeto: estatísticas de jogos e aplicações de conteúdo; gestão de utilizadores (adminstrador, professor, agrupamento, município e aluno); estatísticas globais de um ano letivo, geração de relatórios, entre outras funcionalidades, para que se possa acompanhar o desempenho dos alunos em tempo real e assim contribuir para a melhoria da qualidade da aprendizagem."
    ],
    [
      "superconducting quantum circuits are a promising model for quantum computation, al though their physical implementation faces some adversities due to the hardly unavoidable decoherence of superconducting quantum bits. this problem may be approached from a formal perspective, using logical reasoning to perform software correctness of programs executed in the non-ideal available hardware. this is the motivation for the work devel oped in this dissertation, which is ultimately an attempt to use the formalism of transition systems to design logical tools for the engineering of quantum software. a transition system to capture the possibly unexpected behaviors of quantum circuits needs to consider the phenomena of decoherence as a possible error factor. in this way, we propose a new family of transition systems, the paraconsistent labelled transition systems (plts), to describe processes that may behave differently from what is expected when facing specific contexts. system states are connected through transitions which simultaneously characterize the possibility and impossibility of that being the system’s evolution. this kind of formalism may be used to represent processes whose evolution is impossible to be sharply described and, thus, should be able to cope with inconsistencies, as well as with vagueness or missing information. besides giving the formal definition of plts, we establish how they are related under the notions of morphism, simulation, bisimulation and trace equivalence. it is a common practice to combine transition systems through universal constructions, in a suitable category, which forms a basis for a process description language. in this dis sertation, we define a category of plts and propose a number of constructions to combine them, providing a basis for such a language. transition systems are usually associated with modal logics which provide a formal set ting to express and prove their properties. we also propose a modal logic, more specifically, a modal intuitionistic paraconsistent logic (mipl), to talk about plts and express their properties, studying how the equivalence relations defined for plts extend to relations on mipl models and how the satisfaction of formulas is preserved along related models. finally, we illustrate how superconducting quantum circuits may be represented by a plts and propose the use of plts equivalence relations, namely that of trace equivalence, to compare circuit effectiveness.",
      "os circuitos quânticos que operam qubits supercondutores são um modelo promissor para a arquitetura de computadores quânticos. no entanto, a sua implementação física pode tornar-se ineficaz, devido a fenómenos de decoerência a que os qubits em questão estão altamente sujeitos. uma possível abordagem a este problema consiste em empregar a lógica e as suas ferramentas para a correção de programas a executar nestes dispositivos. a proposta desta dissertação é que se utilize o formalismo dos sistemas de transição para modelar e descrever o comportamento dos circuitos quânticos, que, por vezes, pode ser imprevisível. para tal, considera-se a decoerência de qubits como um possível fator de erro nas computações. assim surge uma nova família de sistemas de transição, os paraconsistent labelled transition systems (plts), como um modelo para descrever processos que, em determinados contextos, se comportam de forma diferente do que é esperado. os estados de um plts estão conectados por transições que caracterizam, simultaneamente, a possibilidade e a impossibilidade de o sistema evoluir transitando de um estado para o outro. este é um modelo em que a informação acerca das transições pode ser incompleta ou mesmo contraditória. além da definição formal dos plts, são também sugeridas, como relações entre plts, as noções de morfismo, simulação, bissimulação e equivalência por traços. muitas vezes, os sistemas de transição são combinados através de construções universais numa categoria adequada, de forma a definir uma álgebra de processos. também neste trabalho é definida uma categoria de plts e são propostas algumas construções, típicas nas álgebras de processos, para os combinar. os sistemas de transição são geralmente associados a lógicas modais, que permitem expressar e provar as suas propriedades. a definição dos plts conduziu à definição de uma lógica modal, mipl, que permitiu determinar de que forma as relações de equivalência definidas para plts, e estendidas para modelos da logica mipl, se refletem na preservação da satisfação de fórmulas sobre os modelos relacionados. por fim, propõe-se utilizar plts para a representação de circuitos quânticos e comparar a eficácia dos circuitos através da relação de equivalência por traços."
    ],
    0.06666666666666667
  ],
  [
    [
      "tutores artificiais são agentes de software que auxiliam nos processos de ensino e formação. nos dias de hoje, esses sistemas têm como objetivo fornecer instruções aos alunos sem a intervenção direta de um professor. para que isso aconteça com sucesso, é necessário que estes sistemas possuam um sistema de conversação que seja capaz de interagir com os alunos de forma simples e cativante, mantendo um diálogo adequado em todos os momentos de ensino que concedem. essencialmente, os sistemas de conversação são assistentes virtuais que utilizam interfaces de comunicação que interagem com os utilizadores através de frases escritas e orais, e, geralmente, possuem a capacidade de compreender o utilizador. por norma, o computador assume o papel de professor. no entanto, o aluno também “ensina” o sistema fornecendo dados e a sua perspetiva em relação ao problema, que podem posteriormente ser utilizados para personalizar o seu ensino. é neste contexto que foram realizados os trabalhos desta dissertação, cujo principal objetivo foi a implementação de um componente de conversação para um sistema educacional, que fosse capaz de criar novas dinâmicas entre o sistema e o utilizador, incentivando-o, assim, a despender mais tempo na ferramenta.",
      "artificial tutors are software agents that assist in teaching and training processes. nowadays, these systems aim to provide instructions to students without the direct intervention of a teacher. for this to happen successfully, these systems must have a conversational system that can interact with the students simply and engagingly while maintaining a fitting dialogue in all moments the user goes through. conversational systems are virtual assistants that use communication interfaces that interact with users using written and oral phrases, generally having the ability to understand the user. by default, the computer assumes the role of a teacher, however, the students can also \"teach\" the system by providing data and their perspective on the problem, which can be used later to customize their teaching. it is in this context that the work of this dissertation will be carried out, which will have the main objective of implementing a dialogue component for an educational system capable of creating new dynamics between the system and the user, thus encouraging the user to spend more time using the tool."
    ],
    [
      "nowadays, the assimilation of web content, by each individual, has a considerable impact on our’ everyday life. with the undeniable success of online social networks and microblogs, such as facebook, instagram and twitter, the phenomenon of influence exerted by users of such platforms on other users, and how it propagates in the network, has been attracting, for some years computer scientists, information technicians, and marketing specialists. increased connectivity, multi-model access and the rise of social media shortened the distance between almost every person in the world, more and more content is generated. extracting and analyzing a significant amount of data is not a trivial task, big data techniques are essential. through the analysis of this interaction, an exchange of information and feelings, it is entirely imaginable its usefulness in understanding complex human behaviours and so, help diverse organization’s decision-making. influence maximization and viral marketing are among the possibilities. this work is intended to study what is the impact and role that an event’s social influence has and how does it propagate, particularly on its surrounding territory. this influence is inferred by analysis of the online platform’s data, by applying intelligent techniques, right after its extraction. the final step is to validate the results with data from different sources. helping businesses through actionable and valuable knowledge is the ultimate goal. this document contemplates an introductory section where the study subject and its state of the art are addressed. next, the problem and what direction to take to solve it are discussed.",
      "atualmente, a assimilação de conteúdo web, por cada individuo, tem um impacto considerável no nosso quotidiano. com o inegável sucesso de redes sociais e microblogs, como por exemplo facebook, instagram e twitter, o fenómeno de influência exercida, por utilizadores de tais plataformas, em outros utilizadores e como se propaga na rede tem atraído, por alguns anos, informáticos, técnicos de informação e especialistas em marketing. o aumento da conectividade, o acesso multi-modal e a proliferação dos meios de comunicação social reduziram a distância entre quase todas as pessoas do mundo, mais e mais conteúdo é gerado. extrair e analisar uma grande quantidade de dados não é uma tarefa trivial, são essenciais técnicas de big data. através da análise desta interação, troca de informações e emoções, é perfeitamente imaginável a sua utilidade na compreensão de complexos comportamentos humanos e, portanto, ajudar na tomada de decisão de diversas organizações. a maximização da influência e o marketing viral estão entre as possibilidades. este trabalho destina-se a estudar qual é o impacto e o papel que a influência social de um evento tem e como se propaga, particularmente no território envolvente. esta influência é inferida pela análise dos dados de plataformas online, aplicando técnicas inteligentes, logo após a sua extração . o passo final é validar os resultados com dados de diferentes fontes. ajudar empresas através do conhecimento valioso e atuável é o objetivo final. este documento contempla uma seção introdutória, onde o assunto de estudo e o seu estado da arte são abordados. de seguida, é discutido o problema e a direção a seguir para o solucionar."
    ],
    0.06666666666666667
  ],
  [
    [
      "mathematical models are fundamental tools for explaining biological behaviors. dynamical and constraint-based models are two different formulations that attempt to capture the phenotypic capabilities of organisms. dynamic models are formulated as ordinary differential equations (odes) that simulate metabolic concentration over time. these models, however, only depict changes in metabolic concentration and rely on mechanistic details and kinetic parameters that are not always available. constraint-based models, on the other hand, have a better cellular perspective. by performing constraint-based optimizations, they simulate cell behavior under different genetic and environmental conditions. metabolic models also have some drawbacks. in addition to providing no mechanical knowledge of any chemical reactions (beyond their stoichiometry) and no information regarding metabolic concentrations or reaction flux dynamics, they are based on a steady-state assumption that production and consumption of metabolites are balanced within the cell. constraint-based optimizations, flux balance analysis (fba) methods, generally return an infinite set of solutions, requiring the imposition of additional assumptions to identify unique flux distributions. while individually, both modeling approaches have several advantages, one lacks the benefits provided by the other. with this in mind, we implemented a tool in mewpy capable of hybridizing kinetic and constraint-based models. with it, we were able to reduce the constraint-based model solution space by overlapping the kinetic solution space and sampling the kinetic model, analyze the impact of different standard deviation values on the sampling, perform hybridization of enzymatic constrained models, and further compare distinct hybridization approaches. to demonstrate the potential of our tool and its applicability in strain optimization, we performed hybrid optimization of succinate production, where we discovered a set of genetic mutations that boosted its production.",
      "os modelos matemáticos são ferramentas fundamentais para explicar os comportamentos biológicos. os modelos dinâmicos e com base em restrições são duas formulações diferentes que tentam captar as capacidades fenotípicas dos organismos. os modelos dinâmicos são formulados como equações diferenciais comuns (odes) que simulam a concentração metabólica ao longo do tempo. estes modelos, contudo, apenas retratam as alterações da concentração metabólica e dependem de detalhes mecanicistas e parâmetros cinéticos nem sempre disponíveis. os modelos com base em restrições, por outro lado, têm uma melhor perspetiva celular. ao efetuarem otimizações baseadas em restrições, simulam o comportamento celular sob diferentes condições genéticas e ambientais. os modelos metabólicos também têm alguns inconvenientes. para além de não fornecerem qualquer conhecimento mecânico de quaisquer reações químicas (para além da sua estequiometria) e nenhuma informação relativa a concentrações metabólicas ou dinâmicas de fluxos de reação, baseiam-se numa suposição de estado estável de que a produção e consumo de metabolitos são equilibrados dentro da célula. as otimizações baseadas em restrições, métodos de análise de equilíbrio de fluxo (fba), devolvem geralmente um conjunto infinito de soluções, exigindo a imposição de pressupostos adicionais para identificar distribuições de fluxo únicas. embora individualmente, ambas as abordagens de modelação tenham várias vantagens, uma carece dos benefícios proporcionados pela outra. com isto em mente, implementámos uma ferramenta em mewpy capaz de hibridizar modelos cinéticos e baseados em constrangimentos. com ela, conseguimos reduzir o espaço de solução do modelo baseado em restrições através da sobreposição do espaço de solução cinética e da amostragem do modelo cinético, da análise do impacto de diferentes valores de desvio padrão na amostragem, da realização da hibridação de modelos com restrições enzimáticas, e ainda da comparação com mais abordagens de hibridação distintas. para demonstrar o potencial da nossa ferramenta e a sua aplicabilidade na otimização de estirpes, realizámos a otimização híbrida da produção de succinato, onde descobrimos um conjunto de mutações genéticas que impulsionaram a sua produção."
    ],
    [
      "this document presents and discusses a project in the area of managing tools to support teaching. this project is part of the work in the second year of the master degree in informatics engineering accomplished at universidade do minho in braga, portugal. the main result of this master’s project is an application that allows the monitor to plan his courses and activities; it also allows the participant to have a better performance in the course in which he is enrolled. from the point of view of monitor, this tool is intended to facilitate the organization of a course or activity, linking to each task the materials/tools necessary to carry it out. from a participant’s point of view, it aims to provide greater involvement of the participant in the course in which he is enrolled in order to have a better supported evolution. for that purpose, the participant will be able to attend a course selected from the offer promoted by escola competências de vida (e•cv), obtain more personalized and immediate feedback, increasing his motivation and engagement to learn more. upon receiving this constant feedback, the participant feels more involved in the activity, acquiring knowledge more naturally, thus potentiating a faster evolution. on the other hand, monitors, that are responsible for teaching the corses, will find their work easier as all the tasks concerned with each activity (both to provide learning resources to the students or to assess/grade them) are concentrated under the same platform. the system here described is called e•cvoltera. it was designed and fully implemented in order to satisfy the requirements specified by escola competências de vida, an academy created and maintained by a group of professors/researchers of escola de psicologia of universidade do minho. the system was made accessible to the community as a web platform.",
      "este documento apresenta e discute um projeto na área de ferramentas pedagógicas para apoiar o ensino. este projeto insere-se no âmbito de trabalho do segundo ano do mestrado em engenharia informática, realizado na universidade do minho em braga, portugal. o principal resultado deste projeto de mestrado é uma aplicação que permite ao professor planear os seus cursos e atividades e ao participante ter um desempenho melhor no curso em que está inscrito. no ponto de vista de monitor, esta ferramenta visa facilitar a organização de um curso ou atividade, interligando a cada funcionalidade os materiais/ferramentas necessárias para a realização da mesma. num ponto de vista de participante, visa propor cionar um maior envolvimento do participante no curso em que está inscrito de forma, a ter uma evolução mais acompanhada. para o efeito, o participante poderá frequentar um curso seleccionado a partir da oferta promovida pela escola competências de vida (e•cv), obter um feedback mais personalizado e imediato, aumentando a sua motivação e empenho para aprender mais. ao receber esse feedback constante, o participante sente-se mais envolvido na atividade, adquirindo conhecimento com mais naturalidade, potencializando assim uma evolução mais rápida. por outro lado, os monitores, que são responsáveis pela gestão dos cursos, assim terão mais facilidade no seu trabalho quanto a todas as tarefas inerentes a cada atividade (tanto para fornecer recursos de aprendizagem aos alunos como para os avaliar/classificar) estão concentrados na mesma plataforma. o sistema aqui descrito é denominado e•cvoltera. foi desenhado e totalmente implementado de forma a satisfazer os requisitos especificados pela escola competências de vida, uma academia criada e mantida por um grupo de professores/investigadores da escola de psicologia da universidade do minho. o sistema foi disponibilizado para a comunidade como uma plataforma web."
    ],
    0.06666666666666667
  ],
  [
    [
      "the recent sequencing techniques and omics approaches are generating huge amounts of data that can provide ways to extract meaningful knowledge, by resorting to appropriate computational tools. one important technique resorts to the use of genome scale model reconstructions. these models are widely used in metabolic engineering, attempting to optimize an organism's functions, genetically modifying it to produce compounds of industrial interest. another area that became widely important within the fields of systems biology and bioinformatics was network analysis and visualization. networks can provide a way to better understand the relationships between biological entities, by allowing their visual representation. however, biological networks usually comprise a large number of entities and interactions, that cannot be easily interpreted by the human eye. integrating visualization and analysis is, therefore, a goal of high interest in several scientific areas, and this has been tackled by several visualization tools available. however, regarding the integration of metabolic engineering techniques with metabolic network visualization, there are still few examples of success. usually, it is necessary to use more than one tool and the agility of the methods is limited. in this work, a metabolic network visualization framework is presented, with the goal of being a tool that will help researchers in metabolic engineering projects. this framework is divided in two layers: the first deals with the importation and exportation of networks in different formats, while the other layer provides all the visualization and edition features. a metabolic layout is based on the reactions contained in the metabolic model, and it can represent just a part of the metabolism of an organism. to have the possibility to use the same layout in different models, a strategy was defined to map the entities of the visualization with the entities of the model. the layouts are displayed in a bipartite graph, with different node types and colors. it is possible to visualize additional information of the network by clicking the nodes. some of the features include dragging, zooming and highlighting. on top of all this, it is also possible to apply filters and overlap information over these networks. the filters can change what is visible in the network, while the overlaps allow defining new labels, colors and shapes to the nodes, and new colors and thickness to the edges. finally, the framework was also integrated within optflux, an open-source software to support metabolic engineering available at www.optflux.org, to provide a connection between visualization and metabolic simulation methods.",
      "as recentes técnicas de sequenciação e as abordagens \"ómicas\" estão a gerar enormes quantidades de dados que, através do uso de ferramentas computacionais adequadas, podem fornecer formas de extraccão de conhecimento biológico significativo. uma importante metodologia recorre à reconstrução de modelos metabólicos à escala genómica. estes modelos são muito usados na engenharia metabólica, tentado-se optimizar o funcionamento do organismo, modificando-o geneticamente, de forma a maximizar a produção de compostos de interesse industrial. outra área de estudo que tem ganho bastante importância nos campos da biologia de sistemas e bioinformática é a análise e visualização de redes. as redes podem oferecer formas de melhor compreender as relações existentes entre entidades biológicas, fornecendo uma representação visual destes relacionamentos. no entanto, as redes biológicas, usualmente, são compostas por um elevado número de entidades e relacionamentos, o que pode tornar difícil a sua interpretação a \"olho nu\". a integração de visualização e análise sempre foi um objectivo de interesse em todas as áreas científicas, e respostas a este problema têm surgido sob a forma de diferentes ferramentas. no entanto, no que se refere à integração de técnicas de engenharia metabólica com visualização de redes metabólicas, existem ainda poucos exemplos com sucesso. usualmente, é necessário o uso de diversas ferramentas e as funcionalidades e flexibilidade é ainda limitada. neste trabalho é apresentada uma plataforma para a visualização de redes metabólicas, com o objectivo de ser uma ferramenta que assista investigadores em projectos de engenharia metabólica. esta plataforma está dividida em duas camadas: a primeira lida com a importação e exportação de redes em diferentes formatos, enquanto a outra camada oferece todas as funcionalidades de visualização e edição. um layout metabólico é baseado nas reaccões contidas num modelo metabólico, e pode representar apenas uma parte do metabolismo do organismo. de forma a ser possível utilizar o mesmo layout em modelos diferentes, foi definida uma estratégia para mapear as entidades da visualização com as entidades do modelo. os layouts são representados sob a forma de um grafo bi-partido, com diferentes tipos de nodos e cores. é possível visualizar informação adicional sobre a rede clicando nos nodos. algumas das funcionalidades incluem arrastar, focar e realçar partes da rede. para além de tudo isto, é possível aplicar filtros e sobrepor informação sobre a rede. os filtros permitem definir o que é visível na rede, enquanto a sobreposição permite definir novas etiquetas, formas e cores dos nodos e cores e espessura das conecções. finalmente, a plataforma foi integrada no optflux, uma ferramenta de código aberto para engenharia metabólica que está disponível em www.optflux.org, de forma a estabelecer uma conexão entre a visualização de redes metabólicas e métodos de simulação do metabolismo."
    ],
    [
      "a mineração de dados tem como principal objetivo descobrir padrões escondidos num conjunto de dados, isto é, informação útil que ajude a tomar decisões sobre um determinado tema. nesta dissertação usa-se a mineração numa base de dados que descreve os certificados adquiridos pelas empresas portuguesas no período 2008-2010. a certificação é um processo voluntário, que, apesar de ser bastante demorado, custoso e envolver demasiada burocracia, pode ser crucial para a sobrevivência das empresas. os certificados demonstram o compromisso da empresa com a qualidade dos produtos e dos serviços, e com o meio ambiente, a saúde e a segurança dos trabalhadores, entre outros. quais os certificados que uma empresa deve adquirir é uma das principais questões que se colocam a uma nova empresa no mercado. a resposta a esta pergunta pode variar com vários fatores, como a região onde a empresa se encontra localizada ou o seu sector de atividade. é necessária uma análise prévia que forneça um conhecimento do estado do mercado para tomar as melhores decisões. a aplicação de técnicas de mineração de dados permite obter uma descrição do estado do mercado e uma previsão de quais os certificados a adquirir consoante as características de uma empresa. a informação extraída facilita a tomada de decisão relativamente ao conjunto de certificados que melhor se adapta às características da empresa. este conjunto de certificados varia com a competitividade resultante do número de empresas na região e do número de empresas no sector de atividade em que a nova empresa se encontra inserida. os resultados apresentados nesta dissertação fornecem às novas empresas uma orientação inicial no mercado competitivo.",
      "the main objective of data mining is to discover patterns in a database, i.e., useful information to support decision making on a given subject. in this dissertation data mining is used in a database that describes the certificates acquired by portuguese companies in the period 2008 – 2010. certification is a voluntary process, which, despite the time taken in the process, the costs and the bureaucracy, may be crucial for the survival of a company. certificates are pledges of the commitment of the company with the quality of services and products, and with the environment, the health and the security of the workers, among others. which are the certificates that a company must acquire is one of the main questions that is placed to a new company in the market. the answer to this question may depend on several factors, as the region where the company is located or its activity sector. a previous analysis is necessary to get the knowledge about the state of the market that enables to take the best decisions. the application of data mining techniques allows a description of the state of the market and a forecast of which are the certificates to acquire depending on the characteristics of a company. the information extracted supports the decision making relatively to the set of certificates that better suit the characteristics of the company. this set of certificates varies with the competitiveness resulting from the number of companies in the region and with the number of companies in the activity sector to which the new company belongs. the results presented in this dissertation provide an initial orientation to the new companies in the competitive market."
    ],
    0.0
  ],
  [
    [
      "a utilização de sistemas robóticos tem vindo a crescer nos últimos anos e o software destes sistemas tem se tornado cada vez mais importante para o seu funcionamento. o robot operating system (ros) é um middleware que simplifica a implementação destes sistemas, fornecendo várias primitivas que facilitam a escrita de software e a coordenação dos diversos componentes que os constituem. os sistemas ros são distribuídos, com uma arquitetura organizada a partir de nós que comunicam entre si através da passagem de mensagens. estes sistemas robóticos são fortemente configuráveis pois necessitam de se ajustar a ambientes de trabalho cada vez mais diversificados e adversos. em sistemas ros existem ficheiros que incluem a configuração do sistema e é através destes que se pensa que é gerida a variabilidade. com esta tese pretende-se estudar empiricamente o modo como, de facto, é gerida a variabilidade destes sistemas visto que existe muito pouca informação sobre como é feita essa gestão. em particular, pretende-se estudar a viabilidade da extração automática de feature models (modelos gráficos que podem ajudar na quantificação da variabilidade) a partir dos ficheiros de configuração de um sistema ros. durante todo o processo de análise conseguiram-se identificar algumas técnicas de gestão de variabili dade. foi também possível desenvolver uma ferramenta capaz de extrair feature models automaticamente, apenas através da análise do código de sistemas ros. foram escolhidos cinco sistemas ros para avaliar a ferramenta desenvolvida, tendo sido possível obter resultados interessantes sobre a variabilidade dos mesmos.",
      "the use of robotic systems has been growing in the last years and the software of these systems has become increasingly important for their operation. the robot operating system (ros) is a middleware that simplifies the implementation of these systems, by providing several primitives that ease the writing of software and the coordinatation of the various components that constitute them. ros systems are distributed, with an architecture organized in nodes that communicate with each other through message passing. these robotic systems are highly configurable, as they need to adjust to increasingly diverse and adverse work environments. in ros systems there are files that include the system configuration and it is through these that the variability is thought to be managed. this thesis intends to empirically study how the variability of these systems is managed, since informa tion about this management is almost nonexistent nowadays. in particular, we intend to study the viability of automatic extraction of feature models (graphic models that can help in the quantification of variability) from the configuration files of a ros system. during the entire analysis process, it was possible to identify some variability management techniques. it was also possible to develop a tool capable of extracting feature models automatically, just by analyzing the code of ros systems. five ros systems were chosen to evaluate the developed tool, and some interesting results were obtained concerning their variability."
    ],
    [
      "com o aumento da nossa dependência nos sistemas de informação também aumenta a necessidade de sistemas mais seguros e resilientes. a pandemia que vivemos, há mais de um ano, veio agravar a situação e mostrou que temos de preparar os sistemas que suportam o nosso dia-a-dia para situações inesperadas e que podem comprometer o seu bom funcionamento. para proteger os sistemas é importante aplicar medidas preventivas. existem standards que definem as melhores práticas para a segurança dos sistemas, que podem ser implementados pelas organizações para melhor se prepararem contra situações adversas. destacam-se os standards desenvolvidos pelo international organization for standardization (iso), na área de gestão de segurança de informação, e pelo national institute of standards and technology (nist), na área de sistemas de confiança seguros. cada vez mais a preocupação com a segurança da informação tem-se reflectido na legislação e regulamentação europeia e portuguesa. esta dissertação pretende analisar as melhores práticas na área da segurança de informação, através dessa análise, propor uma abordagem para a sua implementação e utilizá-la num caso prático, sendo este a infraestrutura de chave pública do cartão de cidadão. desta forma, ao longo desta dissertação são analisados os standards relevantes desen volvidos pelo iso e nist. além disso, com o objectivo de contextualizar o caso prático é analisada a regulamentação e legislação aplicável às infraestruturas de chave pública na europa e em portugal bem como as componentes da infraestrutura de chave pública do cartão de cidadão. com esta análise, foi possível apresentar uma abordagem que reduz a complexidade do processo de implementação dos standards e colocá-la em prática num projecto de reestrutura ção e actualização da gestão de segurança da informação da infraestrutura de chave pública do cartão de cidadão.",
      "as our dependence on information systems increases, so does the need for more secure and resilient systems. the pandemic that we have been experiencing, for over a year, has aggravated the situation and showed that we have to prepare the systems that support our day-to-day activities for unexpected situations that can compromise their proper functioning. to protect systems it is important to apply preventive measures. there are standards that define the best practices for system security, which can be implemented by organizations to better prepare themselves against adverse situations. the standards developed by iso, in the subject of information security management, and by nist, in the subject of secure trust systems, are worth noting. the concern with information security has been increasingly reflected in european and portuguese legislation and regulations. this dissertation intends to analyze the best practices in the area of information security, through this analysis, propose an approach for its implementation and use it in a practical case, this being the public key infrastructure of the cartão de cidadão. thus, throughout this dissertation, the relevant standards developed by iso and nist are analyzed. in addition, in order to contextualize the practical case, the regulations and legislation applicable to public key infrastructures in europe and portugal are analyzed, as well as the components of the public key infrastructure of the cartão de cidadão. with this analysis, it was possible to present an approach that reduces the complexity of the standards implementation process and put it into practice in a project to restructure and update the information security management of the cartão de cidadão public key infrastructure."
    ],
    0.3
  ],
  [
    [
      "this dissertation reports on a masters’ project in the field of computing engineering. the objective of this project is based on the forecast of possible shortages of goods and possible damage that the vending machines can have, optimizing the profitability of the devices, as well as the management of the tasks of the employees. for the execution of this project we intend to design and develop a support system of predictive analysis that allows to expand the functionalities provided by the existing application. the existing application meets all the requirements initially presented by the company’s customer, but does not take advantage of all the capabilities that the sap hana cloud platform (sap hcp) has available. it is possible and intended in this phase to add new functionalities in order to monetize the devices including the predictive analytical capability.",
      "este documento descreve um projeto de tese do mestrado integrado em engenharia informática. o objetivo deste projeto e o desenvolvimento de um sistema de previsão de eventuais faltas de mercadorias e possíveis avarias que as maquinas de venda de produtos possam vir a ter, otimizando a rentabilidade dos dispositivos, assim como a gestão das tarefas dos funcionários. para a execução deste projeto estudou-se e planeou-se um sistema de suporte de análise preditiva para ser integrado numa aplicação de gestão de máquinas de venda automática de produtos já existente, permitindo expandir as suas funcionalidades originais. a aplicação existente, implementada na plataforma systems, applications and products in data processing (sap) responde a todos os requisitos apresentados inicialmente pelo cliente da empresa, contudo não tira proveito de todas as capacidades que a plataforma sap hcp tem disponíveis, sendo possível adicionar novas funcionalidades com o objetivo de rentabilizar os dispositivos adicionando a capacidade de análise preditiva."
    ],
    [
      "modern society is relying more and more on electronic devices, most of which are em bedded systems and are sometimes responsible for performing safety-critical tasks. as the complexity of such systems increases due to concurrency concerns and real-time con straints, their design is more prone to errors which can lead to catastrophic outcomes. in order to reduce the risk of such outcomes, a model-based methodology is commonly used. the model describes the behaviour of the system and is subject to verification tech niques such as simulation and model checking in order to verify it behaves according to the requirements. common problems that arise with this methodology is the ambiguity of requirements written in natural language and the translation of a requirement to a property that can be verified along with the model. this thesis proposes a tool that, after the translation of the requirements to temporal formalism, allows the automatic generation of monitors in order to verify the model. our target platform is simulink, which is widely used in this domain to model, simulate and analyze dynamic systems.",
      "a sociedade de hoje depende cada vez mais de dispositivos eletrónicos, a maioria dos quais são sistemas embebidos e, por vezes, responsáveis pela realização de tarefas críticas. à medida que a complexidade destes sistemas aumenta devido a problemas de concorrência ou restrições de tempo real, o design torna-se mais suscetível a erros que podem levar a resultados catastróficos. a fim de reduzir estes riscos, recorre-se a uma metodologia de desenvolvimento baseada em modelos. o modelo descreve o comportamento do sistema e pode ser sujeito a técnicas de verificação, tais como simulação ou model checking, a fim de verificar que este exibe o comportamento descrito nos requisitos. problemas comuns que surgem com esta metodologia devem-se a ambiguidade dos requisitos, tipicamente escritos em linguagem natural, e a tradução destes para uma propriedade que pode ser verificada em conjunto com o modelo. esta dissertação propõe uma ferramenta que, após a tradução dos requisitos para uma linguagem de especificação formal, permite a geração automática de monitores para verificar o modelo. a plataforma para a qual os monitores são gerados e o simulink, que é tipicamente utilizado neste domínio para modelar, simular e analisar sistemas dinâmicos."
    ],
    0.0
  ],
  [
    [
      "with the increase in deep learning (dl) popularity, the need for efficient and effective model development and optimization has emerged. neural architecture search (nas) is a research area that aims to automate the time-consuming and iterative tasks involved in building and tuning dl models. this approach has already proven to be effective in various fields, such as computer vision and natural language processing, and holds potential in the field of cancer treatment. cancer is a group of diseases characterized by the uncontrolled growth and spread of abnormal cells in the body. the high degree of diversity among tumors, along with drug resistance acquired during (or before) treatment, makes it challenging to find effective therapies for all types of cancer. high-throughput screening (hts) is a laboratory technique used to rapidly test large numbers of compounds against a biological target or assay to identify potential drug candidates. in cancer research, hts is often used to identify compounds that have the potential to inhibit the growth of cancer cells or interfere with specific cancer-associated processes. since the number of compounds with drug-like properties is much greater than the number of compounds that can be analyzed in hts processes, dl has been used recently to predict drug response based on omics data. this work integrated a dl-based framework capable of handling single and multi-input datasets of various types into the company’s internal platform, where the work was carried out. using automated machine learning (automl) tools to reach this goal, namely tools with nas capabilities, this new module automates the search for the best dl models that produce the most suitable results across a range of prediction tasks. the effectiveness of the developed work was validated in the context of drug response prediction by comparing the performance of the nas-generated models with manually tuned models previously developed. the models found using this approach achieved comparable performance without the need for human expertise, proving the potential of nas in cancer treatment, but also as a useful tool for automating and optimizing the model selection process in machine learning.",
      "com o aumento da popularidade de aprendizagem profunda, surgiu a necessidade de desenvolver e otimizar modelos de forma eficiente e eficaz. a pesquisa de arquiteturas neuronais é uma área de investigação que visa automatizar as tarefas demoradas e iterativas envolvidas na construção e afinação de modelos de aprendizagem profunda. esta abordagem já provou ser eficaz em vários domínios, como a visão por computador e o processamento de linguagem natural, e tem potencial no domínio do tratamento do cancro. o cancro é um grupo de doenças caracterizadas pelo crescimento descontrolado e pela disseminação de células anormais no corpo. o elevado grau de diversidade entre os tumores, juntamente com a resistência aos medicamentos adquirida durante (ou antes) do tratamento, torna difícil encontrar terapias eficazes para todos os tipos de cancro. a triagem de alto rendimento é uma técnica laboratorial utilizada para testar rapidamente um grande número de compostos contra um alvo biológico ou um ensaio para identificar potenciais candidatos a medicamentos. na investigação do cancro, esta técnica é frequentemente utilizada para identificar compostos com potencial para inibir o crescimento de células cancerosas ou interferir com processos específicos associados ao cancro. uma vez que o número de compostos com propriedades semelhantes a medicamentos é muito maior do que o número de compostos que podem ser analisados em processos de triagem de alto rendimento, métodos de aprendizagem profunda tem sido usados recentemente para prever a resposta a medicamentos com base em dados ómicos. este trabalho integrou uma framework baseada em aprendizagem profunda capaz de lidar com conjuntos de dados de vários tipos na plataforma interna da empresa, onde o trabalho foi desenvolvido. recorrendo a ferramentas de aprendizagem de máquina automática para atingir este objetivo, nomeadamente ferramentas com capacidades de pesquisa de arquiteturas neuronais, este novo módulo automatiza a procura dos melhores modelos de aprendizagem profunda que produzam os resultados mais adequados num conjunto de tarefas de previsão. a eficácia do trabalho desenvolvido foi validada no contexto da previsão da resposta a fármacos, comparando o desempenho dos modelos gerados por pesquisa de arquiteturas neuronais com modelos afinados manualmente desenvolvidos anteriormente. os modelos encontrados através desta abordagem obtiveram um desempenho comparável sem necessidade de recorrer a especialistas humanos, comprovando o potencial da pesquisa de arquiteturas neuronais no tratamento do cancro, mas também como uma ferramenta útil para automatizar e otimizar o processo de seleção de modelos em aprendizagem de máquina."
    ],
    [
      "o mais recente progresso reconhecido pela comunidade europeia por indústria 5.0 atende as evoluções imergentes no mundo da indústria e revela haver uma necessidade de evolução nos sistemas de erp’s (enterprise resource planning). é esperado que estes sistemas que auxiliem na gestão das empresas de uma forma mais dinâmica, assim tornando-se mais autónomos, atendendo a toda a informação aglomerada no sistema. o produtech, formalmente conhecido como o “cluster” português das tecnologias de produção, é uma rede estabelecida por empresas de tecnologia de produção capazes de reagir às dificuldades do sector de produção com soluções criativas, adaptáveis, integradas e competitivas. incluído dentro deste consórcio está a primavera, sendo uma empresa portuguesa pioneira no desenvolvimento de soluções de gestão, mais particularmente, sistemas erp. estes sistemas constam com elevados volumes de dados, o que poderá levar a operações complexas no tratamento e análise dos mesmos. neste relatório de dissertação de mestrado é documentada uma visão de solução para dar resposta a assistência computacional na escolha de fornecedores, bem como a aglomeração dos conhecimentos dos diversos conceitos que envolvem a indústria, visando a exploração de técnicas de data science e de machine learning. esta implementação teve em consideração aspetos importantes na gestão estratégica das empresas atendendo as necessidades das mesmas, sendo possível a adaptação da solução para cada caso em particular.",
      "the most recent advancement recognized by the european community as industry 5.0 attends the immerging evolutions in the industry world and reveals a need for evolution in the erp’s (enterprise resource planning) systems. it is expected that these systems will assist in the management of companies in a more dynamic way, thus becoming more autonomous, attending to all the information agglomerated in the system. produtech, formally known as the ”portuguese cluster of manufacturing technologies”, is a network established by production technology companies capable of responding to industry challenges with creative, adaptable, and integrated solutions. primavera, a pioneering portuguese company in the development of management solutions, particularly erp systems, is included in this consortium. these systems constantly deal with high volumes of data, which can lead to complex operations in their treatment and analysis. this master’s thesis report documents a proposed solution for providing computational assistance in the selection of suppliers, as well as the aggregation of knowledge from various industry concepts, with an emphasis on data science and machine learning techniques. this implementation took essential aspects into account in the strategic management of businesses, meeting their needs while allowing for the solution to be amended for each individual case."
    ],
    0.3
  ],
  [
    [
      "na nossa sociedade existem várias formas de crimes que ocorrem com bastante frequência. no combate ao crime, vários sistemas safe-return estão a ser desenvolvidos para pessoas vulneráveis. contudo estes sistemas focam-se principalmente na resposta a situações em que o perigo já aconteceu, não é feita nenhuma previsão de um possível crime. estas previsões de situações de perigo podem ser feitas com a informação recolhida, relativa às proximidades do utilizador, em tempo real. um sistema de safe-return-home deve permitir que os utilizadores do serviço sejam notificados, atempadamente, de possíveis situações perigosas que ocorram nas proximidades da localização do utilizador, em tempo real. estas previsões são feitas através da análise de vários tipos de dados relativos ao que se encontra em redor do utilizador. a recolha e análise de dados deverá incluir informação relativa a notícias relacionadas com crimes, acidentes e desastres, incluindo também a localização do utilizador. a análise dos dados permite então fazer uma previsão de perigos, de forma que possam ser visualmente reconhecidos no smartphone do utilizador, sendo assim possível reagir de forma ativa a esses perigos, em tempo real.",
      "in our society there are various forms of crimes that occur quite often. to diminish crime, a number of safe-return systems are being developed for vulnerable people. however these systems are focused mainly in response to situations in which the danger has occurred, with no effort to predict a possible crime. these predictions of danger situations can be done with the gathering of information concerning the surroundings of the user, in real time. a safe-return-home system should allow the users of the service, to be notified in a timely fashion of possible dangerous situations that occur near the user’s location, in real time. these predictions are made through the analysis of various types of data about the surroundings of the user. the data collection and analysis should include information regarding news about crimes, accidents and disasters, and the user’s location. data analysis then allows to predict dangers, so that they can be visually recognized on the user’s smartphone, making it possible to respond actively to these dangers, in real time."
    ],
    [
      "with the rapid growth in the amount of video data, an increasing need for efficient video retrieval systems has become an important problem in the multimedia management topic. despite having a long past, the increase in file size of video collections, caused mostly by the increase of video resolution and quantity of videos, originated a big push for applying machine learning on the video retrieval subject. in today’s world, when dealing with big data, it’s unfeasible to still rely on video metadata and manually annotated videos to provide an accurate video retrieval engine, seeing as the sheer quantity of videos overwhelms an inept search and browse system, unable to provide the video the user wants. therefore, by relying on machine algorithms to accurately mass tag the video collection we achieve great improvements. the process of allocating the video information to the video retrieval framework is severely less time consuming and the viewer has at his disposal more precise and semantically accurate filters. this in turn, drastically reduces the quantity of redundant videos that are pulled from the user’s queries. another way to also ease the time it takes to analyze an immense quantity of videos, is by summarizing the content that is present on them. condensing dozens of hours, pulled from one or more video streams, into a more accessible source of information that displays the most relevant data, is considerably a more efficient viewing experience for the user as it unburdens him of the task of surveying a grotesque amount of media content. the main focus of this thesis is to implement a video summarization method for recapping footage from the interior of a vehicle, that will be integrated on a video retrieval platform that is also being developed in parallel.",
      "com o rápido crescimento na quantidade de videos, uma necessidade crescente de sistemas efici entes de recuperação de vídeo tornou-se num problema importante na gestão de multimédia. apesar de ter um longo passado, o aumento do tamanho de ficheiro das coleções de vídeos, causado principal mente pelo aumento da resolução e quantidade de vídeos, originou um grande impulso na aplicação de aprendizagem automática na área de recuperação de vídeos. no mundo de hoje, ao lidar com big data, é inexequível ainda depender de metadados de vídeo e de vídeos anotados manualmente para fornecer um sistema de recuperação de vídeo preciso, visto que a grande quantidade de vídeos sobrecarrega um sistema de pesquisa e navegação inepto, incapaz de provisionar o vídeo que o utilizador deseja. por conseguinte, ao basearmo-nos em algoritmos de máquina para etiquetar com precisão a coleção de ví deos, conseguimos grandes melhorias. o processo de atribuição da informação do vídeo à estrutura de recuperação de vídeo é muito menos moroso e o espetador tem à sua disposição filtros mais precisos e semanticamente exactos. isto, por sua vez, reduz drasticamente a quantidade de vídeos redundantes que são retirados das consultas do utilizador. outra forma de também diminuir o tempo de análise de uma imensa quantidade de vídeos, é resumir o conteúdo que está presente neles. condensar dezenas de horas, extraídas de um ou mais fluxos de vídeo, em uma fonte de informação mais acessível que exibe os dados mais relevantes, é uma experiência de visualização consideravelmente mais eficiente para o usuário, pois alivia-o da tarefa de pesquisar uma quantidade imensa de conteúdo de média. o foco principal desta tese é implementar um método de sumarização de vídeo para recapitular gravações do interior de um veículo, que será integrado numa plataforma de recuperação de vídeo que também está a ser desenvolvida em paralelo."
    ],
    0.06666666666666667
  ],
  [
    [
      "existe uma preocupação cada vez maior em relação a quantidade de informação gerada e recebida por diversas instituições. não só no que toca ao consumo excessivo de papel, como também a gestão de grandes quantidades de informação. com o intuito de simplificar a gestão documental, o governo tem desenvolvido diversas estratégias. nomeadamente, na administração pública (ap), com base em normas e orientações provenientes da comissão europeia. o projeto m51-clav-arquivo digital: plataforma modular de classificação e avaliação da informação pública (clav), surge como uma dessas estratégias. este visa a classificação e a avaliação de toda a documentação que circula na administração pública portuguesa, utilizando um referencial comum que permite o desenvolvimento de instrumentos de natureza transversal a aplicar em contexto organizacional. esta dissertação tem como objetivo primário, a integração dos serviços da plataforma autenticação.gov no clav, bem como a criação de estratégias apropriadas de gestão de utilizadores, autenticação de pedidos referentes à api pública disponibilizada, autenticação no backend e segurança da aplicação.",
      "there’s a ever growing worry about the quantity of information that’s generated and received by multiple institutions. not only related to the excessive ammount of paper consumed, but as well as the management of such big quantities of infor mation. with the purpose of simplifying the process of managing such documents, the portuguese government has been developing various strategies. namely, in the public administration, with directives provided by the european comission. the clav project comes up as one of those strategies. this allows to classify and evaluate all the documentation that circulates in the portuguese public administration, using a common referential, that allows for the development of instruments of transversal nature that are applied in an organizational context. this dissertation has as its primary objective, the integration of the services pro vided by the autenticação.gov platform in the clav project, as well as the creation of appropriate strategies of user management, authentication of requests refering to the public api, as well as backend authentication and overall security."
    ],
    [
      "a área da ehealth tem ganho uma enorme importância nos últimos anos. tal advém não só da progressiva evolução tecnológica que se tem verificado ao longo dos tempos, mas também da necessidade de melhorar a qualidade de vida das pessoas que necessitam de assistência médica permanente ou ocasional. mais especificamente, a telemonitorização permite aos pacientes estarem mais envolvidos no seu processo de recuperação e agirem mais rapidamente quando surgem novos problemas ou complicações. neste tipo de serviços, existem geralmente três entidades, o paciente, o cuidador e os dispositivos tecnológicos, que funcionam como facilitadores entre ambos. atualmente, existem cada vez mais serviços de telemonitorização, mas, na maioria dos casos, estes serviços seguem uma abordagem genérica, que pode ser demasiado complexa para alguns utilizadores. tal deve-se ao facto de incluírem por vezes funcionalidades desnecessárias para certos doentes, o que não lhes permite a melhor experiência de navegação e utilização. no âmbito da dissertação, desenvolveu-se uma aplicação web que permite aos utentes registar as suas medições através de dispositivos bluetooth, e responder a questionários providenciados pelos seus médicos. quer as medições, quer as respostas aos questionários poderão ser consultadas por parte dos médicos dos pacientes, permitindo assim, tal como pretendido, uma melhor monitorização e controlo do estado de saúde dos pacientes. esta aplicação pretende ser mais simples que as restantes, utilizando serviços modulares, para que os pacientes possam escolher o que querem utilizar, de acordo com o seu perfil, sempre e quando precisarem. cada funcionalidade principal da aplicação pode ser vista como um micro-serviço independente. adicionalmente, o projeto segue as mais recentes abordagens tecnológicas e arquiteturais. em termos de interface e usabilidade, procurou-se desenvolver uma aplicação atrativa e intuitiva. por fim, é importante mencionar que o presente projeto se enquadra no âmbito de desenvolvimento da solução de telemonitorização da altice labs, denominada smartal, que permite recolher dados do paciente a partir de dispositivos inteligentes e questionários.",
      "the ehealth area has gained an enormous importance in recent years. that comes not only from the progressive technological evolution that has been happening over time, but also from the need to improve the quality of life of people who need permanent or ocasional medical assitance. telemonitoring in particular allows patients to be more envolved in their process of recovery and to quickly act when a new problem appears. in this types of services, there are usally three entities, the pacient, the caretaker and the technological devices, that work as facilitators between both. in todays world, more and more telemonitoring services are beggining to appear, but most often, this services follow a generic approach, that can be too complex for some users, because it includes many unnecessary features, that do not allow an easy navigation and orientation flow through the apps. in this dissertation scope, the goal is to develop a web application that allow patients to register their meausurements through bluetooth devices and also answer questionnaires given by their doctors. both the measurements and the questionnaires will be available to the patients doctors, that allow for a better control of their patients health, just as intended. this applications is simpler, and uses modular services, so that the patients can choose what they want to control whenever they need to. each functionality can be seen as an independent microservice. the project also follows the most recent technological and architectural approaches. in terms of interface and usablility, the app aims to be attractive and intuitive. finally, it is importante to refer that the present project fits in the smartal scope, which is a telemonitoring solution developed in altice labs, that has features that allow the gathering and analysis of data, collected throught the use of smart devices."
    ],
    0.3
  ],
  [
    [
      "nesta dissertação descreve-se o desenvolvimento de uma plataforma de fidelização de clientes, em colaboração com a wintouch, que servirá como produto complementar aos já oferecidos pela empresa no mercado de gestão comercial. a plataforma deverá funcionar em dois níveis, empresa e consumidor, de modo a responder aos requisitos de ambos no processo da fidelização. esta necessidade traduz-se na plataforma ser composta por dois componentes distintos mas complementares. tratando-se de uma plataforma que tem como fim ser comercializada no mercado, esta terá que cumprir todos os requisitos de performance e funcionalidade associados, de modo a cumprir os padrões de qualidade esperados pelos parceiros comerciais da wintouch o componente da empresa deverá fornecer as funcionalidades necessárias para implementar e manter campanhas de fidelização de diversos tipos e configurações, bem como permitir uma análise estatística aprofundada dos resultados das mesmas, visando permitir às empresas tomar decisões informadas e ver os efeitos concretos das campanhas nas métricas de negócio. este componente deverá estar totalmente integrado com os restantes produtos fornecidos pela wintouch, nomeadamente as soluções de gestão comercial e de retalho, de modo a poder ser adotado pelos parceiros sem que daí advenham mudanças radicais ao fluxo de trabalho pré-estabelecido. o componente do consumidor deverá permitir a este descobrir e usufruir de campanhas de fidelização de clientes, bem como servir de canal de comunicação entre empresa e consumidor, permitindo ao consumidor aceder a informações relevantes sobre campanhas de fidelização.",
      "this dissertation describes the development of a fidelization platform, in collaboration with wintouch, which will serve as a complementary product to those already offered by the company in the commercial management market. the platform should work on two levels, company and consumer, in order to meet the requirements of both in terms of fidelization. this need translates into the platform being composed of two distinct but complementary components. as this is a platform that is intended to made available on the market, it must meet all the associated performance and functionality requirements, in order to meet the quality standards expected by wintouch’s commercial partners the company’s component should provide the necessary functionalities to implement and maintain loyalty campaigns of different types and configurations, as well as allow an in-depth statistical analysis of their results, in order to allow companies to make informed decisions and see the concrete effects of the campaigns on relevant business metrics. this component should be fully integrated with the other products provided by wintouch, namely the commercial management and retail solutions, so that it can be adopted by partners without causing radical changes to the pre-established workflow. the consumer component should allow them to discover and benefit from customer loyalty campaigns, as well as being a communication channel between company and consumer, allowing the consumer to access relevant information about the fidelization campaigns, amongst many other."
    ],
    [
      "in an ever more connected world, smart cities are becoming ever more present in our society. in these smart cities, use cases in which innovations that will benefit its inhabitants are also growing, improving their quality of life. one of these areas is safety, in which machine learning (ml) models reveal potential in real-time video-stream analysis in order to determine if violence exists in them. these ml approaches concern the field of computer vision, a field responsible for traducing digital images and videos, and be able to extract knowledge and understandable information from them, in order to be used in diverse contexts. some of the available alternatives to recognise actions in video streams are based on ml approaches, such as deep learning (dl), that grew in popularity in the last years, as it was realised that it had massive potential in several applications that could benefit from having a machine recognising diverse human actions. in this project, the creation of a ml model that can determine if violence exists in a video-stream is proposed. this model will leverage technology being used in state of the art methods, such as video classifiers, but also audio classifiers, and early/late fusion (ef / lf) schemes that allow the merging different modalities, in the case of the present work: audio and video. conclusions will also be drawn as to the accuracy rates of the different types of classifiers, to determine if any other type of classifiers should have more prominence in the state of the art. this document begins with an introduction to the work being conducted, in which both the its context, mo tivation and objectives are explained. afterwards, the methodology used in order to more efficiently conduct the research in this thesis is clarified. following that, the state of the art concerning ml based approaches to action recognition and violence detection is explored. after being brought to date in what are the state of the art approaches, one is able to move forward to the following chapter, in which the training method that will be employed to train the models that were seen as the best candidates to detect violence is detailed. subsequently, the selected models are scrutinized in an effort to better understand their architecture, and why they are suited to detect violence. afterwards, the results achieved by these models are explored, in order to better comprehend how well these performed. lastly, the conclusions that were reached after conducting this research are stated, and possibilities for expanding this work further are also presented. the obtained results prove the success and prevalence of video classifiers, and also show the efficacy of models that make use of some kind of fusion.",
      "num mundo cada vez mais conetado, as cidades inteligentes tornam-se cada vez mais presentes na nossa sociedade. nestas cidades inteligentes, crescem também os casos de uso nos quais podem ser aplicadas inovações que beneficiarão os seus habitantes, melhorando a sua qualidade de vida. uma dessas áreas é a da segurança, na qual modelos de aprendizagem máquina (am) apresentam potencial para analisar streams de vídeo em tempo real e determinar se nestas existe violência. estas abordagens de am são referentes ao campo de visão por computador, um campo responsável pela tradução de imagens e vídeos digitais, e pela extração de conhecimento e informação inteligível dos mesmos, de modo a ser utilizada em diversos contextos. algumas das alternativas disponíveis para reconhecer ações em streams de vídeo são baseados em abordagens de am, tais como aprendizagem profunda (ap), que cresceu em popularidade nos últimos anos, à medida que se tornou claro o massivo potencial que tinha em diversas aplicações, que poderiam beneficiar de ter uma máquina a reconhecer diversas ações humanas. neste projeto, é proposta a criação de um modelo de machine learning que permita determinar a existência de violência numa stream de vídeo. este modelo tomará partido de tecnologia utilizada em métodos do estado da arte como classificadores de vídeo, mas também de classificadores áudio, e esquemas de fusão antecipada / tardia (fa / ft) que permitem a combinação de várias modalidades de dados, neste caso: áudio e vídeo. serão tiradas também conclusões sobre as taxas de acerto dos diversos tipos de classificadores, de modo a determinar se algum outro tipo de classificador deveria de ter mais prominência este documento começa com uma introdução ao trabalho levado a cabo, em que o seu contexto, motivação, e objetivos são explicados. seguidamente, a metodologia utilizada de modo a mais eficientemente levar a cabo a pesquisa nesta tese é clarificada. após isso, o estado da arte no que concerne abordagens baseadas em am para reconhecimento de ações e deteção de violência é explorado. depois de ser atualizado em relação a quais são consideradas abordagens de estado da arte, é possível avançar para o capítulo seguinte, onde o método utilisado para treinar os modelos que foram considerados como os melhores candidatos para detetar violência é detalhado. subsequentemente, os modelos selecionados são escrutinizados de modo a melhor entender a sua arquitetura, e porque são adequados para detetar violência. depois, os resultados conseguidos por estes modelos são explorados, de modo a melhor compreender o desempenho conseguido. finalmente, as conclusões que foram chegadas a são apresentadas, tais como possibilidades para expandir e melhorar esta pesquisa. os resultados obtidos comprovam o sucesso e a prevalência dos classificadores de vídeo, e mostram também a eficácia dos modelos que tomam partido de algum tipo de fusão."
    ],
    0.3
  ],
  [
    [
      "we aim to integrate new “suites”, using post-quantum authentication and encryption tech niques, in the tls protocol. namely, this project is dedicated to integrating algorithms belonging to the ntru family of cryptossystems in the openssl library and in the python package “cryptography”. even though all the algorithms included in this project have already been imple mented as part of their submissions to the nist post-quantum standartization project, currently there doesn’t seem to exist a way to perform prototyping and testing of these cryp tossystems in real-life use cases, and it would be interesting to create such tools. we also aim to test if these algorithms could be further optimized for speed and efficiency by comparing the reference implementations (submited to nist and publicly avail able) with our own implementations that perform some required mathematical operations in a very efficient manner (by using specialized number theory libraries).",
      "pretende-se integrar novas “suites” no protocolo tls que usem técnicas de autenticação e cifra na categoria de técnicas pós-quanticas. nomeadamente, este projecto é dedicado à integração de algoritmos da família ntru na biblioteca openssl e na “package” cryptography para o python. apesar de todos os algoritmos contemplados neste projeto já terem sido implementa dos no âmbito da sua submissão ao nist post-quantum standartization project, actualmente não parece existir forma de testar e prototipar estes criptossistemas em casos de uso realistas, e seria interessante desenvolver ferramentas que o permitam. pretende-se também aferir se estes algoritmos podem ser optimizados em eficiência e velocidade de execução, comparando as implementações de referência (submetidas ao nist e disponiveis publicamente) com as nossas implementações, que efectuam algumas operações matemáticas necessárias de forma muito eficiente (com recusro a bibliotecas de teoria de números especializadas)."
    ],
    [
      "despite their rising usage in classrooms, most automatic grading tools for programming exercises are quite simple, using only output comparison or unit tests to evaluate a solution, in contrast with manual grading methods used by teachers, which also look at the code itself, even if it doesn’t produce a correct solution. static analysis methods for code have been around for a while, but largely ignored in assessment software. the master’s project here reported proposes an automatic grading method for pro gramming exercises that, in addition to dynamic analysis, uses static analysis to evaluate submissions. this method benefits both teachers and students, since, by scoring solutions that produce the wrong output, it provides a more comprehensive evaluation of student submitted programs while also making it easier to see exactly what needs to be improved. moreover, it makes evaluation more rigorous, by requiring more than just a program that solely produces the correct result. a prototype application called ace grader was created to demonstrate the efficacy of this grading strategy. this dissertation describes a bibliographic review of existing automatic grading tools, proposes and introduces ace grader through an overview of its architecture and its development process. as an initial version of the application was deployed in the middle of the second semester of university classes, experiments with students in a real classroom setting are also presented and discussed.",
      "apesar do seu uso crescente em salas de aula, a maioria das ferramentas de avaliação automática de exercícios de programação são relativamente simples, usando apenas com paração de output ou testes unitários para avaliar uma solução, em contraste com métodos de avaliação manual usados por professores, que também têm em conta o código em si, mesmo que este não produza uma solução correta. métodos de análise estática para código, que tentam preencher esta lacuna, já existem há algum tempo, mas são na sua maioria ignorados em software de correção. o projeto de mestrado aqui descrito propôe um método de avaliação automática de exercícios de programação que, para além de análise dinâmica, usa análise estática para avaliar submissões. este método beneficia tanto professores como estudantes, visto que, ao pontuar soluções que produzem o output errado, avalia de forma mais compreensiva os programas submetidos por estudantes e permite saber exatamente o que pode ser melhorado. para além disso, torna a avaliação mais rigorosa, ao exigir mais do que um programa que apenas produz o resultado correto. um protótipo de uma aplicação chamado ace grader foi desenvolvido para demonstrar a eficácia desta estratégia de correção. esta dissertação descreve uma revisão bibliográfica de ferramentas de avaliação automática existentes, propôe e introduz o ace grader através de uma visão geral da sua arquitetura e do seu processo de desenvolvimento. uma versão inicial da aplicação foi disponibilizada para testes a meio do segundo semestre letivo, o que permitiu que esta fosse usada em sala de aula com estudantes, em experiências que são aqui apresentadas e discutidas."
    ],
    0.3
  ],
  [
    [
      "shadow mapping has been one of the most used algorithms for real time calculation of shadows, since it is extremely simple and quick in calculating said shadows, but not always presents the best results. on the other hand, ray-tracing presents pixel-perfect shadows, but it is more demanding from a computational point of view. shadow mapping has seen many proposals to increase its accuracy, while retaining its high performance nature. some of the methods proposed, based solely on the standard shadow mapping technique, do improve significantly the standard shadow mapping result at the expense of a minor decrease in performance. other approaches propose hybrid methods, using shadow mapping as a way of limiting the number of pixels that require ray-tracing. one of such approaches uses texel coherence to reduce the number of pixels that require testing. these latter approaches establish the theme for this work. the goal is to narrow down as much as possible the amount of pixels that require a ray-tracer to determine its shadow status. the first step was to identify the location of the errors present in a shadow map. the tests confirmed the intuition that most of these errors should be located in the contours of the shadow areas. the next step focuses on these contour areas and looks for ways to determine the correctness of a pixel’s shadow status. several methods were proposed to achieve this goal. some methods were capable of confirming pixels in shadow. some were capable of correcting pixels in light. each method, with the exception of texel coherence, uses a very selective ray-tracer, i.e. only very few triangles are tested for intersection with a single light ray. since each method has its strengths and weaknesses an algorithm was proposed, chaining all these methods together. the first step is to determine the set of pixels in the contours of the shadow areas. then each method is applied in turn, so that only the pixels the remaining unconfirmed/uncorrected pass on to the next stage. at the end of the algorithm a very large percentage of pixels in shadow were confirmed and a significant number of pixels in light were corrected. the remaining pixels could then be fed to a full ray-tracer. the load of the ray-tracer is severely reduced under this approach making it an affordable solution to obtain pixel perfect shadows in the contours of the shadowed areas.",
      "o shadow mapping tem sido um dos algoritmos mais utilizados para o cálculo de sombras em tempo real, já que é extremamente simples e rápido em calcular estas sombras, mas nem sempre apresenta os melhores resultados. por outro lado, ray-tracing apresenta sombras perfeitas ao nível do pixel mas é mais exigente de um ponto de vista computacional. têm havido muitas propostas para o aumento de qualidade do shadow mapping sem afetar o seu desempenho. alguns dos métodos propostos, baseados somente na técnica de shadow mapping padrão, de facto melhoram significativamente o resultado do shadow mapping padrão ao custo de uma pequena diminuição no desempenho. outras abordagens propõem métodos híbridos, usando o shadow mapping para limitar o número de pixéis que requerem ray-tracing. uma destas abordagens usa o texel coherence para reduzir o número de pixéis que precisam de ser testados. estas últimas abordagens estabelecem o tema deste trabalho. o objetivo é limitar o máximo possível a quantidade de pixéis que requerem um ray-tracer para determinar o seu sombreamento. o primeiro passo foi identificar a localização dos erros presentes num shadow map. os testes confirmaram a intuição de que a maior parte destes erros se deveriam encontrar nos contornos das zonas sombreadas. o próximo passo foca-se nestas áreas de contorno e procura maneiras de determinar se o sombreamento de um pixel está correto. vários métodos foram propostos para conseguir este objetivo. alguns métodos foram capazes de confirmar pixéis em sombra. alguns foram capazes de corrigir pixéis em luz. cada método, com a exceção do texel coherence, usa um ray-tracer muito seletivo, isto é, apenas uma muito pequena quantidade de triângulos é testada para interseção com cada raio de luz. como cada método tem as suas vantagens e desvantagens um algoritmo que encadeia todos estes métodos foi proposto. o primeiro passo é determinar o conjunto de pixéis nos contornos das áreas sombreadas. depois cada método é aplicado à vez de modo a que os pixéis que se mantêm por confirmar ou corrigir passem para o próximo passo. no fim do algoritmo uma grande percentagem de pixéis em sombra foi confirmada e um número significativo de pixéis em luz foi corrigido. o resto dos pixéis poderia então passar por um ray-tracer completo. a carga do ray-tracer é severamente reduzida sob esta abordagem tornando-o numa solução acessível à obtenção de sombras perfeitas ao nível do pixel nos contornos das áreas sombreadas."
    ],
    [
      "a mineração de processos - process mining - define-se como uma técnica de extração de informação em que, de uma forma automática, se extrai a partir de um registo de eventos informação relevante acerca do desempenho de uma dada organização ou sistema numa dada área de negócio. as técnicas de mineração de processos podem ser aplicadas em diversos contextos aplicacionais, como a informática, a medicina ou o marketing. relativamente a esta última área, a mineração de processos pode ser utilizada para estabelecer um conjunto de ações que permitam lançar uma dada campanha na web, tendo como base pontos de maior intensidade de ações web. deste modo, nesta dissertação realizou-se um estudo pormenorizado acerca de como estabelecer perfis e preferências de exploração web a partir da informação obtida através dos processos de navegação e exploração web dos utilizadores, e definir um conjunto de ações específicas para suporte a uma dada campanha de web marketing de forma (semi)automática.",
      "process mining is defined as a technique for extracting information in which information about the performance of a particular organization or system in a given business area is automatically extracted from an event log. process mining techniques can be applied in a variety of application contexts, such as computer science, medicine, or marketing. with regard to this last area, process mining can be used to establish a set of actions that allow launching a given campaign on the web, based on points of greater intensity of web actions. thus, in this dissertation a detailed study was carried out on how to establish profiles and preferences of web exploration from the information obtained through the processes of navigation and web exploration of the users, and to define in a (semi) automatic form a set of specific actions to support a given campaign of web marketing."
    ],
    0.0
  ],
  [
    [
      "o aumento exponencial da informação médica e a procura pela otimização da prestação de cuidados de saúde, conduziu à introdução das tecnologias de informação na área da saúde. não existe qualquer dúvida de que a introdução do processo clínico eletrónico (pce) na área da saúde é uma mais valia para a qualidade dos cuidados prestados aos utentes. para além de promover uma maior qualidade dos serviços prestados, a implementação e integração deste tipo de sistemas em ambiente hospitalar, aumenta a segurança dos utentes e reduz os custos. no entanto, a adoção e aceitação de sistemas de pce na área médica não se tem revelado uma tarefa fácil. um dos principais fatores apontados para o fracasso da adoção destes sistemas está associado ao seu baixo nível de usabilidade. dentro desta problemática, a presente dissertação pretende avaliar a usabilidade e a aceitação dos sistemas de informação em ambiente hospitalar que garantem o registo eletrónico da informação relativa aos seus utentes. para além disto, é realizada também uma avaliação global das funcionalidades inerentes ao pce implementado no centro hospitalar do alto ave (chaa), com o intuito de alcançar um ambiente hospitalar livre de papel.",
      "the exponential increase of medical information and the demand to optimize the provision of peoples care, led to the beginning of information technologies in healthcare. there is no doubt that the introduction of electronic health record (ehr) in this area it's a credit to the quality of the service provided to users. in addition to promoting a higher quality of services, the implementation and integration of such systems in hospitals, increases user safety and reduces the costs. however, the adoption and acceptance of ehr systems in the medical eld hasn't been an easy task. one of the main reason to the failure of the adoption of these systems is related to his low level of usability. about this issue, this project aims to evaluate the usability and acceptance of ehr in hospitals to ensure the electronic record of their user's information. in addition, it's held also an overall evaluation of the ehr functionalities implemented at chaa, with the purpose of reaching a hospital environment free of clinical records on paper."
    ],
    [
      "formal verification of software has been an active topic in the area of computer science. several techniques to verify software are now available, and many tools have been created over the years for different languages and using different techniques. however, for spark, a programming language broadly used in critical systems, only deductive verification tools based on contracts are available. the main downside of this approach is the lack of a full automation. in this dissertation we propose an automated verification tool for spark code, thus contributing to fill the gap identified above. our tool bases on an alternative technique, called bounded model checking, that sacrifices completeness in exchange for automation. through grounding our work in the highly popular and successful cbmc tool for verification of c code, we investigate how to perform bounded model checking of spark programs, and, in particular, we present our implementation of a bounded model checker for spark programs called spark-bmc. experiments performed with our tool show that automatic verification of spark programs is feasible and useful, even though is not complete. as far as we know, there is no tool based on such an automated technique for spark. the tool is freely available and based on open-source technologies.",
      "a verificação formal de software tem sido um tópico bastante ativo na área das ciências da computação. várias técnicas podem ser aplicadas para verificar software e ao longo dos anos surgiram várias ferramentas para diferentes linguagens usando diferentes técnicas. para a linguagem de programação spark, que é especialmente usada em sistemas críticos, existem ferramentas dedutivas baseadas em contratos. porém, esta técnica de verificação tem uma desvantagem: fraca automação. nesta dissertação, propomos uma ferramenta de verificação automática para código spark, assim contribuindo para suprir a limitação antes referida. esta ferramenta baseia-se numa técnica alternativa denominada por ‘bounded model checking’ que embora não sendo completa permite automação. baseando o nosso trabalho na popular e bem sucedida ferramenta cbmc para a verificação de código c, estudamos como levar a cabo o ‘bounded model checking’ de programas spark e, em particular apresentamos a respectiva implementação que designamos por spark-bmc. as experiências que levamos a cabo com a nossa ferramenta mostram que a verificação automática de programas spark, ainda que não seja completa, é praticável e útil. pelo que nos é dado a conhecer, não há nenhuma ferramenta baseada numa tal técnica automatizada para programas spark. a ferramenta é de acesso livre e baseia-se em tecnologias ‘open-source’."
    ],
    0.0
  ],
  [
    [
      "a internet das coisas é um fenómeno que, embora não seja recente, tem sentido um enorme crescimento nos últimos anos. atualmente existem cerca de 10 biliões de dispositivos ligados à internet com a expectativa de se alcançar entre 20 a 50 biliões de dispositivos ligados dentro dos próximos cinco anos. através da introdução de serviços inovadores concebidos para diversas áreas, tais como a indústria, os cuidados de saúde, a domótica, os transportes, a agricultura, o retalho, a segurança, entre muitas outras áreas do nosso quotidiano, a internet das coisas promete melhorar as nossas vidas, pois com as capacidades de monitorização, processamento e comunicação dos dispositivos iot é possível tornar as “coisas” do nosso dia-a-dia parte de algo maior. a monitorização, sendo uma parte integrante das soluções baseadas em iot, pode ser utilizada para medir vários parâmetros. alguns dos mais comuns são a temperatura, humidade, pressão, som e a vibração. embora a vibração possa ser vista de diferentes formas, na prática geotécnica a vibração corresponde a uma resposta elástica do terreno (solos e/ou rochas) aquando da passagem de uma onda de tensão, tendo como origem um evento de génese natural (como por exemplo sismos ou o deslizamento súbito de massas rochosas ao longo de falhas geológicas) ou artificial (explosões, cravação de estacas, trabalhos de construção, utilização de equipamentos diversos, linhas ferroviárias, tráfego rodoviário, entre outros). esta vibração pode ser monitorizada recorrendo a diferentes tipos de sensores, pelo que a proposta apresentada opta por recorrer aos acelerómetros mems, tirando partido do facto de estes serem extremamente pequenos, baratos e com uma baixa necessidade de consumo de energia. a proposta está dividida em três componentes principais: o coletor, o servidor e o monitor. o coletor é o componente físico da proposta e tem como responsabilidade registar os eventos de natureza vibratória ao longo do tempo. o servidor é o componente central e é responsável por armazenar o histórico de toda a informação recolhida pelo coletor. por fim, o monitor é o componente que é responsável por fornecer uma interface capaz de aceder à informação recolhida pelo coletor e gravada pelo servidor. com o trabalho desenvolvido foram executados alguns testes de forma a avaliar o funcionamento dos diferentes componentes da proposta, em especial o desempenho do coletor",
      "the internet of things is a concept that, although it isn’t new, it had a huge growth in recent years. currently there are about 10 billion devices connected to the internet with the expectation of achieving between 20 to 50 billion connected devices within the next five years. through the introduction of innovative services designed for several areas such as industry, health care, home automation, transport, agriculture, retail, security, among many other areas of quotidian, the internet of things promises to improve our lives by using the capabilities of the iot devices, such as monitoring, processing and communication. this capabilities can make these \"things\" of our daily lives part of something bigger. monitoring, being an integral part of the solutions based on the iot concept, may be used to measure several parameters. some of the most common are temperature, humidity, pressure, sound and vibration. although vibration can be seen in several ways, in geotechnical practice vibration corresponds to an elastic response (soil and/or rock) upon the passage of a wave which can have its origin in natural events (such as earthquakes or sudden sliding of rock masses along geological faults) or artificial events (explosions, pile driving and construction work, usage of several equipments, railways, road traffic, etc.). this vibration can be monitored using different types of sensors, but this proposal opted to use the new mems accelerometers, taking advantage of the fact that they are extremely small, cheap and have low consumption needs. this proposal is divided into three main components: the collector, server and monitor. the collector is the physical component of this proposal and is responsible for recording the vibratory events over time. server is the central component and it is responsible for storing all the information registered by the collector. finally, the monitor is the component that provides an interface to the user, so he can access the information registered by the collector and recorded by the server. the monitor consists in a mobile application developed for android. with the porposal finished, some tests were performed which allowed us to assess the validity of the various components of this proposal, in particular the performance of the collector."
    ],
    [
      "the appearance of the sky is defined by the position of the elements we can see in it. with that in mind, it’s interesting to comprehensively understand the procedures needed to accurately compute the position of said elements in the sky, given the latitude, longitude of the observer and the date. alongside the accurate position of these elements, we can take into consideration the atmospheric scattering and attenuation, as well as taking advantage of our virtual environment to implement non-realistic features that enhance the visualization experience. two applications, one for desktop and for android, were developed using multiple free publicly available tools and resources. the 3 most relevant resources are a star catalogue (nash), giving us the positions of over 140.000 stars, the set of equations presented in paul schlyter’s website (schlyter) which compute the positions of the planets of the solar system, and the sampa algorithm developed by nrel (2012). the latter both computes the position of the sun and moon and allows us to translate the position of celestial elements from a geocentric coordinate system to a topocentric one. this thesis mostly focuses on describing how to use these resources to accurately compute the position of and display the stars, planets, moon and sun. besides the accurate positioning of these elements, atmospheric scattering is also taken into account, during the day, which gives our sky its familiar color, as well as atmospheric attenuation, during the night, which color shifts the planets, moon and stars depending on their position on the sky. for ease of visualiza tion, non-realistic features were added that increase the visibility of usually imperceptible elements, such as increasing the scale of the planets, increasing the brightness of the dimmest stars and overlaying the artwork of the 88 constellations over the night sky.",
      "a aparência do céu é definida pela posição dos elementos que conseguimos ver nele. com isto em mente, é interessante perceber compreensivamente os processos para calcular a posição desses elementos no céu, dada a latitude e longitude do observador e data. para além da posição exata destes elementos, podemos ter em consideração a dispersão atmosférica e atenuação, assim como tirar vantagem do nosso ambiente virtual para implementar funcionalidades não realistas que melhoram a experiência de visualiza ção. duas aplicações, uma para desktop e uma para android, foram desenvolvidas com o uso de múltiplas ferramentas e recursos disponíveis gratuitamente online. os 3 recursos mais relevantes são um catálogo de estrelas (nash), que contém as posições de mais de 140.000 estrelas, as equações apresentadas no website do paul schlyter (schlyter) que calculam a posição dos planetas do sistema solar, e o algorimto sampa desenvolvido pelo nrel (2012). este último calcula a posição do sol e da lua, assim como nos permite fazer a translação da posição de elementos celestiais de um sistema de coordenadas geocên trico para um sistema topocênctrico. esta dissertação foca-se principalmente em descrever como usar estes recursos para calcular com precisão a localização das estrelas, planetas, lua e sol. para além do posicionamente preciso destes elementos, dispersão atmosférica também é tida em conta, durante o dia, fenómeno que dá ao nosso céu as suas cores, assim como atenuação atmosférica, durante a noite, que al tera as cores dos planetas, lua e estrelas dependendo da sua posição no céu. para facilitar a vizualização, funcionalidades não realistas foram adicionadas para aumentar a visibilidade de elementos normalmente impercetíveis, tais como aumentar a escala dos planetas, aumentar a luminosidade das estrelas menos brilhantes e desenhar a arte das 88 constelações sobre o céu."
    ],
    0.3
  ],
  [
    [
      "in kinship testing powerful statistical results are usually obtained when genetic information is expected to be shared between a pair of samples, which happens in paternity and identification testing. however, there are other pedigrees where genetic information sharing is not required, such as when a pair of full-siblings or avuncular, is analyzed. studying these pedigrees, where the sharing of genetic information is not mandatory, will be the focus of this work. we will consider several kinship problems where two (exhaustive and mutually exclusive) hypotheses will be compared, through a statistical evaluation based on the calculation of a likelihood ratio (lr) where the probabilities of genotypic configurations, assuming one or another kinship hypothesis, are compared. this analysis will allow the identification of the proportion of cases where the statistical evaluation had weak results, and those where lr favored the false hypothesis of kinship for a widely used commercial kit of genetic markers, considering simulated profiles assuming the pedigrees in question. in addition, we will compare the statistical gain of increasing the battery of analyzed markers and infer the impact of considering the genetic information given by the knowledge of the genetic profile of a relative, as the undoubted mother in the case where the hypotheses ”individuals a and b are related as full-siblings“ and ”the individuals a and b are unrelated“ are asked to be compared. furthermore, a validation of the familias software for two individuals will be performed for the simplest assumptions - absence of mutation and absence of silent allele - through the implementation of the algebraic formulas already established.",
      "em testes de parentesco, resultados estatísticos poderosos são geralmente obtidos quando a partilha de informação genética é esperada entre um par de amostras, o que acontece em testes de paternidade e de identificação. no entanto, existem outros pedigrees onde a partilha de informação genética não é requerida, como quando um par de irmãos ou tia(o)/sobrinha(o) é analisado. estudar estes pedigrees, onde a partilha de informação genética não é obrigatória, será o foco deste trabalho. consideraremos vários problemas de parentesco em que duas hipóteses (exaustivas e mutuamente exclusivas) serão comparadas, através de uma avaliação estatística com base no cálculo de razões de verossimilhança (lr, do inglês likelihood ratio) onde as probabilidades das configurações fenotípicas, assumindo uma ou outra hipótese de parentesco, são comparadas. esta análise permitirá a identificação da proporção de casos em que a avaliação estatística teve resultados fracos, e aqueles onde o lr favoreceu a hipótese falsa de parentesco para um kit comercial amplamente utilizado de marcadores genéticos, considerando perfis simulados assumindo os pedigrees em questão. adicionalmente, será comparado o ganho estatístico de aumentar a bateria dos marcadores analisados e será também inferido o impacto de considerar a informação genética dada pelo conhecimento do perfil genético de um parente, como a mãe indubitada no caso em que as hipóteses ”indivíduos a e b estão relacionadas como irmãos“ e ”os indivíduos a e b não estão relacionados” são comparadas. além disso, uma validação do software famílias para dois indivíduos será realizada para os pressupostos mais simples - ausência de mutação e inexistência de alelo silencioso - através da implementação de fórmulas algébricas já estabelecidas."
    ],
    [
      "electron microscopy (em) of nanomaterials relies on grey-scale images to display the material’s atomic arrangement, and a high resolution em can simultaneously capture multiple atomic structures into a single image. however, the extraction of useful information from these images is still limited to the determination of the material’s orientations, an underutilisation of the powerful features of an em equipment and less productive em sessions. this is due to the compute-intense tasks that have not been automated yet. this dissertation aims to significantly reduce the time required to extract useful data from em images and to remove the user bias when analysing high resolution (s)tem images, by automating most user routine tasks and integrating them into a software tool, im2cr. the deployed im2cr tool aimed to aid an em user to find the most probable atomic structure orientation of a nanomaterial in a single 2d image from a set of pre-defined materials, with a minimal user interaction. im2cr was designed and built with a simple and intuitive graphical user interface (gui) that runs on a common modern laptop. it takes as input a high resolution (s)tem image and multiple cif files with candidate atomic structures to describe the material under observation. after performing the fourier transform (ft) on selected regions of interest (roi) in the image, the tool automatically detects periodic information related to the atom’s positions by the brighter spots on the image ft. with a set of geometric computations it tries to match the theoretical values computed with the measured ones by assigning a custom made merit index. this quantitative evaluation avoids possible user bias and/or errors on image characterisation. im2cr outputs at the end a report with the best matching crystallographic structure, its orientation and the indexation table. this tool was successfully tested for robustness and execution efficiency in a wide range of high resolution (s)tem images from crystalline nanomaterials, with domain size ranging from 4 to 100 nm. the autonomous indexation with preset parameters has a very high success rate and runs in a small fraction of typical (s)tem images acquisition time by taking advantage of the inherent hardware parallelism. alternatively, the user can change some relevant parameters related to the roi selection on the (s)tem image and on the ft peaks detection. im2cr promising results point to the possibility of real-time image analysis with reduced user interaction, allowing for an increased (s)tem characterisation yield and also enabling the interpretation of complex images, such as those from nanocrystalline materials imaged in high-order zone axis orientations.",
      "a microscopia eletrônica (em) de nanomateriais usa imagens em escala de cinza para representar a estrutura atómica de um material, sendo a em de alta resolução capaz de capturar simultaneamente múltiplas estruturas atómicas numa única imagem. no entanto, a extração de informação útil destas imagens ainda é limitada pela determinação das orientações do material representado, o que resulta numa subutilização de equipamentos de em e em sessões menos produtivas. a grande razão para esta limitação deve-se à atual baixa automação de tarefas computacionalmente intensivas necessárias para a caracterização do material. esta dissertação tem como objetivo reduzir significativamente o tempo necessário para extrair informação das imagens em, removendo da equação uma possível análise tendenciosa inconsciente do utilizador através da automação deste processo numa nova ferramenta, imacr. o ernacr tem como objetivo ajudar o utilizador a encontrar a orientação da estrutura atómica mais provável de determinado nano material a partir de um conjunto de materiais pré-definidos e uma (mica imagem 2d, com o mínimo possível de interação do utilizador. o imacr foi desenhado e construído para fazer uso de uma simples e intuitiva interface gráfica (gui) capaz de ser executada num normal computador pessoal. esta aplicação recebe como dados de entrada uma imagem (s)tem de alta resolução e vários ficheiros cif com as estruturas atómicas candidatas para descrever o material observado. após aplicação da transformada de fourier (ft) na região de interesse (roi) selecionada na imagem, a ferramenta é capaz de detetar automaticamente informação periódicas relativa às posições dos átomos através dos pontos mais claros na imagem ft. com base num conjunto de cálculos geométricos, a aplicação tenta combinar os valores teóricos calculados com os valores medidos, avaliando assim as correlações com base num recém-criado índice de mérito. esta avaliação quantitativa evita possíveis influências do utilizador e/ou erros de cálculo na caracterização da imagem. no final, o im2cr exporta um relatório com a melhor estrutura cristalográfica encontrada, a sua orientação e a respetiva tabela de indexação. esta ferramenta foi submetida a testes de robustez e eficiência de execução com base numa ampla variedade de imagens (s)tem de alta resolução de nanomateriais cristalinos, cujo tamanho variava entre 4 a 100 nm. a indexação autónoma com uso de parâmetros predefinidos tem uma taxa de sucesso significativa e é executada numa fração do tempo quando comparada com o tempo típico de captura de imagens (s)tem, fazendo uso do paralelismo de hardware existente. alternativamente, o utilizador tem o poder de poder alterar alguns parâmetros relevantes relacionados à seleção da roi na imagem e à deteção de picos na ft. os resultados obtidos apontam para a possibilidade da análise de imagens em tempo real com uma reduzida interação do utilizador, permitindo assim um aumento do desempenho na caracterização de imagens (s)tem e possibilitando ainda a interpretação de imagens mais complexas, nomeadamente imagens de materiais nanocristalinos com orientações menos convencionais."
    ],
    0.3
  ],
  [
    [
      "from the beginning, humans have sought to develop tools that facilitate their work. from the first tools designed for hunting or agriculture, to the industrial revolution and the use of computers in the context of work or even personal life, one of the goals has been to improve the quality of life regarding the impact of work. in various areas, from banking, commerce, to health and customer support, it is quite common to see the presence of chatbots to provide assistance in various functions. whether it is for help with navigation, problem resolution, or even for the sale of a product, sometimes we don’t even notice it, but it’s there. the most common forms are an embedded chatbot on a website or in a support chat. using a tool like a chatbot can be very useful in assisting the end customer, but not only that. in assisting with repetitive tasks that can be automated and due to its total availability, a chatbot can allow for a decrease in the workload of employees in tasks that would have had to be manually performed by them in the past. with these tasks being performed automatically, employees can focus on something that truly requires their participation. this implementation will allow companies to save resources, particularly time and money, which can be applied to less automation-prone areas. with this goal in mind, deloitte decided to support this dissertation by creating a project in the form of a proof of concept to obtain answers about whether a chatbot with these functionalities would be useful within the company and if its integration into one of the preferred platforms, servicenow, would be feasible. thus, it was proposed to develop the chatbot integrated into servicenow, referred to by the platform as the virtual agent and as ally on this project.",
      "desde sempre, o ser humano procura desenvolver ferramentas que facilitem o seu trabalho. desde as primeiras ferramentas destinadas à caça ou à agricultura, até à revolução industrial e à utilização do computador no contexto de trabalho ou mesmo pessoal, um dos objetivos passa por melhorar a qualidade de vida no que diz respeito ao impacto do trabalho. em diversas áreas, desde a banca, comércio, até à saúde e ao apoio ao cliente, é bastante comum vermos a presença de chatbots para prestar auxílio em diversas funções. seja para ajuda na navegação, resolução de problemas ou até mesmo para a venda de algum produto, por vezes nem notamos, mas está presente. as formas mais comuns são, num website ou num chat de apoio estar um chatbot incorporado. utilizar uma ferramenta como um chatbot pode ser bastante útil no auxílio ao cliente final, mas não só. no auxílio à realização de tarefas repetitivas que podem ser automatizadas e devido à sua disponibilidade total, um chatbot pode permitir a diminuição da carga de trabalho dos funcionários na realização de tarefas que, outrora, teriam de ser realizadas manualmente por eles. com essas tarefas realizadas automaticamente, o foco dos funcionários pode ser despendido em algo que realmente requeira a sua participação. essa implementação irá permitir às empresas poupar recursos, em particular tempo e dinheiro, que podem ser aplicados em áreas menos propensas à automação. com esse intuito, a deloitte decidiu apoiar este trabalho, criando um projeto sob a forma de uma prova de conceito para obter respostas sobre se um chatbot com essas funcionalidades seria útil no seio da empresa e se a sua integração numa das plataformas de eleição, servicenow, seria viável. foi assim proposto desenvolver o chatbot integrado no servicenow, denominado pela plataforma de virtual agent neste projeto denominado por ally."
    ],
    [
      "the security of most digital systems is under serious threats due to major technology breakthroughs we are experienced in nowadays. lattice-based cryptosystems are one of the most promising post-quantum types of cryptography, since it is believed to be secure against quantum computer attacks. their security is based on the hardness of the shortest vector problem and closest vector problem. lattice basis reduction algorithms are used in several fields, such as lattice-based cryptography and signal processing. they aim to make the problem easier to solve by obtaining shorter and more orthogonal basis. some case studies work with numbers with hundreds of digits to ensure harder problems, which require multiple precision (mp) arithmetic. this dissertation presents a novel integer representation for mp arithmetic and the algorithms for the associated operations, mpim. it also compares these implementations with other libraries, such as gnu multiple precision arithmetic library, where our experimental results display a similar performance and for some operations better performances. this dissertation also describes a novel lattice basis reduction module, lattbred, which included a novel efficient implementation of the qiao’s jacobi method, a lenstra-lenstralovasz (lll) algorithm and associated parallel implementations, a parallel variant of the ´ block korkine-zolotarev (bkz) algorithm and its implementation and mp versions of the the qiao’s jacobi method, the lll and bkz algorithms. experimental performances measurements with the set of implemented modifications of the qiao’s jacobi method show some performance improvements and some degradations but speedups greater than 100 in ajtai-type bases.",
      "atualmente existe um grande avanço tecnológico que poderá colocar em causa a segurança da maioria dos sistemas informáticos. sistemas criptográficos baseados em reticulados são um dos mais promissores tipos de criptografia pós-quântica, uma vez que se acredita que estes sistemas são seguros contra possíveis ataques de computadores quânticos. a segurança destes sistemas está baseada na dificuldade de resolver o problema do vetor mais curto e o problema do vetor mais próximo. algoritmos de redução de bases de reticulados são usados em muitos campos científicos, tais como criptografia baseada em reticulados. o seu principal objetivo e tornar o problema mais fácil de resolver, tornando a base do reticulado mais curta e ortogonal. alguns casos de estudo requerem o uso de números com centenas de dígitos para garantir problemas mais difíceis. portanto, é importante o uso de módulos de precisão múltipla. esta dissertação apresenta uma nova representação de inteiros para aritmética de precisão múltipla e todas as respetivas funções de um módulo, ‘mpim’. também comparamos as nossas implementações com outras bibliotecas de precisão múltipla, tais como ‘gnu multiple precision arithmetic library’, em que obtivemos desempenhos semelhantes e em alguns casos melhores. a dissertação também apresenta um novo módulo para a redução de bases de reticulados, ‘mpim’, que inclui uma nova e eficiente implementação do ‘qiao’s jacobi method’, o algoritmo ‘lenstra-lenstra-lovasz’ (lll) e respetiva implementação paralela, uma variante paralela do algoritmo ‘block korkine-zolotarev’ (bkz) e a sua versão sequencial e versões de precisão múltipla do ‘qiao’s jacobi method’, lll e bkz. trabalhos experimentais mostraram que a versão do ‘qiao’s jacobi method’ que implementa todas as modificações sugeridas mostra ganhos e degradações de desempenho, contudo com aumentos de desempenho superiores a 100 vezes em bases ‘ajtai-type’."
    ],
    0.3
  ],
  [
    [
      "cidades inteligentes é um conceito que ainda não se encontra muito bem estabelecido. contudo, é de consenso comum que este consiste em fornecer a todos os seus cidadãos uma melhor qualidade de vida. nestas cidades inteligentes encontramos múltiplos cená rios aplicacionais que podem ser explorados de forma a alcançar os objectivos necessários. um exemplo destes cenários são os sistemas energéticos que, dada a evolução constante, sofrem uma grande sobrecarga. consequentemente, é necessário recorrer a certas medidas para aumentar a eficiência destes. uma solução é a implementação de sistemas de análise de consumo de energia e de previsão de dados. os dados são extraídos de fontes bastante diversas (sensores, câmaras e outros), dados estes que, posteriormente, são processados e exportados para data warehouses. contudo, com a evolução tecnológica que se tem vindo a verificar, a quantidade de dados oportunos aumentou significativamente, bem como as características relativas à forma de como são recolhidos e tratados. hoje em dia, a diver sidade destes dados é intensa, dependendo muito das circunstâncias operacionais e dos sistemas envolvidos, o que gera vulgarmente cenários aplicacionais novos e estranhos para os sistemas que usualmente estão envolvidos no seu tratamento. neste sentido, existe a necessidade de inovar os processos de tratamento destes dados e, deste modo, aumentar a operacionalidade e possibilidade de suportar uma cidade inteligente. para tal, é necessário implementar processos de etl e de previsão de dados para haver a capacidade de toma das de decisões de forma a manter uma cidade inteligente. abordamos, nesta dissertação, os sistemas de análise de consumo de energia, visto serem um dos cenários aplicacionais mais explorados com a evolução tecnológica. havendo a necessidade de aumentar a efi ciência desta área identificamos, planeamos e testamos algumas das medidas passíveis de implementar para a previsão de dados futuros que permitam ajudar à tomada de decisão. alcançando, por fim, a seleção de um algoritmo bastante preciso para a previsão destes dados.",
      "smart cities is a concept that isn’t very well established yet. however, there is a common consensus that it is about providing all its citizens with a better quality of life. in these smart cities we find multiple application scenarios that can be explored in order to achieve the necessary goals. one example of such scenarios are the energy systems which, given the constant evolution, are under great strain. consequently, certain measures need to be taken to increase their efficiency. one solution is the implementation of energy consump tion analysis systems and data forecasting. the data is extracted from quite diverse sources (sensors, cameras and others), which are then processed and exported to data warehouses. however, with technological developments, the amount of opportune data has increased significantly, as well as the characteristics regarding the way it is collected and processed. today, the diversity of this data is intense, depending a lot on the operational circumstances and the systems involved, which commonly generates new and strange application scena rios for the systems that are usually involved in their treatment. in this sense, there is a need to innovate the procedures for processing this data and thus increase the operability and possibility of supporting a smart city. for this, it is necessary to implement etl and data forecasting processes in such a way as to have the capacity to make decisions in order to maintain a smart city. in this dissertation, we address energy consumption analysis sys tems, since they are one of the most explored application scenarios with the technological evolution. there is a need to increase efficiency in this area and so we identify, plan and test some of the measures that can be implemented to forecast future data to help decision making. at last, we achieve the selection of a very precise algorithm for the prediction of this data."
    ],
    [
      "as redes sociais online mudaram para sempre a forma como as pessoas comunicam, partilham experiências e vivem umas com as outras. na área dos sistemas ubíquos, a comunidade tem demonstrado interesse em trazer para espaços públicos inteligentes componentes de interação social que advêm das redes sociais online. estas integrações, onde omundo social virtual se cruza com um mundo de interações sociais físicas e localizadas têm motivações óbvias, pelo impacto positivo que podem ter em melhorar as interações sociais localizadas, no entanto, introduzem várias questões que devem ser estudadas. porque as interações sociais que ocorrem num mundo virtual são necessariamente diferentes das que ocorrem no mundo físico real, ainda não é evidente a melhor forma de introduzir componentes de interação social online num espaço com ecrãs públicos. este estudo tem como objectivo avaliar as hipóteses de integração de redes sociais em espaços com ecrãs públicos em três áreas, a capacidade técnica dos mecanismos de integração, o mapeamento de dados e entidades das redes sociais em ecrãs públicos e a adaptação de processos existentes nas redes sociais nos ecrãs públicos. partindo das motivações, necessidades e possibilidades de integração são definidos uma série de casos de uso representativos, expostos modelos de integração, assim como são implementadas aplicações no âmbito desses casos de uso. desta forma espera-se ter uma noção clara e genérica das possibilidades de integração de componentes sociais virtuais em espaços com ecrãs públicos.",
      "social networking sites have forever changed the way people communicate, share experiences and live with each other. in the ubiquitous systems field, the scientific community has shown interest in bring components from social networks into places with public displays. this integration, where a social virtual world bridges with social interactions occurring in the physical world, has obvious motivations, because of the positive impact it can have enhancing localized social interactions, however, it introduces issues that needs to be addressed. because a social experience occurring in an online world is being brought into a physical and public space, it is still not quite understandable the impact for people of such data and experience enabled in public displays. this study aims to evaluate the integration possibilities between social networks and public displays in three domains, the technical capability of the integration mechanisms, the mapping between social data and entities from social networks in public displays and the adaptation of social network processes in public displays. beginning with the motivation, needs and integration possibilities, it is defined representative use cases, exposed integration models, as also it are implemented applications based on these use cases. in this way it is expected to have a clear and generic notion about the integration possibilities between social networks and public displays."
    ],
    0.0
  ],
  [
    [
      "metabolic reprogramming is recognized as a critical hallmark of cancer, influencing cancer initiation and progression. emerging evidence suggests that the metabolism of non-cancer cells within the tumor microenvironment plays a pivotal role in modulating tumor development, underscoring the importance of metabolic variables for better understanding cancer. the main goal of this study is to identify genes exhibiting differential expression in cancer, with a specific emphasis on distinguishing between organs with high metabolic rates (brain, liver, and kidneys) and organs with low metabolic rates (bladder, colon, and skin), particularly focusing on genes encoding mitochondrial proteins. for this, we used two databases containing rna-seq samples from normal and cancer tissues, obtained from the genotype-tissue expression (gtex) and the cancer genome atlas (tcga) projects, respectively. general linear models (glms) were applied for differential expression analysis, and hierarchical clustering e soft fuzzy clustering to identify distinct gene expression profiles. our research showed that many of the differentially expressed mitochondrial genes, such as acsm1 and acsm5, and prodh, represent potential adaptations of cancer cells to metabolic and micro environmental stress. additionally, fdx2, a crucial player in iron-sulfur protein biogenesis, and acsm2b, responsible for catalyzing the activation of free fatty acids (ffas) to coa, showed substantial expression differences, highlighting the importance of these two pathways for the oncogenic process. the most sub stantial genetic expression differences were observed between normal and cancer tissues, rather than between high and low metabolic rate organs, suggesting that the signal from the metabolic rate could be masked by the pronounced changes that cancer induces in cells. despite the unequal sample sizes and the usage of two different data sources, our findings provide valuable insights into the complex interplay between metabolism and gene expression in cancer.",
      "a reprogramação metabólica é reconhecida como um hallmark do cancro, influenciando a sua iniciação e progressão. estudos recentes mostram que o metabolismo das células não cancerígenas desempenha um papel crucial no microambiente tumoral e na modulação do seu desenvolvimento; demonstrando a importância do metabolismo neste processo. neste estudo identificaram-se genes mitocondriais que exibem expressão diferencial em cancro, com particular ênfase na distinção entre órgãos com elevada taxa metabólicas (cérebro, fígado e rins) e órgãos com baixa taxa metabólica (bexiga, cólon e pele). para tal, foram utilizados dados de rna-seq provenientes de duas bases de dados: genotype-tissue expression (gtex) e the cancer genome atlas (tcga), contendo amostras de tecidos normais e cancerígenos, respetivamente. os genes diferencialmente expressos foram obtidos através de uma análise de expressão diferencial usando general linear models (glms), e os perfis de expressão foram obtidos por hierarchical clustering e soft fuzzy clustering. os resultados demonstraram que muitos dos genes mitocondriais diferencialmente expressos, tais como acsm1 e acsm5, e prodh, poderão representar potenciais adaptações das células cancerígenas ao stress metabólico e microambiental. adicionalmente, a fdx2, uma proteína crucial para a biogénese de proteínas ferro-enxofre, e a acsm2b, responsável pela ativação de ácidos gordos livres (ffas) transformando-os em coa, mostraram diferenças significativas de expressão, demonstrando a importância destes dois processos na carcinogénese. as diferenças de expressão entre tecidos normais e cancerígenos mostraram ser mais acentuadas do que entre órgãos com taxas metabólicas alternativas, sugerindo que a magnitude do sinal gerado pelas diferenças moleculares produzidas pelo tipo de taxa metabólica poderá não ser suficiente para se sobrepor à magnitude do sinal provocado pelo cancro. apesar do tamanho diferente das amostras, e da utilização de duas bases de dados diferentes, estes resultados contribuem para elucidar a complexa relação entre metabolismo e expressão genética em cancro."
    ],
    [
      "existem hoje em dia diversas soluções de business intelligence no mercado que permitem a análise de informação de forma intuitiva, permitindo o acesso a utilizadores de negócio das mais diversas áreas. estas soluções vieram assim tornar o processo de análise de informação ágil, permitindo que esteja presente em mais processos de negócio, executados por utilizadores não especialistas. estes processos de negócio exigem por vezes a manipulação de dados manualmente. a tarefa de manipulação manual de dados provenientes de diversas fontes é um processo complexo, onde há a necessidade de proceder à implementação de queries, pipelines customizados, entre outras tarefas específicas. estas tarefas estão assim fora do alcance de profissionais sem conhecimento técnico de programação e desenvolvimento de software. existe assim a necessidade de construção de uma ferramenta que permita a interação e manipulação de dados sem conhecimento técnico que seja transversal a diferentes contextos com a possibilidade destes também terem diferenças ao nível de tecnologias de base de dados. o tableau, ferramenta de visualização e interação com dados provenientes de diversas fontes, anunciou que iria disponibilizar uma api para aceder e interagir com a informação presente nos dashboards, possibilitando assim o desenvolvimento de software por terceiros. por conseguinte, surgiu então a oportunidade de criar um produto que no formato de plugin tivesse como objetivo preencher a lacuna de manipular e interagir com os dados presentes numa base de dados sem estar constrangido a um cenário ou tecnologia específica. desta maneira conseguimos expandir o conjunto de ações disponíveis aos utilizadores para além das atuais que estão de momento restritas apenas à visualização e interação com dados provenientes de diversas fontes, sem qualquer possibilidade de alteração e preservação de nova informação. este documento relata o plano de trabalhos sobre a manipulação de dados e estruturas, em múltiplos contextos com a possibilidade de alteração de tecnologia de base de dados. tendo isto em conta, está estipulado o desenvolvimento de um produto de um motor dinâmico de manipulação de queries dinâmico, que permita a interação por parte dos utilizadores com diferentes contextos e que secundariamente seja de fácil integração em diferentes ferramentas de visualização através de interfaces gráficas.",
      "there are now several business intelligence solutions on the market that allow information analysis in an intuitive way, allowing access to business users from the most diverse areas. these solutions have thus made the information analysis process agile, allowing it to be present in more business processes, performed by non-specialist users. these business processes sometimes require the manipulation of data manually. the task of manually manipulating data from different sources is a complex process, where there is a need to implement queries, customized pipelines, among other technical tasks. these tasks are therefore out of reach for professionals without technical knowledge of programming and software development. thus, there is a need in the market for a tool that allows the interaction and manipulation of data without technical knowledge that is transversal to different contexts with the possibility of these also having differences in terms of database technologies. tableau, a tool for visualization and interaction with data from different sources, announced that it would provide an api to access and interact with the information present in the dashboards, thus enabling the development of software by third parties. therefore, the opportunity arose then to create a product that, in the format of a plugin, aimed to fill the gap of manipulating and interacting with the data present in a database without being constrained by a scenario or technology. in this way, we were able to expand the set of actions available to users in addition to the current ones, which are currently restricted only to viewing and interacting with data from different sources, without any possibility of altering and preserving new information. this document reports the work plan on the manipulation of data and structures, in multiple contexts with the possibility of changing database technology. taking this into account, it is stipulated the development of a product for a dynamic query handling engine, which allows users to interact with different contexts and that secondarily is easy to integrate into different visualization tools through graphical interfaces."
    ],
    0.06666666666666667
  ],
  [
    [
      "in the last few years, data management engines have become increasingly modular, separating some of its main layers, such as data storage and transactional management. the exposure of the transactional manage ment component brings new challenges, in particular its correct configuration and tuning when running different workloads. in this sense, this dissertation focuses on the autonomous optimization of a particular transactional middleware, ph1, while keeping in mind the tuning of other similar systems. it is becoming more and more important to develop algorithms that can automatically optimize these systems whose performance is heavily dependent on a proper configuration. the use of machine learning techniques for similar problems (database knob tuning) has become common in the literature [1, 2, 3, 4], especially in a black box perspective where it does not have visibility over particular details of the system. usually, these systems are located in realistic online environments, where workloads can change at different times. even though there are numerous research projects for automatic knob tuning, these projects have not entirely addressed this problem and are mostly developed for offline training when the workloads remain static. we propose opal as the component that when executing transactional workloads is able to dynamically adjust its configurations in an online environment with a continuous space. our approach allows for online changes and uses reinforcement learning as a starting point taking into consideration tuning algorithms in continuous spaces, as is the case of ddpg [5].",
      "nos últimos anos, os motores de gestão de dados têm-se tornado cada vez mais modulares, separando algumas de suas camadas, principais, como o armazenamento de dados e a gestão transacional. a exposição do componente de gestão transacional traz novos desafios, em particular a sua correta configuração e ajuste durante a execução de diferentes cargas de trabalho. neste sentido, esta dissertação foca-se na otimização autónoma de um middleware transacional específico, ph1, tendo em mente o ajuste de outros sistemas semelhantes. é cada vez mais importante desenvolver algoritmos que possam ajustar automaticamente estes sistemas cujo desempenho depende muito de uma configuração adequada. a utilização de técnicas de machine learning para problemas semelhantes (ajuste de parâmetros em bases de dados) tem-se tornado comum na literatura [1, 2, 3, 4], sobretudo recorrendo a uma visão black-box onde não se tem visibilidade sobre detalhes particulares do sistema. normalmente, estes sistemas encontram-se em ambientes online realistas, onde as cargas de trabalho podem mudar em momentos diferentes. apesar de existirem inúmeros projetos de pesquisa para o ajuste automático de parâmetros, estes projetos não abordam totalmente este problema e são desenvolvidos principal mente para treino offline quando as cargas de trabalho permanecem estáticas. opal é proposto como o componente que ao executar cargas de trabalho transacionais é capaz de ajustar dinamicamente as suas configurações num ambiente online com um espaço contínuo. a nossa abordagem permite mudanças online e utiliza reinforcement learning como ponto de partida tendo em consideração algoritmos para ajuste de parâmetros em espaços contínuos, como é o caso do ddpg [5]."
    ],
    [
      "o objetivo deste estudo é avaliar e comparar o desempenho de dois indicadores de previsão de risco de mortalidade neonatal para recém-nascidos de muito baixo peso (<1500g), o crib (clinical risk index for babies) e o snappe ii (score for neonatal acute physiology-perinatal extension ii), com recurso à metodologia roc (receiver operating characteristic). a execução prática deste estudo foi suportada com auxílio a programas estatísticos próprios para a análise da metodologia roc, como o spss, rocnpa, comp2roc, rocr e catools. os dados que contemplam o presente estudo foram recolhidos pelas unidades de cuidados intensivos neonatais do território português entre 2010 e 2012, e enviados para o registo nacional de recém-nascidos de muito baixo peso (rnmbp), que é a entidade responsável pelo armazenamento desta informação. será aferida também a comparação e avaliação de variáveis de elevada expressão na previsão da mortalidade, que compõem os indicadores de mortalidade em estudo, sendo elas, o peso à nascença e a idade gestacional. a amostra em estudo é composta por 789 recém-nascidos de muito baixo peso, dos quais 51,3% são do género masculino. em média os recém-nascidos em questão apresentam um peso médio ao nascimento de 1214 g ±343,1 e 29,8 ±2,5 semanas de gestação e, dos integrantes na amostra 11,3% foram declarados óbitos hospitalares. a exatidão dos indicadores de mortalidade e das variáveis foi obtida através do cálculo da auc, área abaixo da curvaroc,queparaocribfoide0,876±0,025,paraosnappe-iide0,867±0,026,seguindo-sedasvariáveisidade gestacional e o peso ao nascimento com 0,785 ±0,032 e 0,782 ±0,028, respetivamente. com base nos resultados obtidos durante a elaboração do presente estudo, o crib provou ser melhor em pre dizer a mortalidade para recém-nascidos de muito baixo peso, e tem a seu favor um menor número de variáveis comparativamente ao snappe-ii.",
      "the aim of this study is to evaluate and compare the performance of two scores of neonatal mortality risk prediction in very low birth weight infants (<1500g), crib (clinical risk index for babies) and the snappe-ii (score for neonatal acute physiology perinatal extension-ii), using the roc (receiver operating characteristic) methodology. the execution of this study was supported with statistical programs for the analysis of roc methodology, such as spss, rocnpa, comp2roc, rocr and catools. the dataset used in present work was collected by the neonatal intensive care units of the portuguese territory between 2010 and 2012, and sent to the registo nacional de recém-nascidos de muito baixo peso (rnmbp), orga nization responsible by the storage of this information. itwillbealsoassessthecomparisonandevaluationofhighexpressionvariablesinthemortalityprediction,which are part of the risk mortality scores in question, that are, the birth weight and gestational age. the dataset consists in 789 very low birth weight infants, of which 51.3% are males. the newborns in study have an average birth weight of 1214 g ±343.1g and an average gestational age of 29.8 ±2.5 weeks, and 11.3% of selected newborns were declared hospital deaths. theaccuracyoftheriskmortalityscoresandvariableswasobtainedbycalculatingtheareaundertheroccurve (auc), for the crib was 0.876 ±0.025 and 0.867 snappe-ii ±0.026 following gestational age and birth weight with 0.785 ±0.032 and 0.782 ±0.028, respectively. based on the results obtained during the elaboration of this study, crib proved be better to predict mortality in very low birth weight infants, and has a minor number of variables compared to snappe-ii."
    ],
    0.3
  ],
  [
    [
      "a administração pública portuguesa tem desmaterializado processos e tem promovido a adoção de sistemas de gestão documental eletrónica bem como a digitalização de documentos destinados a serem arquivados. estas medidas pretendem atingir a otimização de processos, a modernização de procedimentos administrativos e a redução de papel. com o propósito de atingir estes objetivos e simplificar a gestão documental na administração pública, a classificação e avaliação da informação pública (clav) nasce como uma das medidas. a clav tem como finalidade a classificação e a avaliação da informação pública por forma a auxiliar os sistemas de informação das entidades públicas alertando-as quando determinado documento deve ser arquivado ou eliminado. para tal esta possui um referencial comum, a lista consolidada, com as funções e processos de negócio das entidades públicas associadas a um catálogo de legislação e de organismos. nos últimos dois anos, a clav tem vindo a ser desenvolvida no departamento de informática da universidade do minho em estreita colaboração com a equipa de investigação da área na direção-geral do livro, dos arquivos e das bibliotecas. à data de início deste trabalho, a clav era constituída por dois servidores de bases de dados que tinham como interlocutor o servidor da api de dados da clav. era com este servidor da api de dados que toda a interação com o exterior passava: acesso de aplicações de terceiras partes e acessos da interface cliente desenvolvida para a clav. nesta dissertação, o grande objetivo era fazer evoluir a arquitetura aplicacional dando resposta a uma série de requisitos e tentando simplificar ao máximo o processo da sua manutenção futura. nesse sentido, especificou-se e implementou-se um serviço para a proteção da api de dados da clav, especificou-se a documentação desta api de dados, definiram-se os formatos de exportação e implementaram-se os exportadores desta api por forma a permitir uma maior interoperabilidade dos dados, implementou-se a autenticação com a chave móvel digital recorrendo ao autenticação.gov, criaram-se os mecanismos necessários à migração de http para https e, por fim, adicionou-se uma api gateway na clav por forma a simplificar o funcionamento e gestão da plataforma. todos estes desenvolvimentos estão em produção e podem ser observados acedendo ao sítio web oficial da clav: https://clav.dglab.gov.pt",
      "the portuguese public administration has dematerialized processes and promoted the adoption of electronic document management systems as well as the scanning of documents intended to be archived. this measures aim to optimize and modernize administrative procedures and reduce paper usage. in order to achieve these objectives and simplify the document management in public administration, clav was born as one of the measures. clav’s main purpose is the classification and evaluation of the public information in order to help the information systems of public entities, alerting them when certain documents must be filed or deleted. to this end, a common reference, called the consolidated list (lista consolidada), is used, with the business functions and processes of public entities associated with a catalogue of legislation and entities. in the last two years, clav have been develop in computing department of um in strict collaboration with the area investigation team at direção-geral do livro, dos arquivos e das bibliotecas. at start date of this work, clav was constituted by two database servers that had as interlocutor the data api server of clav. was with this data api server that all exterior interaction passed: access from third party applications and access from client interface developed for clav. in this dissertation, the big goal was make evolve the application architecture giving answer to a series of requirements and trying to simplify to maximum the process of it future maintenance. in this sense, a protection service for data api of clav was specified and developed, the data api documentation was specified, the exportation formats were defined and the api exporters were developed in order to allow a bigger data interoperability, the authentication with chave móvel digital using the autenticação.gov was developed, the necessary mechanisms of http to https migration were created and, lastly, an api gateway on clav was added in order to simplify the operation and management of the platform. all these developments are in production and can be observed accessing to the official web page of clav: https://clav.dglab.gov.pt"
    ],
    [
      "this master dissertation addresses the problem of spreadsheet errors by using typed linear algebra in spreadsheet design. the study builds on previous efforts to solve this issue and presents an approach to improve the quality and reliability of spreadsheet systems. the outcome of this study shows that the adoption of a typed linear algebra approach in spreadsheet design can significantly reduce the risk of errors and improve the reliability of spreadsheet-based systems. the tool developed in this dissertation allows users to derive spreadsheet models in haskell from formal specifications, which are then translated into a particular spreadsheet format. this process helps to ensure the accuracy and consistency of the generated spreadsheets, as it is based on precise and well typed specifications. additionally, the use of typed linear algebra in the semantics of spreadsheet functions and constructions such as e.g. running totals provides a solid foundation for the correctness. overall, the results of this study demonstrate the effectiveness of the typed linear algebra approach in improving the quality and reliability of spreadsheet systems.",
      "esta dissertação de mestrado aborda o problema de erros em folhas de cálculo, utilizando álgebra linear tipada no design de folhas de cálculo. o estudo baseia-se em esforços anteriores para resolver esta questão e apresenta uma abordagem para melhorar a qualidade e a confiabilidade de sistemas de folha de cálculo. os resultados deste estudo mostram que a introdução de uma abordagem de álgebra linear tipada no design de folhas de cálculo pode reduzir significativamente o risco de erros e melhorar a confiabilidade dos sistemas de folhas de cálculo. a ferramenta desenvolvida nesta dissertação permite que os utilizadores criem especificações em haskell de folhas de cálculo, que são então traduzidas para uma folha de cálculo correspondente. este processo ajuda a garantir a precisão e a consistência da folha de cálculo gerada, pois ela é baseada numa especificação precisa e bem definida. além disso, o uso de álgebra linear tipada na semântica de funções de folhas de cálculo e construções, como totais acumulados, proporciona uma base sólida para a correção destes elementos. em geral, os resultados deste estudo demonstram a eficácia da abordagem de álgebra linear tipada na melhoria da qualidade e confiabilidade dos sistemas de folhas de cálculo."
    ],
    0.3
  ],
  [
    [
      "software’s structure profoundly affects its development and maintenance costs. poor software’s structure may lead to well-known design flaws, such as large modules or long methods. a possible approach to reduce a module’s complexity is the extract method refactoring technique. this technique allows the decomposition of a large and complex method into smaller and simpler ones, while reducing the original method’s size and improving its readability and comprehension. nowadays, it’s almost mandatory that integrated development environments (ides) support this and other refactoring techniques. despite the wide availability of the extract method operation on ides, the identification of portions of code that are worthwhile to refactor still relies mostly on developer knowledge and expertise. thus, the purpose of this dissertation is to empower the outsystems platform with a system that is able to analyse modules complexity and automatically suggest extract method refactoring opportunities.",
      "a estrutura das aplicações de software afeta significativamente os seus custos de desenvolvimento e de manutenção. uma fraca estruturação do código pode levar a problemas de design, como é o exemplo de módulos de grande dimensão, ou long methods. uma das abordagens para reduzir a complexidade de módulos é a aplicação da técnica de refatorização extract method. esta técnica permite extrair comportamento de um método complexo em métodos mais simples, reduzindo o tamanho do método original e aprimorando a sua leitura e compreensão. hoje em dia, é quase imperativo que os ides suportem esta e outras técnicas de refatorização. apesar dos ides disponibilizarem a operação de extração de métodos, a identificação de secções do código potencialmente úteis para refatorizar continua a ser uma operação que depende do conhecimento e experiência do programador. assim, o objetivo desta dissertação é prover a plataforma outsystems de um sistema capaz de analisar a complexidade de módulos e automaticamente sugerir oportunidades de refatorização extract method."
    ],
    [
      "a implementação da interoperabilidade nos sistemas de informação hospitalar (sih) é cada vez mais um requisito e não uma opção. a agência para a integração, difusão e arquivo de informação médica e clínica (aida) consiste numa plataforma de interoperabilidade hospitalar desenvolvida por investigadores da universidade do minho e que se encontra instalada no centro hospitalar do porto (chp). a aida assegura a interoperabilidade entre os sih e para alémdisto, assegura também a confidencialidade, integridade e disponibilidade dos dados. a aida deve possuir um elevado nível de disponibilidade e um funcionamento eficiente 24 horas por dia. um pequeno período de paragem poderá trazer graves consequências para a qualidade dos serviços prestados. esta plataforma possui mecanismos de recuperação e tolerância de falhas, contudo devido à sua elevada importância, é preciso agir antes da ocorrência das falhas, evitando sérios danos. os processos de monitorização e prevenção de falhas devem ser implementados nos “órgãos vitais” da aida, que são as base de dados, máquinas e agentes inteligentes. uma vez que a prevenção de falhas em base de dados da aida já ter sido alvo de estudo, esta dissertação aborda a monitorização e prevenção de falhas nas máquinas e agentes. para prever as falhas, foram criados modelos baseados no modified early warning score (mews). este modelo através da recolha frequente dos valores dos sinais vitais, calcula um conjunto de scores para determinar o nível de risco a que o paciente está submetido. foram desenvolvidos sistemas de monitorização de prevenção para as máquinas e agentes que permitem não só prevenir falhas, mas também observar e avaliar o comportamento destes componentes através de dashboards de monitorização. a prevenção de falhas nos agentes foi baseada na frequência com que estes registam as suas atividades nos seus ficheiros log, enquanto que para as máquinas a prevenção foi baseada em indicadores de desempenho como a memória e o cpu. apurou-se que os componentes, em geral, encontram-se com os seus principais recursos bem balanceados e que os sistemas de prevenção desenvolvidos detetaram situações críticas com sucesso, contribuindo para um aumento da integridade e disponibilidade da aida do chp.",
      "implementing interoperability in health information systems (his) it is increasingly a requirement rather than an option. the agency for integration, diffusion and archive of medical information (aida) is an interoperability healthcare platform developed by researchers at the minho university and it is installed in the centro hospitalar do porto (chp). aida ensures the interoperability among his and besides this, it also ensures the confidentiality, integrity and availability of data. aida should have a high level of availability and an efficient operation 24 hours a day. a small stop period will be able to bring serious consequences for the quality of services provided. this platform has mechanisms for fault tolerance and recovery, however due to its high importance, it is needed to act before the occurrence of faults, avoiding serious damage. the processes of monitoring and failure prevention should be implemented in the aida’s “vital organs”, which are the databases, machines and intelligent agents. once the prevention of failures in the aida’s database has already been studied, this dissertation addresses monitoring and preventing failures in machines and agents. to forecast faults, models were created based on modified early warning score (mews). this model through the frequent collection of values of vital signs, calculates a set of scores to determine the level of risk to which the patient is subjected. monitoring and prevention systems were developed for machines and agents, which allow not only prevent faults, but also watch and evaluate the behaviour of these components through monitoring dashboards. the prevention of failures in agents was based on the frequency that they record their activities in their log files, while for the machines prevention was based on performance indicators such as memory and cpu. it was found that components, in general, are with their main resources well balanced and the prevention systems developed detected critical situations successfully, contributing to an increase in the integrity and availability of aida in chp."
    ],
    0.0
  ],
  [
    [
      "o erro humano está frequentemente associado ao calculo manual de dosagem de medicamentos, sendo a sua incidência maior na população pediátrica do que na população adulta. desta forma, esta dissertação tem como objetivo a finalização e implementação de uma plataforma de apoio à decisão médica de modo a reduzir esse erro. a codificação de episódios de internamento é de extrema importância para a gestão financeira hospitalar, uma vez que esta é utilizada para o calculo de financiamento hospitalar através da utilização de gdh. neste sentido, foi desenvolvida uma plataforma de apoio à gestão financeira que permite auxiliar na codificação de episódios de internamento de qualquer unidade da instituição de saúde, mesmo das unidades de neonatologia e pediatria. o desenvolvimento de ambas as plataformas foi acompanhado por um médico responsável, tendo sido testadas e realizadas as devidas alterações. encontrandose a primeira na fase final de testes e a segunda em funcionamento em ambiente hospitalar. por fim foram lançados dois questionários direcionados à avaliação de cada plataforma.",
      "human error is often associated with the calculation of dosage of drugs, and its highest incidence in the pediatric population than in adults. thus, this work aims to reduce this error, by completed and implement medical decision support platform to support. the codification of inpatient episodes is extremely important for the hospital financial management, since it is used to calculate the hospital financing using gdh. therefore, a support platform was developed to financial management that allows assist the inpatient episodes codification of any unit of the health institution, even the neonatology and pediatrics units. the development of both platforms was accompanied by a responsible physician, having been tested and made the appropriate changes. finding the first in the final testing phase and the second in operation in hospital. finally it was launched two questionnaires to evaluate each platform."
    ],
    [
      "currently, most computing systems have access to more than one type of processing unit, typically a multicore cpu device and a computing accelerator, such as a gpu. however, the vast majority of the existing implementations of advanced path tracing algorithms only take advantage of one of these processing units. the implementation of these algorithms in such heterogeneous platforms while efficiently using both types of computing units already proved to provide improved performance results. this dissertation examines four path tracing algorithms (path tracing aka pt, bidirectional path tracing aka bpt, bidirectional photon mapping aka bpm and vertex connection and merging aka vcm) and extends previous work by exploring a richer heterogeneous environment with more gpu accelerators and with manycore x86 devices (i.e., xeon phi knights corner), complemented with an insight into the challenges introduced by each computing architecture and their programming environment. it also shows how these are combined together to perform heterogeneous computation managed by a simple scheduling algorithm, created to take advantage of each device’s features. this work proved that a fully heterogeneous approach to these four path tracing algorithms is feasible and the performance results are significantly improved.",
      "atualmente, muitos dos sistemas de computação conseguem tirar proveito de mais do que um tipo de processador (tipicamente o multicore e o gpu). contudo, a maioria das implementações de algoritmos de path tracing avançados aproveitam apenas um destes processadores. a implementação destes algoritmos de path tracing em plataformas heterogéneas tem resultados comprovados que se mostram mais eficientes. esta dissertação analisa quatro algoritmos de path tracing avançados: o path tracing (pt), o bidirectional path tracing (bpt), o bidirectional photon mapping (bpm) e o vertex connection and merging (vcm). expande também o trabalho previamente desenvolvido explorando um ambiente heterogéneo mais rico, com mais gpus e com manycores (i.e., xeon phi knights corner), e apresenta os desafio que estas arquiteturas e os seus ambientes de programação podem trazer. mostra ainda como estas contribuem em conjunto para o mesmo sistema heterogéneo com um simples algoritmo de escalonamento, implementado para tirar partido do melhor de cada arquitetura. no final mostra-se que uma abordagem heterogénea para estes quatro algoritmos de path tracing avançado consegue ser viável e ainda trazer ganhos significativos de performance."
    ],
    0.021428571428571425
  ],
  [
    [
      "through the exponential growth of the internet service usage comes the inevitability of the creation of a tool that can both be useful and versatile in the management of all of the created traffic volume. from this necessity arises the concept of software-defined networking (sdn), that aims to offer a set of protocols and technologies capable of easing the management and the efficiency of maintenance of the various network infrastructures that require its usage. this work aims for the initial comparison of the multiple capacities of several sdn controllers existent in the market, specifying their programming languages and characteristics. after such comparison follows the selection of one of the sdn controllers, in order for a prototype of a traffic forwarding solution to be developed, as to get the most out of the chosen controller’s characteristics and of the global sdn approaches. that same developed solution should allow for multiple characteristics, highlighting the possibility of its capacity to deal with traffic forwarding under specific parameters. initially, the solution should incorporate the dijkstra algorithm to calculate the shortest paths and inject its results into the network, as it also should immediately converge after link failure. then, it should also be conducive to events that occur in real time, it should allow for link or router protection in the infrastructure, effectively react to link failure, react to different load levels in the network, flow traffic forwarding through specific routes, topology multiplexing through different virtual networks, among others. that way, the developed prototype may come up as an alternative sdn approach to certain well-known interior routing protocols, such as routing information protocol (rip) and open shortest path first (ospf).",
      "através do crescimento exponencial da utilização dos serviços internei, advém a inevita-bilidade da criação de uma ferramenta que seja simultaneamente útil e versátil na gestão de todo o volume de tráfego criado. é desta necessidade que surge o conceito de software-defined networking (sdn), que visa oferecer um conjunto de protocolos e tecnologias, capazes de facilitar a gestão e a eficácia de manutenção das demais infraestruturas de rede que procurem a sua utilização. este trabalho visa a comparação inicial das várias capacidades dos demais controlado-res sdn existentes no mercado, enunciando as suas linguagens de programação e carac-terísticas. após tal comparação, será feita a escolha de um dos controladores sdn de forma a que seja desenvolvido um protótipo de uma solução de encaminhamento de tráfego, tirando o máximo de proveito das características do controlador e das abordagens sdn. a solução desenvolvida deverá permitir uma plenitude de características, destacando-se a possibilidade de poder estar vocacionado para lidar com encaminhamento de tráfego consoante vários parâmetros. inicialmente deverá incorporar o algoritmo de dijkstra para cálculo de caminhos mais curtos e injeção de rotas, assim como deverá convergir de imedi-ato após falhas de links. seguidamente, deverá também ser propício a eventos que ocorram em tempo real, permitir a proteção de links ou routers da infraestrutura, reação eficaz após falhas de links, reação consoante diversos níveis de carga na rede, encaminhamento de fluxos de tráfego através de rotas específicas, multiplexagem da topologia por diferentes redes virtuais, entre outros. deste modo, o protótipo desenvolvido surge, assim, como uma abordagem sdn alternativa a certos protocolos de encaminhamento interno conhecidos, tais como o routing information protocol (rip) e open shortest path first (ospf)."
    ],
    [
      "once medical images were scanned and uploaded to a computer, researchers began to create automated medical imaging systems. from the 1970’s to the 1990’s, medical imaging was performed with sequential application of low-level pixel processing and mathematical modeling to solve specific tasks such as organ segmentation. at the end of the 1990’s, supervised techniques began to appear, where data extracted from the images were used to train models and classification systems. one example is the use of automated classifiers to build support systems for cancer detection and diagnosis. this pattern recognition and / or machine learning approach is still very popular and represented a shift from systems that were completely human-engineered to computer-trained systems with the use of specific (manually drawn) features and automatically extracted from the training data (example). the following step would be enabling the algorithms to directly learn characteristics of the pixels of the images. this is the basic concept of deep learning algorithms: multi-layered models that transform input data (images) into outputs (e.g. the presence or absence of pathological lesions or cancer). this study intends to present ways of using deep learning algorithms in the analysis of medical images, like the particular case of pathological lesions representative of breast cancer phenotypes.",
      "assim que foi possível digitalizar e carregar imagens médicas num computador, os investigadores começaram a criar sistemas automatizados para análise de imagens médicas. no intervalo dos anos 70 até aos anos 90, a análise de imagens médicas foi feita com a aplicação sequencial de processamento de pixeis de baixo nível e modelação matemática para resolver tarefas específicas como, por exemplo, a segmentação de órgãos. no final dos anos 90, começam a aparecer as técnicas supervisionadas, onde os dados extraídos das imagens são usados para treinar modelos e sistemas de classificação. um exemplo é o uso de classificadores automáticos para construir sistemas de apoio à deteção e diagnóstico do cancro. esta abordagem de reconhecimento de padrões e/ou machine learning ainda é muito popular e representou uma mudança nos sistemas que eram completamente projetados por seres humanos para sistemas treinados por computadores com recurso ao uso de características específicas (manualmente desenhadas) e extraídas automaticamente dos dados de treino (exemplo). o passo seguinte a alcançar é que os algoritmos aprendam directamente as características dos pixeis das imagens. é este o conceito base dos algoritmos de deep learning: modelos (redes) compostos por muitas camadas que transformam dados de entrada (imagens) em saídas (por exemplo, a presença ou ausência de lesões patológicas ou cancro). pretende-se com este estudo apresentar formas de usar algoritmos de deep learning na análise de imagens médicas, em particular para a classificação de lesões patológicas representativas de fenotipos de cancro da mama."
    ],
    0.3
  ],
  [
    [
      "the 2d convection-diffusion is a well-known problem in scientific simulation that often uses a direct method to solve a system of n linear equations, which requires n3 operations. this problem can be solved using a more efficient computational method, known as the alternating direction implicit (adi). it solves a system of n linear equations in 2n times with n operations each, implemented in two steps, one to solve row by row, the other column by column. each n operation is fully independent in each step, which opens an opportunity to an embarrassingly parallel solution. this method also explores the way matrices are stored in computer memory, either in row-major or column-major, by splitting each iteration in two. the major bottleneck of this method is solving the system of linear equations. these systems of linear equations can be described as tridiagonal matrices since the elements are always stored on the three main diagonals of the matrices. algorithms tailored for tridiagonal matrices, can significantly improve the performance. these can be sequential (i.e. the thomas algorithm) or parallel (i.e. the cyclic reduction cr, and the parallel cyclic reduction pcr). current vector extensions in conventional scalar processing units, such as x86-64 and arm devices, require the vector elements to be in contiguous memory locations to avoid performance penalties. to overcome these limitations in dot products several approaches are proposed and evaluated in this work, both in general-purpose processing units and in specific accelerators, namely nvidia gpus. profiling the code execution on a server based on x86-64 devices showed that the adi method needs a combination of cpu computation power and memory transfer speed. this is best showed on a server based on the intel manycore device, knl, where the algorithm scales until the memory bandwidth is no longer enough to feed all 64 computing cores. a dual-socket server based on 16-core xeon skylakes, with avx-512 vector support, proved to be a better choice: the algorithm executes in less time and scales better. the introduction of gpu computing to further improve the execution performance (and also using other optimisation techniques, namely a different thread scheme and shared memory to speed up the process) showed better results for larger grid sizes (above 32ki x 32ki). the cuda development environment also showed a better performance than using opencl, in most cases. the largest difference was using a hybrid cr-pcr, where the opencl code displayed a major performance improvement when compared to cuda. but even with this speedup, the better average time for the adi method on all tested configurations on a nvidia gpu was using cuda on an available updated gpu (with a pascal architecture) and the cr as the auxiliary method.",
      "o problema da convecção-difusão é utilizado em simulaçãos cientificas que regularmente utilizam métodos diretos para solucionar um sistema de n equações lineares e necessitam de n3 operações. o problema pode ser resolvido utilizando um método computacionalmente mais eficiente para resolver um sistema de n equações lineares com n operações cada, implementado em dois passos, um solucionando linha a linha e outro solucionando coluna a coluna. cada par de n operações são independentes em cada passo, havendo assim uma oportunidade de utilizar uma solução em baraçosamente paralela. este método também explora o modo de guardar as matrizes na memória do computados, sendo esta por linhas ou em colunas, dividindo cada iteração em duas, este método é conhecido como o método de direção alternada. o maior bottleneck deste problema é a resolução dos sistemas de equações lineares criados pelo adi. estes sistemas podem ser descritos como matrizes tridiagonais, visto todos os seus elementos se encontrarem nas 3 diagonais interiores e a utilização de métodos estudados para este caso é necessário para conseguir atingir a melhor performance possível. esses métodos podem ser sequenciais (como o algoritmo de thomas) ou paralelos (como o cr e o pcr) as extensões vectoriais utilizadas nas atuais unidades de processamento, como dispositivos x86-64 e arm, necessitam que os elementos do vetor estejam em blocos de memória contíguos para não sofrer penalizações. algumas abordagens foram estudadas neste trabalho para as ultrapassar, tanto em processadores convencionais como em aceleradores de computação. os registos do tempo em servidores baseado em dispositivos x86-64 mostram que o adi necessitam de uma combinação de poder de processamento assim como velocidade de transferência de dados. isto é demonstrado especialmente no servidor baseado no dispositivo knl da intel, no qual o algoritmo escala até que a largura de banda deixe de ser suficiente para o problema. um servidor com dois sockets em que cada é composto por um dispositivo com 16 cores baseado na arquitetura xeon skylake, com acesso ao avx-512, mostrou ser a melhor escolha: o algoritmo faz as mesmas operações em menos tempo e escala melhor. com a introdução de computação com gpus para melhorar a performance do programa mostrou melhores resultados para problemas de maiores dimensões (tamanho acima de 32ki x 32ki celulas). o desenvolvimento em cuda também mostrou melhores resultados que em opencl na maioria dos casos. a maior divergência foi observada ao utilizar o método cr-pcr, onde o opencl mostrou melhor performance que em cuda. mas mesmo com este método sendo mais eficaz que o mesmo em cuda, o melhor performance com o método adi foi observado utilizando cuda no gpu mais recente estudado com o método cr."
    ],
    [
      "a visualização de relatórios de meios complementares de diagnóstico e terapêutica (mcdts) é importante para uma boa prática médica, no sentido em que estes fornecem indicações importantes sobre o estado de saúde do paciente. torna-se, então, crucial o desenvolvimento de uma aplicação que permita a médicos em centros de saúde aceder de uma forma segura à informação guardada numa base de dados hospitalar. essa necessidade foi encontrada no centro hospitalar do porto, organização para a qual foi implementado este projeto. com a presente dissertação, procurou-se, ainda, desenvolver uma análise de segurança ao sistema para que o trabalho nal fosse projetado e elaborado de acordo com as reais necessidades do hospital. o sistema é apresentado e analisado neste documento. foram, também, feitos testes à segurança do mesmo, sendo que não foi possível averiguar quanto à sua performance.",
      "complementary diagnostic and therapeutic methods (cdtnm) reports are important to a good medical practice as they provide crucial directions about the health's state of a patient. it is, therefore, critical to develop a software application that allows physicians in local health facilities to access information stored in a hospital's database in a secure way. this need was spotted in cento hospitalar do porto for which this project was developed. in this dissertation was implemented a security analysis so that the nal work was designed and developed according to the real needs of the hospital. the system is presented and analyzed in this document. there were also made safety tests to prove its strength. it was not possible to evaluate the system's performance."
    ],
    0.09999999999999999
  ],
  [
    [
      "users expect access to programs and business information anywhere in the simplest way possible using a device. with the diversification of devices, the standard is disappearing and we are going towards a more heterogeneous world of mobile devices. with this divergence increasing, it gets more difficult to update, support and control applications through all these new platforms. therefore it is important to facilitate these tasks. the solution to these problems lies on the mobile device management (mdm) programs that can control what devices install and configure, providing remote tasks and access. this dissertation aims not to compete with the current products on the market, but to propose a different way to distribute content to the devices registered on the platform using a rule system. this system will prioritize the newest rules by the device and its location characteristics. as so, providing a different way of grouping devices and distributing content to them.",
      "os utilizadores esperam acesso aos programas e informações corporativas em qualquer lugar da forma mais simples possível, utilizando um dispositivo. com a diversificação de dispositivos, o standard está a desaparecer e estamos a ir em direção a um mundo mais heterogéneo de dispositivos móveis. com esta crescente divergência, torna-se mais difícil de atualizar, dar suporte e controlar aplicações através de todas estas novas plataformas. é então importante que estas tarefas sejam facilitadas. a solução para estes problemas reside nos programas de mdm que podem controlar o que os dispositivos instalam e configuraram, proporcionando acesso e tarefas remotas. esta dissertação não pretende competir com os produtos existentes no mercado, mas para propor uma forma diferente de distribuir conteúdo para os dispositivos registados na plataforma através de um sistema de regras. este sistema vai priorizar as regras mais recentes por dispositivo e as características da sua localização. proporcionando uma forma diferente de agrupar dispositivos e distribuição de conteúdo para eles."
    ],
    [
      "a inteligência artificial (ia) e a ciência de dados estão cada vez mais presentes no nosso quotidiano e os benefícios que trouxeram para a sociedade nos últimos anos são notáveis. o sucesso da ia foi impulsionado pela capacidade adaptativa que as máquinas adquiriram e está estreitamente relacionada com a sua habilidade para aprender. os sistemas conexionistas, apresentados na forma de redes neurais artificiais (rnas), que se inspiram no sistema nervoso humano, são um dos mais importantes modelos que permitem a aprendizagem. estes são utilizados em diversas áreas, como em problemas de previsão ou classificação, apresentando resultados cada vez mais satisfatórios. uma das áreas em que esta tecnologia se tem destacado é a visão computacional (computer vision (cv)), permitindo, por exemplo, a localização de objetos em imagens e a sua correta identificação. a deteção de anomalias (anomaly detection (ad)) é outro campo onde as rnas vêm surgindo como uma das tecnologias para a resolução de problemas. em cada área são utilizadas diferentes arquiteturas de acordo com o tipo de dados e o problema a resolver. combinando o processamento de imagens e a deteção de anomalias, verifica-se uma convergência de metodologias que utilizam módulos convolucionais em arquiteturas dedicadas a ad. o objetivo principal desta dissertação é estudar as técnicas existentes nestes domínios, desenvolvendo diferentes arquiteturas e modelos, aplicando-as a casos práticos de forma a comparar os resultados obtidos em cada abordagem. o caso prático principal consiste na monitorização de pavimentos rodoviários por meio de imagens para a identificação automática de áreas degradadas. para isso, dois protótipos de software são propostos para recolher e visualizar os dados adquiridos. o estudo de arquiteturas de rnas para o diagnóstico da condição do asfalto por meio de imagens é o foco central no processo científico apresentado. os métodos de machine learning (ml) utilizados incluem classificadores binários, autoencoders (aes) e variational autoencoders (vaes). para os dois últimos modelos, práticas supervisionadas e não supervisionadas são também comparadas, comprovando a sua utilidade em cenários onde não há dados rotulados disponíveis. usando o modelo vae num ambiente supervisionado, este apresenta uma excelente distinção entre áreas de pavimentação em boas condições e degradadas. quando não existem dados rotulados disponíveis, a melhor opção é utilizar o modelo ae, utilizando a distribuição de semelhanças das reconstruções para calcular o threshold de separação, atingindo accuracy e precision superiores a 94%). o processo completo de desenvolvimento mostra que é possível construir uma solução alternativa para diminuir os custos de operação em relação aos sistemas comerciais existentes e melhorar a usabilidade quando comparada às soluções tradicionais. adicionalmente, dois estudos demonstram a versatilidade dos sistemas conexionistas na resolução de problemas, nomeadamente no projeto de estruturas mecânicas, possibilitando a modelação de campos de deslocamento e pressão em placas reforçadas; e na utilização de ad para identificar locais de aglomeração de pessoas através de técnicas de crowdsensing.",
      "artificial intelligence (ai) and data science (ds) have become increasingly present in our daily lives, and the benefits it has brought to society in recent years are remarkable. the success of ai was driven by the adaptive capacity that machines gained, and it is closely related to their ability to learn. connectionist systems, presented in the form of artificial neural networks (anns), which are inspired by the human nervous system, are one of the principal models that allows learning. these models are used in several areas, like forecasting or classification problems, presenting increasingly satisfactory results. one area in which this technology has excelled is com puter vision (cv), allowing, for example, the location of objects in images and their correct identification. anomaly detection (ad) is another field where anns have been emerging as one technology for problem solving. in each area, different architectures are used according to the type of data and the problem to be solved. combining im age processing and the finding of anomalies in this type of data, there is a convergence of methodologies using convolutional modules in architectures dedicated to ad. the main objective of this dissertation is to study the existent techniques in these domains, developing different model architectures, and applying them to practical case studies in order to compare the results obtained in each approach. the major practical use case consists of monitoring road pavements using images to automatically identify degraded areas. for that, two software prototypes are proposed to gather and visualise the acquired data. moreover, the study of ann architectures to diagnose the asphalt condition through images is the central focus of this work. the experimented methods for ad in images include a binary classifier network as a baseline, autoencoders (aes) and variational autoen coders (vaes). supervised and unsupervised practises are also compared, proving their utility also in scenarios where there is no labelled data available. using the vae model in a supervised setting, it presents a excellent distinction between good and bad pavement areas. when labelled data is not available, using the ae and the distribution of similarities of good pavement reconstructions to calculate the threshold is the best option with both accuracy and precision above 94%. the full development process shows it is possible to build an alternative solution to decrease the operation costs relatively to expensive commercial systems and improve usability when compared with traditional solutions. additionally, two case studies demonstrate the versatility of connectionist systems to solve problems, namely in mechanical structural design enabling the modelling of displacement and pressure fields in reinforced plates; and using ad to identify crowded places through crowd-sensing techniques."
    ],
    0.3
  ],
  [
    [
      "in some contexts, namely in the development of autonomous vehicles in robotics, the exploitation of the arduino platform for the development of embedded systems allows a simpler and faster implementation of the desired functionality than would be possible using the devices known as programmable logic controllers (plcs), typically used. however, the soft ware certification is a much more complicated task than the plcs certification, due mainly to the complexity of the software and its difficult analysis. although the software testing is the most used technique to validate it, it can not be used to prove the absence of errors. on the other hand, the formal verification of software is usually used to demonstrate the correctness of critical systems as, for example, aerospace industry and banking industry, where a failure means the loss of human lives or high monetary values. as the embedded systems are increasingly used in robotic systems, whose criticality is increasingly high, it is important to ensure their correctness. with this, the intervention area focused on the communication protocols because they have an appropriate dimension given the scope of the dissertation. these are quite impor tant, particularly in the context of robotic, since embedded systems are in constant commu nication with several peripherals, being fundamental the correction of the communication protocols. this dissertation aims at the application of formal verification tools to commu nication libraries of the arduino platform. the adopted verification process comprises the application of the frama-c tool, together with the wp plug-in, to perform a static analysis of annotated ansi-c code with acsl properties. furthermore, another objective is the exploration of the experimental frama-clang plug-in because of the widespread interest in achieving c++ code certification.",
      "em alguns contextos, nomeadamente no desenvolvimento de veículos autónomos em robó tica, a exploração da plataforma arduino para desenvolvimento de embedded systems possi bilita uma implementação mais simples e rápida da funcionalidade desejada do que seria possível utilizando os dispositivos conhecidos como programmable logic controllers (plcs), tipicamente utilizados. contudo, a certificação de software é uma tarefa bastante mais com plicada do que a certificação de plcs, devido principalmente à complexidade do software e à sua difícil análise. embora o teste de software seja a técnica mais utilizada para o validar, esta não pode ser utilizada para provar a ausência de erros. por outro lado, a verificação formal de software é normalmente utilizada para demonstrar a correção de sistemas críticos como, por exemplo, indústria aeroespacial e indústria bancária, onde uma falha significa a perda de vidas humanas ou de valores monetários elevados. como os embedded systems são cada vez mais utilizados em sistemas robóticos, cuja criticalidade é cada vez mais elevada, é importante garantir a sua correção. com isto, a área de intervenção incidiu sobre os protocolos de comunicação porque estes apresentam uma dimensão adequada dado o âmbito da dissertação. estes são bastante importantes, em particular no contexto da robótica, uma vez que os embedded systems estão em constante comunicação com diversos periféricos, sendo fundamental a correção dos protocolos de comunicação. esta dissertação tem como objetivo a aplicação de ferramentas de verificação formal a bibliotecas de comunicação da plataforma arduino. o processo de verificação adotado compreende a aplicação da ferramenta frama-c, em conjunto com o plug-in wp, para realizar uma análise estática de código ansi-c anotado com propriedades acsl. para além disso, é ainda objetivo a exploração do plug-in experimental frama-clang devido ao interesse generalizado em alcançar a certificação de código c++."
    ],
    [
      "a análise genómica das populações tem contribuído significativamente para o aumento do número de snvs descritos em bases de dados. estudos populacionais prévios têm contribuído com 18 a 57% novas variantes. a nova informação genética é particularmente relevante enquanto referência para propósitos clínicos. iniciativas à escala global como o 1000 genomes project (1kg) incluem populações ibéricas, contudo, nenhum indivíduo português foi incluído no mesmo grupo. tanto quanto se sabe, nenhum indivíduo português foi incluído no projeto gnomad, o maior conjunto de dados genómicos atual. acreditamos que uma coleção de informação genómica referente à população portuguesa poderia trazer grandes benefícios ao diagnóstico molecular em pacientes portugueses. as alterações genéticas detetadas em 70 indivíduos portugueses foram inseridas em uma base de dados não-relacional. a informação publicada pelos projetos 1kg e gnomad para cada alteração incluída nas mesmas foi adicionada à referida base de dados. frequências alélicas reportadas para sete populações incluídas na base de dados do gnomad, cinco populações do 1kg e 5 subpopulações europeias do mesmo projeto foram comparadas contra os valores calculados para os nossos dados. as diferenças das distribuições alélicas foram testadas com o fisher’s exact test. os p-values obtidos foram corrigidos de acordo com a sua false discovery rate (fdr). os exomas de indivíduos portugueses analisados continham 224,155 alterações genéticas filtradas de acordo com critérios de qualidade definidos no presente estudo. aproximadamente 16,4% das variantes não se encontravam descritas nas bases de dados dos projetos 1kg e gnomad. os resultados obtidos endossam evidências, previamente descritas na literatura, de uma correlação entre as diferenças genéticas das populações comparadas em relação à população portuguesa e a distância geográfica das mesmas a portugal. diferenças significativas entre distribuições alélicas da população estudada e outras subpopulações europeias foram encontradas para 7,284 alterações genéticas distribuídas por 2,571 genes. os resultados obtidos sugerem a existência de marcadores genéticos populacionais e podem motivar futuros estudos com vista a detetar marcadores genéticos específicos da população portuguesa. o estudo apresentado representa uma contribuição significativa para, não só enriquecer iniciativas genómicas de grande escala, mas também para estabelecer uma referência auxiliar para análises genéticas a doentes portugueses.",
      "the in-depth study of the genomics of single populations has contributed significantly to the enlargement of known snvs in databases. each single population study has contributed with 18 to 57% of novel snvs. the new genetic information is particularly relevant as a reference for clinical purposes. global-scale initiatives as the 1000 genomes project (1kg) already include iberian population; however, no portuguese individuals were included in this cohort. furthermore, to our knowledge, gnomad, the most extensive genomic dataset, does not include portuguese individuals either. we believe that a portuguese collection of genomic information would greatly benefit molecular diagnosis in portuguese patients. variants detected in 70 portuguese individuals were inserted in a mongodb no-sql database. the 1kg and gnomad information for each variant were uploaded to the same database. allele frequencies for seven gnomad populations, five 1kg populations, and five 1kg european subpopulations were compared to the values calculated for our data. allele distribution differences were tested with fisher’s exact test. p-values were corrected for false discovery rate (fdr). the exomes of the portuguese individuals contained 224,155 variants filtered accordingly to defined quality criteria. approximately 16.4% of the variants had not been previously reported by 1kg or gnomad projects. the present work endorsed the evidence for a correlation between genetic and geographic distance previously reported in the literature. finally, significative differences were found for the allele distribution between our population and the other 1kg european subpopulations in 7,284 variants distributed by 2,571 genes. results suggest the existence of populational genetic markers and may prompt future studies for detection of portuguese-specific genetic markers. the present study is a significant contribution to enrich large-scale genomic initiatives and, to stand as a useful auxiliary reference for genetic analyses of portuguese patients."
    ],
    0.0
  ],
  [
    [
      "a bloom filter is a probabilistic data structure designed to check, rapidly and memory-efficiently, whether an element is present in a set. it has been vastly used in various computing areas and several variants, allowing deletions, dynamic sets and working with sliding windows, have surfaced over the years. in many systems, it becomes relevant to identify the more recent information in the data stream. however, the majority of the sliding window schemes consider the most recent elements of a data stream without taking into account time as a factor. while this allows, e.g., saving the most recent 10000 elements, it does not easily translate into storing data received in the last 60 seconds, unless the insertion rate is stable and known in advance. in this thesis, a new technique is explored, unproved in terms of time complexity and memory usage compared to the already existing ones, that can save information of a given time period and correctly identify it as present when queried, while also being able to retire data when it becomes stale. this new solution can be employed in a wide range of real world applications, such as in advertising, networking, fraud detection and distributed denial of service attacks prevention.",
      "um bloom filter é uma estrutura de dados probabilistica, cuja função é verificar, rapida-mente e eficientemente em termos de memória, se um elemento está presente no set. tem sido vastamente utilizado em diversas áreas da informática e várias variantes, que permitem remoções, seis dinâmicos e funcionam com sliding windows, têm surgido ao longo dos anos. em inúmeros sistemas, toma-se relevante identificar a informação mais recente do fluxo de dados. contudo, a maioria dos esquemas em sliding windows considera os elementos mais recentes de um fluxo de dados sem ter em conta o tempo como um fator. enquanto que isto permite, e.g., guardar os 10000 elementos mais recentes, não se traduz facilmente em armazenar dados recebidos nos últimos 60 segundos, a menos que a taxa de inserção seja estável e conhecida antecipadamente. nesta dissertação, uma nova técnica é explorada, melhorada em termos de complexidade temporal e uso de memória comparativamente a outras já existentes, que consegue guardar informação de um dado período de tempo e identificá-la corretamente como presente quando uma consulta é feita, como também é capaz de eliminar dados quando estes \"envelhecem\". esta nova solução pode ser utilizada numa vasta gama de aplicações do mundo real, tais como em publicidade, networking, deteção de fraude e prevenção de ataques distribuídos de negação de serviço."
    ],
    [
      "as empresas do sector dos transportes públicos têm procurado introduzir novas tecnologias e aplicações com o objetivo de melhorar o serviço disponibilizado aos passageiros. neste sentido, os operadores deste tipo de serviço têm investido no aumento da segurança e conforto do passageiro, na diversificação das funcionalidades disponibilizadas, no acrescento de novos destinos ou paragens e na eficácia do cumprimento de horários planeados, entre outros. do lado das tecnologias de informação, uma das funcionalidades que mais tem recebido atenção é o sistema de informação e monitorização acessível aos utilizadores. enquanto uma parte substancial da informação mantida por estes sistemas é atualizada pouco frequentemente, podendo ser, inclusive, denominada de informação de carácter fixo, os dados utilizados no sistema de monitorização devem ser atualizados o mais frequentemente possível. mas, o objetivo de aumentar a qualidade do serviço de informação prestado melhorando a qualidade da monitorização do sistema, implica, em geral, custos elevados. no planeamento e construção de sistemas e aplicações de informação para monitorização de serviços de transporte de passageiros numa escala urbana ou regional, a escolha do tipo de infra-estrutura que torna possível obter, processar e utilizar os dados em tempo real, ou com frequência funcionalmente útil, é, assim, fulcral. uma má escolha das tecnologias associadas e dos sistemas que integram o serviço, podem tornar os custos inerentes à sua implementação no terreno insustentáveis e podem comprometer seriamente a sustentabilidade das empresas do ramo. o presente trabalho começa com um estudo aprofundado das várias soluções e tecnologias já existentes no mercado, analisando-as criticamente. desse estudo resultou o desafio de conceber uma proposta dum sistema aplicativo integrado de gestão de informação que disponibiliza ferramentas para a gestão, optimização e administração do serviço de transportes públicos com base em tecnologias normalizadas, abertas e de sem custo de utilização. a solução desenhada, que se pretendeu modular e escalável, consiste num sistema integrado baseado inteiramente no protocolo simple network management protocol (snmp) e que combina aplicações informáticas com a capacidade de obter dados em tempo real dos veículos e também disponibilizar informação diferenciada aos diversos intervenientes do sistema (passageiros, gestores, motoristas). após a implementação e teste dum sistema completo de aplicações protótipo, fica provada a efetividade funcional da arquitetura proposta. sendo esta solução modular e baseada numa tecnologia normalizada e aberta e de utilização livre de direitos de autor, espera-se, em consequência, que também seja eficaz no que concerne a eventuais custos de implementação e adoção no mundo real.",
      "public transportation companies have been searching for new technologies and applications with the goal of improving the service available to its passengers. in this regard, the operators have invested in the increase of the passenger safety and comfort, diversification of available features, the addition of new destinations or stops, and mostly on the efficiency and compliance of timetables, among others. when it comes to information technologies, one of the features which has been given more attention is the information system and the monitoring available to users. while a substantial portion of the information kept by these systems is infrequently updated, which can even be called fixed nature information, the data used in the monitoring systems must be updated as often as possible. however, enhancing the service quality and delivering real time information to the users usually implies huge costs. when planning and building systems and applications to monitor public transport services at a urban or regional scale, the decision on which infrastructure a company will have to invest in order to obtain and process real time data is crucial. besides, a wrong choice regarding the associated technologies and systems that assemble the service, combined with the involved implementation costs can seriously compromise the company sustainability in the market. the present work starts with an in-depth study of the different solutions and technologies that we encounter on today’s market, critically analyzing them. from that study, the challenge of conceiving an integrated information management system arose. it provides management tools, optimization and administration of the public transport services based on normalized technologies, open-source and usage costs free. the designed solution, modular and highly scalable, consists in a system entirely based on the simple network management protocol (snmp) which combines applications with the ability to attain real time data from the vehicles and therefore provides differentiated information to the various users (passengers, managers, drivers). after implementation and testing the complete prototype applications, the proposed architecture has been proven to be effectively functional. being a modular solution based on a normalized, open source and free of copyright rights technology, it is consequently expected to be also effective in terms of eventual implementation costs and adoption in the real world."
    ],
    0.3
  ],
  [
    [
      "the population density in the urban environment has increased significantly, consequently increasing the number of vehicles and people on the public road. possible monitoring of this flow allows better problem management, and the enhancement of solutions in a smart city context, solutions that promote regular traffic in a city. this work presents a solution for counting vehicles and people in a video to use the solution developed in cities of portugal. the solution combines deep learning techniques and traditional computer vision techniques, combining object detection, classification, ob ject tracking, and fingerprint concepts. for each concept is presented the state of the art techniques and techniques used in similar problems. to choose the best fingerprint methods, a comparative study of different techniques was produced. with a dataset of vehicle and people images, the following techniques were con sidered: fourier transform, scale invariant feature transform (sift), color co-occurrence histogram (ccoh), and autoencoders, of which ccoh showed better results. the solution pipeline consists of the yolov3 algorithm for the object detection part, hav ing the algorithm a convolutional neuronal network for object classification; kalman filter for object tracking was chosen in conjunction with the ccoh technique for object finger print. the pipeline ends with the matching of the newly detected objects with previously detected objects, using the hungarian algorithm for this correspondence. in order to extract features using the defined pipeline, a python library has been devel oped, allowing visualization of its operation and easy integration with video sources (video files and cameras). object counting, area definition, line intersection, heatmap’s, and object collision are examples of features that can be obtained by the library. as a global solution, a web application was developed, including a frontend application, a backend, a relational database, and a service to perform video processing with the help of the developed library. the web application is in use and in a production environment.",
      "a densidade populacional em contexto urbano tem aumentado significativamente, aumentando assim, o número de veículos e pessoas na via pública. uma possível monitorização deste fluxo permite uma melhor gestão do problema e a potenciação de soluções em um contexto de smart city, soluções que promovam o normal tráfego em uma cidade. este trabalho apresenta uma solução para a contagem de veículos e pessoas em vídeo com o objectivo de utilizar a solução desenvolvida em cidades de portugal. a solução combina técnicas de deep learning e técnicas tradicionais de visão por computador, combinando conceitos de deteção de objetos, classificação, tracing de objectos e fingerprint, sendo que para cada conceito são apresentadas as técnicas estado de arte e as técnicas utilizadas em problemas semelhantes. para escolher os melhores métodos de fingerprint um estudo comparativo de várias técnicas foi desenvolvido. com um dataset de imagens de veículos e pessoas foram consideradas as técnicas: fourier transform, scale invariant feature transform (sift), color co-occurrence histogram (ccoh) e autoencoders, dos quais o ccoh apresentou melhores resultados. a pipeline da solução é composta pelo algoritmo yolov3 para a parte de deteção de objetos, tendo este algoritmo uma rede neuronal convolucional para a classificação de ob-jectos; foi escolhido o kalman filter para tracking de objectos em conjunto com a técnica de ccoh para fingerprint de objectos. a pipeline termina com a correspondência dos novos objectos detectados com objectos previamente detectados, sendo utilizado o algoritmo húngaro para esta correspondência. de modo a extrair features com o uso da pipeline definida, foi desenvolvida uma biblioteca python que permite a visualização do seu funcionamento e uma fácil integração com fontes de vídeo (ficheiros de vídeo e cameras). contagem de objectos, definição de áreas, intersecção de linhas, heatmap's e a colisão de objectos são, entre outras, exemplos de features que podem ser obtidas pela biblioteca. como solução global foi desenvolvida uma aplicação web, englobando assim, uma aplicação frontend, uma backend, uma base de dados relacional e um serviço para executar o processamento de vídeo com o auxílio da biblioteca desenvolvida. a aplicação web encontra-se em utilização e em ambiente de produção."
    ],
    [
      "nowadays, companies live in a scenario of strong competitiveness. the telecommunications market is not an exception and it is possible to offer a differentiation from competition through better service quality, differentiated support and even better value proposals. with the evolution of technologies, companies have more data about their customers and the usage profile of each one of them. with this information it is possible to establish a better relationship with the customer through a more efficient support service. the evolution of artificial intelligence and computational power, combined with existing data, al lows for several comparisons between different machine learning algorithms. in this dissertation, a prediction model capable of predicting recurrences of contacts with the customer service is pro posed. the aim is to predict whether a particular problem reported by the customer will repeat and require a new contact, so that it is possible to correct those problems in advance, making the user experience more pleasant and fluid. in order to achieve the best possible model, different classical machine learning approaches were tested, along with several deep neural network architectures. in recent years, deep neural networks have shown interesting results in several non-tabular appli cations, therefore being interesting to test them in tabular applications like the one present in this work. tabnet, developed by google, is a deep neural network adjusted to perform the better in tabu lar datasets, and was also tested, as it has shown better performance than several neural networks or decision-tree bases algorithms. the used data were collected by various internal systems, the most important of which being the one related to customer support calls. the customer service, due to its size and complexity, has a system that monitors all calls and their motivations, as well as the parties involved (both operator and customer) and other additional data such as time spent and the call outcome. data from other systems is related to billing, service usage and customer profile, and is added to help to understand the context of the call. the model that shown the best results was catboost, a decision trees based algorithm, showing an auc_roc of 79%, with a recall of 61% and a precision of 62%, allowing the identification of about 8,6% of the 3.9 million calls made to the support service as recurrences even before they occur, about 340k cases. in an ideal scenario, all these calls would be avoided, allowing a substantial cost reduction for the company, as well as a consequent increase in customer satisfaction in relation to the service. the catboost model showed better training times and less memory needs, while achieving a better performance than the different architectures of deep neural networks proposed. only tabnet was able to achieve a similar performance, while maintaining a higher training time. however, in futures uses, where the catboost model achieves a plateau and is not benefiting for the increasing data, it could be useful to use tabnet as the model in production. tabnet has the advantage of being a neural network and, for that reason, being more capable of breaking the plateau that classical models often achieve.",
      "atualmente, as empresas vivem num cenário de forte competitividade. o mercado das telecomunicações não é uma exceção e é possível oferecer uma diferenciação da competição através de melhor qualidade de serviço, suporte diferenciado e até melhores propostas de valor. com a evolução das tecnologias, as empresas possuem também cada vez mais dados acerca dos seus clientes e sobre o perfil de uso de cada um deles. com esta informação é possível estabelecer uma melhor relação com o cliente através de um suporte mais eficiente. a evolução da inteligência artificial e do poder computacional, aliada aos dados existentes, permitem fazer várias comparações entre diferentes algoritmos de machine learning. nesta dissertação, é proposto um modelo de previsão capaz de prever reincidências de contactos com o serviço de apoio ao cliente. o objetivo é, então, prever se um determinado problema reportado pelo cliente se vai tornar reincidente e exigir um novo contacto, para que seja possível proceder à correção antecipada desses problemas, tornando a experiência de utilizador mais agradável e fluida. a fim de alcançar o melhor modelo possível, foram testadas diferentes abordagens clássicas de machine learning, juntamente com várias arquitecturas de deepneuralnetworks. nos últimos anos, as deep neuralnetworks mostraram resultados interessantes em várias aplicações não tabulares, pelo que é interessante testá-las em aplicações tabulares como a presente neste trabalho. o tabnet, desenvolvido pela google, é uma deepneuralnetwork ajustada para ter um melhor desempenho em conjuntos de dados tabulares, e também foi testada, uma vez que mostrou um melhor desempenho do que várias redes neuronais e algoritmos baseados em árvores de decisão. os dados usados são recolhidos por diversos sistemas internos, sendo que os de maior importância são os dados relativos a chamadas para o apoio ao cliente. o serviço de apoio ao cliente, devido à sua dimensão e complexidade, possui um sistema que monitoriza todas as chamadas e as suas motivações, bem como os intervenientes e outros dados acessórios como tempo dispensado e soluções encontradas. os dados provenientes de outros sistemas estão relacionados com a faturação, uso e perfil do cliente, com vista a fornecer um contexto para a situação. o modelo que obteve o melhor resultado foi o catboost, baseado em árvores de decisão, com um roc_auc de 79%, com uma recall de 61% e uma precision de 62%, permitindo identificar cerca de 8,6% das 3,9 milhões de chamadas feitas ao serviço de suporte como reincidências mesmo antes de elas ocorrerem, ou seja, 334 mil casos. num cenário ideal, todas essas chamadas seriam evitadas, possibilitando uma redução de custos substancial para a empresa, bem como um consequente aumento na satisfação do cliente em relação ao serviço.o catboost foi também o modelo que mostrou melhores tempos de treino e menor exigência de memória, conseguindo ao mesmo tempo um melhor desempenho do que as diferentes arquitecturas de deepneuralnetworks propostas. apenas o tabnet conseguiu um desempenho semelhante, apesar de manter um tempo de treino superior. contudo, em utilizações futuras, onde o modelo catboost atinge um patamar de performance e já não beneficie com o aumento de dados, poderá ser útil utilizar o tabnet como modelo em produção. o tabnet tem a vantagem de ser uma rede neural e, por essa razão, ser mais capaz de quebrar o patamar de performance que os modelos clássicos frequentemente alcançam e não conseguem quebrar."
    ],
    0.3
  ],
  [
    [
      "complex microbial communities are essential to all ecosystems, and by linking microbial identity to function, meta-omics technologies facilitate the interpretation of the processes cat alyzed by microorganisms. mosca is a command-line pipeline that performs bioinformatics analyses of metagenomics, metatranscriptomics, and metaproteomics. mosguito is a web based tool developed in react, which allows the configuration of mosca’s workflow and the visualization of mosca outputs. although the metadata and the configuration options of mosca could be easily customized and downloaded through mosguito, mosguito was unable to interact with mosca automatically. in this thesis, a third-tier client-server architecture was developed containing the client mosguito, the server mosca, and a database. mosguito as a client-side can retrieve, store and delete data from the database and start running analysis on mosca as a server. mosca as a server can receive files from the client-side and start an analysis run. the database can store results from mosca, input files from users, and respective user information from their login session. a full guide to how to utilize this new version of mosguito is provided. mosguito client-side can interact with mosca as a server using flask apis, end users don’t need to have knowledge on command-line pipelines to use mosca, nor the computer resources to download it. there fore users using mosguito can optimize the usage and configuration of mosca, being able to analyze the data from omics experiments with a simple interaction with mosguito.",
      "comunidades microbianas complexas são essenciais em todos os ecossistemas, as tecnologias metaómicas facilitam a interpretação dos processos catalisados pelos microrganismos, pois permitem ligar a identidade dos microrganismos a sua função. mosca é um pipeline que funciona a base de linha de comandos que realiza análises de bioinformática de meta- genómica, metatranscriptómica e metaproteómica. o mosguito é uma ferramenta web desenvolvida em react, que permite a configuração do fluxo de trabalho do mosca e a visualização dos resultados. embora os metadados e as opções de configuração do mosca possam ser facilmente personalizados e transferidas através do mosguito, o mosguito não conseguia interagir com o mosca automaticamente. nesta tese, foi desenvolvida uma arquitetura cliente-servidor de terceiro nível contendo o cliente mosguito, o servidor mosca e uma base de dados. o mosguito como cliente pode recuperar, armazenar e excluir dados da base de dados e começar a executar análises no mosca como servidor. o mosca como servidor pode receber arquivos do lado do cliente e iniciar uma execução de análise. a base de dados pode armazenar resultados do mosca, ficheiros de input submetidos pelos utilizadores e respetivas informações da sessão de login do utilizador. e apresentado um guia completo de como utilizar esta nova versão do mosguito. o lado do cliente mosguito pode interagir com o mosca como um servidor usando apis construídas utilizando a framework flask. os utilizadores finais não precisam ter conhecimento sobre linhas de comando para usar o mosca e sem a necessidade de recursos de computador para o transferir. assim, os utilizadores do mosguito otimizam o uso e a configuração do mosca, podendo analisar seus dados com uma simples interação com o mosguito."
    ],
    [
      "the middleware layer is an abstraction method that acts as an intermediary in a software infastructure implementing interoperability between existing applications, operating systems, networks and the hardware of a distributed system. it is considered a cross-platform tool capable of providing an essential programming abstraction to this type of systems, allowing for easier management of the inherent heterogeneity in these. a middleware solution application allows a responsible user to orchestrate message flows, to prepare their contents so that they always reach their destination in the format they need. it also provides the users with the possibility to obtain information in real time regarding the performance of the systems it encompasses, allowing evaluation and consequent action to improve efficiency, in order to achieve the requirements of the systems’ operation. the project to be developed, aims to take advantage of internet of things and middleware technologies and concepts, applying them to the creation of a service monitoring tool essential to a more efficient performance of a distributed system. the services and architectures to focus with greater attention are micro-services, soa architecture, etl processes and events, of which one will be chosen to be the focus in the development of the project.",
      "a camada de middleware é uma camada de abstração que atua como intermediária no software de uma infraestrutura. esta implementa a interoperabilidade entre as aplicações existentes, os sistemas operacionais, as redes e o hardware de um sistema distribuído. (farahzadi et al. (2018)) é considerada uma ferramenta cross-platform capaz de fornecer uma abstração de programação essencial a este tipo de sistemas, permitindo mais facilmente gerir a heterogeneidade inerente nestes. uma aplicação de solução de middleware permite a um utilizador responsável orquestrar fluxos de mensagens e preparar os conteúdos destas de modo que cheguem sempre ao destino no formato que lhe é necessário. também permite obter informações em tempo real sobre o funcionamento dos sistemas que engloba, permitindo avaliação e consequente ação de melhorar a eficiência, de modo a alcançar os requisitos de funcionamento dos sistemas. o projeto a desenvolver, visa tirar partido das tecnologias e conceitos de internet of things e middleware, aplicando-as à criação de uma ferramenta de monitorização de serviços e plataformas essenciais ao funcionamento de um sistema distribuído. os serviços a focar com mais atenção são micro-services, arquitetura soa, processos etl e events, sendo que micro-services serão mais aprofundados no desenvolvimento do projeto."
    ],
    0.02727272727272727
  ],
  [
    [
      "um sistema de raciocínio pode ser caracterizado como um agregado de componentes de software que realizam em conjunto processos de tomada de decisão complexos. este tipo de sistema está bastante ligado a uma das áreas de trabalho mais mediáticas atualmente, a inteligência artificial. algumas iniciativas de desenvolvimento dentro desta área tendem a incorporar este tipo de ferramenta em sistemas de avaliação, mais concretamente em tutores inteligentes, com o intuito de ajudar os estudantes no seu processo de aprendizagem. nesta dissertação apresenta-se a conceção e a implementação de um conjunto de mecanismos de raciocínio baseado em casos e baseado em regras. estes dois tipos de mecanismos foram idealizados para integrar o atual módulo de avaliação do sistema leonardo, uma plataforma que complementa o estudo presencial dos alunos da universidade do minho. os novos mecanismos, em particular os de raciocínio baseados em casos, complementam o processo de avaliação do sistema leonardo aumentando as suas capacidades de raciocínio aquando da realização dos processos de avaliação estendendo as sessões de quizz. quanto aos mecanismos baseados em regras, estes representam uma importante camada entre o módulo de avaliação e a interface do tutor do sistema, visto que não permite apresentar questões de escolha múltipla na interface que não estejam de acordo com critérios estabelecidos por peritos. nesta dissertação veremos como tais mecanismos foram fundamentados, desenvolvidos e integrados no sistema leonardo.",
      "a reasoning system can be characterized as an aggregate of software components that jointly perform com plex decision-making processes. this type of elements is closely linked to one of the most popular areas today, artificial intelligence. some development initiatives within this area tend to incorporate this type of tool into evalu ation systems, specifically in intelligent tutors, in order to help students in their learning process. this dissertation presents the design and implementation of a set of case-based and rule-based reasoning mechanisms. this two types of mechanisms were design in order to integrate the current evaluation module of the leonardo system, a platform that complements the in person study of students of university of minho. new mechanisms, in particular case-based reasoning, complements the evaluation process of the leonardo system increasing their reasoning skills when carring out evaluation processes by extend the quizz sessions. as for rule-based mechanisms, these represent an important layer between the evaluation module and the system tutor interface, as it does not allow to present multiple choice questions in the interface that are not in accordance with criteria established by ex perts. in this dissertation we will see how such mechanisms were substantiated, developed and integrated in the leonardo system."
    ],
    [
      "hoje, a controlar fornece para a bosch a intelligent functional test system machine, um sistema ciber-físico desenvolvido para realizar diferentes níveis de testes funcionais em dispositivos e componentes electrónicos. a bosch utiliza-a para testar o correto funcionamento dos auto-rádios produzidos. durante este processo, os auto-rádios são submetidos a vários testes e o problema surge quando a máquina detecta erros em vários auto-rádios consecutivos e não é possível saber se a própria máquina está com problemas, pois não possui nenhum módulo que permita saber se está a funcionar corretamente ou não. a origem deste trabalho surge da necessidade de encontrar uma solução que resolva o problema enunciado, mas também, inovadora e com contribuições para o mundo da investigação em sistemas ciber-físicos e sistemas de testes de autodiagnóstico. a solução é integrar um sistema de autodiagnóstico na máquina que possa testar o seu funcionamento para que a bosch possa ter certeza se o problema está na máquina ou nos auto-rádios. como a máquina é um sistema ciber-físico, permite a integração de um sistema de software que possa gerir a execução de testes, sendo capaz de detectar falhas nas máquinas. o trabalho aqui apresentado aborda o problema criando um novo sistema de testes de autodiagnóstico que garantirá a confiabilidade e integridade do sistema ciber-físico. em detalhe, esta dissertação começa por expôr um estudo sobre o estado da arte atual de sistemas ciber-físicos, automação de testes, metodo logia de teste keyword-driven e mais alguns conceitos relacionados a linguagens específicas de domínio que serão relevantes para a solução final. são apresentadas a especificação e análise do sistema, a fim de definir bem os seus componentes. uma nova arquitetura modular e extensível é proposta para siste mas de testes de autodiagnóstico, bem como uma arquitetura para estendê-lo e integrá-lo num sistema ciber-físico. foi proposto um novo sistema de testes de autodiagnóstico que aplica a arquitetura proposta provando que é possível realizar o autodiagnóstico em tempo real do sistema ciber-físico e permitindo a integração de qualquer tipo de teste. para validar o sistema, foram realizados 28 casos de teste, abran gendo todas as suas funcionalidades. os resultados mostram que todos os casos de teste passaram e, portanto, o sistema cumpre todos os objetivos propostos.",
      "nowadays, controlar supplies with bosch the intelligent functional test system machine, a cyber physical system developed to perform different levels of functional tests on electronic devices and compo nents. bosch uses it to test the correct functioning of the produced car radios. during this process, the car radios are subjected to several tests and the problem arises when the machine detects errors in several consecutive car radios and it is not possible to know if the machine itself has any problems, as it does not have any module that allows knowing whether it’s working correctly or not. the origin of this work arises from the need to find a solution that solves the referred problem, but also, innovative and with contributions to the world of research in cyber-physical systems and self-diagnosis tests systems. the solution is to integrate a self-diagnosis system into the machine that can test its functionality so that when these car radio failures appear, bosch can be sure whether the problem is with the machine or the car radio. as the machine is a cyber-physical system, it allows the integration of a software system to control and manage all its actions. therefore, it is necessary to develop a system to manage the tests and their execution, being able to detect internal failures in the machines. the work presented here addresses the problem by creating a new self-diagnosis tests system that will guarantee the reliability and integrity of the cyber-physical system. in detail, this dissertation begins by exposing a study on the current state of the art of cyber-physical systems, test automation, keyword-driven test methodology and some more concepts related to domain-specific languages that will be relevant to the final solution. the specification and analysis of the system are presented, to define well its compo nents. a new modular and extensible architecture is proposed for self-diagnosis test systems, as well as a methodology for extending and integrate it into a cyber-physical system. a new self-diagnosis tests sys tem has been proposed that applies the proposed architecture proving that it is possible to carry out the self-diagnosis in real-time of the cyber-physical system and allowing the integration of any type of test. to validate the implementation of the system, 28 test cases were carried out to cover all its functionalities. the results show that all test cases passed and, therefore, the system meets all the proposed objectives."
    ],
    0.3
  ],
  [
    [
      "os avanços recentes das tecnologias de inteligência artificial e de processamento de dados mudaram radicalmente o paradigma do setor de saúde, dando origem a soluções digitais que prometem transformar os vários processos clínicos, permitindo um aumento da sua eficiência e qualidade enquanto capazes de reduzir os custos a eles associados. com os profissionais de saúde a enfrentarem diariamente o problema de possuírem recursos limitados, fazendo com que não sejam capazes de monitorzar e apoiar diariamente todos os seus pacientes, cada vez mais se torna importante o desenvolvimento de alternativas válidas e fidignas, capazes de ajudar os vários pacientes, no mínimo tempo possível. uma das soluções mais adotadas de maneira a solucionar o problema referido, reside no desenvolvimento de sistemas conversacionais, comumente chamados de chatbots, que se assumem como capazes de efetuar o esclarecimento de questões de âmbito clínico, incorporando uma função semelhante a um assistente virtual e preenchendo, desta forma, a lacuna existente na comunicação entre os vários pacientes e os profissionais de saúde. esta dissertação tem como foco, a sugestão de uma arquitetura relativa a um sistema conversacional com o objetivo de efetuar aconselhamento psiquiátrico. o chatbot proposto tem o nome de yec, acrónimo em inglês para “your everyday companion”, representativo de uma abordagem híbrida, pela combinação de técnicas de processamento de linguagem natural e de um modelo deep learning, para a geração da sua resposta. o sistema é desenhado para efetuar tratamento diferenciado por utilizador, permitindo desta forma a inferência do seu estado emocional, bem como do estabelecimento de um grau elevado de confiança e proximidade. de forma a provar que o sistema apresentado na teoria, representa uma solução prática viável, procedeu-se ao desenvolvimento de uma fase primária do motor conversacional presente no sistema, expondo as diferentes abordagens realizadas de forma a que, pela análise dos seus resultados, fosse possível inferir sobre a sua melhor implementação. a realização desta dissertação permitiu assim concluír acerca do poder inerente à combinação de técnicas de dl e de nlp para modelação conversacional, aferindo assim da sua capacidade para ajudar a resolver os diferentes problemas clínicos observados nos dias de hoje, sendo no entanto necessário mais investigação de modo a enfatizar esta afirmação.",
      "the recent advances of technologies for artificial intelligence and data processing have radically changed the healthcare industry, giving rise to digital healthcare solutions, promising to transform the whole healthcare process to become more efficient, less expensive and with higher quality. nowadays, health professionals have to deal with the lack of resources, not being able to personally monitor and support patients in their everyday life, so it’s becoming more and more important, to find and develop alternative ways to instantaneously help patients, corresponding to their needs. one of the solutions for the problem above, resides in the form of dialogue systems, called chatbots, that could play a leading role, by embodying the function of a virtual assistant and bridging the gap between patients and clinicians. this thesis focus on the suggestion of a chatbot architecture, for both psychiatric counseling and elderly monitoring, combining methodologies to emotion recognition and intent understanding. the system proposed is called yec, acronym for “your everyday companion”, and represents a hybrid approach that integrates both nlp techniques and an encoder-decoder deep learning model, in order to generate the appropriated response. the system is designed to perform a differentiated treatment for every user in the system, thus allowing the establishment of trust and confidence between them. to prove that the proposed approach is a viable solution to the presented problem, it is demonstrated the practical implementation of the yec model, for an initial phase of the system, demonstrating how the use of different recurrent neural networks in our model results in dissimilar performance results for our chatbot, allowing us to infer which one is better suitable. this work allowed to realize the potential of combining deep learning and natural language processing for conversational modeling and their capability for solving some real life problems associated with healthcare, being nevertheless necessary more future work to give emphasis to this affirmation."
    ],
    [
      "a monitorização da qualidade da água é uma tarefa fundamental que tem de ser incluída em qualquer processo operacional de uma estação de tratamento de águas residuais (etar), pois permite verificar se os efluentes, quando descarregados no meio ambiente, cumprem os valores padrão definidos por lei. se estes valores não forem monitorizados com eficácia, a poluição da água continua a intensificar-se e, consequentemente, a água torna-se cada vez mais escassa. devido ao problema da poluição da água, tem-se verificado um aumento na escassez da água, em portugal, durante as últimas décadas. a introdução de abordagens de machine learning (ml) neste tipo de operações pode vir a ser bastante importante, devido à sua capacidade de melhorar a eficiência da monitorização da qualidade da água e da previsão das substâncias da mesma. uma das principais características que esta abordagem oferece é a interpretação de padrões e tendências nos dados, que não são facilmente identificáveis por outras técnicas, e ainda a interpretação de relações não lineares nos dados. deste modo, este trabalho visa a implementação de modelos de ml baseados em convolutional neural networks (cnn), long short-term memory (lstm), transformer, e transformer-lstm, para a previsão dos valores da condutividade e do caudal, no afluente duma etar, de modo a apoiar a monitorização da qualidade da água, e a celeridade do processo de tomada de decisão neste tipo de infraestruturas. diante dos melhores modelos candidatos obtidos, verificou-se que os modelos baseados em transformer alcançaram os melhores resultados na previsão da condutividade, nas duas abordagens consideradas (multivariate e univariate), enquanto que os modelos baseados em cnn alcançaram o melhor desempenho na previsão do caudal, também nas duas abordagens supramencionadas.",
      "water quality monitoring is a fundamental task that must be included in any operational process of a wastewater treatment plant (wwtp), since it allows checking whether the effluents, when returned to the environment, meet the required standard values. if these values are not monitored effectively, water pollution continues to intensify and, consequently, water becomes increasingly scarce. as a consequence of water pollution, there has been an increase in water scarcity, in portugal, during the last decades. the introduction of machine learning (ml) approaches in this type of operations can be very important, due to its ability to improve the efficiency of water quality monitoring and prediction of water components. one of the main features that this approach offers is the interpretation of patterns and trends in the data, which are not easily identifiable by other techniques, and also the interpretation of non-linear relationships in the data. in this way, this work aims to implement ml models based in convolutional neural networks (cnn), long short-term memory (lstm), transformer, and transformer-lstm, to predict electrical conductivity and water flow values, in the affluent of a wwtp, to support water quality monitoring, and accelerate the decision-making process in such infrastructure. among the best candidate models obtained, transformer based models achieved the best results considering conductivity forecasting, in both approaches (multi variate and univariate), while cnn-based models achieved the best performance in the prediction of water flow values, also considering the approaches mentioned before."
    ],
    0.12857142857142856
  ],
  [
    [
      "hoje em dia, a tomada de decisões de forma rápida e eficaz é essencial nas organizações de saúde. neste sentido, surgem os sistemas de apoio à decisão, as plataformas de business intelligence e os sistemas de recomenda ção. de forma a apoiar a decisão no âmbito da ginecologia e obstetrícia, no centro materno infantil do porto para além de existir o processo clínico eletrónico, encontra-se também implementado um sistema de pré-triagem que permite distinguir as utentes em dois níveis (urgência e consulta). no âmbito desta dissertação e tendo por base os sistemas existentes, foi realizado um projeto com o objetivo de extrair conhecimento de uma forma automática e em tempo real e desenvolver um sistema de prioridades para a triagem obstétrica. este projeto seguiu a metodologia de investigação design research, cujo objetivo é orientar e validar a construção de artefactos. neste sentido, foi desenvolvida e implementada uma plataforma de business intelligence e elaborada uma proposta de um sistema prioridades de triagem obstétrica. no primeiro caso, a plataforma desenvolvida permite obter indicadores em duas vertentes: indicadores de monitorização do sistema de pré-triagem existente e indicadores obstétricos. no total, foram produzidos cerca de 50 indicadores. a nível técnico foi seguida a metodologia de kimball e foi utilizado o pentaho community edition como ferramenta open source de business intelligence, que lhe conferiu várias características, entre elas, o pervasive. em relação ao segundo artefacto, inicialmente, procedeu-se à exploração, validação e divulgação do sistema de pré-triagem para a comunidade médica e científica. numa fase posterior foram desenvolvidos modelos de data mining, que permitiram aferir se o sistema de pré-triagem estava calibrado para triar utentes em dois níveis (acuidades superiores a 81,75%). foi também desenvolvido um algoritmo de simulação que permitiu aferir se seria necessário e viável o sistema de pré-triagem evoluir para um sistema de triagem de prioridades com triagem de 5 níveis. por último, foi desenvolvida e apresentada uma proposta de um sistema de triagem prioridades obstétrica. após validação dos artefactos produzidos com as equipas clínica, verifcou-se que este trabalho contribui para o apoio à decisão nas áreas abordadas.",
      "nowadays, making decisions quickly and e ffectively it is essential in healthcare organizations. in this sense, arise decision support systems, business intelligence platforms and recommender systems. in order to support clinical and management decisions in gynaecology and obstetrics in centro materno infantil do norte beyond the existence of the electronic health record, it was also implemented the pre-triage system allowing health professionals to distinguish between two levels (emergency and output patients). within the scope of this thesis and based on the existing systems it was carried out a project with the goal to extract knowledge in an automated way and in real time and develop a priority triage system for obstetric. this project followed the research design research methodology, whose purpose is to guide and validate the construction of artefacts. in this sense, it was developed and implemented a platform for business intelligence and elaborated a proposal of a obstetric triage priorities system. in the fi rst case, the platform allows obtaining indicators in two areas: indicators for monitoring the existing pretriage system and obstetric indicators. in total they were produced about 50 indicators. in the technical level it was followed the kimball methodology and it was used the pentaho community edition as open source business intelligence tool, which gave to the platform several characteristics (e.g pervasive). on the second artefact, initially it was conducted the exploration, validation and dissemination of pre-triage system for the medical and scientifi c community. at a later stage, they were induced data mining models, which allowing to conclude that the pre-triage system is calibrated to triage patients on two levels (higher than 81.75 % ) but it was also found that this system needs some improvements. in this way, it was developed a simulation algorithm, that allowed to assess and analyse the viability of the pre-triage system be improved to a five leaves priority system. finally, it was developed and presented a proposal for a obstetric triage priorities system. the artefacts produced ware validate with the clinical teams, where they assess the work quality and proved that this work contributes to decision support in the areas addressed."
    ],
    [
      "understanding the evolutionary relationships between organisms is a complex issue that has gained importance not only in evolution but also in clinical and biological inferences, only possible with technological advances that require new analytical tools. in this work, the main focus is to study the genome of mycobacterium tuberculosis, the causal agent of tuberculosis, establishing a detailed evolutionary framework.the secondary objective, regardless of the focus on the evolution of mtb, is that the pipeline created can be applied to any other organism. this dissertation presents some basic facts about tuberculosis, an infectious bacterial disease caused by the mycobacterium tuberculosis complex, its constitution, evolution, pathology, drug resistance and genetic variation. our objectives were contextualized considering the most up-to-date tools of alignment and phylogenetics, an area in constant progress due to the growing needs of bioinformatics tools in the area of genomics and evolution. taxonomic and genetic data were compiled from all organisms with complete genomes in ncbi. this database was subsequently cured by eliminating redundant genomes, i.e., containing only one representative element of each species with the complete proteome. a search of each mycobacterium tuberculosis protein in this local database using blast allowed the detection of probable homologs in a large number of taxonomically informative organisms. the search results were limited to two hundred homologues, which were aligned using muscle. phylogenetic trees, based on maximum likelihood were constructed for the approximately four thousand mycobacterium tuberculosis proteins. the phylogenetic relationship and monophyly of mycobacterium tuberculosis with the remaining bacteria of the same genus(mycobacterium) and the same family (corynebacteriaceae) were studied to understand possible processes of acquisition of genes by horizontal transference. finally, positive selection processes were studied by searching for excess or deficit of non-synonymous mutations in relation to the synonymous (ka / ks) using the codeml software, in order to identify branches with an accelerated evolution in the establishment of the pathogenic species mycobacterium tuberculosis. these genes may form the basis of the physiological and biochemical characteristics that make this bacterium pathogenic to humans.",
      "compreender as relações evolutivas entre os organismos é uma questão complexa que ganhou cada vez mais relevo no âmbito não apenas da evolução mas para inferências clínicas e biológicas, só possíveis com os avanços tecnológicos, que exigem novas ferramentas analíticas. neste trabalho,o foco principal é estudar o genoma da mycobacterium tuberculosis, o agente causal da tuberculose, estabelecendo-se um quadro evolutivo detalhado. o objetivo secundário, independentemente do foco na evolução da mtb, é que a pipeline criada possa ser aplicada a qualquer outro organismo. esta dissertação apresenta alguns fatos básicos sobre a tuberculose, uma doença infeciosa bacteriana causada pelo complexo mycobacterium tuberculosis, sobre a sua constituição, evolução, patologia, resistência aos medicamentos e variação genética. os nossos objetivos foram contextualizados considerando as mais atualizadas ferramentas de alinhamento e de filogenética, uma área em constante progresso devido às crescentes necessidades de ferramentas bioinformáticas na área da genómica e evolução. foram compilados dados taxonómicos e genéticos de todos os organismos com genomas completos no ncbi. esta base de dados foi posteriormente curada eliminando-se genomas redundantes, ou seja, contendo apenas um elemento representativo de cada espécie com o proteoma completo. uma busca de cada proteína da mycobacterium tuberculosis nesta base de dados local utilizando o blastpermitiu a deteção de prováveis homólogos num vasto número de organismos taxonomicamente informativos. os resultados da busca foram limitados a duzentos homólogos, que foram alinhados recorrendo ao muscle. árvores filogenéticas, baseadas em máxima verossimilhança foram construídas para as cerca de quatro mil proteínas da mycobacterium tuberculosis. a relação filogenética e monofilia da mycobacterium tuberculosis com as restantes bactérias do mesmo género (mycobacterium) e da mesma família (corynebacteriaceae) foram estudadas para compreender possíveis processos de aquisição de genes por transferência horizontal. finalmente, processos de seleção positiva foram estudados através da procura de excesso ou défice de mutações não sinónimas em relação às sinónimas (ka/ks) usando o software codeml, de modo a identificar ramos com uma evolução acelerada no estabelecimento da espécie patogénica mycobacterium tuberculosis. esses genes podem estar na base das características fisiológicas e bioquímicas que tornam esta bactéria patogénica para o homem."
    ],
    0.024999999999999998
  ],
  [
    [
      "this document constitutes the final report of a master’s thesis focused on developing a working soft ware product for the company wintouch, with the purpose of managing the delivery of prepared/cooked meals at restaurants. the software product developed and here discussed (the working process and the final product) manages the entire process of deliveries in restaurants, allowing clients to place orders online, via a phone call or in person, and keeping track of the subsequent tasks until the order is delivered to the client. this management includes tasks such as deciding when to start preparing the order, sending the couriers to clients’ homes, while managing their routes and maximizing the number of orders they take to a certain area. the software package and application developed and under discussion also ensures a proper interaction with wintouch’s products, allowing restaurants to save information about clients, so as to increase the efficiency of future contacts with clients.",
      "este documento constitui o relatório de uma tese de mestrado focada no desenvolvimento de um produto de software para a empresa wintouch, com o propósito de gerir as entregas de refeições preparadas/cozinhadas em restaurantes. o produto deverá gerir todo o processo de preparar entregas em restaurantes, deixando clientes fazer pedidos online, por telefone ou presencialmente, e após o pedido ser realizado, gerir todas as tarefas subsequentes até que o pedido seja entregue ao cliente, tal como gerir quando começar a preparar a entrega do pedido, enviar estafetas para as casas dos clientes, gerir as suas rotas e maximizar o número de pedidos entregues numa determinada área. deverá também interagir com os restantes produtos da wintouch, permitindo que os restaurantes guardem informações sobre os seus clientes, de modo a maximizar a eficiencia dos contactos futuros com os mesmos."
    ],
    [
      "a eficiência e desempenho das operações de entrada/saída(e/s) são aspetos fundamentais na implementação de sistemas de armazenamento. a maioria das soluções atuais são implementadas em kernel, obrigando a trocas de contexto entre espaço de utilizador e kernel por parte das aplicações. estas trocas de contexto são custosas e, por isso, limitam o desempenho do sistema de armazenamento. a plataforma storage performance development kit (spdk) disponibiliza uma forma de construir estes sistemas evitando o acesso a kernel, realizando todas as operações de e/s necessárias diretamente do espaço de utilizador para o disco físico. contudo, os dados continuam a ter que ser guardados com garantias de persistência. assim sendo, os sistemas construídos com spdk devem se tolerantes a faltas para garantir resiliência em cenários de falta. a inexistência de uma ferramenta capaz de testar essa resiliência em sistemas de armazenamento construídos com spdk é um problema para os programadores que querem testar a resiliência dos seus sistemas. de forma a resolver este problema, esta dissertação propõe o fault injector in spdk (fispdk), uma ferramenta que estende o spdk e fornece injeção de faltas determinística ao nível do block device. para injetar faltas deterministicamente, fispdk utiliza diferenciação de pedidos e/s de forma a identificar quais pedidos devem (ou não) ser injetados com uma falta. para isso, o fispdk implementa mecanismos de propagação de contexto, que permitem passar informação da aplicação para os níveis mais baixos das pilhas de e/s, e é baseado numa extensão da block device application programming interface (api) original do spdk. para providenciar injeção de faltas, o fispdk apresenta um block device virtual que interceta pedidos e/s e injeta corrupção de dados ou atraso neles. o block device virtual pode ser configurado pelos utilizadores para apontar os tipos de faltas e quais os pedidos que devem ser injetados com essas faltas. uma avaliação compreensiva do fispdk demonstra que a nossa solução consegue injetar faltas de forma determinística e avaliar a tolerância a faltas de sistemas de armazenamento que usam spdk, sem adicionar uma sobrecarga significativa à pilha de armazenamento.",
      "the efficiency and performance of input/output (i/o) operations are two of the most important aspects in the implementation of a storage system. most of the current solutions are implemented in kernel, forcing context switches between user-space and kernel by applications. these context switches are costly and, therefore, limit the performance of the storage system. the spdk framework provides a way to build these systems by bypassing all kernel components, performing all necessary i/o operations from user level applications directly to the physical device. however, data still needs to be stored persistently. therefore, systems built with spdk must guarantee fault tolerance, to ensure resilience under fault scenarios. the nonexistence of a tool that is able to test the fault tolerance of spdk-compliant systems is a problem to developers that want to test the resilience their systems. to solve this problem, this dissertation presents fispdk, a tool that extends spdk and provides deter ministic fault injection at the block device level. to inject faults in a deterministic way, fispdk resorts to i/o differentiation to be able to identify which requests must be (or not) injected with a fault. for that, fis pdk implements a context propagation mechanisms,which enables passing application-level information to the lower-levels of the i/o stacks, and is based on an extension of the original block device api of spdk. to provide fault injection, fispdk presents a virtual block device that intecepts i/o requests and injects data corruption and delay into these. the virtual block device can be configured by users to pinpoint the type of faults and the i/o requests targeted by these. a comprehensive experimental evaluation of fispdk shows that our solution is able to deterministi cally inject faults and evaluate the fault tolerance of the spdk-compliant storage systems, without adding significant performance overhead into the storage stack."
    ],
    0.3
  ],
  [
    [
      "cerebral amyloid angiopathy is a cerebrovascular disorder resulting from the deposition of an amyloidogenic protein in small and medium sized cortical and leptomeningeal vessels. a primary cause of spontaneous intracerebral haemorrhages, it manifests predominantly in the elder population. although caa is a common neuropathological finding on itself, it is also known to frequently occur in conjunction with alzheimer’s disease, being sometimes misdiagnosed. currently, caa diagnosis is generally conducted by post-mortem examination or, in live patients by the examination of an evacuated hematoma or brain biopsy samples, which are typically unavailable. therefore, a reliable and non-invasive method for diagnosing caa would facilitate the clinical decision making and accelerate the clinical intervention. the main goal of this dissertation is to study the application of machine learning (ml) to reveal possible biomarkers to aid the diagnosis and early medical intervention, and better understand the disease. therefore, three scenarios were tested: classification of four neurodegenerative diseases with annotation data obtained from visual rating scores, age and gender; classification of the diseases with radiomic data derived from the patient’s mri; and a combination of the previous experiments. the results show that the application of artificial intelligence in the medical field brings advantages to support the physicians in the decision making process and, at some point, make a correct prediction of the disease label. although the results are satisfactory, there are still improvements to be done. for instance, image segmentation of cerebral lesions or brain regions and additional clinical information of the patients would be of value.",
      "angiopatia amiloide cerebral (aac) é uma doença vascular cerebral resultante da deposição de matéria amiloide. principal causa de hemorragias cerebral espontâneas, a aac manifesta se predominantemente na população idosa. embora a aac seja uma doença que por si só tem um grande impacto no grupo etário referido, ocorre em simultâneo com inúmeras outras doenças neurodegenerativas, como a doença de alzheimer. atualmente, o diagnóstico de aac realiza-se quer em post-mortem, quer em pacientes vivos. no entanto, o diagnóstico em vida é conseguido por meio de biópsias de tecidos cerebrais, sendo um método invasivo, o que dificulta a intervenção clínica. deste modo, torna-se imperativa a procura de alternativas fiáveis e não invasivas em vida para auxiliar o diagnóstico da doença e permitir a melhoria da qualidade de vida do paciente. perante os progressos na área da tecnologia e medicina, esta dissertação propõe o estudo da aplicação de algoritmos de machine learning (ml) para revelar possíveis biomarcadores para auxiliar o diagnóstico e permitir uma intervenção precoce. deste modo, foram testados três cenários distintos: a classificação de quatro doenças neurodegenerativas com dados anotados obtidos a partir de métricas visuais de avaliação da atrofia, idade e sexo; a classificação das doenças com dados gerados a partir de métodos radiómicos; e uma combinação das duas abordagens anteriores. neste documento apresenta-se e discute-se os resultados obtidos com a aplicação de quatro diferentes algoritmos de ml que visam a deteção automática da doença associada à imagem testada. adicionalmente, é feita uma análise crítica de quais as características mais relevantes que levaram à tomada de decisão por parte do algoritmo. os resultados demonstram que através de aplicação de metodologias automáticas é possível o auxílio ao diagnostico médico por especialistas e, no limite, a realização de diagnostico automático com elevada precisão. finalmente, são apresentadas possíveis alternativas de trabalho futuro para que os resultados possam ser aperfeiçoados, como por exemplo, a segmentação das regiões de interesse, i.e., identificação das lesões, aquando da anotação por especialistas. mediante a inclusão dessa segmentação, uma vez que será mais especifica, os resultados serão, por sua vez, aprimorados."
    ],
    [
      "em 2008 é posto em marcha um projecto de inovação e desenvolvimento denominado por inpact entre a efacec, a edp distribuição e a universidade do minho. o objectivo deste projecto é fornecer um conjunto de ferramentas de engenharia para programação dos sistemas de protecção, automação e controlo de sistemas de energia. pretendia-se que as ferramentas suportassem a gestão completa dos sistemas da efacec, baseadas nas normas internacionais iec 61850, iec 61131-3, iec 61499 e iec 60870-5. nesse âmbito, a efacec criou o automation studio, um ambiente de desenvolvimento integrado, desenvolvido em linguagem c# da framework .net da microsoft, sendo as ferramentas integradas nesse ambiente como plugins. entre as ferramentas desenvolvidas, conta-se um editor de sinópticos. tendo este sido desenvolvido em c# .net, utiliza xaml para a descrição dos diagramas. no entanto, dos equipamentos produzidos pela efacec, mais concretamente, a plataforma para automação e controlo de sistemas de energia e gestor de sistemas scada uc 500, utiliza svg para a visualização e interacção com os sinópticos. assim, embora o editor permita criar os sinópticos para a plataforma uc 500, as linguagens utilizadas não são compatíveis. para ultrapassar estes problemas de interoperabilidade, entre o editor e a plataforma, surgiu a necessidade de desenvolver um compilador xaml para svg. o objectivo do trabalho, desenvolvido no âmbito desta dissertação, foi então o desenvolvimento do referido compilador de xaml para svg. este deveria ser integrável no ambiente de edição do automation studio para, desta forma, permitir a configuração de diversos equipamentos da efacec, em particular da plataforma uc 500, a partir desse ambiente de desenvolvimento integrado. após várias fases de testes e de melhoramentos, o compilador foi definitivamente integrado no editor automation studio na sua versão 2.0 e seguintes. o resultado positivo deste projecto é visível pela utilização actual em dois exemplos reais, um na subestação de ermesinde e outro no bahrain, ambos apresentados neste documento.",
      "in 2008 an innovation and development project, inpact, is set in motion between efacec, edp distribution and the university of minho. the aim of this project is to provide a set of engineering tools for programming of power systems’ protection systems, automation and control solutions. it was intended that the tools could support the complete management of efacec systems, based on international standards iec 61850, iec 61131-3, iec 61499 and iec 60870-5. in this context, efacec created automation studio, an integrated development environment, developed in the c# language using .net technology. different tools were integrated into this environment as plugins. among the tools developed, there was a synoptics editor. this editor was developed in c# .net, and xaml is used to describe the diagrams. however, the equipment produced by efacec, more specifically, the uc 500 platform for power systems automation and control and scada systems manager, uses svg for the visualization and interaction with the synoptics. thus, although the editor supports creating the synoptics for the uc 500 platform, the languages used are not compatible. to overcome these interoperability problems between the publisher and the platform, the need arose to develop a xaml to svg compiler. the aim of the work developed within this dissertation, then, was the development of the xaml to svg compiler. this compiler should be integrated in the editing environment for automation studio, thus allowing configuration of several efacec devices, particularly the uc 500 platform, from this integrated development environment. after several phases of testing and improvement, the compiler was definitely integrated into the automation studio editor in its 2.0 version. the positive outcome of this project is visible in its current application in two concrete examples, one in the ermesinde substation and another in bahrain, both presented in this paper."
    ],
    0.0
  ],
  [
    [
      "the aim of this master work is to implement liss language in antlr compiler generator system using an attribute grammar which create an abstract syntax tree (ast) and generate mips assembly code for mars (mips assembler and runtime simulator) . using that ast, it is possible to create a syntax directed editor (sde) in order to provide the typical help of a structured editor which controls the writing according to language syntax as defined by the underlying context free grammar.",
      "o tema desta dissertação é implementar a linguagem liss em antlr com um gramática de atributos e no qual, irá criar uma árvore sintática abstrata e gerar mips assembly código para mars (mips assembler and runtime simulator). usando esta árvore sintática abstrata, criaremos uma sde (editor dirigido a sintaxe) no qual fornecerá toda a ajuda típica de um editor estruturado que controlará a escrita de acordo com a gramática."
    ],
    [
      "at the moment, there is a vast amount of archival data spread across the portuguese archives, which keeps information from our ancestors’ times to the present day. most of this information was already transcribed to digital format, and the public can access it through archives’ online repositories. despite that, some of these documents are structured with many plain text fields without any annotations, making their content analyses difficult. in this thesis, we implemented several named entity recognition solutions to perform a semantic interpretation of the archival finding aids by extracting named entities like person, place, date, profession, and organization. these entities translate into crucial information about the context in which they are inserted. they can be used for several purposes with high confidence results, such as creating smart browsing tools by using entity linking and record linking techniques. in this way, the main challenge of this work was the creation of powerful ner models capable of producing high confidence results. in order to achieve high result scores, we annotated several corpora to train our machine learning algorithms in the archival domain. we also used different ml architectures such as maxent, cnns, lstms, and bert models. during the model’s validation, we created different environments to test the effect of the context proximity in the training data. finally, during the model’s training, we noticed a lack of available portuguese annotated data, limiting the potential of several nlp tasks. in this way, we developed an intelligent corpus annotator that uses one of our ner models to assist and accelerate the annotation process.",
      "de momento, existe uma vasta quantidade de dados arquivísticos espalhados pelos arquivos portugueses, que guardam informações desde os tempos dos nossos antepassados até aos dias de hoje. a maior parte desta informação já foi transcrita para o formato digital e encontra-se disponível ao público através de repositórios online dos arquivos. apesar disso, alguns destes documentos estão estruturados com muitos campos de texto livre, sem quaisquer anotações, o que pode dificultar a análise do seu conteúdo. nesta tese, implementamos várias soluções de reconhecimento de entidades mencionadas, a fim de se realizar uma interpretação semântica sobre descrições arquivísticas, extraindo entidades tais como pessoa, local, data, profissão e organização. estes tipos de entidades traduzem-se em informação crucial sobre o contexto em que estão inseridas. com métricas de confiança suficientemente elevadas, estas entidades podem ser utilizadas para diversos fins, como a criação de ferramentas de navegação inteligente por meio de técnicas de entity linking e record linking. desta forma, o principal desafio deste trabalho consistiu na criação de poderosos modelos ner que fossem capazes de produzir resultados de elevada confiança. para alcançar tais resultados, anotamos vários datasets para treinar os nossos próprios algoritmos de aprendizado de máquina no contexto arquivístico. para além disso, usamos diferentes arquiteturas de ml tais como maxent, cnns, lstms e bert. durante a validação do modelo, criamos diferentes ambientes de teste de modo a testar o efeito da proximidade de contexto nos dados de treino. por fim, durante o treino dos modelos verificamos que existe pouca quantidade de dados disponíveis anotados em português, o que pode limitar o potencial de várias tarefas de nlp. desta forma, desenvolvemos um anotador de datasets inteligente que utiliza um dos nossos modelos de ner para auxiliar e acelerar o processo de anotação."
    ],
    0.3
  ],
  [
    [
      "the chest radiography is one of the most frequently requested hospital exams, playing a pivotal role in the diagnosis, management, and monitoring of respiratory diseases in patients presenting with respiratory symptoms. harnessing the power of deep learning methodologies has revolutionized the field of medical imaging analysis, enabling the development of robust models for accurate disease prediction and clas sification, encompassing an extensive array of conditions. the inherent strength of deep learning lies in its ability to automatically extract intricate features from complex datasets, thereby facilitating enhanced accuracy without the need for extensive human intervention. this study delves into the intricate mechanisms of a chest x-ray classification model, highlighting a noteworthy inclination toward seemingly inconsequential regions of the x-rays, particularly fixating on non-diagnostic elements such as letters and symbols. while the elimination of these artifacts did not substantially alter the model’s classification outcomes, it raises pertinent inquiries about the potential benefits of a preprocessed dataset devoid of such distractions. this research also scrutinizes the visualization methods employed, unraveling an intriguing tendency of the model to focus on regions comprised entirely of black pixels in certain cases. this observation insti gated a comprehensive examination of the efficacy of the visualization methods, uncovering a substantial misalignment between the generated hotspots and the annotations provided by radiologists. notably, the study identifies that the size of the bounding box and the model’s classification success rate share a strong correlation, indicating that larger bounding boxes generally demonstrate better alignment. additionally, we establish a direct relationship between the model’s classification accuracy and the alignment with visualization techniques. this analysis not only deepens our understanding of the dynamics at play within chest x-ray analysis but also underscores the essential considerations necessary for refining the training and visualization processes in machine learning-based diagnostic tools.",
      "a radiografia torácica é um dos exames hospitalares mais frequentemente solicitados, desempenhando um papel crucial no diagnóstico, tratamento e monitorização de doenças respiratórias em pacientes com sintomas respiratórios. a utilização de metodologias de aprendizagem profunda revolucionou o campo da análise de imagens médicas, permitindo o desenvolvimento de modelos robustos para previsão e classifi cação precisa de doenças, abrangendo uma ampla gama de condições. a força inerente da aprendizagem profunda reside na sua capacidade de extrair automaticamente características complexas de conjuntos de dados complexos, facilitando assim uma precisão aprimorada sem a necessidade de uma extensa intervenção humana. este estudo explora os mecanismos intricados de um modelo de classificação de radiografia torá cica, destacando uma inclinação notável para regiões aparentemente inconsequentes das radiografias, fixando-se particularmente em elementos não diagnósticos como letras e símbolos. embora a eliminação desses artefatos não tenha alterado substancialmente os resultados de classificação do modelo, levanta questões pertinentes sobre os potenciais benefícios de um conjunto de dados pré-processado livre de tais distrações. esta pesquisa também analisa os métodos de visualização utilizados, revelando uma tendência intri gante do modelo de se concentrar em regiões compostas inteiramente por pixels pretos em certos casos. esta observação instigou um exame abrangente da eficácia dos métodos de visualização, revelando um desalinhamento substancial entre os pontos quentes gerados e as anotações fornecidas pelos radiolo gistas. notavelmente, o estudo identifica que o tamanho da caixa delimitadora e a taxa de sucesso de classificação do modelo compartilham uma forte correlação, indicando que caixas delimitadoras maiores geralmente demonstram melhor alinhamento. além disso, estabelecemos uma relação direta entre a precisão de classificação do modelo e o alinhamento com as técnicas de visualização. esta análise não só aprofunda a nossa compreensão da dinâmica em jogo na análise de radiografias torácicas, mas também destaca as considerações essenciais necessárias para aprimorar os processos de treinamento e visualização em ferramentas de diagnóstico baseadas em aprendizagem de máquina."
    ],
    [
      "o crescimento da complexidade de aplicações informáticas tornou falhas e erros de software uma inevitabilidade. para ajudar a garantir que uma aplicação funciona como previsto, profissionais recorrem a modelos de software para detetar e corrigir problemas nas fases iniciais de desenvolvimento. especificações formais são modelos de software que permitem a desenvolvedores especificar rigorosamente estruturas e comportamentos de software. infelizmente, a sua complexidade inerente também pode impor problemas nos principiantes que as tentam aprender. uma maneira possível de abordar este problema seria o emprego de práticas de reparação de especificações e geração automática de sugestões para ajudar os alunos a corrigir tentativas erradas. alloy4fun é uma plataforma online para a aprendizagem de alloy, uma linguagem de especificação formal com capacidades de analise automática. alloy4fun permite a instrutores criar e partilhar desafios de especificação formal com avaliação automática. recentemente, uma técnica de geração automática de sugestões for desenvolvida para esta plataforma, mas provou ser insatisfatória devido ao seu fraco desempenho. o objetivo desta tese foi explorar outras técnicas para geração de sugestões, nomeadamente técnicas de geração de sugestões baseadas em dados, que poderiam usar o conjunto de dados publico de submissões históricas de estudantes do aloy4fun para fornecer dicas de forma mais eficiente. o principal resultado desta tese, specassistant, é um novo sistema de geração de dicas baseado em dados para alloy. este extrai informação do conjunto de dados do alloy4fun para construir grafos de submissões, dos quais são extraídas sugestões a partir de regras personalizadas pelos desenvolvedores de cada desafio. para avaliar o specassistant, realizamos uma série de experiências quantitativas, com o objetivo de avaliar a disponibilidade e o desempenho do nosso sistema. as nossas descobertas demostram que o specassistant consegue fornecer dicas para uma porção significativa de submissões, apresentado um desempenho que supera o sistema de sugestões precedente.",
      "the increasing complexity of software applications has made software bugs and errors an inevitability. to help ensure that software functions as intended, professionals rely on software models to detect and correct faults early in the development process. formal specifications are software models that allow developers to precisely specify software structures and behaviors. unfortunately, their inherent complexity can also pose problems to newcomers while learning them. one possible way to address this issue could be to employ automated hint and specification repair techniques to help students fix incorrect attempts. alloy4fun is an online platform for learning alloy, a formal specification language with automated analysis features. alloy4fun allows educators to create and share specification challenges with automated assessment. recently, a hint generation technique based on automated repair has been developed for this platform, but proved unsatisfactory due to its poor performance. the goal of this thesis was to explore other techniques for hint generation, namely data-driven hint generation techniques which could leverage the alloy4fun public data-set of past student submissions to provide hints more efficiently. the main outcome of this thesis, specassistant, is a new data-driven hint generation system for alloy. it mines the alloy4fun data-set to build submission graphs, from which hints are extracted using policy rules customized by the challenge developers. to evaluate specassistant we performed a series of quantitative experiments, with the goal of assessing our system’s availability and performance. our findings show that specassistant can provide hints for a significant portion of invalid submissions with a performance that greatly surpasses that of the previous hint system."
    ],
    0.3
  ],
  [
    [
      "o koha é um software com interface web para gestão de bibliotecas. este software é utilizado em várias bibliotecas, incluindo as bibliotecas de várias universidades como a universidade do minho, e entidades governamentais como o catálogo coletivo das bibliotecas nos açores. nesta dissertação pretende-se adi cionar um sistema de recomendação, onde após se fazer uma pesquisa e selecionar uma obra, mostrar uma lista de recomendações similares de acordo com as várias informações da obra, podendo este sis tema eventualmente ser separado do koha e complementar outros sistemas de software de gestão de bibliotecas. para a criação deste sistema de recomendação foram estudados vários exemplos de outros sistemas de recomendação já existentes. adicionalmente, também foram estudados os requisitos reque ridos por estes e por este projeto em particular. estes estudos ajudaram a obter o melhor funcionamento possível do sistema de recomendação.",
      "koha is a software with a web interface for library management. this software is utilized in various libraries including various universities such as university of minho and governmental institutions such as catálogo coletivo de bibliotecas in açores. the objective of this dissertation is adding a recomendaton system such that after researching and choosing the book, it shows a list of similar recomendations according to the various informations of the book, potentially this system might be separated from koha, and complement other online library management software. for the creation of the recommendation system various already existing recommendation systems were analysed including the necessary requirements needed for their and this project’s correct operation."
    ],
    [
      "devido à elevada proliferação tecnológica, existe software criptográfico implementado numa miríade de plataformas. plataformas essas que podem ter características de computação bastante diferentes. no entanto, o software criptográfico deverá ser imperceptível ao utilizador final, uma vez que deve funcionar como uma camada de protecção às tarefas que o utilizador possa estar a realizar e não aumentar a sua pegada computacional. essa exigência leva muitas vezes a que o software criptográfico tenha que ser re-implementado de acordo com as capacidades computacionais da plataforma/dispositivo-alvo (sendo até comum recorrer-se a assembly para atingir tal objectivo). o desenvolvimento de software criptográfico também exige que o programador seja versado em diversas áreas da ciência, devendo assim ser realizado por criptógrafos especializados. o cao é uma dsl imperativa para a área da criptografia. munido de um compilador, interpretador e uma ferramenta de verificação formal, o cao permite a passagem de conhecimento criptográfico para um programador com menos experiência na área da criptografia através da automatização da validação formal das implementações. o compilador cao possui um back-end altamente configurável que permite geração de executáveis dedicados às plataformas destino. com a grande utilização actual de processadores arm, torna-se interessante estudar formas de explorar as características destes processadores e desenvolver um back-end que as implemente com o objectivo de obter melhor desempenho do software criptográfico. nesta dissertação explorou-se a utilização dos tipos de dados vectoriais e instruções do co-processador neon de forma a obter paralelismo ao nível dos dados. também foi estudada a possibilidade de inclusão de paralelismo nativo ao nível das tarefas no cao, de forma a tirar partido das arquitecturas multi-core.",
      "with the current technological proliferation, cryptography started to be incorporated in wide range of devices, from embedded processors to high-end servers. as cryptographic software needs to be as close to invisible as possible in terms of computational footprint, it must be highly optimised for each specific platform/device. it is not uncommon to see cryptographic code written in assembly to achieve such goal. furthermore, the development of cryptographic software needs the programmer to be well-versed in various domains of science as mathematics, computer science and electrical engineering. with that in mind, the cao language was developed. the cao language is a imperative dsl tailored for the cryptographic domain. backed-up by a compiler, interactive interpreter and a formal verification tool, the cao language is able to help an inexperienced programmer in the development of cryptographic software with the automisation of the formal validation of the implementations. the cao compiler has a highly-tunable back-end that can be optimized to deliver executables tailored for specific platforms/devices. since the arm architectures are one of the most common processor architectures used in embedded devices, the main goal of this dissertation is the development of a cao back-end that takes advantage of the characteristics of these processors in order to generate executables of higher performance for cryptographic software. this dissertation explores the usage of the neon co-processor’s instructions and datatypes as means to attain data-level parallelism. it also studies the possibility of enabling the use of task-level parallelism as a feature of the cao language, in order to better explore multi-core architectures."
    ],
    0.3
  ],
  [
    [
      "atualmente, principalmente nos países mais desenvolvidos, as águas residuais são tratadas através das estação de tratamento de águas residuais (etars) de forma a tentar atenuar os efeitos da atividade humana na poluição da água potável. as etars são infraestruturas que desempenham um papel funda mental e imprescindível para a sociedade, pois estas permitem que a água potável já utilizada em diversas atividades e no uso doméstico possa retornar ao seu habitat natural nas melhores condições, podendo ser reaproveitada. contudo, uma etar para realizar a sua função requer elevados consumos energéticos, devido a todo o processo de tratamento que é auxiliado com um número elevado de equipamentos, desde das águas residuais afluentes até ao seu destino final. esses consumos oscilam consoante o tipo de afluente, sendo importante analisar através de um controlo analítico, quais as substâncias que, quando presentes no afluente, requerem maior tratamento, e consequentemente maior consumo de energia. com isto, o objetivo desta dissertação é, através do uso de data science, elaborar um forte controlo analítico às substâncias do afluente e relacionar o mesmo com a oscilação do consumo energético, tornando as etars mais eficientes e sustentáveis.",
      "currently, mainly in more developed countries, wastewater is treated through wastewater treatment plants (wwtps) to mitigate the effects of human activity on the source of drinking water. wwtps are infrastructures that play a fundamental and indispensable role for society, as these allow the water supplied in various activities and for domestic use to return to its natural habitat in the best conditions, to be able to reuse it. however, a wwtps to perform its function requires high energy consumption due to the entire treat ment process assisted with an increased number of equipment, from the inflow of the affluent wastewater to its final destination. these consumptions vary depending on the affluent, being essential the analy sis through an analytical control, which substances, when present in the affluent, require more effective treatment and consequently greater energy consumption. with this, this dissertation’s objective is, through the use of data science, to elaborate a robust ana lytical control to the affluent substances and relate it with the oscillation of energy consumption, making the wwtps more efficient and sustainable."
    ],
    [
      "a literatura científica na biomedicina é um elemento fundamental no processo de obtenção de conhecimento, uma vez que é a maior e mais confiável fonte de informação. com os avanços tecnológicos e o aumento da competição profissional, o volume e diversidade de documentos médicos científicos tem vindo a aumentar consideravelmente, impedindo que os investigadores acompanhem o crescimento da bibliografia. para contornar esta situação e reduzir o tempo gasto pelos profissionais na extração dos dados e na revisão da literatura, surgiram os conceitos de web crawling, web scraping e processamento de linguagem natural, que permitem, respetivamente, a procura, extração e processamento automático de grandes quantidades de texto, abrangendo uma maior gama de documentos científicos do que os normalmente analisados de forma manual. o trabalho desenvolvido para a presente dissertação teve como foco principal o rastreamento e recolha de documentos científicos completos, do campo da biomedicina. como a maioria dos repositórios da web não disponibiliza, gratuitamente, a totalidade de um documento, mas sim apenas o resumo da publicação, foi importante a seleção de uma base de dados adequada. por este motivo, as páginas web alvo de rastreamento foram restringidas ao domínio dos repositórios da editora biomed central, que disponibilizam por completo, milhares de documentos científicos na área da biomedicina. a arquitetura do sistema desenvolvido divide-se em duas partes principais: fase online e a fase offline. a primeira inclui a procura e extração dos urls das páginas candidatas a serem extraídas, a recolha dos campos de texto pretendidos e o seu armazenamento numa base de dados. a segunda fase consiste no tratamento e limpeza dos documentos recolhidos, deixando-os num formato estruturado e válido para ser utilizado como entrada de qualquer sistema de análise de texto. para a concretização da primeira parte, foram utilizadas a framework scrapy, como base para a construção do scraper, e a base de dados de documentos mongodb, para o armazenamento das publicações científicas recolhidas. na segunda etapa do processo, ou seja, na aplicação de técnicas de limpeza e padronização dos dados, foram aproveitadas algumas das inúmeras bibliotecas e funcionalidades que a linguagem python oferece. para demonstrar o funcionamento do sistema de extração e tratamento de documentos da área médica, foi estudado o caso prático de recolha de publicações científicas relacionadas com transtornos obsessivo compulsivos. como resultado de todo o procedimento, foi obtida uma base de dados com quatro coleções de documentos com diferentes níveis de processamento.",
      "the scientific literature in biomedicine is a fundamental element in the process of obtaining knowledge, since it is the largest and most reliable source of information. with technological advances and increasing professional competition, the volume and diversity of scientific medical documents increased considerably, preventing researchers from keeping up with the growth of bibliography. to circumvent this situation and reduce the time spent by professionals in data extraction and literature review, the concepts of web crawling, web scraping and natural language processing have emerged, which allow, respectively, the search, extraction and automatic processing of large text, covering a wider range of scientific documents than those normally handled. the work developed for the present dissertation focused on the crawling and collection of complete scientific documents from the field of biomedicine. as most web repositories do not provide the entire document for free, but only the abstract of the publication, it was important to select an appropriate database. for this reason, the crawled web pages have been restricted to the domain of biomed central repositories, which provide thousands of scientific papers in the field of biomedicine. the system architecture in question is divided into two main parts: the online phase and the offline phase. the first one includes searching and extracting the urls of the candidate pages to be extracted, collecting the desired text fields and storing them in a database. the second phase is the handling and cleaning of the collected documents, leaving them in a structured and valid format to be used as input to any text analysis system. for the realization of the first part, it was used the scrapy framework as the basis for the construction of the scraper and the mongodb document database for storing the collected scientific publications. in the second step of the process, that is, for the application of data cleaning and standardization techniques, some of the numerous libraries and functionalities that the python language offers are taken advantage of. in order to demonstrate the operation of the document extraction system, the practical case of collecting scientific publications related to obsessive compulsive disorders was studied. as a result of the entire procedure, a database with four document collections with different processing levels was obtained."
    ],
    0.12857142857142856
  ],
  [
    [
      "o termo microserviços não é propriamente recente, existem inúmeras referências ao longo da última década sobre este conceito, no entanto não existe um verdadeiro consenso sobre quem foi o primeiro a introduzir esta abordagem. independentemente da indefinição sobre o autor, as vantagens e os desafios da sua utilização como base ao desenvolvimento de novas aplicações são hoje bem conhecidos. é também possível verificar que esta arquitetura de software, que inicialmente era mais utilizada em desenvolvimentos nativos para a cloud, é cada vez mais utilizada em centros de dados locais, o que lança novos desafios às infraestruturas de rede dos centros de dados. o simples facto dos microserviços serem independentes entre si, permite que sejam desenvolvidos, distribuídos e atualizados individualmente, desta forma conseguimos atualizações mais rápidas e com maior frequência, endereçando a constante mudança de requisitos aplicacionais que se verifica em variadíssimas áreas de negócio. no entanto a adoção de novas plataformas deve garantir que estes novos paradigmas integram, e idealmente beneficiam de tecnologias ou soluções já existentes. num ambiente altamente distribuído, como é o caso de arquiteturas baseadas em microserviços, é evidente que a componente de comunicações tem um papel preponderante na qualidade do serviço, pelo que nos casos em que o centro de dados onde se pretende utilizar a plataforma de orquestração utiliza redes baseadas em software (sdn), o ideal é que as soluções integrem de forma bastante profunda. esta integração é ainda mais relevante se o referido centro de dados apresentar uma arquitetura híbrida, isto é, composto por capacidade de computação em múltiplos datacenter físicos, mas também em provedores de clouds públicas (azure, aws, google cloud, etc.). este trabalho pretende enumerar os principais desafios à utilização de containers em centros de dados, bem como descrever a melhor forma de integrar a solução de gestão de rede de centros de dados do fabricante cisco (aci - application centric infrastructure) com a solução de orquestração de containers mais utilizada atualmente (kubernetes). é também âmbito deste trabalho apresentar uma proposta à integração do ambiente descrito anteriormente (aci+kubernetes) com soluções de orquestração de containers alojados em clouds públicas, nomeadamente na cloud da microsoft (azure).",
      "the term microservices isn't exactly new, there are innumerous references in the last decade about this concept, however there isn't a true understanding about who was the first to introduce this approach. nevertheless, the advantageous and challenges of adopting this methodology, that started to be use more in cloud native applications, but rapidly extended to all type of applications regardless of the location where they will be executed, creating a new challenge to the network infrastructure in the datacenter. the simple fact that the microservices are independent among themselves, allows them to be developed, distributed and updated individually, this way we can have faster and frequent updates, addressing the constant need of reviewing application requisites that we see happening in several business areas. however, the adoption of this new platforms must consider or even benefit, with the integration with the already existing technologies. in a highly distributed environment, like the microservices architecture, it's clear that the communication aspect has a preponderant role in the overall quality of service, so in scenarios where the data center already as software defined network solution has, the ideal scenario is that this solution integrates with every solution that we add to the datacenter. this integration is even more relevant if the data center already has an hybrid architecture, combining several physical locations with public cloud providers (azure, aws, google cloud, etc.). this paper pretends to identify the main challenges to the usage of containers in data centers, as well as describe the best way of integrating the datacenter network management tool from cisco (aci - application centric infrastructure) with the main container orchestrator (kubernetes). it will also be presented in this paper a proposal on how to integrate the previous mentioned environment (aci+kubernetes) with orchestrators solutions managed by public cloud providers, namely microsoft azure."
    ],
    [
      "o processo de produção humana de voz é, resumidamente, o resultado da convolução entre o sinal de excitação, o impulso glótico, e a resposta impulsiva resultante da função de transferência do trato vocal. este modelo de produção de voz é frequentemente referido na literatura como um modelo fontefiltro, em que a fonte representa o fluxo de ar que sai dos pulmões e passa pela glote (espaço entre as pregas vocais), e o filtro retrata as ressonâncias do trato vocal e a radiação labial/nasal. estimar a forma do impulso glótico a partir do sinal de voz é de importância significativa em diversas áreas e aplicações, uma vez que as características de voz relacionadas, por exemplo, com a qualidade da voz, esforço vocal e distúrbios da voz, devem-se, principalmente, ao fluxo glotal. no entanto, este fluxo é um sinal difícil de determinar de forma direta e não invasiva. ao longo das últimas décadas foram desenvolvidos vários métodos para estimar o impulso glótico mas sem o desenvolvimento de um algoritmo eficiente e automático. a maioria dos métodos desenvolvidos baseia-se num processo designado por filtragem inversa. a filtragem inversa representa a desconvolução, ou seja, procura obter o sinal de entrada aplicando o inverso da função de transferência do trato vocal ao sinal de saída. apesar da simplicidade do conceito, o processo de filtragem inversa não é simples uma vez que o sinal de saída pode incluir ruído e não é alcançável modelar com precisão as características do filtro do trato vocal. nesta dissertação apresentamos um novo método de filtragem de um sinal de modo a melhorar um método robusto de estimação da fonte glótica, no domínio das frequências, que usa uma característica de fase baseada nos atrasos relativos normalizados (nrd) dos harmónicos. este modelo é aplicado a diversos sinais de voz (sintéticos e reais), e os resultados obtidos da estimação do impulso glótico são comparados com os obtidos usando outros métodos analisados no estado da arte com e sem o referido método de filtragem.",
      "the human speech production system is, briefly, the result of the convolution between the excitation signal, the glottal pulse, and the impulse response resulting from the transfer function of the vocal tract. this model of voice production is often mentioned in the literature as a source-filter model, where the source represents the flow of the air leaving the lungs and passing through the glottis (space between the vocal folds), and the filter stands for the resonances of the vocal tract and the lip/nostrils radiation. the estimation of the shape of the glottal pulse from the speech signal is of significant importance in many fields and applications, since the most important features of speech related to voice quality, vocal effort and speech disorders, for example, are mainly due to the voice source. unfortunately, the glottal flow waveform which is at the origin of the glottal pulse, is a very difficult signal to measure directly and non-invasively. several methods to achieve the estimation of the glottal flow have been proposed over the last decades, but an efficient and automatic algorithm which performs reliably is not yet available. most of the developed methods are based on the inverse filtering method. the inverse filtering approach represents a deconvolution process, i.e., it seeks to obtain the source signal by applying the inverse of the vocal tract transfer function to the output speech signal. despite the simplicity of the concept, the inverse filtering procedure is complex because the output signal may include noise and it is not straightforward to accurately model the characteristics of the vocal tract filter. in this dissertation we discuss a new filtering method for voiced signals with the goal to improve the assessment of a robust frequency-domain algorithm for glottal source estimation that uses a phaserelated feature based on the normalized relative delays (nrds) of the harmonics. this model is applied to several speech signals (synthetic and real), and the results of the estimation of the glottal pulse are compared with the ones obtained using other state-of-the-art methods with and without the presence of that filtering method."
    ],
    0.06666666666666667
  ],
  [
    [
      "o tema desta dissertação é a especificação de uma ontologia de genealogia, sendo que o principal objetivo é desenvolver uma ontologia. a ontologia será capaz de suportar a descrição da genealogia de um conjunto de indivíduos, desde dados pessoais passando pelas relações de parentesco até eventos significativos da vida de uma pessoa, podendo associar fotografias ou registos documentais que sejam de interesse. para além disso, pretendia-se desenvolver uma aplicação web que suportasse todas as operações de gestão e manutenção da ontologia. este tipo de aplicação pode ser usada como passatempo ou exercício de simples curiosidade pelo passado familiar ou como suporte de histórico familiar para fins médicos, genéticos, entre outros.",
      "the subject of this dissertation is the specification of an ontology of genealogy, the main objective being to develop an ontology. the ontology will be able to support the description of the genealogy of a group of individuals, from personal data going through the relations of kinship to significant events of a person’s life, and may also associate photographs or documentary records that are of interest. in addition, we intend to develop a web application that supports all ontology management and maintenance operations. this type of application can be used as a hobby or exercise of simple curiosity by the family past or as support of family history for medical, genetic, among others."
    ],
    [
      "traditional cryptographic standards are designed with a desktop and server environment in mind, so, with the relatively recent proliferation of small, resource constrained devices in the internet of things, sensor networks, embedded systems, and more, there has been a call for lightweight cryptographic standards with security, performance and resource requirements tailored for the highly-constrained environments these devices find themselves in. in 2015 the national institute of standards and technology began a standardization process in order to select one or more lightweight cryptographic algorithms. out of the original 57 submissions ten finalists remain, with ascon and romulus being among the most scrutinized out of them. in this dissertation i will introduce some concepts required for easy understanding of the body of work, do an up-to-date revision on the current situation on the standardization process from a security and performance standpoint, a description of ascon and romulus, and new best known analysis, and a comparison of the two, with their advantages, drawbacks, and unique traits.",
      "os padrões criptográficos tradicionais foram elaborados com um ambiente de computador e servidor em mente. com a proliferação de dispositivos de pequenas dimensões tanto na internet of things, redes de sensores e sistemas embutidos, apareceu uma necessidade para se definir padrões para algoritmos de criptografia leve, com prioridades de segurança, performance e gasto de recursos equilibrados para os ambientes altamente limitados em que estes dispositivos operam. em 2015 o national institute of standards and technology lançou um processo de estandardização com o objectivo de escolher um ou mais algoritmos de criptografia leve. das cinquenta e sete candidaturas originais sobram apenas dez finalistas, sendo ascon e romulus dois desses finalistas mais examinados. nesta dissertação irei introduzir alguns conceitos necessários para uma fácil compreensão do corpo deste trabalho, assim como uma revisão atualizada da situação atual do processo de estandardização de um ponto de vista tanto de segurança como de performance, uma descrição do ascon e do romulus assim como as suas melhores análises recentes e uma comparação entre os dois, frisando as suas vantagens, desvantagens e aspectos únicos."
    ],
    0.06666666666666667
  ],
  [
    [
      "a criptografia desempenha um papel importante na nossa sociedade, visto que é utilizada em sistemas de computação designados como críticos, que têm que funcionar mesmo na presença de erros. áreas como os sistemas bancários ou de saúde usam software e hardware, que têm que funcionar em todas as circunstâncias. o principal objetivo para o uso de criptografia nesses sistemas, é o de garantir a segurança da informação, que em muitos casos é sensível. nos últimos anos, foram surgindo linguagens de programação que se focam num domínio específico, chamadas de linguagens de domínio específico (dsls). no domínio da criptografia, apareceram as linguagens cryptol e cao, ambas ambicionando aumentar a produtividade dos programadores, mas também aumentar a comunicação entre estes e os especialistas do domínio. o cryptol é uma linguagem funcional e tem um conjunto de ferramentas associadas, compostas por um conjunto de ferramentas de verificação e de compilação para linguagens como c ou vhdl, que é uma linguagem descritiva de hardware. o cao é uma linguagem imperativa, com uma sintaxe idêntica à do c, e tem também um conjunto de ferramentas associado, que permite a introdução de operações de alto nível na linguagem, por exemplo. neste trabalho, essas duas linguagens foram abordadas, em particular as suas funcionalidades, e como podem ser usadas para implementar um algoritmo através da sua especificação. além disso, foi desenvolvida uma ferramenta de compilação que pretende transformar código fonte cao em código cryptol, de forma a compilá-lo para vhdl posteriormente. por fim, um caso de estudo que foca curvas elípticas para criptografia, foi utilizado para comparar as duas dsls e também para testar a ferramenta desenvolvida.",
      "cryptography plays an important role in our society, essentially because it is used in critical computer systems that must work properly, even in the face of errors or human mistakes. banking or health care are examples of areas which use hardware and software that must work in every situation. the main goal of the use of cryptography in those systems is to achieve information security, which in most cases is sensitive. in the last few years, programming languages that focus on a particular domain emerged, denominated as domain-specific languages (dsls). two dsls for cryptography appeared, cryptol and cao, both aiming to increase the productivity of developers, but also to improve the communication between them and domain experts. cryptol is a functional language and has an associated toolkit composed by a verification suite and compilation back ends to languages such as c or vhdl (a hardware description language). cao is an imperative language, with a syntax similar to c’s and also has an associated toolkit with tools that allows, for instance, the introduction of higher-order operations into the language. in this work, these two dsls for cryptography will be analysed, focusing on its features and how they can re-target a published algorithm into a specific implementation. furthermore, it was developed a compiler tool that aims to translate cao source code into cryptol, in order to compile it afterwards to vhdl. finally, a case study focusing on elliptic curve cryptography, was used to compare the two dsls and to test the developed tool."
    ],
    [
      "as estruturas de precipitados desempenham uma função fundamental nas ciências dos materiais devido à capacidade de obstruir o movimento de deslocamentos dentro do material. esta tese de mestrado debruça-se sobre uma aplicação baseada na mecânica estatística, nomeadamente o método monte carlo, no estudo e previsão do fenómeno de precipitação em ligas de alumínio. a liga de alumínio em estudo é a liga alumínio escândio. esta tese aborda temas como a mecânica computacional, mecânica estatística, ciências dos materiais, difusão, e ainda métodos de mineração de dados (data mining). a tese descreve as condições que influenciam a precipitação e como controlar este fenómeno. o resultado desta tese é um conjunto de aplicações de software que permitem (i) efetuar a simulação de monte carlo, (ii) analisar os resultados usando a técnica de mineração dbscan e (iii) comparar os resultados da simulação com a teoria clássica da nucleação. os resultados práticos obtidos com estas aplicações são: - relatórios da simulação, da análise dos clusters/precipitados com o algoritmo dbscan e da aplicação da teoria clássica da nucleação; - ficheiros para visualização 3d da simulação (em vários pontos ao longo do tempo); - ficheiros para visualização 3d dos precipitados. larry gonick é um cartoonista que desenhou cartoons para a revista discover. é autor de um vasto conjunto de livros das quais quero mencionar: “the cartoon guide to statistics” da qual esta figura foi retirada e que me parece adequado na forma como ilustra vários temas que esta tese aborda: estatística, aleatoriedade, “salto” e “barreira”.",
      "precipitate structures play a fundamental function in the material science due to the capacity of representing strong obstacles for dislocations movements within the material. this master thesis focuses on the elaboration and application of mechanical statistics knowledge, namely the kinetic monte carlo method, on the study and prediction of the phenomenon of precipitation in an aluminum alloy. the alloy under analysis is the aluminum scandium alloy. this thesis tackles subjects such as computational mechanics, mechanical statistics (the kinetic monte carlo method), material science, the precipitation phenomenon, the diffusion phenomenon, what influences this phenomenon and how to control it and also predict it, as well as data mining (namely clustering). the outcome of this thesis is a set of software applications that allow (i) to perform monte carlo simulations, (ii) to analyze the results using the dbscan clustering technique, and (iii) to compare the simulation results with the classical nucleation theory. practical results obtained with these applications are: - reports about the simulation, the analysis of clusters/precipitates with dbscan algorithm, and the application of the classical nucleation theory; - files for 3d visualization of the simulation (at various points over time); - files for 3d visualization of the precipitates. larry gonick is a cartoonist that sketched the bimonthly “science classics” cartoon for the science magazine discover besides being the author of several books as for example: the cartoon guide to statistics. the figure above was taken from this book and by which i do think it is very adequate in illustrating subjects addressed by this thesis, such as statistics, randomness, jump and barrier."
    ],
    0.0
  ],
  [
    [
      "uma fault tolerant network é uma estrutura de redes que tem como objetivo garantir a comunicação entre nodos de uma rede mesmo que esta seja propícia à perda de datagramas e à consequente perda de informação. estas perdas podem acontecer por vários motivos, mas este projeto tem como alvo analisar três casos, o da mobilidade de nodos na rede, o da conexão intermitente e o da conexão esporádica. todos estes ambientes onde se pretende assegurar a troca de informação entre nodos apresentam uma característica em comum, um possível volume elevado de perda de datagramas a qualquer instante que limita a quantidade de dados que podem ser trocados bem como a qualidade de serviço destas mesmas trocas de dados. esta é a principal característica que se pretende atenuar com o desenvolvimento deste projeto, porém existem outras relacionadas como a interrupção prolongada de uma transmissão e a sua retoma que também foram analisadas. como todos os ambientes referidos anteriormente apresentam adversidades semelhantes ou que podem ser tratadas como tal, optou-se pelo desenvolvimento de um protocolo de transferência de dados adaptado a tais adversidades. este encontra-se entre a camada de transporte e a camada de aplicações da network stack e pode ser utilizado como base para o desenvolvimento de arquiteturas que possibilitem a troca organizada de informação entre nodos. neste projeto foi desenhado e implementado um protocolo de transferência de dados que possibilita a troca de informação nos ambientes anteriormente referidos, apresentando resiliência a drops de datagramas, grandes delays na transmissão destes e movimentações de nodos na rede. para além deste protocolo, foi desenhada uma simples arquitetura de redes baseada em redes ad hoc onde cada nodo tem uma visualização da rede centralizada nele próprio e os restantes nodos visíveis encontram-se organizados em níveis de vizinhança consoante a sua distância ao nodo central. foi tamb´em definido o objetivo de desenho de uma arquitetura de redes baseada em redes dtn, que fortemente influenciou o protocolo de transferência de dados devido às suas grandes exigências. por fim foram realizados testes em determinados cenários reais pertinentes ao protótipo implementado de forma a provar que os objetivos delineados inicialmente foram atingidos.",
      "a fault tolerant network is a network structure that aims to guarantee node communication in certain network environments that are prone to datagram drops and consequent loss of information. these drops can be present due to a multitude of reasons but this project aims to analyze three specific cases where these can occur, node mobility, intermittent network connection and sporadic network connection. all these cases present the same challenge to node communication, there’s a possibility of a high volume of datagram drops that can happen unexpectedly, limits the amount of data that can be exchanged between nodes and lowers the quality of service of said exchanges. the main objective of this project is to address these limitations that are innate in these specific cases as well as other topics that are derivative of said limitations such as extensive interruptions in data transfers and subsequent resume of these. since all the cases described previously present the same challenges, or they can be treated like they are similar, it was decided to develop a data transfer protocol fitted for these limitations. this protocol is between the transport and application layers of the network stack and it’s purpose is to improve data transfer and reception between nodes. in this project, a data transfer protocol was designed and implemented that enables data transmission between nodes in the cases previously referred, showing resilience against datagram drops, transmission delays and node mobility. in addition to this protocol, a simple ad hoc network architecture was designed using said protocol where each node has a view of the network centralized onto itself and the neighboring nodes organized in neighbor levels depending on their distance to the central node. the objective of designing a simple version of a dtn was also established bringing new challenges that strongly influenced the design of the data transfer protocol due to its strict requirements. lastly, some tests were performed in real world scenarios in such ways that all the features designed and developed in this project could be shown to be working properly and that all objectives delined were met."
    ],
    "delay tolerant network (dtn) is a small regional network designed to provide better communications when the end-to-end connection is not always possible. dtn is well known for intermittent connections and long delays. nodes store data packets in the buffers and forward later when the connection is restored. recently, named data networking (ndn) has been drawing wide attention as a future internet architecture. this architecture shifts the emphasis from host to content and pays little attention to where is the content. routing in ndn is based on the name of the content. named data-delay tolerant (nd-dt) network is an integration of dtn and ndn. it takes the advantages of both architectures by applying named data approach in dtn scenarios. in nd-dt network, distributed databases are maintained by a group of fixed or moving nodes. data inconsistency always exists because of the intermittent connections and long delays. however, data synchronization solutions can minimize this inconsistency, helping to reduce the data access delay. chronosync is a well-known ndn state synchronization protocol. data synchronization in nd-dt networks are challenging because of the intermittent connections and the nodes’ mobility. moreover, the connection between nodes is not assured, which may make synchronization to fail. in this work, it is assumed that there is at least one path between each pair of database nodes. the aim of this work is to improve the recovery process of chronosync in order to enhance its adaptability to nd-dt network scenarios. for this goal, chronosync and our improved solution were implemented and tested on an nd-dt network simulator. the results show that our improved chronosync is more adaptable to nd-dt networks. the improved chronosync consumes less time to finish synchronization tasks in all the scenarios. what’s more, in three database scenarios, ichronosync decreasing about 83% of the synchronization time while chronosync decreases 62% when changed from sparse network to dense network. what’s more, improved chronosync generates 27% fewer data packets, which can increase the probability of other network nodes getting connected.",
    0.3
  ],
  [
    [
      "the number of personal data circulating through computer applications and web today is quite large, leading the european parliament to propose and approve a regulation aimed at protecting this data. the general data protection regulation (gdpr) is a new european legal framework that came into force on may 25, 2018 that focuses on the protection, collection and management of personal data, i.e. data about individuals. this regulation applies to all companies and organizations in the european union this master’s thesis in computer engineering focused on creating a solution that has as its main objective to facilitate the work of the people who are responsible for monitoring and ensuring that the regulation is being complied with. this solution emerged in a work context from a sharing of ideas between me and the idealmais entity, which proposed my integration in the architecture and development team of the solution. the developed backend has as main features the management of measures, customers and compliance reports, i.e. reports intended to indicate the measures, their status and the actions that should be taken, serving as support for certification.",
      "o número de dados pessoais que hoje em dia circula pelas aplicações informáticas e web é bastante grande, levando o parlamento europeu a propor e aprovar um regulamento destinado à proteção desses mesmos dados. o regulamento geral de proteção de dados (rgpd) é um novo quadro jurídico europeu que entrou em vigor a 25 de maio de 2018 e que se centra na proteção, na recolha e na gestão de dados pessoais, ou seja, dados sobre indivíduos. este regulamento aplica-se a todas as empresas e organizações da união europeia esta tese de mestrado em engenharia informática incidiu na criação de uma solução que tem como principal objetivo facilitar o trabalho das pessoas que são responsáveis por controlar e garantir que o regulamento está a ser cumprido. esta solução surgiu em contexto laboral a partir de uma partilha de ideias entre mim e a entidade idealmais, que propôs a minha integração na equipa de arquitetura e desenvolvimento de backend da solução em causa. o backend desenvolvido tem como principais características a gestão de medidas, clientes e relatórios de conformidade, isto é, relatórios destinados a indicar as medidas, o seu estado e as ações que devem ser tomadas, servindo de apoio à certificação."
    ],
    [
      "o ambiente de grande competitividade característico do sector do retalho e crescente dificuldade na captação de novos clientes leva as empresas a apostar na implementação de estratégias adequadas para promover a satisfação dos clientes adquiridos para motivar a sua lealdade. é neste contexto que se começa a reconhecer a importância de combater o fenómeno de churn, ou seja, a perda de clientes. é necessário identificar os clientes que estão em risco de churn e, para isso, é necessário criar um método que o permita fazer com antecedência para que possam recair sobre eles as campanhas de retenção proactivas. quanto mais eficaz for o método a identificar os clientes em riscos, maior será o retorno da aplicação da campanha. muitos trabalhos têm sido desenvolvidos na área de previsão de churn nos mais diversos sectores. contudo, na área do retalho a pesquisa têm sido muito limitada. assim, com este trabalho de dissertação pretendeu-se estudar o fenómeno da perda de clientes com o objectivo de definir e implementar um modelo de churning para o sector do retalho recorrendo a técnicas de mineração de dados. pretendeu-se fazer um levantamento das principais questões envolvidas na previsão de churn no retalho, na construção do conjunto de dados (assinaturas dos clientes) e na aplicação de técnicas de mineração de dados no processo de previsão. nesse sentido, foram construídos alguns modelos para fazer a previsão de casos de churn baseados em cinco das técnicas de classificação mais utilizadas em trabalhos de previsão de churn: árvores de decisão, regressão logística, redes neuronais, random forests e svm. a avaliação e comparação da performance dos modelos elaborados foi feita de acordo com várias medidas como accuracy, precision, sensitivity, specificity, f-measure e auc e, para além disso, foi testado o impacto, na precisão do modelo, da alteração da densidade de eventos de churn no conjunto de treino.",
      "the great competitive environment characteristic of the retail sector and increasing difficulty in attracting new customers leads firms to invest in the implementation of appropriate strategies to promote customer satisfaction to motivate their loyalty. it is in this context that we begin to recognize the importance of combating the phenomenon of churn, i.e., the loss of clients. it is necessary to identify customers who are at risk of churn and, therefore, it is necessary to create a method that allows to do it in advance so that they can be covered by the proactive retention campaigns. the more effective the method to identify customers at risk, the higher the return of applying the campaign. many studies have been developed in the area of churn prediction in various sectors. however, in the area of retail the research has been very limited. so with this dissertation work was intended to study the phenomenon of loss of customers to define and implement a model of churning to the retail sector using data mining techniques. the intention was to make a survey of the main issues involved in the prediction of churn in retail, construction of the dataset (customer signatures) and applying data mining techniques in the forecasting process. accordingly, some models were constructed to forecast cases of churn based on five of the most commonly used classification techniques in churn prediction: decision trees, logistic regression, neural networks, random forests and svm. the evaluation and comparison of the performance of models developed has been made according to several measures as accuracy, precision, sensitivity, specificity, f-measure and auc and, furthermore, has been tested the impact of the change in the density of churn events in the training set."
    ],
    0.0
  ],
  [
    "altice labs is developing a new internet protocol television (iptv) platform to replace the one it currently possesses, being based on standard technologies and open standards allowing for greater evolution and resilience. to check if it is capable of incorporating services used in production, the company decided to implement one of these services on the new platform. the chosen service was a back office service, designated by landing pages (lp) that allow the creation of applications based on predefined templates. applications are then presented through an interpreter present on the client, as in their set-top boxes. this master’s dissertation aims to implement this interpreter of lp applications on the new platform, and also check if it is able to support the services that are currently available. the implementation was made in javascript using google chrome as a web browser and the nw.js platform (node-webkit) as tests environments, since the specification of the set-top box, where the platform should run, is still unknown. an analysis of the service and its templates led to their division into groups. each group was then implemented sequentially in an iterative and incremental process. the implemen tation revealed that the platform is capable of incorporating the intended features but it has flaws that must be corrected. this dissertation presents the process followed to recreate all templates on the new platform and corresponding analysis of its features on the new platform. this process provides pointers on how the interpreter and platform should evolve to enable deployment of the services that the company altice labs intends to make available.",
    [
      "throughout the years, companies have continuously sought ways to improve their customer service, and help desk systems eventually evolved to meet these needs. recent advancements in technology such as the introduction of chatbots have led to more automated and self-service options in help desk systems. implementing an effective help desk system can be a great way to attract new customers, through onboarding methods, and strengthen relationships with existing clients by addressing issues quickly and efficiently, resulting in increased customer satisfaction and loyalty. this dissertation aims to establish the optimal help desk system that will not only assist in preserving a positive relationship with existing clients but also attract new ones, ultimately contributing to enhanced customer satisfaction, retention, and operational efficiency for wintouch’s cloud application. with this in mind, wintouch is looking for ways to make its help desk system better in its cloud application. they are hoping to have automated self-service customer support options as well as the more traditional customer service techniques, so any user can have the ability to choose their preferred method, ensuring their customer relationship remains in the best shape possible. hence, this dissertation aims to establish the optimal help desk system that will assist in preserving a positive relationship with existing clients while also attracting new ones.",
      "ao longo dos anos, as empresas têm procurado formas de melhorar o seu atendimento ao cliente e os sistemas de help desk evoluíram de forma a atender essas necessidades. os sistemas de help desk integraram-se com novas tecnologias como chatbots, tornando-se mais automatizados para fornecer um melhor atendimento ao cliente. implementar um bom sistema de help desk pode ser uma excelente forma de atrair novos clientes, através de métodos de onboarding, e melhorar as relações com clientes existentes. um sistema de help desk bem planeado pode ajudar a garantir que os problemas dos clientes sejam resolvidos rapidamente e eficientemente, o que pode levar a um aumento da satisfação e lealdade dos clientes. esta dissertação visa estabelecer o melhor sistema de help desk que não só ajudará a preservar uma relação positiva com os clientes existentes, mas também atrairá novos, contribuindo assim para uma satisfação, retenção e eficiência operacional aprimoradas para a aplicação da wintouch. com isso em mente, a wintouch está à procura de formas de melhorar o seu sistema de help desk na sua aplicação cloud-based. a empresa espera ter opções de suporte ao cliente automatizadas e de self-service, assim como técnicas de atendimento ao cliente mais tradicionais, para que qualquer utilizador possa escolher o seu método preferido, garantindo que a relação com o cliente continue da melhor forma possível. assim, esta dissertação visa estabelecer o sistema de help desk ideal que ajudará a preservar uma relação positiva com clientes existentes e, ao mesmo tempo, atrair novos."
    ],
    0.3
  ],
  [
    [
      "some road users have been targeted in a way to try and create systems that help increase their safety. we are referring to, in this instance, pedestrians, cyclists and even motorcyclists whom we call vulnerable road users (vru) due to the fragility they present when compared to other users that circulate on the same roads, in particular cars. these systems fall within the ambit of the smart cities that are increasingly growing all over the world. the objective of this dissertation is to take advantage of geofencing technology, i.e., to create a virtual perimeter for a real geographical location in which it is desired to detect at any moment the entrance or exit of an user. to achieve such goals a mobile app was deve loped with the purpose of notifying the user as soon as he enters or exits such geofences. a web-based platform was also conceived and designed for city hall administrators where geofences may be created, updated, deleted and classified. the main focus went towards the creation of geofences in specific points of interest for vrus, i.e., areas that may be considered dangerous for pedestrians when crossing the road, due to bad visibility for instance, signaling road blocks or construction areas that may cause disruption in traffic or even a road accident.",
      "certos utilizadores dos meios rodoviários têm sido alvo de estudo de forma a tentar criar sistemas que ajudem a aumentar a segurança dos mesmos. estamos a falar em concreto de peões, ciclistas e até mesmo motociclistas e estes denominam-se vru sendo que o seu nome advém da fragilidade que apresentam perante os demais utilizadores que circulam nas mesmas estradas, em particular os carros. estes sistemas inserem-se no âmbito das smart cities, cidades tecnológicas que cada vez mais estão em crescendo pelo mundo inteiro. o objetivo desta dissertação é tirar partido das vantagens da tecnologia das geofences, isto é, criar um perímetro virtual para unia localização geográfica real na qual se pretende detetar a todo o momento a entrada ou saída de um utilizador. para atingir estes objetivos desenvolveu-se uma mobile app com o propósito de notificar um utilizador, seja este um pedestre, um ciclista ou um condutor, sempre que este entre ou saia de uma dessas geofences. foi também desenvolvida uma plataforma web para um administrador por exemplo um gestor de unia câmara municipal, onde pode criar e remover geofences. o foco principal desta dissertação estará direcionado para a criação de geofences em pontos de interesse específicos para os vru, isto é, áreas que podem ser consideradas perigosas para travessias de peões, devido a má visibilidade, de forma a sinalizar acidentes rodoviários ou ainda obras na via pública que possam causar disrupções no tráfego."
    ],
    [
      "hoje em dia a comunicação assíncrona entre serviços, independentemente da plataforma (desktop, mobile, smart tv, smartwatch, etc), é cada vez mais frequente. parte do software produzido pelas empresas de telecomunicações, neste caso, pela empresa celfocus, consiste em realizar operações assíncronas e, por conseguinte, receber notificações sobre o estado dessas operações. a título de exemplo, quando um funcionário numa loja de telecomunicações realiza uma operação (assíncrona) como alterar o tarifário de um cliente, é necessário feedback dessa operação (ou das várias operações espoletadas pela mesma), através de uma notificação com origem no servidor e destino para o browser do funcionário. para conseguir implementar esta comunicação em tempo real, tecnologias denominadas por server push foram desenvolvidas. cada uma delas tem um cenário ideal de uso, diferentes características, vantagens e desvantagens. a presente dissertação consiste em investigar as tecnologias existentes para a comunicação entre o frontend e backend e, depois, desenvolver um sistema que permite enviar e receber notificações. por fim, para comprovar que a solução conceptual proposta, efetivamente, funciona na prática, são realizados testes funcionais.",
      "in the present age, asynchronous communication between services regardless of the platform (desktop, mobile, smart tv, smartwatch, etc) is increasingly common. part of the software produced by the telecommunications companies, in this case by the company celfocus, consists of performing asynchronous operations and therefore receiving notifications about the status of these operations. for example, when a telecommunication's store employee performs an (asynchronous) operation such as changing a customer's tariff, feedback from this operation is required (or from the multiple operations triggered) through a notification originating from a server and destination to the employee browser. to achieve this real time communication, server push technologies have been developed. each has an ideal use case scenario, different characteristics, advantages, and disadvantages. this thesis consists of investigating different technologies for frontend-backend communication, and then to develop a system that allows to send and receive notifications. finally, to prove that the proposed conceptual solution actually works in practice, functional tests are carried out."
    ],
    0.3
  ],
  [
    [
      "recognizing sentences of a language in an efficient and precise manner has always been a strong subject within computer science. many theories, algorithms and techniques have been proposed along computing history, but at the end it all comes down to performing lexical and syntactic analysis of the source, originating a parse tree as the result. sometimes there is no need for full precision or even a full parse tree. a good example of one of these cases is architecture extraction from source code. in this case only a small portion of the code is of interest. another good example is recognizing handwritten expressions, because it is entirely impossible to predict the kind of calligraphy that will be analyzed, it is also impossible to perform an one hundred percent precise recognition. this need for tolerant parsing lead to the development of many forms of tolerant parsing along the years. this master work will focus on one form of tolerant parsing in particular, fuzzy parsing. from this work it is expected the emergence of a new fuzzy parsing technique based on automata, where automata states would represent context and edges would represent potential matches inside that context. the hypothesis of this work is that such an approach reduces uncertainty and recognition time. it is also expected the creation of a tool suit that facilitates the process of developing fuzzy parsers. we believe that such a tool will be a great addition to areas such as program comprehension or ide construction.",
      "reconhecer frases de uma linguagem, de uma forma eficiente e precisa, tem sido sempre um tópico de interesse dentro da informática. diversas teorias, algoritmos e técnicas foram propostas ao longo dos anos, mas no fim de contas tudo se baseia em análises léxicas, sintáticas e semânticas da fonte, originando uma árvore de parsing como resultado. por vezes não há necessidade de um reconhecimento cem por cento preciso, nem de uma árvore de parsing completa. um bom exemplo de um desses casos, pode ser encontrado na extração de modelos arquiteturais a partir de código fonte. neste caso, apenas uma parte reduzida do código tem interesse. outro bom exemplo pode ser encontrado no reconhecimento de expressões matemáticas escritas manualmente. uma vez que é impossível prever o tipo de caligrafia a ser analisada, é também impossível realizar um reconhecimento cem por cento preciso. esta necessidade por tolerância no reconhecimento levou ao desenvolvimento de várias técnicas de parsing tolerante ao longo dos anos. esta dissertação irá focar-se particularmente numa forma de parsing tolerante, fuzzy parsing. deste trabalho é esperado que surja uma nova técnica de fuzzy parsing baseada em autómatos, onde os estados representarão contexto e os arcos representarão possíveis correspondências dentro desse contexto. espera-se que esta abordagem reduza a incerteza e o tempo de reconhecimento. é também esperado que seja criada uma ferramenta que facilite o processo de criação de outros fuzzy parsers. pensamos que uma ferramenta do género será uma bela adição ao para áreas como análise de programas e construção de ambientes de desenvolvimento de software."
    ],
    [
      "na última década, tem sido cada vez mais frequente a utilização de testes de personalidade, tais como o ocean/big five, desenvolvido por goldenberg em 1992, para avaliar aquilo que os psicólogos consideram ser as cinco principais dimensões da personalidade (extroversão, agradabilidade, abertura, consciencioso e estabilidade). assim sendo, o propósito fundamental desta dissertação é o de conceber e desenvolver uma aplicação móvel focada no tratamento de vários dados biométricos do utilizador assim como na sua personalidade. para isso, será implementado na aplicação o teste de personalidade, baseado no modelo de saucier, que permite ao utilizador escolher de entre quarenta adjetivos, os que melhor se adequam a si. aquando da realização do teste, que se repetirá ao longo do tempo, será tida em consideração a informação biométrica do utilizador e o seu histórico. o objetivo é o de oferecer, ao utilizador, a possibilidade de conhecer os traços que compõe a sua personalidade ao mesmo tempo que lhe é fornecido informação biométrica, sendo possível analisar relações e correlações entre as cinco dimensões fundamentais da personalidade e os dados biométricos recolhidos, assim como a evolução de cada um destes atributos ao longo do tempo.",
      "in the last decade, there has been an increased use of personality tests, such as the ocean/big five, developed by goldenberg in 1992, to evaluate what psychologists consider to be the five traits, or dimensions, of personality (extroversion, agreeableness, openness, conscientiousness and neuroticism). therefore, the main purpose of this dissertation is to design and develop a mobile application focused on the collection and treatment of several biometric data of the user as well as on his personality. for this, a personality test, based on the saucier model, will be implemented, which allows the user to choose from forty adjectives the ones that best suit him. when performing the test, which will be repeated over time, the user’s biometric information and history will be taken into consideration. the goal is to offer, the user, the possibility of knowing the dimensions that make his personality while providing biometric data, being than possible to understand relations and correlations between the five fundamental personality dimensions and the collected data, as well as the evolution of these features over time."
    ],
    0.3
  ],
  [
    [
      "a iminente escassez de recursos naturais e o constante aumento populacional tem assolado o presente século. tal crescente habitacional contribui para uma concentração nos grandes centros urbanos e, consequentemente, um maior nível de poluição quer em contextos habitacionais como industriais. nesta vertente, as estações de tratamento de águas residuais, desempenham um papel crucial no controlo do nível de qualidade da água que é reutilizada ou descarregada para o exterior. estas instalações recebem ininterruptamente cargas de afluentes extremamente poluentes que são provenientes da rede pública de esgotos e que carecem de um tratamento faseado para a purificação das mesmas. porém, para garantir a qualidade da água que é reaproveitada ou devolvida ao meio ambiente, é necessária monitorização contínua destas estações de forma a permitir o processo de tomada de decisão. posto isto, esta dissertação visa implementar modelos de machine learning com o intuito de detetar possíveis anomalias nas substâncias presentes no efluente destas infraestruturas. assim sendo, são aplicados modelos como isolation forest (if), one class support vector machine (ocsvm) e long short-term memory autoencoder (lstm-ae) para identificar os registos do azoto total, nitratos e ph que possam ser anómalos. no caso em específico das lstm-ae, são considerados três thresholds para classificar os registos, dos quais, dois utilizam valores estáticos e um consiste em valores dinâmicos. de entre os melhores modelos candidatos, no global, os modelos de if e ocsvm alcançaram resultados superiores aos modelos baseados em lstm-ae. no que diz respeito aos thresholds, as abordagens com valores estáticas de forma geral, atingiram resultados ligeiramente superiores. em suma, os vários cenários aplicados permitiram concluir que os modelos concebidos conseguiram detetar as várias anomalias presentes nas substâncias referidas.",
      "the imminent scarcity of natural resources and the constant population increase have plagued the present century. such populational growth contributes to a concentration in large urban centres and, consequently, a higher pollution level in housing and industrial contexts. in this regard, the wastewater treatment plants play a crucial role in controlling the water quality that is reused or discharged abroad. these installations receive uninterrupted loads of extremely polluting affluents from the public sewage system that need a phased treatment to purify them. however, to guarantee the quality of the water reused or discharged into the environment, continuous monitoring of these facilities is necessary to allow the decision-making process. that said, this dissertation aims to implement machine learning models to detect possible anomalies in the substances present in the effluent of these infrastructures. therefore, models such as isolation forest (if), one-class support vector machine (ocsvm) and long short-term memory autoencoder (lstm-ae) are applied to identify the records of the total nitrogen, nitrates and ph that may be anomalous. in the specific case of lstm-ae, three thresholds are considered to classify the records, of which two use static values, and one consists of dynamic values. among the best candidate models, overall, the if and ocsvm models achieved superior results to the models based on lstm-ae. regarding thresholds, the approaches with static values generally achieved slightly better results. the various scenarios applied allowed us to conclude that the designed models could detect various anomalies in the substances mentioned."
    ],
    [
      "globalmente, 25% da população sofre de distúrbios mentais, sendo possível implementar metodologias que possibilitem a deteção e previsão numa fase mais precoce. concretamente, o delirium é uma disfunção neuropsiquiátrica aguda, prevalente em doentes admitidos em contexto hospitalar de internamento e terapia intensiva. sendo uma manifestação multifatorial é normalmente subdiagnosticada e negligenciada. o delirium pode ser categorizado, de acordo com o perfil de atividade motora, em hipoativo e hiperativo. neste contexto, surge o tema da dissertação que visa desenvolver uma aplicação capaz de prever a ocorrência de delirium e dos seus subtipos, com base na metodologia dos glms. os modelos de regressão logística multinomial são frequentemente implementados para identificar as variáveis mais contributivas, dado que permitem modelar a relação entre os preditores e uma variável dependente multicategórica. as etapas que precedem a implementação do algoritmo dizem respeito ao pré-processamento dos dados. no decorrer do processo de modelação, aplicou-se o adasyn para gerar amostras sintéticas devido ao desbalanceamento das classes da variável dependente. posteriormente, foi realizada a seleção de variáveis recorrendo a diversas técnicas, sendo que o método elastic net com um alpha de 0,1 foi o que demonstrou um melhor desempenho. para tal, este modelo foi implementado na aplicação disponível em https://alexandra-coelho.shinyapps.io/delirium_detection/. para o subtipo hipoativo, permitiu a seleção de 27 variáveis, tendo obtido uma auc-pr de 0,307 e uma auc-roc de 0,691. as variáveis mais contributivas incluem o período de internamento em dias, o alcoolismo, os analgésicos, os cardiotónicos, assim como, o grupo de diagnóstico referente à toxicidade e drogas. relativamente ao subtipo hiperativo, o modelo determinou 29 variáveis relevantes, onde obteve um valor de auc-pr de 0,074 e de 0,531 para a auc-roc. das variáveis mais impactantes destacam-se a pcr, a idade, a po2, os critérios sirs e o local de proveniência no su, nomeadamente, o udc1. especula-se que os baixos valores associados essencialmente ao subtipo hiperativo são devidos à baixa representatividade desta categoria. apesar deste modelo preditivo ainda poder ser melhorado, assume-se como uma ferramenta útil para os profissionais de saúde aquando o diagnóstico do delirium no su.",
      "globally, 25% of the population suffers from mental disorders, and it is possible to implement methodologies that enable the detection and prediction at an earlier stage. specifically, delirium is an acute neuropsychiatric dysfunction prevalent in patients admitted to inpatient and intensive care hospital settings. as a multifactorial manifestation, it is typically underdiagnosed and overlooked. delirium can be categorized, based on motor activity profile, into hypoactive and hyperactive subtypes. in this context, the dissertation topic arises, aiming to develop an application capable of predicting the occurrence of delirium and its subtypes using the glms methodology. multinomial logistic regression predictive models are often implemented to identify the most influential variables, as they allow for modelling the relationship between predictors and a multinomial dependent variable. the steps preceding the algorithm’s implementation relate to data preprocessing. during the modelling process, adasyn was applied to generate synthetic samples due to the imbalance in the classes of the dependent variable. subsequently, variable selection was performed using various techniques, with the elastic net method having an alpha value of 0,1 showing the best performance. to achieve this, this model was integrated into the application available at https://alexandra-coelho.shinyapps. io/delirium_detection/. for the hypoactive subtype, it allowed the selection of 27 variables, resulting in an auc-pr of 0,307 and an auc-roc of 0,691. the most influential variables include the length of hospitalization in days, alcoholism, analgesics, cardiotonics, as well as the diagnostic group related to toxicity and drugs. regarding the hyperactive subtype, the model identified 29 relevant variables, with an auc-pr of 0,074 and an auc-roc of 0,531. the most impactful variables include pcr, age, po2, sirs criteria, and the source location in the er, specifically udc1. it is speculated that the low values, especially for the hyperactive subtype, are due to the limited representation of this category. despite the potential for further improvement in this predictive model, it is considered a useful tool for healthcare professionals when diagnosing delirium in the emergency room."
    ],
    0.06666666666666667
  ],
  [
    [
      "com o atual crescimento do desenvolvimento de software e aumento da procura para solução de diversos problemas, começa-se a verificar a incapacidade de certas abordagens arquiteturais para lidarem com alguns dos desafios atuais. efetivamente, alguns destes desafios estão relacionados com o aumento da adesão das pessoas à tecnologia, personalização das aplicações, implementação rápida de novas funcionalidades, crescimento e complexidade das aplicações, otimização da produtividade das equipas de desenvolvimento, adequação das melhores tecnologias, entre outros. deste modo, as arquiteturas orientadas a microsserviços resolvem alguns destes problemas atuais. este projeto de dissertação tem como objetivo estudar as arquiteturas orientadas a microsserviços, os seus princípios, padrões e testar a aplicabilidade dos mesmos a um caso de estudo do mundo real, um sistema e commerce, com a finalidade de resolver alguns dos problemas e desafios desta abordagem arquitetural. as categorias de padrões de arquiteturas orientadas a microsserviços estudadas são: decomposição, manutenção de dados, mensagens transacionais, apis externas, descoberta de serviços e segurança. dos padrões estudados e implementados, os que tiveram resultados interessantes foram o saga e cqrs, devido a resolverem problemas relacionados com a manutenção dos dados, que se torna complexa com a característica distribuída deste tipo de arquiteturas. a avaliação da aplicabilidade destes padrões ao caso de estudo, faz-se em comparação do desenho e implementação, com o estudo do estado de arte, através dos pontos positivos e negativos, bem como são efetuados testes à aplicação desenvolvida para conclusão de resultados. por fim, foram efetuados testes de carga, para verificar a capacidade de escalabilidade, mas principalmente o impacto que as decisões arquiteturais têm na performance e disponibilidade das funcionalidades.",
      "with the current growth of software development and increased demand for solutions of multiple problems, we start to verify the inability of certain architectural approaches to deal with some of the current challenges. effectively, some of these challenges are related with people's adherence to technology, application customization, quick implementation of new features, application growth and complexity, productivity's optimization of the development teams, adaption of the best technologies, etc. the microservices architectures solve some of these current problems. this dissertation project has the objective to study the microservices architectures, their principles, patterns and test their applicability in one real world case study, an e-commerce system, to solve some problems and challenges of that architectural approach. the categories of microservices architecture patterns studied are: decomposition, data management, transactional messaging, external apis, service discovery and security. of the patterns studied and implemented, the ones that had interesting results were saga and cqrs, due to solving problems related with data management, which becomes complex with distribution characteristic of these architectures. the applicability of these patterns to the case study is evaluated by comparing the design and implementation with the state of the art study, with positive and negative points, as well with application testing to get the conclusion results. finally, were carried out charge tests, to verify the scalability capacity, but mainly the impact of the architecture decisions has in features performance and availability."
    ],
    [
      "human kind has proven how challenging and volatile the technological market can be, growing at an exponential rate. the benefits of such evolution are directly reflected in many ways in our everyday life. robots are a clear example of an advanced technology that may be completely integrated in our societies in a near future, hopefully in such a way that their actions will be considered as trustable as human actions are. these machines are permanently relying on software, which has a development process that many times cannot be considered trustworthy. this may cause the final product to have multiple malfunctions, which in turn may result in tremendous economic losses or even harm human lives. bearing this in mind, software industry and academia have been trying to establish new standards and techniques that considerably lower the occurrence of the latter problems. the solution is to apply certain formal methodologies and tools when developing software, namely when developing critical software that controls machinery used, for example, in healthcare sector, aeronautical industry, or in military operations. the present dissertation aims to explore and improve techniques and tools to help devel opers in the process of building robotic systems, namely those developed with the robot operating system (ros). the focus will be on a specific framework named haros, which performs different types of analyses of ros-based code. although it has a solid set of useful features, some need to be upgraded to enhance efficiency and also to promote a better experi ence to their users, in particular when the the software has many variants, as is often the case with robotic applications. the proposed extension offers ros and haros users a practical methodology that, by merging existing ros and software product line (spl) development tools and concepts, considerably improves the understanding of the variability in a robotic application, without requiring a steep learning curve.",
      "a humanidade tem provado o quão desafiador e volátil consegue ser o mercado tecnológico, que cresce a um ritmo exponencial. os benefícios dessa evolução refletem-se diretamente de várias formas no nosso quotidiano. os robôs são um exemplo evidente de uma tecnolo gia vanguardista que num futuro próximo poderá estar totalmente integrada nas nossas sociedades, de tal forma que as suas ações serão consideradas tão confiáveis quanto as dos seres humanos. não obstante, estas máquinas estão constantemente dependentes de software cujo processo de desenvolvimento é muitas vezes pouco credível. isto leva a que o produto final tenha muitas anomalias, que por sua vez podem resultar em prejuízos económicos avultados ou até pôr em perigo vidas humanas. tendo isso em conta, a indústria de software e a academia têm tentado estabelecer novos padrões e técnicas que reduzem consideravelmente a ocorrência de problemas futuros. a solução passa por aplicar determinadas metodologias e ferramentas formais durante o de senvolvimento de software, nomeadamente no desenvolvimento de software sensível que controla aparelhos usados, por exemplo, no setor da saúde, na indústria aeronáutica, ou até em operações militares. o propósito desta dissertação é explorar e melhorar técnicas e ferramentas que auxiliem os técnicos no processo de construção de sistemas robóticos, nomeadamente os desenvolvidos com o robot operating system (ros). o foco vai para uma ferramenta chamada haros, que exerce diferentes tipos de análises em código ros. apesar da mesma já ter um conjunto consistente de funcionalidades, algumas precisam de ser otimizadas para melhorar a eficiência e também para promover uma melhor experiência aos seus utilizadores, em particular quando o software tem muitas variantes, como é frequentemente o caso nas aplicações robóticas. a extensão desenvolvida oferece aos utilizadores do ros e do haros uma metodologia prática que, combinando ferramentas e conceitos já usados no ros e no desenvolvimento de linhas de produtos de software (lpss), melhora consideravelmente a compreensão da variabilidade existente numa aplicação robótica, não requerendo uma curva de aprendizagem muito elevada."
    ],
    0.3
  ],
  [
    [
      "actualmente as aplicações primavera incluem componentes de reporting que exigem demasiado esforço de implementação e de manutenção, na medida em que todo o seu desenvolvimento é manual, repetitivo e assente em tecnologia desactualizada. estes componentes de reporting são baseados nas soluções crystal reports, sendo necessária a construção/desenho em tempo de desenvolvimento de todos os relatórios que são pretendidos para um determinado produto. cada um destes relatórios tem o seu desenho próprio, a suas próprias características e configurações, não existindo qualquer forma de partilhar determinadas propriedades que possam ser comuns aos vários relatórios. por norma pretende-se que todos os relatórios de um produto tenham um aspecto uniforme, como por exemplo o layout ou fonte utilizada para determinados campos (por exemplo o título do relatório). significa isto que é necessário na construção de cada um dos relatórios replicar todas estas características que são comuns, o que exige um esforço significativo e pode ser propício ao erro quando as regras de desenho de relatórios não estão bem definidas no início do desenvolvimento. este problema torna-se mais evidente quando por exemplo num produto com um elevado número de relatórios se pretende fazer uma alteração numa destas características comuns. a simples alteração do tipo de fonte do título do relatório acaba por ser um processo bastante dispendioso, uma vez que é necessário editar todos os relatórios individualmente. esta dissertação surgiu da necessidade de desenvolver um novo componente de reporting que possa responder às limitações actuais. no âmbito do projecto primavera athena, está inserida a framework de reporting, cuja finalidade é dar suporte à criação, geração e apresentação de relatórios nos produtos desenvolvidos sobre a framework athena. um dos principais objectivos da framework de reporting é a geração automática de relatórios a partir dos modelos das aplicações, acabando assim com todo o processo manual de criação de relatórios.",
      "currently, primavera applications include components for reporting that requires too much effort of implementation and maintenance, because the development is manual, repetitive and based on out dated technology. these components are based on crystal reports solutions, which require the construction/design at development time of all reports that are intended for a particular product. each one of these reports has its own design, its own characteristics and settings, and there is no way to share certain properties that may be common to multiple reports. usually it is intended that all reports of a product share a uniform appearance, such as the layout and font used for certain fields (for example the report title). this means that in the construction of each report is necessary to replicate all of these common characteristics, which requires a significant effort and may cause more errors if the design rules are not correctly defined in the beginning of the development. this problem becomes even more evident when in a product with a high number of reports is necessary to make a change in one of these common characteristics. the simple change of the report title font turns out to be a very expensive process, since it is necessary to individually edit all reports. this work arose from a need to develop a new reporting component that can respond to the current limitations. within the scope of primavera athena project, is the reporting framework, which aims to support the creation, generation and presentation of reports on products developed in the athena framework. one of the main objectives of the reporting framework is to provide automatic generation of reports from the applications model, ending with all the manual process in reports development."
    ],
    [
      "achieving great and undeniable success in a great variety of industries and businesses has made the term big data very popular among the scientific community. big data (bd) refers to the ever fast-growing research area in computer science (cs) that comprises many work areas across the world. the healthcare sector is widely known to be highly proficient in the production of big quantities of data. it can go from health information, such as the patient’s blood pressure and cholesterol levels, to more private and sensitive data, such as the medical procedures history or the report of ongoing diseases. the application of sophisticated techniques enables a profound and rigorous analysis of data, something a human cannot do in real-time. however, a machine is capable of rapidly collect, group, storage and examine vast amounts of data and extract unknown and possi bly interesting knowledge from it. the algorithms used can discover hidden relationships between attributes that prove to be very useful for a corporation’s work. buried structures within the produced data can also be detected by these techniques. machine learning (ml) methods can be adjusted and modelled to different input representations - this adaptability is one of the factors that contributes to its blooming prosperity. the main goal is to make predictions on data, by building utterly efficient models that can accurately take in the data and thus predict a certain outcome. this is especially important to the healthcare industry since it can considerably improve the lives of many patients. everything from detecting a type of disease, predicting the chance of morbidity after a hospital stay, to aid in the decision making of treatment strategies are vital to patients as well as to clinicians. any improvement over established methods that have been previously studied, tested and published are an asset that will improve the patient’s satisfaction about the healthcare performance in medical institutions. this can be achieved by refining those algorithms or implementing new approaches that will make better predictions on the given data. the main objective of this dissertation is to propose ml approaches having acknowledged and evaluated the existent methods used in clinical data. in order to fulfill this goal, an analysis of the state of the art of medical knowledge repositories and scientific papers published related to the selected keywords selected was performed. in this line of work, it is crucial to understand, compare and discuss the results obtained to those previously published. thus, one of the goals is to suggest new ways of solving those problems and measuring them up against the existent ones.",
      "obter um sucesso enorme e inegável numa grande variedade de indústrias e companhias, tomou o termo big data (bd) muito popular entre a comunidade científica. big data refere-se à área de investigação em engenharia informática que revela um crescimento rápido e está envolvida em várias áreas em todo o mundo. o setor da saúde é universalmente con-hecido por ser altamente frutífero na produção de grandes quantidades de dados. podem variar desde dados de saúde, tais como, o valor da pressão sanguínea e nível de coles-terol do paciente, até dados mais confidenciais, como o histórico de cirurgias realizadas e doenças diagnosticadas. a aplicação de técnicas sofisticadas permite uma análise profunda e rigorosa dos dados -algo que um ser humano não consegue fazer em tempo real. no entanto, uma máquina não tem dificuldades em recolher, agrupar, armazenar e analisar rapidamente grandes quanti-dades de dados e extrair deles conhecimento que era desconhecido e, possivelmente, interessante. os algoritmos usados podem ser usados para descobrir relações desconhecidas entre os vários atributos, que se podem revelar bastante úteis para o dia-a-dia de uma empresa. estruturas e padrões escondidos nos dados podem ser também detetados através das mesmas técnicas. os métodos de machine learning (ml) podem ser ajustados e modela-dos de forma a aceitar diferentes representações de dados de entrada - esta adaptabilidade é um dos fatores mais proeminentes que contribui para a sua prosperidade. o principal objetivo é fazer previsões sobre os dados, de modo a construir modelos totalmente eficientes que possam analisar os dados de forma precisa, e, assim, prever um determinado resultado. isto é especialmente importante para o setor da saúde, uma vez que pode melhorar consideravelmente a vida de muitos pacientes. tudo, desde a deteção de um certo tipo de doença, prever a probabilidade de morbilidade após um internamento até a auxiliar na tomada de decisão em relação a estratégias de tratamento, é vital para os pacientes, bem como para os médicos. portanto, qualquer melhoria em relação a métodos já estabelecidos que foram previamente estudados, testados e publicados é uma mais-valia que melhorará a satisfação do paciente em relação à sua experiência com os serviços de saúde. tal pode ser alcançado refinando esses algoritmos ou mesmo implementando novas abordagens que farão melhores previsões sobre os dados. o principal objetivo desta dissertação é propor abordagens de ml, fazendo um reconhecimento e avaliando os métodos existentes utilizados em dados médicos. desta forma, foi posta em prática uma análise ao estado da arte de repositórios de conhecimento médico, bem como a artigos científicos relacionados com esses conjuntos de dados. assim, é fundamental compreender, comparar e discutir os resultados obtidos com os publicados anteriormente. portanto, um dos objetivos é sugerir novas formas de resolver os problemas, tecendo uma comparação com os existentes."
    ],
    0.0
  ],
  [
    [
      "taking into account the characteristics of industrial environments, sometimes resistant to innovation so as not to harm productive factors, technologies that are outdated are often adopted. in addition, there is a clear need and desire for obtaining data related to the pro duction process in order to optimize it and, therefore, obtaining a better performance of the industrial equipment. complementary, there is the need for transferring information about refined concepts among multiple departments to assist tasks such as process engineering and order management. the purpose of this dissertation is to build a platform developed by setlevel, called coreflux, where the building modules are capable of providing not so up-to-date technologies with the capabilities to integrate industry 4.0 concepts. based on the concept of internet of things, the work aims at providing obsolete equipment and technologies with the capacity to behave as newer equipment, while maintaining the same benefit that these older technologies offer to the industrial sector, such as reliability and availability.",
      "tendo em conta as características do ambiente industrial, resistente a inovação de forma a não prejudicar fatores produtivos, é possível observar a preferência por tecnologias muitas vezes desatualizadas. apesar da resistência a novas tecnologias, são notórios a necessidade e o desejo de obtenção de dados relativos ao processo produtivo de forma a otimizar o mesmo e por consequente obter um melhor desempenho dos equipamentos existentes. para além disso, existe a necessidade de transferência de informações sobre conceitos refinados entre vários departamentos para auxiliar tarefas, departamentos estes como engenharia de processos e gestão comercial. o objetivo desta dissertação é construir uma plataforma desenvolvida pela setlevel, denominada coreflux, onde os diferentes módulos construídos são capazes de fornecer a tecnologias não tão atualizadas a capacidade de integrar conceitos da indústria 4.0. com fundamento no conceito de internet of things, o trabalho visa dotar equipamentos e tecnologias obsoletas com capacidade de se comportarem como equipamentos mais novos, mantendo assim os benefícios que essas tecnologias mais antigas oferecem ao setor industrial, como a confiança e disponibilidade."
    ],
    [
      "a grande quantidade de dados que é gerada diariamente em empresas ou por pessoas em termos individuais despertou a atenção de algumas entidades que viram o grande interesse e potencial da exploração dessa informação. o desenvolvimento de soluções orientadas para esse tipo de exploração começou, assim, a ser incentivado de forma muito dinâmica. na maioria dos casos, essa exploração tem como objetivo alimentar sistemas de profiling, que posteriormente tentam estabelecer algum tipo de padrão comportamental através da utilização de uma ou mais técnicas de análise de dados. a análise de sentimentos presentes em textos é uma das áreas de análise de dados que também tem despertado muito interesse nos últimos anos, tendo sido gradualmente aplicada sobre uma gama de problemas muito diversificada para determinar, por exemplo, como é que um dado produto está a ser aceite pelas pessoas. contudo, embora existam já vários modelos desenvolvidos para este tipo de análise, a sua precisão ainda é muito questionada, em parte devido às dificuldades que existem na realização deste tipo de análise, na qual, de certa forma, é necessário que a linguagem escrita seja compreendida de forma natural por um dado conjunto de algoritmos. neste trabalho de dissertação explorámos esta vertente de análise de dados, com particular ênfase na análise de sentimentos em conteúdos textuais. foi aplicado um conjunto de transformações responsáveis pelo pré-processamento e transformação dos dados para um formato apropriado para serem utilizados pelos modelos. ao longo da construção do pré-processamento foi, ainda, demonstrada a importância desta fase, para qualquer problema de análise de dados, que sem ela não é possível compreender o problema de análise o que frequentemente leva a que os resultados obtidos não sejam os melhores possíveis. após o pré-processamento dos dados, foram desenvolvidos três modelos de análise de sentimentos em textos: modelo supervisionado de aprendizagem automática, modelo baseado em dicionários de sentimentos e modelo híbrido. qualquer um dos modelos faz uso de técnicas de análise de textos de modo a serem reconhecidos sentimentos e respetivas polaridades, aspetos a que os sentimentos se referem, entre outros. dos três modelos desenvolvidos, o modelo híbrido foi o que obteve melhores resultados, com uma percentagem de classificações incorretas aproximadamente igual a 6% do total dos dados de teste.",
      "the huge amount of data that is generated on a daily basis by companies and individuals has raised the interest of entities that saw the oportunities in exploiting that information. soon, the development of data analysis solutions started to emerge rapidly and dynamically. in most cases, these forms of exploiting data are used by profiling systems, as a way of feeding them relevant data, in order to establish some behavioral pattern. sentiment analysis in texts is a field of data analysis which has raised much interest in recent years, having been gradually applied over a wide range of problems in order to determine, for example, how a given product is being accepted by people. however, although there are already several models developed for this type of text analysis, their accuracy is still much questioned, in some way due to the difficulties that exist in the accomplishment of this type of analysis in which, in a certain way, it is necessary that written language can be understood, in a natural way, by a given set of algorithms. in this dissertation this aspect of data analysis will be explored. it is created a set of transformations that are applied to the data, which is in textual format, representative of the pre-processing to be applied in order to transform the data into an appropriate format to be processed by the models. throughout the preprocessing construction it is also demonstrated the importance of this phase, for any data analysis problem, that without it, it is not possible to understand the analysis problem and the results obtained are not the best possible. once the data is pre-processed, it is formed a set of models that use techniques of text analysis with the aim of recognizing feelings in it. these models can be summarized to three main ones: supervised model of machine learning, model based on dictionaries and a hybrid model. in any of the models it is sought to extract the maximum possible amount of information, besides the recognition of feelings and its polarity, as recognition of the aspects to which the feelings refer, among others. of the three models developed, the hybrid model was the one that obtained the best results, with a percentage of incorrect classifications approximately equal to 6% of the total of the test data."
    ],
    0.06666666666666667
  ],
  [
    [
      "virtual globes have a number of key bene ts as a platform for communicating and visualizing geospatial data over traditional technologies. virtual globes have increased in popularity and several implementations are available that cater to di erent audiences from education to industry. despite these advantages, an open source virtual globe solution is still not available for mobile environments. our goal is the development on an open source globe for android, able to receive 3d scenes from a w3ds server. we present the architecture and the implementation decisions. we choose to develop the virtual globe on top of osgearth which takes advantage of the openscenegraph toolkit. based on this decision, we explain how osgearth was extended to consume new 3d data sources and how it was ported to the android platform. porting to android requires major changes in the opengl api usage. embedded devices only support a subset of the opengl api. we provide a virtual globe application that runs natively on the android operating system. it is implemented on top of the osgearth framework. osgearth was ported to android and expanded to support additional features. pointers to the source code repositories are provided. with the work developed in this project, mobile virtual globe solutions can be customized and deployed, providing powerful visualizations and more intuitive interactions.",
      "nos últimos anos, aplicações de globo virtual sofreram um grande aumento na sua popularidade e proliferação. este tipo de aplicação oferece um grande conjunto de vantagens em relação às soluções tradicionais para a visualização e interação com dados geoespaciais. estas vantagens levaram a um elevado interesse na presença desta solução em ambientes móveis. no entanto, uma solução open source para globos virtuais em android ainda não se encontra disponível. o objectivo principal deste trabalho é então disponibilizar em android uma solução de globo virtual open source. o globo implementado terá também de ser capaz de consumir o serviço w3ds recentemente especi cado. apresentamos a arquitectura da nossa solução e as escolhas realizadas. escolhemos basear a nossa solução no osgearth, framework de globos virtuais que recorre ao openscenegraph para as suas necessidades de rendering. esta decisão implicou um processo de porting destas libraries para android, efectuando todas as adaptações necessárias. de especial importância a adaptação do código dos shaders responsáveis pelo rendering grá co, uma vez que em android apenas há disponível o opengl es, especi cação limitada do opengl. o osgearth foi também expandido de forma a ser capaz de consumir o w3ds. disponibilizamos uma solução de globo virtual que corre nativamente em android e é capaz de consumir o w3ds. a framework osgearth foi assim expandida com novas funcionalidades e passou também a estar disponivél para android. com o trabalho realizado, globos virtuais móveis podem ser personalizados e implementados fácilmente."
    ],
    [
      "a sustentabilidade está dependente das decisões que o ser humano toma no ambiente em que se envolve. por outro lado, é necessário ter consciência sobre o impacto das suas ações no meio ambiente. o crescimento da tecnologia e da área científica de sistemas inteligentes tem sido cada vez maior, tornando-se parceiras do ser humano, e o seu potencial para ambientes inteligentes e sustentabilidade tem sido evidenciado nos últimos tempos. para enriquecer a resposta do ambiente inteligente aos seus utilizadores poder-se-á recorrer a sensores dispostos no ambiente e à fusão sensorial e de informação. as recomendações e previsões produzidas têm como objetivo a avaliação e o estudo da sustentabilidade do ambiente, nomeadamente, da utilização equilibrada da energia. dentro dos contextos de sensibilização, de prevenção das ações do utilizador e da sustentabilidade do ambiente em que o ser humano esteja inserido, existem outros objetivos a alcançar, nomeadamente, a fusão de informação como ferramenta na utilização em suporte tecnológicos. a necessidade de aliar este processamento de dados/informação ao desenvolvimento de um conjunto de plataformas que permitam ao utilizador perceber efeitos negativos ou positivos que as condições analisadas têm, passando por possíveis recomendações ao utilizador. esta plataforma deliberativa e reativa apoiará, processos e práticas de consciencialização para a sustentabilidade, por forma a conseguir mudanças nos padrões de estilo de vida, de produção e consumo de energia. o presente trabalho incide sobre a integração das tecnologias da informação e comunicação (tic) no meio envolvente, com estratégias de sustentabilidade dentro da dimensão social, ambiental e económica. as tic associadas a conceitos de inteligência ambiente e elementos físicos, como por exemplo, edifícios, permitem obter formas de melhorar aspectos como o consumo energético e o impacto ambiental, na medida em que podem gerir de forma eficiente o consumo de recursos e contribuir para a redução de desperdícios. um exemplo de aplicação pode ser encontrado na plataforma de agentes inteligentes, denominada por phess, que com ligação a sensores que permite centralizar a recolha de dados e obter decisões através de processos deliberativos automaticamente. esta dissertação foca-se na extensão da plataforma phess com processos de fusão sensorial e de informação e, ainda, na criação e da monitorização dos novos indicadores que permitem promover a sustentabilidade social, económica e ambiental proporcionando aos utilizadores novas possibilidades de acesso a serviços e de participação na comunidade. a utilização de sistemas inteligentes, auxiliam na ação sobre o meio e são, por isso, uma mais-valia para o conforto das pessoas e para a sustentabilidade do ambiente.",
      "sustainability is dependent on the decisions that humans take in the environment in which they are involved. on the other hand, it is necessary to be aware of the impact of their actions on the environment. the growth of the technologicy and scientific area of intelligent systems has been greater, becoming a partner of the human being, and its potential for sustainability and intelligent environments have been evidenced in recent times. to enrich responses from intelligent environments to their users, sensor and information fusion techniques can be used with the help of sensor dispersed across the environment. recommendations and forecasts produced aim to evaluate and study of environmental sustainability, namely balanced use of energy. within a context of awareness, prevention of user’s actions and sustainability of the environment in which the human being is inserted, there are other objectives to be achieve, for instance, information fusion as a tool for use in technological support. the need to combine this data/information processing to develop a set of platforms to enable the user to assess the negative or positive conditions has been done through suggestions to the user. this deliberative and reactive platform should support processes and practices of sustainability awareness to promote changes in lifestyle patterns of production and consumption of energy. this work focuses on the integration of information and communication technologies (ict) in the environment with sustainability strategies within the social, environmental and economic dimensions. ict associated with physical elements, such as buildings, can reduce energy consumption and environmental impact, in that it can efficiently manage resource consumption and help to reduce waste. an example can be demonstrated by the intelligent agents platform, called for phess, which with connection to sensors, provides a centralized collection of data and obtain decision through deliberative processes automatically. the work here detailed is focused on the extension of the phess platform with processes from sensor and information fusion and the creation and monitoring of new indicators that can promote social, economic and environmental sustainability giving people new access to services and to participate in the community. the use of intelligent systems, which help to act in the environment and therefore, are an asset for the comfort of people and the sustainability of the environment."
    ],
    0.049999999999999996
  ],
  [
    [
      "diabetes mellitus, uma doença caracterizada por níveis elevados de açúcar no sangue, é frequentemente monitorizada através de protocolos invasivos e dolorosos, o que resulta em redundância entre os doentes. esta monitorização é geralmente efetuada através do rastreio dos níveis de açúcar no sangue. devido à sua grande intimidade com o sangue, as lágrimas estão a ser consideradas como um outro potencial bio fluído de diagnóstico. as lágrimas contêm múltiplos biomarcadores relacionados com várias doenças sistémicas e, como tal, podem ser utilizadas para monitorizar, diagnosticar e tratar estas doenças utilizando protocolos não invasivos. no entanto, as concentrações dos analitos são muito mais baixas nas lágrimas do que no sangue. estão a ser desenvolvidos dispositivos específicos para detetar essas baixas concentrações. as lentes de contacto, em particular, são utilizadas por milhões de pessoas com problemas de visão e têm potencial para serem transformadas em dispositivos portáteis funcionais. as lentes de contacto necessitam de materiais de elétrodos biocompatíveis e estáveis para serem integrados em plataformas de biossensores. o grafeno, uma camada atomicamente fina de átomos de carbono, apresenta propriedades favoráveis, como a biocompatibilidade, a elevada condutividade elétrica, a fácil funcionalização e a flexibilidade. devido à sua espessura atómica e excelente mobilidade de portadores, este material pode ser utilizado para fabricar transístores de efeito de campo de grafeno para bio deteção, com potencial para atingir uma sensibilidade ultraelevada. este trabalho explora tecnologias baseadas em grafeno para permitir uma plataforma inovadora para a deteção de glucose, potencialmente integrada em lentes de contacto. para tal, foram testadas e otimizadas as condições para o fabrico de uma lente de contacto à base de grafeno. em primeiro lugar, foi abordada a transferência do grafeno para a superfície altamente complexa das lentes de contacto. em segundo lugar, foi estudada uma funcionalização do grafeno, concebida para induzir as reações químicas envolvidas na deteção da glucose, nas várias fases intermédias, para compreender as interações fundamentais que ocorrem. por último, foram utilizados transístores de efeito de campo de grafeno com gate eletrolítica, fabricados com o grafeno funcionalizado, para detetar a glucose numa vasta gama de concentrações.",
      "diabetes mellitus, a disease characterized by high levels of blood sugar, is often monitored by invasive and painful protocols, which results in redundancy among patients. this monitoring is usually performed by tracking the blood-sugar levels. because of their high intimacy with blood, tears are being regarded as another potential diagnostic biofluid. tears contain multiple biomarkers related to several systemic diseases and, as such, they can be used to monitor, diagnose and treat these diseases using non-invasive protocols. however, the analytes’ concentrations are much lower in tears than in blood. dedicated devices are being developed to detect such low concentrations. contact lenses, in particular, are utilized by millions of people with vision conditions and have the potential to be transformed into functional wearable devices. contact lenses need biocompatible and stable electrode materials to be integrated within biosensing platforms. graphene, an atomically-thin layer of carbon atoms, presents favorable properties, such as biocompatibility, high electrical conductivity, easy functionalization, and flexibility. due to its one atom thickness and excellent carrier mobility, this material can be used to fabricate graphene field-effect transistors for biosensing, potentially targeting ultra-high sensitivity. this work explores graphene-based technologies to enable an innovative platform for the glucose-sensing, potentially integrated in contact lenses. to this end, the conditions for the fabrication of a graphene-based contact lens were tested and optimized. firstly, the graphene transfer onto the highly complex contact lenses surface was addressed. secondly, a functionalization of graphene, designed to induce the chemical reactions involved in glucose sensing, was studied in the various intermediate stages to understand the fundamental interactions taking place. finally, electrolyte-gated graphene field-effect transistors fabricated with the functionalized graphene were used to detect glucose in a wide range of concentrations."
    ],
    [
      "when building safety-critical systems, guaranteeing properties like correctness and security are one of the most important goals to achieve. thus, from a scientific point of view, one of the hardest problems in cryptography is to build systems whose security properties can be formally demonstrated. in the last few years we have assisted an exponential growth in the use of tools to formalize security proofs of primitives and cryptographic protocols, clearly showing the strong connection between cryptography and formal methods. this necessity comes from the great complexity and sometimes careless presentation of many security proofs, which often contain holes or rely on hidden assumptions that may reveal unknown weaknesses. in this context, interactive theorem provers appear as the perfect tool to aid in the formal certification of programs due to their capability of producing proofs without glitches and providing additional evidence that the proof process is correct. hence, it is the purpose of this thesis to document the development of a framework for reasoning over information theoretic concepts, which are particularly useful to derive results on the security properties of cryptographic systems. for this it is first necessary to understand, and formalize, the underlying probability theoretic notions. the framework is implemented on top of the fintype and finfun modules of ssreflect, which is a small scale reflection extension for the coq proof assistant, in order to take advantage of the formalization of big operators and finite sets that are available.",
      "na construção de sistemas críticos, a garantia de propriedades como a correção e segurança assume-se como um dos principais objetivos. deste modo, e de um ponto de vista científico, um dos problemas criptográficos mais complicados é o de construir sistemas cujas propriedades possam ser demonstradas formalmente. nos últimos anos temos assistido a um crescimento enorme no uso de ferramentas para formalizar provas de segurança de primitivas e protocolos criptográficos, o que revela a forte ligação entre a criptografia e os métodos formais. urge esta necessidade devido à grande complexidade, e apresentação por vezes descuidada, de algumas provas de segurança que muitas vezes contêm erros ou se baseiam em pressupostos escondidos que podem revelar falhas desconhecidas. desta forma, os provers interativos revelam-se como a ferramenta ideal para certificar programas formalmente devido à sua capacidade de produzir provas sem erros e de conferir uma maior confiança na correção dos processos de prova. neste contexto, o propósito deste documento é o de documentar e apresentar o desenvolvimento de uma plataforma para raciocinar sobre conceitos da teoria de informação, que são particularmente úteis para derivar resultados sobre as propriedades de sistemas criptográficos. para tal é necessário, em primeiro lugar, entender e formalizar os conceitos de teoria de probabilidades subjacentes. a plataforma é implementada sobre as bibliotecas fintype e finfun do ssreflect, que é uma extensão à ferramenta de provas assistidas coq, por forma a aproveitar a formalização dos somatórios e conjuntos finitos disponíveis."
    ],
    0.0
  ],
  [
    [
      "o data mining é um processo de exploração de grandes quantidades de dados, com um potencial enorme para ajudar as empresas na extração de conhecimento que está oculto nos mais diversos sistemas de dados. esta tecnologia é utilizada pelas empresas nos mais variados domínios, com o intuito de as ajudar em atividades de tomada de decisões. entre os diversos campos de aplicações encontramos o domínio da biologia e do ambiente, em particular, as questões relacionadas com as estações de tratamento de águas residuais (etar). as etar são infraestruturas essenciais para manter o equilíbrio do meio-ambiente, sendo caracterizadas por terem várias fases de tratamento, nas quais são removidas impurezas como sólidos, matéria orgânica e nutrientes. todo este processo dinâmico e complexo deve ser processado de forma eficiente, permitindo que o efluente final que nelas é tratado tenha a melhor qualidade possível. a previsão da qualidade da água tratada, com base nos vários fluxos que dão entrada nas etar, permite medir a eficácia do tratamento e, assim, obter alguma informação útil para um melhor controle de toda a infraestrutura. a etar em estudo neste trabalho de dissertação, localiza-se no norte de portugal e serve uma população de cerca de 45 mil habitantes. os dados fornecidos para alimentação dos processos de interação levados a cabo são referentes a tratamentos realizados nessa etar durante o período de um ano. este estudo pretendeu explorar técnicas de data mining preditivas, nomeadamente modelos de regressão, por forma a prever com eficácia os valores dos parâmetros de qualidade da etar. as medidas de qualidade do tratamento analisadas neste estudo, basearam-se nos parâmetros de previsão carência bioquímica de oxigénio (cbo) e sólidos suspensos totais (sst). por sua vez, as técnicas de regressão adotadas neste trabalho são baseadas em support vector machines, mais concretamente nos algoritmos support vector regression e numas das suas variantes: sequential minimal optimization. este conjunto de técnicas tem sido aplicadas com sucesso em diferentes áreas, inclusive em alguns trabalhos relacionados com as etar. pretendeu-se assim, à custa da utilização destas técnicas de previsão, definir um modelo comportamental para a etar em questão, por forma a analisar a sua capacidade preditiva neste tipo de sistemas complexos. neste problema, as fases de análise e preparação dos dados mostraram-se determinantes na obtenção dos resultados alcançados. analisaram-se ainda as diversas tarefas de modelação desenvolvidas neste estudo. os modelos desenvolvidos demonstraram uma boa capacidade preditiva, especialmente na previsão do parâmetro do efluente final cbo. as técnicas de previsão utilizadas, para além da capacidade de modelação preditiva não linear, permitem ainda uma análise aos atributos mais influentes à qualidade dos parâmetros de previsão.",
      "data mining is a process of exploration of large data sets with a huge potential to assist companies in the extraction of knowledge that is hidden in their data systems. companies in various fields use this technology, in order to assist them in decision-making. among the various fields are included the biology and environment domains, in particular, issues related to the wastewater treatment plants (wwtp). wwtp are essential infrastructures to maintain the environmental balance. treatment plants are characterized by having several treatment stages in which is done the removal of solids, organic matter and nutrients. all of this dynamic and complex process must be handled efficiently to ensure a good quality effluent. the prediction of the treated wastewater quality, based on the measured inflow parameters, allow the treatment performance evaluation and yet to obtain some useful information for a better control of the entire infrastructure. the data used in this study were collected from a wwtp located in northern portugal that serves a population of about 45,000 inhabitants, whose data was provided regarding the treatments performed in this wwtp during one year. this study aimed to explore data mining techniques for prediction, namely regression models, in order to successfully predict the concentrations of the quality parameters like biochemical oxygen demand (bod) and total suspended solids (tss), which are actually the selected outflow parameters to be predicted in this work. the regression techniques used herein are based on support vector machines (svm), more particularly support vector regression and in one of its variants: sequential minimal optimization. this set of techniques has been successfully applied in different areas, including some wwtp related work, thus we intended to explore the svm and analyze their predictive ability in this type of complex systems. the stages of data preparation and data analysis were shown to be crucial to obtain the results achieved. several regression models for both predictive parameters were analyzed and compared, where the results show that accurate estimates can be achieved especially on the concentrations of bod. the svm, beyond the capability of non-linear predictive modeling, yet allow the analysis of the features that are most related to the quality of the prediction parameters."
    ],
    [
      "over the past few years the use of information technology (it) and computer applications have been distributed among various sectors, including food distributors. the reason for this to be happening is the capability of technology to transform and improve radically the quality of the service of a company. the main objective of food retail/food distribuition companies in healthcare, as well as social services, is the ability to provide the best service to its patients and the capability to deliever high quality meals, while reducing the costs and the waste involved on this long process. therefore, for this to happen, decisions need to be made as fast and effective as possible. this being the case, for a bright future of an it in the food distribution sector it is required the development of systems capable of ma naging all the resources, providing a better service and a greater satisfaction among users, through the use of costumized meal plans, focusing on quality. hereupon, in the context of this master’s dissertation, the aim of this present work was develop and explore web information systems to support decisions making process in nutrition. thus, it includes the development of a web platform that works as a information system for a food retail institution and as a decision support system that creates meal plans for all its clients taking into account detailed information present in the organization. the tools were designed in order to help the administration of cantina social do hospital da misericordia de vila verde in their daily work.",
      "nos últimos anos, o uso de tecnologia da informação (ti) e aplicativos de computador foram distribuídos entre vários setores, incluindo distribuidores de refeições. a causa para isso acontecer deve-se à capacidade da tecnologia transformar e melhorar radicalmente a qualidade do serviço de urna empresa. o principal objetivo das organizações de produção e distribuição de refeições na área da saúde, bem como nos serviços de acção social é oferecer o melhor serviço aos seus utentes e garantir a entrega de refeições de boa qualidade, reduzir os custos associados e desperdícios desnecessários ao longo de todo o processo. assim, para que isso aconteça, as decisões devem ser tomadas rapidamente e de forma eficaz. portanto, acredita-se que o futuro verdadeiramente bem-sucedido de uma ti no setor de distribuição de refeições poderia ser alcançado através da implementação de sistemas capazes de gerir todos os recursos, potenciando um melhor serviço e uma maior satisfação dos utentes, com um planeamento de refeições personalizadas, focado sempre na qualidade. no contexto desta dissertação de mestrado, o objetivo deste trabalho foi desenvolvimento e implementação de sistemas de informação web para o apoio à decisão na área da nutrição. assim, o projeto inclui o desenvolvimento de uma plataforma web suportada por um sistema de informação para unia organização de produção e distribuição de refeições e como um sistema de apoio à decisão para a criação dos planos de refeição para todos os seus utentes, tendo em consideração todas as informações recolhidas nos diferentes sistemas de informação existentes na organização. as ferramentas foram concebidas de forma a auxiliar a administração da cantina social do hospital da misericórdia de vila verde no seu trabalho diário."
    ],
    0.0
  ],
  [
    [
      "a incontinência urinária, é um problema de saúde com múltiplas repercussões, que interferem negativamente na qualidade de vida dos doentes. aliado a este facto, está a dificuldade em realizar diagnósticos corretos acerca do grau de incontinência. sem um diagnóstico correto o tratamento, que já por si é complicado, incorre em dificuldades adicionais. assim, tem-se assistido ao desenvolvimento de vários produtos para incontinência, cujo objetivo é melhorar a qualidade de vida dos pacientes, face aos danos causados pela doença. tratam-se de sistemas concebidos para aumentar a autoestima das pessoas na sua presença em público, face ao receio de algum evento de incontinência. nesse sentido, o principal objetivo deste trabalho, é a criação de um sistema de alarme para incontinentes, que é constituído por uma componente de deteção de líquido, incorporada em roupa interior específica para incontinentes e uma componente de geração de alarme, que corresponde a um dispositivo android . a comunicação entre os dois dispositivos é estabelecida, através do protocolo bluetooth. a componente de deteção de líquido, inclui a implementação de um sensor de líquido baseado em materiais fibrosos, integrado no substrato têxtil específico para incontinentes. é também responsável por, processar a informação proveniente do sensor e por comunicar com o dispositivo android . os resultados provenientes do sensor consistem na resistência elétrica produzida, relacionada com a quantidade de líquido libertado. por sua vez, a componente de geração de alarme, é uma aplicação móvel, concretizada com o propósito de emitir alarmes, quando recebe sinais provenientes do dispositivo. além disso, fornece uma interface de configuração do dispositivo de deteção, para o nível de alarme desejado. permite ainda registar eventos de incontinência numa base de dados local. o trabalho desenvolvido tornou possível a conceção e implementação de um sistema de alarme para incontinentes, sendo que o nível de alarme pode ser configurado em função da quantidade de líquidos. a existência duma base de dados com registo de eventos permite a monitorização em modo offline por períodos mais longos.",
      "urinary incontinence is a health problem with multiple repercussions that negatively affect the life quality of the patients. moreover, it is difficult to perform accurate diagnosis on the degree of incontinence. without a correct diagnosis, the treatment, which already is complicated itself, incurs in additional difficulties. several products have been developed within the last years for incontinence, aiming to improve the life quality of patients, due to the damages caused by the disease. these systems are designed to increase the patient self-esteem when in public situations, due to the afraid of some incontinence event. accordingly, the aim of this work, is the development of an alarm system for incontinence, which comprises a liquid detection component, incorporated into specific underwear for incontinence, and an alarm activation component, which corresponds to an android device. the communication between the two devices is established through the bluetooth protocol. the liquid detection component includes the implementation of a liquid sensor based on fibrous materials, in a specific textile substrate for incontinence. it is also responsible to process the information from the sensor and to communicate with the android device. the results from the sensor consists on the relationship between the produced electrical resistance and the amount of liquid released. in its turn the alarm activation component is a mobile application, carried out with the purpose of emitting an alarm when receive signals from the electronic device. furthermore, provides a configuration interface of the detection device, with the desired alarm level, allowing to register incontinence events in a local database. the developed work made possible the conception and implementation of an alarm system for incontinence. the alarm level can be set depending on the amount of liquid. the existence of an event log to the database allows the offline monitoring for longer periods."
    ],
    [
      "over the past years, research on cancer genomics has been boosted by the advances in high throughput sequencing technologies. the cancer genome atlas (tcga) project is an effort to map the genomic alterations possibly associated with specific types of tumours and aims to improve the prevention, diagnosis and treatment of cancer. the generation of large and heterogeneous datasets, as a result of tcga and other similar projects, creates the need to use advanced bioinformatics and computational tools for the analysis of cancer genomic data. despite different bioinformatics frameworks have been established in order to explore and perform comprehensive analysis of cancer datasets, the area of logic and probabilistic logic programming has not been sufficiently explored in the analysis of cancer data. the main goal of this thesis was to explore problog – a probabilistic logic programming (plp) language – to encode interactions on heterogeneous cancer genomics datasets that may lead to new insights. to accomplish this objective, our work consisted in the elaboration of a python program and a problog framework. the used datasets involved stomach cancer genomic data. the python program – proceomics – aimed to process and format cancer genomic data so it could be used by problog programs. the problog framework – problog knowledge base (kb) – intended to codify the data previously processed by proceomics. to evaluate the consistency of the developed framework and explore possible relations between the different types of genomic data, queries were formulated to the problog kb. thus, this thesis provides a tool that establishes a link between the genomic data contained in public databases with probabilistic logic programs. we hope this work may help to overcome future efforts to use plp on genomic data analysis.",
      "ao longo dos últimos anos, devido aos avanços significativos nas áreas tecnológicas responsáveis pelo estudo do genoma humano, o estudo dos dados genómicos associados a casos de ocorrência de cancro tem crescido exponencialmente. the cancer genome atlas (tcga), é um projeto que consiste no mapeamento de mudanças a nível genómico que possam estar associadas com algum tipo específico de cancro e que, por sua vez, possam fornecer alternativas mais avançadas de prevenção, prognóstico e tratamento relativamente àquelas já existentes. no entanto, a geração de inúmeros e extensivos datasets tem, consequentemente, vindo a aumentar. apesar de já existir um número significativo de ferramentas e metodologias bioinformáticas que têm como objetivo explorar e realizar análises sobre os diferentes datasets relativos a variados tipos de cancro, a área da programação lógica, bem como da programação lógica probabilística, não têm sido frequentemente exploradas de modo a alcançar esse mesmo objetivo. posto isto, o objetivo principal desta tese consistiu na exploração de uma extensão probabilística de uma linguagem lógica – problog – de modo a codificar e explorar interações complexas entre diferentes datasets, visando ainda a descoberta de novas relações entre eles. de modo a alcançar este objetivo, o trabalho desenvolvido consistiu na elaboração de um programa em python e de uma framework em problog. todos os dados utilizados nas análises realizadas nesta tese são relativos à genómica do cancro do estômago. o programa em python – proceomics – teve como objetivo processar e formatar dados genómicos de cancro de modo a ser possível codificar esses mesmos dados em programas problog. por sua vez, a framework em problog – problog kb – foi criada com o intuito de codificar os dados previamente processados pelo programa. de modo a avaliar a consistência da framework desenvolvida e explorar possíveis relações entre os diferentes tipos de dados genómicos, foram colocadas queries à problog kb. assim sendo, esta tese forneceu uma ferramenta que estabelece uma ligação entre os dados genómicos, contidos em base dados públicas, e programas lógico probabilísticos. esta ligação poderá ajudar a ultrapassar os poucos esforços aplicados na utilização deste tipo de linguagem para estudar dados genómicos."
    ],
    0.3
  ],
  [
    [
      "in recent years there have been many improvements to medical procedures, involving the use of augmented reality technology to provide new innovative approaches to difficult tasks that are often required of the patients, requiring less physical exertion from the to achieve the same results or simply looking at the problem in a new perspective. virtual reality technology has the capability of creating an interactive, motivating environment in which practice intensity and feedback can be manipulated to create individualised treatments to retrain movement. currently there is a very large amount of people suffering from minor to severe functional limitations, impairments such as loss of range of motion, decreased reacting times, disordered movement organisation, and impaired force generation create deficits in motor control that effect the personss capacity for independent living and economic self-sufficiency. the use of augmented reality is starting to be used in more medical scenario’s and in the treatment of many diseases generally co-related with motor difficulties or recovery treatments. one of the diseases that has been looked more prominently for augmented reality development is the parkinson’s disease which causes its patients to suffer severe gait constriction and whose generalised gait treatments didn’t produce a significant improvement in the patients gait without the use of heavy medication. one other important detail to take notice is that the parkinsons disease causes the patient to abruptly enter a freezing state without any kind of warning which can lead the patient to fall and severally harm itself depending on the situation at hand. the objective of this thesis is to explore the possibilities of the use of augmented reality in an attempt to improve gait in patients suffering from parkinson’s disease. for this purpose many augmented reality glasses were analysed selecting the best one in terms of affordability, comfort and utility. the application developed has the objective of improving the patients gait by displaying an augmented reality supper- imposed path for the patient to follow matching auditory cues with each of the patients steps and also helping the patient of he suddenly finds himself affected by a ”freezing” episode.",
      "recentemente tem sido feitos vários melhoramentos nos procedimentos médicos, recorrendo ao uso de tecnologias como realidade aumentada para fornecer uma nova abordagem a tarefas complicadas que são frequentemente requeridas aos pacientes, requerendo um menor esforço físico e feedback imediato ou simplesmente para obter uma nova perspetiva sobre o problema em questão. o uso de realidade aumentada tem vindo a ser cada vez maior, sendo usado em cada vez mais procedimentos e para tratamento de variadas condições principalmente focadas em dificuldades motoras e fisioterapia. uma das doenças que despertou maior interesse no uso de realidade aumentada no seu tratamento é a doença de parkinson, conhecida por causa deterioramento nas capacidades motoras dos afetados causando problemas na marcha da pessoa que, afetam varias tarefas do seu dia a dia. outro detalhe importante da doença de parkinson é que os afetados também tem o que são chamados de episódios de ”congelamento” que acontecem quando o paciente de repente e sem nenhum aviso previ-o fica paralisado durante uns instantes, o que pode provocar a queda da pessoa. estes episódios não são constantes podendo variar bastante na ocorrência e na intensidade de pessoa a pessoa. o objetivo desta dissertação é a exploração das possibilidades do uso de realidade aumentada numa tentativa de melhorar a marcha das pessoas afetadas com a doença de parkinson. para este propósito muitas ferramentas de realidade virtual foram examinadas escolhendo uma que seja o menos intrusiva possível para facilitar o uso pelo paciente e que tenha as especificações necessárias para o bem funcionar da aplicação. a aplicação de realidade virtual terá então o objetivo de melhorar a marcha do paciente através do seu uso mostrando ”pégadas” que irão servir para o paciente se orientar e ajudar o paciente quando ele estiver sobre o efeito de congelamento para evitar que cause danos graves a si próprio caso ocorra numa situação complicada."
    ],
    [
      "the language processing course at minho’s university uses a virtual machine implemented in c with its interface being implemented with the gtk toolkit. however, it is neither very informative nor very easy to install. the goal in this master’s project is to analyze and model the entire virtual machine’s system and build a web application with a graphical interface. the new tool offers two main characteristics: compiling and reporting errors in programs written for the virtual machine; and animate its execution, displaying the internal state of the vm and providing the user an interface to control the execution. in this document, a study of existing technologies will be carried out, focusing in detail on the current virtual machine vm. after this analysis, a solution will be proposed, followed by a detailed explanation of its implementation.",
      "na unidade curricular de processamento de linguagens tem-se utilizado uma vm doméstica implementada em c com uma interface gtk. no entanto, esta não é muito informativa nem muito fácil de instalar. o objetivo nesta dissertação é fazer uma análise e modelação de todo o sistema e construir uma aplicação web com uma interface gráfica. a nova ferramenta oferece duas funcionalidades principais: compilar e reportar erros em programas escritos para a vm e, se o programa estiver correto, animar a sua execução mostrando o estado interno da vm e fornecendo ao utilizador uma interface de controlo sobre a execução. neste documento, será realizado um estudo das tecnologias existentes, focando em detalhe a máquina virtual atual vm. após esta análise, será apresentada uma proposta de solução, seguida de uma explicação detalhada da sua implementação."
    ],
    0.06666666666666667
  ],
  [
    [
      "tools that detect security problems are very important nowadays and for the people doing code reviews it is even more important to have tools that identify vulnerabilities in the code. in this way companies that provide applications can be more confident that the code they deploy is almost vulnerabilities free. a subset of these tools, known as static application security testing (sast) tools, rely on the analysis of the source code aiming at looking for patterns that correspond to vulnerabili ties. these analyzers use mainly language processors to help them extract from the source code the information they need. languages exist for many years but they never ceased to exist because they are in constant development.the description of languages is supported by grammars. grammars also evolve to sustain the referred languages evolution. they were primarily used for compilers to analyse the structure of the language and parse it; nowadays they help many other tools like sast for example. having a tool that can detect vulnerabilities is very useful like was said, but to identify those vulnerabilities it is necessary to find patterns in languages. by finding these abstract patterns the work is simplified since all concrete languages will present similar vulnerabilities. for example, sql injection is a vulnerability that is shared across almost all languages; so, it is possible to define one general pattern to capture that common vulnerability in each language. those patterns are defined over abstract syntax trees (ast). to build an ast while parsing a program, checkmarx uses a set of functions called visitors that are associated to the productions of the programming language grammar. in that context, the language factory tool developed by checkmarx generates automatically some visitors and let programmers to generate the others by dragging and dropping rules. the main objective of language factory tool is to aid programmers understanding how to create visitors and, at the same time, to generate as many visitors as possible to be used directly. when checkmarx is working on a new language to to include support for that language in the cxsast tool, the use of language factory will help creating the appropriate visitors making this process simpler and faster. the master’s project reported in this dissertation appears in that framework aiming at the improvement of the checkmarx language factory making it capable of infer more visitors from the new language’s grammar.",
      "ferramentas que detectam problemas de segurança são muito importantes nos dias de hoje. para quem tenta encontrar vulnerabilidades no código é ainda mais importante ter ferramentas que as identifiquem. dessa forma, as empresas que desenvolvem aplicação podem ter mais confiança de que o código que implementaram não contem vulnerabilidades um subconjunto dessas ferramentas, conhecidas como ferramentas de static application security testing (sast), conta com a análise de código-fonte com o objetivo de encontrar padrões que correspondam a vulnerabilidades. esses analisadores usam principalmente processadores de linguagem para os ajudar a extrair do código-fonte as informações de que precisam. as linguagens existem há muitos anos mas nunca deixaram de existir porque estão em constante mudança. a descrição das linguagens é suportada por gramáticas. as gramáticas também evoluem a par da evolução das referidas linguagens. eles foram usados principalmente por compiladores para analisar a estrutura da linguagem e analisá-la; hoje em dia usam-se muitas outras ferramentas como o sast por exemplo. ter uma ferramenta que detecte vulnerabilidades é muito útil como foi dito, mas para iden tificar essas vulnerabilidades é necessário encontrar padrões nas linguagens. ao encontrar esses padrões, simplificamos oo trabalho, pois todas as linguagens compartilham as mesmas vulnerabilidades, por exemplo, sql injection é uma vulnerabilidade que é compartilhada em quase todas as linguagens, portanto, deve ter padrões em comum. assim, o objetivo deste projeto de mestrado é melhorar o número de visitors gerados na ferramenta language factory desenvolvida pela checkmarx. esta ferramenta gera um número de visitors e permite que os programadores gerem estes através de associar as respectivas regras. esta ferramenta tem como principal objetivo fazer com que os programadores entendam como gerar visitors de forma a gerar o maior número possível destes para que possamos usá-los; por exemplo quando a checkmarx está a adicionar uma nova linguagem na ferramenta cxsast, um dos passos é ter uma ast com todos os visitors para a nova linguagem e se tivéssemos quase todos os visitors gerados poderíamos tornar o trabalho mais simples e rápido para os programadores. o projeto de mestrado mencionado nesta dissertação surge nesse enquadramento, visando o aperfeiçoamento da ferramenta language factory da checkmarx tornando-a capaz de inferir mais visitors, através do uso de uma gramática nova de uma linguagem de progra mação."
    ],
    [
      "este trabalho pretende analisar, em simultâneo, um projeto de implementação de um software de gestão de serviços de tecnologias de informação desde o seu início á sua conclusão numa empresa, assim como a própria aplicabilidade do software de gestão escolhido sap itsm (it service management). pretende deixar um contributo à forma e conteúdo da aplicação sap itsm e lançar as bases na empresa, em que o estudo se insere, para aproximação dos serviços prestados às boas práticas itil, com a certificação iso /iec 20000 no horizonte. para cumprir esse objetivo, exploram-se os conceitos e a literatura adjacente ao tema, identificando os principais problemas e preocupações a ter em consideração na aplicação das boas práticas itil. metodologicamente segue-se o estudo de caso, beneficiando da presença do autor no seio do trabalho da equipa de implementação, levando a cabo, para além da observação, a que se junta documentação para análise. por fim apresentam-se os resultados, discutindo o seu significado e concluindo quanto ao problema de investigação identificado.",
      "this work aims to analyze, simultaneously, a project to implement a software service management information technology since its beginning to its conclusion in an organization, as well as the applicability of the chosen management software sap itsm (sap it service management). the contribution to the content of the form of sap itsm application and lay the foundations in the organization in which the study is situated, for approximation of services to itil best practices, with the iso / iec 20000 certification on the horizon. to meet this goal, we explore the concepts and it adjacent to the subject literature, identifying the key issues and concerns to be considered in the implementation of itil best practices. methodologically follows the case study, benefiting from the presence of the author within the work of the implementation team, carrying out, beyond the observation that joins documentation for analysis. finally presents the results and discuss the meaning and completing the investigation as identified problem."
    ],
    0.22499999999999998
  ],
  [
    [
      "os avanços tecnológicos e industriais, as constantes inovações e a necessidade de melhoria contínua ao longo dos últimos anos pelas instituições, têm proporcionado um exponencial aumento na quantidade de informação gerada, e consequentemente armazenada por estas. as instituições de saúde, como qualquer outra organização, geram uma grande quantidade de dados relativos aos seus processos, os quais, muitas das vezes, não são registados e geridos adequadamente, tornando muito difícil a sua posterior gestão e manipulação. por outro lado, torna-se necessária a compreensão dos custos envolvidos na prestação dos diferentes cuidados de saúde por parte dos gestores hospitalares, para a melhoria da qualidade e e ciência dos diversos processos diários neste tipo de instituições. os serviços de informação para gestão (sig) hospitalares têm entre diversas responsabilidades, o registo de todos os procedimentos hospitalares relacionados com a área de gestão hospitalar, originando um grande volume de dados e informação, a qual necessita de ser bem manipulada. estes dados são de extrema importância em tomadas de decisão. como tal, os mesmos precisam de sofrer um processo de modelação e organização através da utilização de sistemas projetados especi camente para esta fun- ção, como é o caso dos sistemas de business intelligence (bi). o conceito de bi emergiu nas instituições hospitalares como medida para solucionar o problema existente no tratamento e processamento dos dados na área da sáude, transformando-os em informação e conhecimento útil para os pro ssionais. de uma forma geral, os sistemas de bi representam um conjunto de tecnologias e aplicações, que atuam na recolha, análise e difusão dos dados existentes, funcionando como suporte para tomadas de decisão e cientes. o principal objetivo deste projeto prende-se, essencialmente, com o desenvolvimento de uma aplicação de suporte multidimensional para sistemas de bi a ser implementada no centro hospitalar do porto, para uso exclusivo por pro ssionais pertencentes aos sig da mesma instituição. esta aplicação permite a importação direta de folhas de excel, que contém registos efetuados por estes pro ssionais, para uma base de dados, alimentando diretamente a data warehouse (dw) e as data marts (dms) existentes e a existir para este propósito. com isto, estes pro ssionais passam a ter total responsabilidade na manutenção e gestão de registos mantidos em folhas de excel, o que não acontecia anteriormente, mantendo as componentes dimensional e factual da dw e dms através de dados mantidos em folhas de cálculo. veri cou-se que a aplicação desenvolvida, enquanto ferramenta de suporte para sistemas de bi, é inteiramente capaz de ser implementada e integrada nas operações diárias da organização hospitalar, facilitando a gestão destes dados e o trabalho dos pro ssionais, proporcionando um aumento da uidez, rapidez, tratamento, recolha e análise da informação.",
      "the technological and industrial advances, the constant innovations and the need for continuous improvement have created an exponential growth in the amount of information created and stored. health institutions, as any other organization, generate a large amount of data regarding their processes, which, most times, are not properly recorded and managed, making its later management and manipulation very hard. on the other hand, the understanding of the costs associated with the di erent health care by the hospital managers in order to improve the quality and e ciency of the daily processes of this kind of institution. the hospital services for information management (sig) have a diverse set of responsibilities, such as the recording of every hospital procedure regarding the eld of hospital management which leads to a huge volume of data and information that needs to be well handled. these data are of extreme importance in the decision making process. as such they need to undergo a process of modeling and organizing through the use of systems developed speci cally for this job, such as the business intelligent (bi) systems. the concept of bi appeared in the hospital institutions as a way to solve the existing problem of processing and handling data in the health area and making them into useful information and knowledge for the professionals. most of the bi systems represent a set of technologies and applications that act on the gathering, analysis and propagation of existing data, working as support for e cient decision making. the main goal of this project lies with development of a multidimensional support application for bi systems for exclusive professional use in oporto hospital center. this application enables the direct import of excel sheets containing records made by the aforementioned professional into a database and feeding directly the existing data warehouse (dw) and the data marts (dms)and the ones that will exist for this purpose. this way the professionals take full responsibility for maintaining and managing the records kept in excel sheets, which did not happen before, keeping the dimensional and factual components of the dw and dms through data kept in spreadsheets. it was veri ed that it is possible for the developed application, while being a support tool for bi systems, to be completely implemented and integrated in daily operations of an hospital institution, making the data management and the professionals work easier and increasing the uidity, quickness, treatement, gathering and analysis of data."
    ],
    [
      "human interaction with machines has never been so frequent as nowadays. in order to reduce the redundant workload of a human being that answers repeated and trivial questions regarding customer support on a digital marketing website, this work has the purpose of replacing this tedious job with an informatics tool, a dialogue tool. a dialogue tool like a chatbot that could handle customer support to a digital marketing website, provides the opportunity of placing human resources on ”non mechanical tasks”. given that chatbots exchange messages directly with customers, they could collect required protocol information in all the interactions. in spite of the possibility of needing human assistance, he will not need to ask these standard questions and will improve its efficiency. by automating these required dialogues to answer questions about certain products, that would otherwise be responded by a human, the organizations will have the opportunity to place human resources in another sectors that are not so easily automated.",
      "a interação humana com máquinas nunca foi tão frequente como nos dias de hoje. com a intenção de reduzir a quantidade de trabalho de um ser humano que receberia ao responder a questões triviais e repetidas no que diz respeito a suporte ao cliente, este trabalho tem o propósito de substituir um trabalho entediante por uma ferramenta informática, uma ferramenta que possibilite o diálogo entre o cliente e o serviço de suporte. uma ferramenta como um chatbot que poderia fornecer suporte ao cliente num website de marketing digital iria providenciar às empresas a oportunidade de alocar trabalhadores para tarefas ”menos mecânicas”. dado que os chatbots trocam mensagens diretamente com os clientes, estes podem recolher informações que são sempre necessárias e protocolares em todas as interações. assim sendo, mesmo que este diálogo requira possivelmente um ser humano, este irá prescindir de fazer estas perguntas padrão, melhorando assim a eficiência deste trabalho (suporte ao cliente). ao automatizar diálogos necessários para responder a questões acerca de produtos que, de outra forma seriam respondidas por um ser humano, as organizações estarão a poupar tempo e dinheiro que podem ser aplicados noutros sectores menos propícios a serem automatizados."
    ],
    0.0
  ],
  [
    [
      "a atual era digital depende de dados numa perspetiva de grande escala e as organizações requerem sistemas de armazenamento que funcionem corretamente sob falhas. por exemplo, falhas de energia podem levar à perda de dados em aplicações cujos ficheiros ainda estão armazenados em memória, isto é, num meio volátil. evitar estes cenários de perda de dados constitui um grande desafio, uma vez que exige que os programadores apliquem primitivas de sincronização (fsync()) que garantem a durabilidade dos dados, a custo de uma potencial diminuição do desempenho das aplicações. as ferramentas de injeção de faltas permitem ajudar os programadores com testes automáticos e consequente validação das suas políticas de durabilidade de dados. no entanto, as abordagens atuais para sistemas de ficheiros focam-se: (1) na manipulação direta de hardware; ou (2) em erros internos de implementação do sistema de ficheiros, e não na interação da aplicação com o mesmo. além disso, estas ferramentas são limitadas quanto à informação disponibilizada ao programador, de forma a este compreender a causa efetiva que levou à perda de dados reportada. para resolver estes desafios, esta dissertação propõe o lazyfs, um sistema de ficheiros que simula a perda de dados utilizando uma abordagem de injeção de faltas em software reprodutível e automática. este sistema tem uma cache dedicada que gere os dados das aplicações e a sua sincronização para uma camada persistente. a pedido, o lazyfs pode limpar todos os dados que não foram previamente sincronizados, fornecendo também aos programadores informações relevantes sobre os dados em risco de serem perdidos com potenciais falhas de energia. o desempenho e validação da correção do protótipo demonstra que a nossa solução consegue avaliar a durabilidade dos dados de aplicações, sem adicionar uma sobrecarga significativa à sua execução normal. foram também reproduzidas quatro anomalias em diferentes bases de dados e o protótipo já se encontra integrado na ferramenta de injeção de faltas jepsen. atualmente, o lazyfs está a ser usado, juntamente com o jepsen, para avaliar sistemas de bases de dados em produção, como o percona mysql server, mongodb e o etcd. adicionalmente, foi descoberta uma possível violação de coerência no etcd, que está a ser estudada pela sua equipa de desenvolvimento.",
      "the digital era we are living in depends on data in a massive scale perspective and organizations require reliable storage systems to perform correctly under failures. for instance, power outages may lead to data loss for applications whose file system (fs) data is still cached and has not yet been flushed to a persistent storage medium. avoiding these data loss scenarios is challenging since it requires application developers to employ synchronization primitives (fsync) to ensure data durability, which, as a consequence, may significantly impact the performance of applications. through fault injection tools one could potentially help developers, by providing automatic testing and validation of their data durability policies, while pinpointing potential issues. however, current testing approaches for assessing the durability of data stored at fss are: (1) done manually through hardware manipulation; or (2) focus on internal errors found at the fs implementation and not on the interaction between the application and the fs. further, these tools provide limited output to help users find the root causes of observed data loss. to solve these challenges, this dissertation presents lazyfs, a fs that simulates data loss using a reproducible and automatic software-based fault injection approach. it has an internal dedicated cache which manages applications data and its flushing to persistent storage. upon request, it can discard all data that was not previously persisted, while helping developers by providing valuable outputs of the data in risk of being lost with a power failure. the performance and correctness validation of lazyfs’s prototype shows that our solution can evaluate applications data durability without imposing significant overhead to their normal execution. also, four different database anomalies were reproduced with lazyfs. the prototype was also integrated with the widely-used jepsen fault injection framework, and is being used for assessing the correctness of production databases such as percona mysql server, mongodb and etcd. a possible consistency violation was detected in the etcd database and is currently being studied by their development team."
    ],
    [
      "nowadays, the products deriving from the biotechnology industry have become quite valu able in the world market. hence, it is highly advantageous to find out how the prices of the different chemical compounds needed for biotechnological processes behave in the bioeconomy. the sisbi project was developed to allow the retrieval and collection of different prices associated with certain chemical compounds through different available sources and databases. with access to this information, some behaviours and patterns can be detected in the price variations, indicating other relevant knowledge, such as the biotechnological interest of this compound in the field. however, it is necessary to take into account that sisbi data, although relevant, have inconsistencies that do not support an efficient analysis of these data, which is the case for the existence of duplicates, different units and problems in the price integration. as a result, this study developed algorithms to identify and solve these problems and to analyze the prices of compounds through time series. to effectively evaluate these data, a new database, bioanalysis, was built based on the data from the sisbi project. then, several preprocessing methods were applied, including the elimination of duplicates, conversion of units, removal of defective and inconsistent prices, which led to the solution of the various complications encountered. consequently, once the data was prepared for analysis, the prices pertaining to two specific metabolites, 4-aminopyridine and methane, were examined. thus, different price variations over time were compared between different configurations (quantity + unit) of the same metabolite and between different metabolites. these variations were divided by the different price providers to identify any specific relationship or pattern depending on where the data originate. however, in this study, no particularly cheap provider was detected between 4-aminopyridine configurations or between the two metabolites. the only association found occurred only between certain methane configurations. in addition, the price variations analyzed are mostly constant, and when they are not, they do not show any pattern or seasonality. these results revealed that, using only the prices available to date, no correlation was determined by identifying the providers associated with low prices when comparing different metabolites or configurations.",
      "atualmente, os produtos resultantes da indústria biotecnológica têm-se tornado bastante importantes no mercado mundial. desta forma, é altamente vantajoso descobrir como se comportam os preços dos diferentes compostos químicos necessários para os processos biotecnológicos na bioeconomia. o projeto sisbi foi desenvolvido de modo a permitir a recolha e coleção de diferentes preços associados a determinados compostos químicos através de diversas fontes e bases de dados disponíveis. com o acesso a esta informação, alguns comportamentos e padrões podem ser detetados na variação dos preços, indicando outras informações relevantes como o interesse biotecnológico desse composto na área. no entanto, é necessário ter em conta que os dados da sisbi, embora relevantes, apresentam inconsistências que não permitem analisar de forma eficaz estes dados, como é o caso da existência de duplicados, diferentes unidades e problemas na integração dos preços. por esta razão, este estudo comprometeu-se a desenvolver algoritmos para identificar e resolver estes problemas, e para analisar os preços dos compostos através de séries temporais. de modo a avaliar eficazmente estes dados, uma nova base de dados, bioanalysis, foi construída com base nos dados do projeto sisbi. de seguida, diversos métodos de pré-processamento foram realizados, incluindo a eliminação de duplicados, conversão de unidades, remoção de preços defeituosos e não consistentes, que levaram à resolução das várias complicações encontradas. por consequência, uma vez os dados prontos para a análise, os preços pertencentes a dois metabolitos específicos, 4-aminopiridina e metano, foram examinados. assim, diferentes variações de preços ao longo do tempo foram comparadas entre diferentes configurações (quantidade + unidade) do mesmo metabolito e entre diferentes metabolitos. estas variações foram agrupadas pelas diferentes fontes de preços de modo a identificar alguma relação ou padrão específico dependente ao local de onde os dados provenieram. contudo, neste estudo, não se detetou nenhuma fonte em particular consistentemente barata entre configurações do 4-aminopiridina ou entre os dois metabolitos. a única associação descoberta ocorreu apenas entre determinadas configurações do metano. para além disso, as variações dos preços analisados são maioritariamente constantes, e quando não são, não demonstram nenhuma tendência ou sazonalidade. estes resultados revelaram que, utilizando apenas os preços disponíveis até à data, nenhuma correlação foi determinada ao identificar as fontes associadas a preços baixos quando comparando diferentes metabolitos ou configurações."
    ],
    0.075
  ],
  [
    [
      "apreciando plataformas on-line de partilha de opiniões ou experiências sobre diversos serviços pode-se reparar que existe a necessidade, por parte dos utilizadores, de saber opiniões de outros. aqui insere-se o sentido de comunidade que é prevalente na internet. desde redes sociais a videojogos este conceito faz-se presente onde a criação de uma comunidade pode ser a chave para o sucesso de uma nova ideia. neste caso a nova ideia trata-se de healthadvisor mobile solutions (head-ms), integrante de healthadvisor ecosystem (head-es), head-ms aponta para a criação de uma plataforma móvel onde é possível partilhar opiniões e avaliações sobre serviços de saúde existentes, estas opiniões e avaliações são feitas pela comunidade e para a comunidade, gerando assim valor para os utilizadores numa espécie de simbiose entre utilizador e plataforma. o que surge com a existência do healthadvisor mobile solutions trata-se de uma solução não só integrante mas ainda parte vital de um ecossistema maior healthadvisor ecosystem que tem como objetivo a criação de uma comunidade como suporte principal e mais valia do sistema, potenciando o seu crescimento e florescimento graças à sua transparência e simplicidade. com o desenvolvimento desta dissertação o que se alcançou foi a criação de uma aplicação móvel, cross-platform, com foco na comunidade. aqui um utilizador pode-se informar sobre serviços de saúde, saber opiniões/avaliações de outros utilizadores e ainda partilhar a sua própria opinião sobre esses mesmos serviços. a aplicação foi desenvolvida em react native servida de dados por uma web rest api.",
      "assessing on-line platforms of opinion or experience sharing on various services it is possible to notice that there is a need, from the users, to know opinions from others. it‘s here where the sense of community emerges. prevailing on the internet from social networks to video games this concept makes itself present where the creation of a community can be the key to the success of an idea. in this case the new idea is healthavisor mobile solutions (head-ms), which integrates the healthadvisor ecosystem (head-es), head-ms aims for the creation of a mobile platform where it is possible to share opinions and ratings on existing health services. these opinions and ratings are made by the community for the community, generating value to the users in a sort of symbiosis between the user and the platform. what emerges with the existence of healthadvisor mobile solutions is a solution that not only belongs but is also a vital part of a bigger ecosystem, healthadvisor ecosystem, that has as its objective the creation of a community as main support and is an important asset to the system, potentiating its growth and allowing it to thrive thanks to its transparency and simplicity with the development of this dissertation what was achieved was the creation of a cross-platform mobile application with a focus on the community. here a user can find out about health services, find out about other users’ reviews and ratings and also share their own opinion about these same services. the application was developed in react native and is served data by a web rest api."
    ],
    [
      "a simulação de componentes é uma importante ferramenta para o auxílio no desenvolvimento de sistemas, realização de testes e uma melhor compreensão acerca desses mesmos componentes por parte de investigadores e desenvolvedores. esta pode ser realizada utilizando diferentes abordagens, mas tem de permitir uma reprodução fiável do ambiente. a presente dissertação assenta sobre uma plataforma já existente, o minha. esta plataforma permite simular sistemas distribuídos e é capaz de simular todas as interações entre várias máquinas ao nível da rede. embora a plataforma seja capaz de realizar a simulação ao nível da rede, esta não era capaz de realizar qualquer simulação ao nível dos discos das máquinas simuladas, até à realização do dissertação. é este o problema que a presente dissertação se propõe resolver, criando um módulo que realize a intercepção das operações sobre o disco e que trate as mesmas de forma a simular a existência de um disco independente para cada uma das máquinas simuladas. esta dissertação tem como objetivo dotar a plataforma de um novo módulo que permita que a mesma consiga simular sistemas que necessitem de recursos do disco, como bases de dados. até à realização da dissertação a plataforma não fazia qualquer controlo sobre os recurso requeridos do disco, o que provocava resultados de simulação inconsistentes devido à partilha não controlada do disco da máquina onde a simulação era realizada. o modelo de simulação apresentado é validado experimentalmente com um microbenchmark e com tpc-b sobre a base de dados hypersql. de realçar que o resultado da dissertação em questão já se encontra integrado na plataforma e disponível no repositório oficial da plataforma minha que se encontra alojado em http://code.google.com/p/minha/.",
      "the simulation components is an important tool to support the development of systems, testing and a better understanding of those components by researchers and developers. this can be accomplished using different approaches, but must allow faithful reproduction of the environment. this dissertation is based on an existing platform, minha. this platform allows simulate distributed systems and is capable of simulating all the interactions between multiple machines at the network level. although the platform is able to perform the simulation at the network level, this was not able to perform any simulation to the level of the disks of machines simulated until completion of the dissertation. is this the problem that this dissertation proposes to solve by creating a module that performs the interception operations on the disk and that treat the same in order to simulate the existence of an independent disk for each of the simulated machines. this dissertation aims to provide a platform for a new module that allows simulate systems that require disk resources, such as databases. until completion of the dissertation platform made no control over the resources required from disk, which caused inconsistent simulation results due to uncontrolled sharing disk of the machine where the simulation was performed. the simulation model is validated experimentally presented with a micro-benchmarks and tpc-b on the hypersql database. note that the result of the dissertation is already integrated into the platform and available in the official repository of minha platform that is available in http://code.google.com/p/minha/."
    ],
    0.0
  ],
  [
    [
      "the growth of technological infrastructures in companies and institutions, along with the lack of specialized human resources to manage them effectively, sometimes creates misinformation about the actual status of these infrastructures, making them unstable or even unsafe. in that context, the need arises to create an autonomous and analytical information collection system. this system should be flexible and extensible enough to adapt to the various configurations of equipments and software available, capable to produce a report with as much information as possible, as well as some recommendations to improve infraestructure overall status, using only open source technology. this dissertation arises from this need and aims to design and implement this platform.",
      "o crescimento das infraestruturas tecnológicas existentes nas empresas e instituições, aliado à falta de colaboradores especializados nos seus quadros que façam a sua gestão de forma eficaz, faz com que por vezes exista desconhecimento do estado dessas infraestruturas, tornando-as instáveis ou até inseguras. nesse contexto, surge a necessidade da criação de uma plataforma autónoma de análise e recolha de informação. esta plataforma deverá ser flexível e expansível o suficiente para se adaptar às diversas configurações de equipamentos e de software disponíveis, produzindo um relatório com o máximo de informação possível, bem como algumas recomendações de melhorias para a infraestrutura, utilizando apenas serviços open source. esta dissertação surge dessa necessidade e tem como objetivo projetar e implementar essa plataforma."
    ],
    [
      "o reconhecimento de atividades utilizando smartphones tem ganho uma atenção redobrada nos últimos anos devido à adoção generalizada destes dispositivos e consequentemente dos seus vários sensores. estes sensores são capazes de fornecer dados bastante relevantes para este fim. os sensores não intrusivos, em particular, oferecem a vantagem de recolher dados sem exigir ao utilizador a realização de quaisquer ações específicas ou o uso de dispositivos adicionais. primeiramente são discutidos os diferentes tipos de sensores habitualmente utilizados em smartphones, incluindo acelerómetros, giroscópios, entre outros, e o tipo de informação que nos podem guarnecer. depois serão apresentados vários algoritmos e técnicas de machine learning, incluindo aprendizagem supervisionada, aprendizagem não supervisionada, e ainda aprendizagem por reforço. dentro destes são também apresentados alguns dos algoritmos mais utilizados. para finalizar esta primeira parte, serão ainda apresentadas alguns trabalhos realizados por outros investigadores na área. o objetivo desta tese passou, então, pela criação de uma aplicação destinada ao reconhecimento de atividades recorrendo exclusivamente ao uso de sensores não intrusivos presentes em qualquer smartphone. os dados colecionados por esses sensores forem submetidos a várias etapas de processamento e, após diversas iterações, obteve-se um conjunto de features altamente favoráveis ao treino dos modelos de machine learning criados. o melhor resultado foi obtido pelo modelo utilizando o algoritmo xgboost, que alcançou uma impressionante taxa de accuracy de 0.979. este resultado bastante sólido, permite verificar a alta eficácia do uso deste tipo de sensores para o reconhecimento de atividades.",
      "activity recognition using smartphones has gained increased attention in recent years due to the widespread adoption of these devices and, consequently, their various sensors. these sensors are capable of providing very relevant data for this purpose. non-intrusive sensors, in particular, offer the advantage of collecting data without requiring the user to perform any specific action or use any additional devices. firstly, we will discuss the different types of sensors commonly used in smartphones, including accelerometers, gyroscopes, among others, and the type of information they can provide us. next, we will introduce several machine learning algorithms and techniques, including supervised learning, unsupervised learning, and reinforcement learning. within these, some of the most commonly used algorithms will also be presented.to conclude this first part, some work carried out by other researchers in the field will also be presented. the objective of this thesis was, therefore, the development of an application designed for activity recognition using exclusively non-intrusive sensors available in any smartphone. the data collected by these sensors underwent several processing stages, and after numerous iterations, a set of highly favorable features for training the machine learning models was obtained. the most prominent result was achieved by the model using the xgboost algorithm, which achieved an impressive accuracy rate of 0.979. this quite robust result confirms the high effectiveness of using this type of sensors for activity recognition."
    ],
    0.3
  ],
  [
    [
      "o aumento exponencial do volume de dados gerados no mundo tecnológico atual é incontestável. a necessidade de armazenar e processar esses grandes volumes de dados levou a indústria a optar por soluções de armazenamento e processamento na nuvem. além disto, os desenvolvedores optam cada vez mais por sistemas de base de dados que permitem melhor desempenho e também tirar partido da variedade estrutural dos dados face aos sistemas relacionais tradicionais. estes sistemas que estão a surgir apresentam modelos de dados baseados em estruturas como, p.e., grafos ou índices chave-valor, e oferecem interfaces que podem ser apenas duas operações (put/get) ou, à semelhança dos sistemas relacionais com o sql, ter linguagens de interrogação específica. contudo, a migração de praticamente todos os componentes das infraestruturas das aplicações para a nuvem implica que os dados sejam processados e armazenados em infraestruturas de terceiros, ficando muitas vezes a privacidade destes comprometida. por outro lado, um dado problema pode ter dados com estruturas diferentes ou partes diferentes de uma aplicação podem ter necessidades diferentes quanto aos dados e, por isso, a diversidade entre sistemas de armazenamento leva uma grande complexidade em desenvolver sistemas que usem várias fontes de dados diferentes e heterogéneas eficientemente. assim, esta dissertação pretende dar uma resposta à problemática da gestão de dados de forma privada nas aplicações web, potencializando a utilização de múltiplos sistemas de fontes de dados heterogéneas. em específico, esta dissertação apresenta uma nova arquitetura, à qual se chamou polyglot, que permite a manutenção da privacidade dos dados, enquanto ao mesmo tempo possibilita a utilização de múltiplas fontes de dados heterogéneas e tira partido da nuvem para grande parte do processamento. esta arquitetura é também implementada sob a forma de um protótipo direcionado a um sistema de monitorização, que consiste no caso de estudo desta dissertação. este protótipo permite comprovar a validade da arquitetura, sendo que a implementação feita demonstra todas as funcionalidades essenciais ao funcionamento do sistema. mais ainda, este protótipo é também avaliado a nível de desempenho e utilização de recursos, permitindo demonstrar a viabilidade deste sistema para uma utilização em cenários reais. por último exploram-se algumas das funcionalidades mais relevantes que se poderiam adicionar ao sistema e os ganhos que estas trariam face à implementação atual, demonstrando o potencial do protótipo.",
      "the exponential growth of the volume of data currently generated in the technological world is undeniable. the need to store and process these large amounts of data lead the industry to choose storing and processing solutions based on cloud services. moreover, developers are increasingly choosing database systems that have better performance and also allow take advantage of the structural variety of data, when compared to traditional relational systems. these surging systems’ data models are based on structures such as, for instance, node graphs or key-value indexes and offer interfaces that can be only two operations (put/get) or, similarly to the relational systems and sql, have its own specific query language. however, the migration of practically every component of an application infrastructure to the cloud means the data is processed and stored in a third party’s infrastructure, usually compromising data privacy. on another hand, a single problem can have data with multiple structures or different parts of a given application may have different needs regarding data, and so, the diversity among database systems leads to great complexity while developing systems that use multiple and disparate data sources efficiently. this dissertation aims at answering the problem of private data management in web applications, while boosting the use of multiple disparate data sources. specifically, this dissertation presents a new architecture, called polyglot, which allows the maintenance of data privacy, whilst allowing the use of disparate data sources and taking advantage of the cloud for most processing. this architecture is then implemented on a prototype, which was developed for integration with a monitoring system, which is the dissertation’s case study. this prototype proves the validity of the architecture, since the implementation showcases all the major features required for the system to work. this prototype is experimentally evaluated, with metrics being taken for performance and resources usage, and showing the validity of this system for a real scenario usage. lastly we explore some of the most relevant features that could be added to the system, showing the increases in performance and resource economization they would bring, when compared to the current implementation, showcasing the potential of this system."
    ],
    [
      "procedural generation of content has been studied for quite some time and it is increasingly relevant in scientific areas and in video-game and film industries. procedural road layout generation has been traditionally approached using l-systems, with some works exploring alternative avenues. although originally conceived for biological systems modelling, the adequacy of l-systems as a base for road generation has been demonstrated in several works. in this context, this work presents an alternative approach for procedural road layout generation that is also inspired by plant generation algorithms: space colonisation. in particular, this work uses the concept of attraction points introduced in space colonisation as its base to produce road layouts, both in urban and inter-city environments. as will be shown, the usage of attraction points provides an intuitive way to parameterise a road layout. the original space colonization algorithm (sca) generates a tree like structure, but in this work, the extensions made aim to fully generate a inter-connected road network. as most previous methods the method has two phases. a first phase generates what is mostly a tree structure growing from user defined road segments. the second phase performs the inter connectivity among the roads created in the first phase. the original sca parameters such as the killradius help to control the capillarity of the road layout, the number of attraction points used by each segment will dictate its relevance establishing a road hierarchy naturally dependent on the distribution of the attraction points on the terrain. an angle control allows the creation of grid like or more organic road layouts. the distribution of the attraction points in the terrain can be conditioned by boundary maps, containing parks, sea, rivers, and other forbidden areas. population density maps can be used to supply an explicit probabilistic distribution to the attraction points. flow-fields can be used to dictate the flow of the road layout. elevation maps provide an additional restriction regarding the steepness of the roads. the tests were executed within a graphic toolbox developed simultaneously. the results are exported to a geographical information file format, geojson, and then maps are rendered using a geospatial visualisation and processing framework called mapnik. for the most part, parameter settings were intuitively reflected on the road layout and this method can be seen as a first step towards fully exploring the usage of attraction points in the context of road layout.",
      "gradualmente a geração procedimental de conteúdo tem-se tornado cada vez mais relevante, sendo maioritariamente aplicada em industrias como a dos vídeo-jogos e cinema. no que toca à geração procedimental de redes de estradas, grande parte das abordagens em torno deste tema são baseadas em l-systems. embora a área de aplicação dos l-systems tenha sido originalmente para produzir modelos de sistemas biológicos, mostrou também ser um algoritmo adequado para a geração procedimental de redes de estradas. este trabalho apresenta uma abordagem alternativa à geração procedimental de redes de estradas que também é inspirada num algoritmo procedimental de geração de plantas, colonização espacial, utilizando o conceito de pontos de atracão como base para gerar padrões de estradas. como será demonstrado, a utilização de pontos de atracão fornece uma maneira intuitiva de parametrizar um padrão de estradas desejado. como a maioria dos trabalhos feitos nesta área, este método tem duas fases. a primeira fase gera uma rede semelhante a uma árvore criada a partir de um ou mais segmentos iniciais da rede determinados pelo utilizador. a segunda fase trata de interligar as estradas geradas na primeira fase. os parâmetros iniciais do algoritmo de colonização espacial, como o kill radius, ajudam a controlar a capilaridade da rede, os pontos de atracão que influenciam cada segmento irão ditar a sua relevância na rede geral, estabelecendo a noção de hierarquia de estradas, dependendo da distribuição de pontos de atracão no terreno. o controlo do ângulo entre segmentos permite a criação de padrões de estradas tanto em forma de grelha como padrões mais orgânicos. a distribuição dos pontos de atracão no terreno pode ser influenciada por mapas de fronteira, que contem as áreas válidas e/ou inválidas, como parques, mar, rios, e outras áreas proibidas. mapas de densidade populacional podem ser usados para fornecer uma distribuição probabilística dos pontos de atracão. campos de forças, podem ser usados para ditar o fluxo da rede de estradas. mapas de elevação oferecem uma restrição adicional tendo em conta a inclinação das estradas. de um modo geral, as definições de parâmetros refletiram-se de um modo intuitivo nos padrões de redes de estradas gerados, e este trabalho pode ser considerado como um primeiro passo na exploração do conceito de pontos de atracão na área da geração de redes de estradas."
    ],
    0.3
  ],
  [
    [
      "a análise de grandes conjuntos de dados categóricos é um problema recorrente nas ciências sociais, comportamentais e biológicas. surge por isso a necessidade de diminuir estes dados conseguindo, contudo, que as perdas de informação sejam mínimas. tendo por base este problema, emergiu o tema desta dissertação, cujo objetivo passa pela análise exploratória do software r em busca de ferramentas para trabalhar com a análise de componentes principais categórica (acpcat), que surge como alternativa à tradicional análise de componentes principais (acp), e permite reduzir a dimensionalidade de variáveis medidas em escalas diferentes. de forma a compreender os princípios fundamentais deste método estatístico, foi feita uma busca de fontes bibliográficas, que permitiu, adicionalmente, destacar o pacote gifi e pacote homals como sendo os únicos que possuem funções que permitem aplicar a acpcat. estes pacotes foram explorados utilizando o mesmo dataset de exemplo, sendo feita uma descrição detalhada dos seus argumentos e dos valores e gráficos obtidos como forma de comparação das suas funcionalidades e recursos. o pacote gifi descende do pacote homals como sendo uma versão mais fácil de manipular e mais flexível devido a uma diferença na formulação da sua função de perda e ao facto deste utilizar b-splines. de modo a explorar as funcionalidades e limitações da função princals() do pacote gifi e para estabelecer de que forma dados biológicos podem ser trabalhados, foi também executada a análise de um conjunto de dados sensoriais recolhidos de provas de vinhos. com o intuito de permitir ao utilizador executar uma acpcat de forma simplificada e intuitiva foi criada uma aplicação web, que está disponível no endereço https://andreiagomes.shinyapps.io/gify/ e pode ser acessada livremente de qualquer dispositivo desde que este tenha acesso à internet. a aplicação, de nome gify, tem por base a função princals() do pacote gifi e permite ao utilizador, mesmo que este não tenha nenhum tipo de conhecimento sobre o software r, carregar o seu conjunto de dados, definir os seus parâmetros de análise, executar a acpcat e consultar os resultados, sendo tudo isto processado recorrendo a botões de seleção e espaços de preenchimento.",
      "the analysis of large sets of categorical data is a recurrent problem in the social, behavioral and biological sciences. therefore, the need arises to be able to reduce these data, however, achieving that the loss of information is minimal. based on this problem, the theme of this dissertation emerged, with the objective of conducting an exploratory analysis of the r software to identify available tools for performing categorical principal component analysis (catpca). catpca serves as an alternative to traditional principal component analysis (pca), allowing for dimensionality reduction of variables measured on different scales. in order to comprehend the fundamental principles of this statistical method, an search of bibliographic sources was conducted. this search also highlighted the gifi and the homals packages as the only ones equipped with functions that enable the application of catpca. both packages were explored using the same sample dataset, and a detailed description of their arguments, as well as the values and plots obtained, was provided for the purpose of comparing their functionalities and features. the gifi package descends from the homals package, designed to be more user-friendly and flexi ble, primarily due to a difference in the loss function formulation and its utilization of b-splines. to further delve into the functionalities and limitations of the function princals() from the gifi package, as well as to illustrate how biological data can be handled, an analysis of sensory data from wine tasting trials was also performed. in order to enable users to perform a simplified and intuitive catpca, a web application named gify has been created. this application is accessible at https://andreiagomes.shinyapps.io/gify/ and can be freely accessed from any device with internet access.the gify application is built upon the princals() function from the gifi package. it allows users, even those without any prior knowledge of the r software, to upload their dataset, define analysis parameters, execute catpca, and view the results obtained. all these operations can be performed using user-friendly buttons and input fields."
    ],
    [
      "this document describes an investigation performed at philips research (eindhoven, the netherlands) which aimed at improving the performance of philips current sleep/wake classification methods using portable devices based on unobtrusive cardiorespiratory signal modalities. particularly, this research focused on improving the detection of the sleep onset latency (sol) parameter. using a data set with recordings of healthy subjects, several alternative classification models were built, evaluated and compared to the current classifier. it was found that the performance of the current classifier, regarding sol detection, decreases with increasing sol, leading to an underestimation of this parameter, and possibly undervaluation of symptoms of sleep disruptions or even sleep disorders, during medical diagnosis. the main issue associated to this fault is that the current classifier is trained with examples from the entire night and therefore, for subjects with extended sol periods, fails to capture the characteristics of wake before the initiation of sleep. in this report a new method of distinguishing sleep from wake, to be applied with recordings of subjects with sol over 30 minutes, is proposed. the new method comprises two steps: one specially dedicated to identify wake before the initiation of sleep (and more accurately detect the moment of sleep onset (so)), and the other one to distinguish wake after sol. hence, it requires the use of two classifiers which differ regarding techniques for feature selection and are trained with examples of different periods of the night recordings.",
      "o presente documento descreve uma investigação desenvolvida na instituição philips research (eindhoven, the netherlands). o objetivo deste trabalho é melhorar o desempenho de atuais métodos philips de classificação sleep/wake que utilizam dispositivos portáteis baseados na aquisição de sinais cardiorrespiratórios. em particular, este trabalho foca o melhoramento do desempenho desta tecnologia na deteção do período de latência de sono. utilizando um dataset que inclui registos de gravações noturnas de sujeitos saudáveis, vários modelos de classificação foram construídos, avaliados e comparados com o modelo atual. verificou-se que o desempenho do classificador atual, no que diz respeito à deteção do período de latência de sono, é inferior para sujeitos com dificuldade em adormecer (com latência de sono superior a 30 minutos [1]) o que conduz a uma subestimação deste parâmetro e, possivelmente, à subestimação de sintomas de distúrbios associados com o sono, aquando do diagnóstico médico. esta falha no desempenho está relacionada com o facto de o modelo de classificação atual ser treinado com exemplos de gravações noturnas completas, fazendo com que, para sujeitos com períodos de latência de sono prolongados, as caraterísticas da classe wake antes da iniciação do sono, não sejam bem capturadas. nesta dissertação é proposto um novo método para a deteção sleep/wake destinado a pessoas com latência de sono superior a 30 minutos. este método inclui dois passos: o primeiro destinado especificamente à identificação da classe wake durante o período de latência de sono (detetando-o a sua duração com maior eficácia) e o segundo com o objectivo de distinguir wake durante o restante tempo da noite de sono. assim, torna-se necessária a utilização de dois classificadores que diferem relativamente às técnicas utilizadas para a seleção de features e utilizam diferentes exemplos de treino, isto é, períodos distintos das gravações noturnas."
    ],
    0.12857142857142856
  ],
  [
    [
      "desde os primórdios dos tempos que o ser humano procura optimizar e automatizar todos os processos que se apresentam como morosos e repetitivos. o processo de pricing de produtos nos retalhistas é um sistema complexo e que consome tempo ao processo interno dos mesmos. este projeto de investigação tem como objectivo o desenvolvimento de uma plataforma que seja capaz de optimizar e automatizar o processo de pricing de produtos, tendo como base um modelo baseado em múltiplas regras. com o aumento do volume de vendas online, surgem, no âmbito do processo de pricing, diversos problemas relativos aos processos associados à expansão de uma plataforma de e-commerce. aparece assim, a necessidade de um sistema capaz de responder a estes problemas e auxiliar na eficácia, rapidez e tomada de decisão de quem lida diariamente com esta questão complexa. assim, propõe-se o desenvolvimento de um sistema que deverá ser capaz de auxiliar a tomada de decisão na definição do preço de venda dos produtos comercializados em qualquer plataforma de e-commerce. a solução que é desenvolvida ao longo deste projeto de investigação, terá que ser passível de gerir, em tempo real, o processo de pricing de um retalhista, bem como auxiliar na decisão da definição de preços. o foco deste projeto de investigação será dado a sistemas de apoio à decisão orientados a modelos, uma vez que é de extrema importância a versatilidade e a adaptação do sistema a múltiplos contextos e variáveis. como tal, e de forma a responder às questões de investigação que orientam este projeto de investigação, estrutura-se o conteúdo em quatro capítulos fundamentais: o estado da arte, a metodologia de investigação e ferramentas de desenvolvimento, o desenvolvimento do trabalho e as conclusões. durante o capítulo dedicado ao estado da arte abordam-se definições e conceitos essenciais ao capítulo de desenvolvimento deste projeto, tal como o conceito de sistemas de apoio à decisão, a definição do conceito de motores de regras e de algoritmos de inferência. para estruturar a forma como se irá conduzir este projeto de investigação, no capítulo de metodologia e ferramentas de desenvolvimento, apresenta-se a metodologia de investigação e as ferramentas de desenvolvimento aplicadas neste estudo, tal como o ambiente no qual a solução final foi desenvolvida. o capítulo de desenvolvimento define-se pela exposição da investigação e lógica aplicada no desenvolvimento de um sistema de apoio à decisão para o processo de pricing. por último, o capítulo em que se expõem as conclusões deste projeto de investigação, tem como objectivo analisar os princípios teóricos que servem de base a provas de conceito, seguindo-se pela exposição da análise swot. o desenvolvimento desta análise é enquadrado na metodologia de investigação definida inicialmente, design research, avaliando a solução desenvolvida de modo a perceber se os requisitos iniciais foram cumpridos. assim, e de forma a concluir esta investigação, e relacionar todos os conceitos abordados e tecnologias utilizadas, as questões de investigação são respondidas de forma a expor a viabilidade da solução apresentada.",
      "since the dawn of time, human beings have sought to optimize and automate all processes that are slow and repetitive. the product pricing process for retailers is a complex system that consumes time for their internal process. this research project aims to develop a platform that is capable of optimizing and automating the product pricing process, based on a model based on multiple rules. with the increase in online sales volume, several pricing problems arise as part of the pricing process associated with the expansion of an e-commerce platform. thus arises the need for a system capable of responding to these problems and assisting in the effectiveness, speed and decision making of those who deal with this complex issue on a daily basis. thus, it is proposed to develop a system that should be able to assist decision making in defining the selling price of products sold on any e-commerce platform. the solution that is developed throughout this research project will have to be able to manage, in real time, the pricing process of a retailer, as well as assist in deciding pricing. the focus of this research project will be on model-driven decision support systems, as the versatility and adaptation of the system to multiple contexts and variables is of utmost importance. as such, and in order to answer the research questions that guide this research project, the content is structured into four fundamental chapters: state of the art, research methodology and development tools, work development and conclusions. the state of the art chapter addresses definitions and concepts that are essential to the development chapter of this project, such as the concept of decision support systems, the definition of the concept of rule engines and inference algorithms. to structure how this research project will be conducted, in the methodology and development tools chapter, we present the research methodology and development tools ap plied in this study, as well as the environment in which the final solution was developed. the development chapter is defined by the research and logic applied in the development of a decision support system for the pricing process. finally, the chapter in which the conclusions of this research project are presented aims to analyze the theoretical principles that underlie proofs of concept, followed by the presentation of a swot analysis. the development of this analysis is framed in the initially defined research methodology, design research, evaluating the developed solution in order to understand if the initial requirements were met. thus, in order to conclude this research, and relate all the concepts covered and tech nologies used, the research questions are answered in order to expose the viability of the presented solution."
    ],
    [
      "this document presents the motivation, development and results of a masters thesis work in informatics focused on computational thinking education, that was accomplished at universidade do minho in braga, portugal. this thesis is based on a big ontology that describes in detail the concepts ’computational thinking’ and ’programming’, which maps those concepts to different education levels, starting with the first year. the main goal is the development of a web platform that, on one hand, helps on collecting in a repository and classifying any kind of resources to be used by teachers on computing classes and, on the other hand, helps on the retrieval from that repository of the most adequate resources to teach a specific subject to a specific level. the classification and the intelligent search mechanism will follow the knowledge description defined by the ontology.",
      "este documento apresenta a tese de mestrado em engenharia informática a qual é focada na educação de pensamento computacional. este trabalho é baseado numa vasta ontologia que descreve em detalhe os conceitos ‘pensamento computacional’ e ’programação’ mapeando esses conceitos em diferentes níveis de educação, a partir do primeiro ano de escolaridade. o maior objetivo desta tese e a construção de uma plataforma web que ajuda a construir um repositório de recursos pedagógicos classificados os quais vão ser usados pelos professores nas aulas. esta plataforma também irá ajudar os professores a procurar o tipo de recurso que querem utilizar nas suas aulas. os mecanismos de classificação e de procura inteligente são baseados na ontologia construída."
    ],
    0.3
  ],
  [
    [
      "tendencialmente as pessoas idosas têm problemas cognitivos e físicos que potencializam a queda. estudos revelam que 30% das pessoas com idades à volta dos 65 anos cai pelo menos uma vez por ano, piorando o cenário para 50% a partir dos 80 anos de idade. quando a assistência à pessoa que caiu tarda em chegar surge evidentemente o agravamento do estado de saúde e das condições de cura e recuperação. recorrendo a técnicas dos sistemas inteligentes, neste trabalho pretende-se estudar e analisar formas computacionais de deteção da queda. para que a deteção seja correcta é necessária a utilização de sensores, e software que interprete a informação proveniente dos mesmos. será ainda necessário estudar diferentes abordagens na utilização dos dados dos sensores para optimizar a eficácia do sistema. neste trabalho dá-se ênfase ao uso de dispositivos de fácil aquisição e uso, e a uma abordagem não invasiva. especificamente, estuda-se a utilização dos acelerómetros de plataformas móveis android para a aquisição de dados sobre os padrões de movimento dos utilizadores por forma a detectar quedas e minimizar o tempo decorrido entre a queda e a chegada de ajuda.",
      "in a general way, elder people suffer from cognitive and physical disorders that potentiate falls. studies show that 30% of people aged around 65 falls at least once a year, with this number worsening to 50% after 80 years of age. when the help to the person that fell takes time to arrive, there is an evident risk to the health of the person and a worsening of the healing and recovering conditions. using intelligent system techniques, in this work we aim to study technology-supported ways of detecting falls. in order for the detection to be correct, there is the need to use the appropriate sensors as well as software that correctly interprets the data generated. it will also be necessary to study different approaches on the use of the sensory data in order to optimize the efficacy of the system. in this work we focus on the use of easy to use and easy to acquire devices, and on a non-invasive approach. specifically, we study the use of the accelerometers that make part of mobile android platforms for data acquisition concerning user movement patterns in order to detect falls and minimize the time between a fall and the arrival of help."
    ],
    "internet security has been targeted in innumerous ways throughout the ages and internet cyber criminality has been changing its ways since the old days where attacks were greatly motivated by recognition and glory. a new era of cyber criminals are on the move. real armies of robots (bots) swarm the internet perpetrating precise, objective and coordinated attacks on individuals and organizations. many of these bots are now coordinated by real cybercrime organizations in an almost open-source driven development resulting in the fast proliferation of many bot variants with refined capabilities and increased detection complexity. one example of such open-source development could be found during the year 2011 in the russian criminal underground. the release of the zeus botnet framework source-code led to the development of, at least, a new and improved botnet framework: ice ix. concerning attack tools, the combination of many well-known techniques has been making botnets an untraceable, effective, dynamic and powerful mean to perpetrate all kinds of malicious activities such as distributed denial of service (ddos) attacks, espionage, email spam, malware spreading, data theft, click and identity frauds, among others. economical and reputation damages are difficult to quantify but the scale is widening. it’s up to one’s own imagination to figure out how much was lost in april of 2007 when estonia suffered a well-known distributed attack on its internet country-wide infrastructure. among the techniques available to mitigate the botnet threat, detection plays an important role. despite recent year’s evolution in botnet detection technology, a definitive solution is far from being found. new constantly appearing bot and worm developments in areas such as host infection, deployment, maintenance, control and dissimulation of bots are permanently changing the detection vectors thought and developed. in that way, research and implementation of anomaly-based botnet detection systems are fundamental to pinpoint and track all the continuously changing polymorphic botnets variants, which are impossible to identify by simple signature-based systems.",
    0.0
  ],
  [
    [
      "the continuous social and economic development has led over time to an increase in consumption, as well as greater demand from the consumer for better and cheaper products. hence, the selling price of a product assumes a fundamental role in the purchase decision by the consumer. in this context, online stores must carefully analyse and define the best price for each product, based on several factors such as production/acquisition cost, positioning of the product (e.g. anchor product) and the competition companies strategy. the work done by market analysts changed drastically over the last years. as the number of web sites increases exponentially, the number of e-commerce web sites also prosperous. web page classification becomes more important in fields like web mining and information retrieval. the traditional classifiers are usually hand-crafted and non-adaptive, that makes them inappropriate to use in a broader context. we introduce an ensemble of methods and the posterior study of its results to create a more generic and modular crawler and scraper for detection and information extraction on e-commerce web pages. the collected information may then be processed and used in the pricing decision. this framework goes by the name prometheus and has the goal of extracting knowledge from e-commerce web sites. the process requires crawling an online store and gathering product pages. this implies that given a web page the framework must be able to determine if it is a product page. in order to achieve this we classify the pages in three categories: catalogue, product and ”spam”. the page classification stage was addressed based on the html text as well as on the visual layout, featuring both traditional methods and deep learning approaches. once a set of product pages has been identified we proceed to the extraction of the pricing information. this is not a trivial task due to the disparity of approaches to create a web page. furthermore, most product pages are dynamic in the sense that they are truly a page for a family of related products. for instance, when visiting a shoe store, for a particular model there are probably a number of sizes and colours available. such a model may be displayed in a single dynamic web page making it necessary for our framework to explore all the relevant combinations. this process is called scraping and is the last stage of the prometheus framework.",
      "o contínuo desenvolvimento social e económico tem conduzido ao longo do tempo a um aumento do consumo, assim como a uma maior exigência do consumidor por produtos melhores e mais baratos. naturalmente, o preço de venda de um produto assume um papel fundamental na decisão de compra por parte de um consumidor. nesse sentido, as lojas online precisam de analisar e definir qual o melhor preço para cada produto, tendo como base diversos fatores, tais como o custo de produção/venda, posicionamento do produto (e.g. produto âncora) e as próprias estratégias das empresas concorrentes. o trabalho dos analistas de mercado mudou drasticamente nos últimos anos. o crescimento de sites na web tem sido exponencial, o número de sites e-commerce também tem prosperado. a classificação de páginas da web torna-se cada vez mais importante, especialmente em campos como mineração de dados na web e coleta/extração de informações. os classificadores tradicionais são geralmente feitos manualmente e não adaptativos, o que os torna inadequados num contexto mais amplo. nós introduzimos um conjunto de métodos e o estudo posterior dos seus resultados para criar um crawler e scraper mais genéricos e modulares para extração de conhecimento em páginas de ecommerce. a informação recolhida pode então ser processada e utilizada na tomada de decisão sobre o preço de venda. esta framework chama-se prometheus e tem como intuito extrair conhecimento de web sites de e-commerce. este processo necessita realizar a navegação sobre lojas online e armazenar páginas de produto. isto implica que dado uma página web a framework seja capaz de determinar se é uma página de produto. para atingir este objetivo nós classificamos as páginas em três categorias: catálogo, produto e spam. a classificação das páginas foi realizada tendo em conta o html e o aspeto visual das páginas, utilizando tanto métodos tradicionais como deep learning. depois de identificar um conjunto de páginas de produto procedemos à extração de informação sobre o preço. este processo não é trivial devido à quantidade de abordagens possíveis para criar uma página web. a maioria dos produtos são dinâmicos no sentido em que um produto é na realidade uma família de produtos relacionados. por exemplo, quando visitamos uma loja online de sapatos, para um modelo em especifico existe a provavelmente um conjunto de tamanhos e cores disponíveis. esse modelo pode ser apresentado numa única página dinâmica fazendo com que seja necessário para a nossa framework explorar estas combinações relevantes. este processo é chamado de scraping e é o último passo da framework prometheus."
    ],
    [
      "o mundo das aplicações web domina cada vez mais, a forma como nos comunicamos e transmitimos informações. o agendamento cirúrgico consiste em programar as cirurgias de forma a utilizar eficientemente os recursos e reduzir o risco das cirurgias canceladas podendo assim obter uma calendarização detalhada do início e fim das atividades de uma intervenção cirúrgica. sendo assim as aplicações associadas à saúde diferenciam-se da generalidade, por se dedicarem exclusivamente à saúde, não só no seu conteúdo e informação, mas também nas suas funcionalidades. devido à variação da duração de uma cirurgia e à chegada de cirurgias de urgência o agendamento cirúrgico é interrompido ao longo do dia e pode levar a uma mudança no horário previsto do início das cirurgias programadas. estas alterações podem resultar em situações indesejáveis tanto para os pacientes como para os prestadores de saúde, daí a necessidade de um ajuste do horário. deste modo, esta dissertação consiste no desenvolvimento do agendamento inteligente de blocos operatórios. a aplicação proposta permitirá aos prestadores de saúde efetuar marcações cirúrgicas, visualizar as marcações das intervenções cirúrgicas num calendário semanal assim como possibilita o acesso a uma lista para editar ou remover as marcações, melhorando a variação da duração das cirurgias. com esta aplicação, é possível afirmar que se trata de uma plataforma de apoio para os utilizadores pelo tipo de partilha de informação que disponibiliza, permite o agendamento/planeamento das cirurgias efetuadas pelo bloco operatório e possui um conjunto de funcionalidades que a tornam numa aplicação útil, cómoda e de fácil acesso para o quotidiano dos utilizadores.",
      "the world of web applications is becoming more and more dominant of the way we communicate and transmit information. surgical scheduling consists of scheduling surgeries efficiently using the resources and reduce the risk of canceled surgeries thus obtaining a detailed timetable of the beginning and ending of the activities of a surgical intervention. therefore, health-related applications differ from the generality, dedicating themselves exclusively to health, not only their content and information but also their functionalities. due to the variation of the duration of a surgery and the arrival of urgent surgeries, surgical scheduling is interrupted throughout the day and can lead to a change in the foreseen timetable for the beginning of the scheduled surgeries. these may result in undesirable situations either for patients or for health care providers, thus the need for an adjustment of the timetable. so this dissertation consists of the development of intelligent scheduling of operating rooms. the proposed application will allow healthcare providers to make the surgical appointments to visualize the appointments of the surgical interventions in a weekly calendar as well as to allow access a list to edit or remove appointments, improving the variation of the duration of the surgeries. with this application it is possible to say that it is a platform of support to users due to type of information shared, which allows permits the scheduling/planning of the surgeries carried out by the operating room and it has a set of functionalities that make it a useful, convenient and easy to access for everyday users."
    ],
    0.03
  ],
  [
    [
      "ethics, as a set of moral principles that guide an individual's behavior, assisting them in doing what is right, must be present in any decision taken by technology and information systems professionals, even if it is an unconscious process intrinsic to the individual. given its importance, ethics in technologies and information systems has been a subject under study for several decades; however, little research has been carried out focused on the ethical challenges of emerging technologies and, in particular, how codes of ethics are concerned with answering them. in order to fill this gap, a literature review was conducted to identify the ethical challenges of emerging technologies and to understand the main topics investigated in the literature on codes of ethics. the current versions of codes of ethics were analysed, followed by a proposal for an updated unified structure of codes of ethics. overall, this research identified forty-five ethical challenges associated with ten emerging technologies, of which eighteen are represented in the unified structure, indicating that, although there is some coverage, there is still room for improvement. this thesis ends with a set of recommendations for future research, encouraging an in-depth study of ethical challenges, combined with an update of the codes of ethics to reflect the results obtained and concluding with the recommendation to study the awareness of ethical challenges by technologies and information systems’ actors.",
      "a ética, enquanto conjunto de princípios morais que guiam o comportamento de um indivíduo, auxiliando-o a fazer o que está certo, deverá estar presente em qualquer decisão tomada pelos profissionais de tecnologias e sistemas de informação, ainda que a mesma possa ser um processo inconsciente, intrínseco ao indivíduo. dada a sua importância, a ética em tecnologias e sistemas de informação é um tema em estudo há várias décadas; contudo, tem sido realizada pouca investigação focada nos desafios éticos das tecnologias emergentes e, em particular, no estudo da resposta que lhes é dada pelos códigos de ética. de modo a colmatar esta lacuna, foi conduzida uma revisão de literatura, de forma a identificar os desafios éticos de tecnologias emergentes e a compreender quais os principais tópicos investigados na literatura sobre códigos de ética. foram também analisados os códigos de ética e apresentada uma estrutura unificada de códigos de ética atualizada. deste trabalho resultou a identificação de quarenta e cinco desafios éticos associados a dez tecnologias emergentes, dos quais dezoito se encontram representados na estrutura unificada, indicando que, embora haja alguma cobertura, ainda há espaço para melhoria. esta dissertação termina com um conjunto de recomendações para pesquisa futura, incentivando um estudo mais aprofundado dos desafios éticos, aliado a uma atualização dos códigos de ética de maneira a refletir os resultados obtidos, concluindo com a recomendação para se estudar o quão consciencializados os atores do universo das tecnologias e sistemas de informação estão deste assunto."
    ],
    [
      "in recent years ontologies have become an integral part of storing information in a structured and formal manner and a way of sharing said information. with this rise in usage, it was only a matter of time before different people tried to use ontologies to represent the same knowledge domain. the area of ontology matching was created with the purpose of finding correspondences between different ontologies that represented information in the same domain area. this document reports a master’s work that started with the study of already existing ontology matching techniques and tools in order to gain knowledge on what techniques exist, as well as understand the advantages and disadvantages of each one. using the knowledge obtained from the study of the bibliography research, a new web-based tool called omt was created to automatically merge two given ontologies. the omt tool processes ontologies written in different ontology representation languages, such as the owl family or any language written according to the rdf web standards. the omt tool provides the user with basic information about the submitted ontologies and after the matching occurs, provides the user with a simplified version of the results focusing on the number of objects that were matched and merged. the user can also download a log file, if he so chooses. this log file contains a detailed description of the matching process and the reasoning behind the decisions the omt tool made. the omt tool was tested throughout its development phase against various different potential inputs to assess its accuracy. lastly, a web application was developed to host the omt tool in order to facilitate the access and use of the tool for the users.",
      "nos últimos tempos, ontologias têm-se tornado fundamentais quando os objetivos são armazenar informação de forma formal e estruturada bem como a partilha de tal informação. com o aumento da procura e utilização de ontologias, tornou-se inevitável que indivíduos diferentes criassem ontologias para representar o mesmo domínio de informação. a área de concordância de ontologias foi criada com o intuito de encontrar correspondências entre ontologias que representem informação no mesmo domínio. este documento reporta o trabalho de uma tese de mestrado que começou pelo estudo de técnicas e ferramentas já existentes na área de concordância de ontologias com o objetivo de obter conhecimento nestas mesmas e perceber as suas vantagens e desvantagens. a partir do conhecimento obtido a partir deste estudo, uma nova ferramenta web chamada omt foi criada para automaticamente alinhar duas ontologias. a ferramenta omt processa ontologias escritas em diferentes linguagens de representação, tal como a familia de linguages owl ou qualquer linguagem que respeite o padrão rdf. a ferramenta omt fornece ao utilizador informação básica sobre as ontologias e após o alinhamento ocorrer, fornece ao utilizador uma versão simplificada dos resultados obtidos, focando no numero de objetos que foram alinhados. o utilizador pode também descarregar um ficheiro log. este ficheiro contém uma descrição destalhada do processo de alinhamento e a justificação para as diferentes decisões tomadas pelo ferramenta omt. a ferramenta omt foi testada durante todo o processo de desenvolvimento com diferentes tipos de ontologia de entrada para avaliar a sua capacidade de alinhamento. por último, foi também desenvolvida uma aplicação web para hospedar a ferramenta omt de forma a facilitar o acesso e uso da ferramenta aos utilizadores."
    ],
    0.0
  ],
  [
    [
      "since the last century, concerns about safety and welfare issues have increased at the same pace as technolo gical advances. synthetic aperture radar (sar) is one way of creating images using active sensors, which has the advantage that it can be used in any climate and time of day. due to its complexity, the use of these images is time consuming and requires an experienced user. this study aims to find a way to handle these images, in terms of geographic registration and image interpretation, in a more efficient way. based on the literature on geographic registration techniques, image comparison and segmentation in sar images, those with the greatest potential to solve the problems identified are developed and tested. the analysis of the solutions demonstrates that the developed methods have the ability to evolve into complete and efficient tools which are capable of answering the identified problems. the results indicate that the sar-harris sar image comparator proves to be a stable enough algorithm to include in a semi-automatic registration tool. concerning the sar image classification and segmentation, the u-net neural network presented values of accuracy near 70%, exhibiting the ability of the network to integrate a sar image classification system. in the case of the c means algorithm, despite not reaching the u-net accuracy values, accuracy values in range 50-60% , it presents itself as an extremely versatile algorithm, also useful in the segmentation task. still, there is a great margin of progress in the automation of the processes of geographic registration and segmentation of sar images.",
      "desde o século passado, as preocupações em assuntos de segurança e bem-estar têm aumentado ao mesmo ritmo que os avanços tecnológicos. synthetic aperture radar (sar) é uma das formas de criar imagens utilizando sensores ativos, que possuem a vantagem de poder ser utilizado em qualquer clima e hora do dia. devido à sua complexidade, a utilização destas imagens é muito custosa em termos de tempo e requer mão-de-obra qualificada. este estudo tem o objetivo de encontrar uma forma de lidar com estas imagens, em termos de registo geográfico e interpretação da imagem, de forma mais eficiente. com base na literatura acerca de técnicas registo geográfico, comparação entre imagens e segmentação em imagens sar, são desenvolvidas e testadas aquelas que apresentam maior potencial para resolver os pro blemas identificados. a análise das soluções demonstram que os métodos estudados e desenvolvidos tem a capacidade de evoluir para ferramentas mais completas e eficientes capazes de responder aos problemas iden tificados. os resultados indicam que o comparador de imagens sar sar-harris revela ser um algoritmo estável e robusto o suficiente para originar uma ferramenta de registo semi-automática. já em termos de classificação e segmentação de imagens sar, a rede neuronal u-net apresenta valores de accuracy próximos dos 70%, exibindo a capacidade da rede de integrar um sistema de classificação de imagens sar. no caso do algoritmo c-means apesar de não atingir os valores de precisão da u-net, atingindo valores de 50-60%, apresenta-se como um algoritmo extremamente versátil e robusto, características úteis na tarefa de segmentação. ainda assim, há uma grande margem de progressão na automatização dos processos de registo geográfico e de segmentação de imagens sar."
    ],
    [
      "recombination rate is an essential parameter for most studies on human variation. linkage disequilibrium (ld) measures the association between two variants in the same chromosome. when a new variant arises by mutation in a germinal line, that variant will be in complete linkage with the variants in the chromosomic background where it arises. recombination through time (occurring during meiosis) will decrease the association decreasing the ld. understanding how recombination occurs throughout the genome is the basis to interpret various association studies (search from causal variants for a given disease) and characterization of selective events. in this project the aim is to establish a novel methodology to estimate rate of recombination along a chromosome using a phylogenetic method. for this to be done, each chromosome will be divided into small overlapping windows of variation containing 20/30 variants. for each of these windows a phylogenetic network will be calculated using the reduced-median algorithm. highly recombining regions will show a higher rate of cycles or reticulations in the network. a linkage map will be constructed for each chromosome using this novel methodology, compare the results with methods already available, locate region of low recombination of possible use for phylogenetic analysis and also explore some properties of the method for evaluation of selection.",
      "a proporção de recombinação é um parâmetro essencial para os estudos baseados na variação encontrada nos humanos. o linkage disequilibrium (ld) mede a associação entre duas variantes no mesmo cromossoma. quando uma nova variante aparece devido a uma mutação na linha germinativa, esta mesma variante irá estar em linkage completo com as outras variantes presentes no cromossoma onde esta apareceu. com o passar do tempo, a recombinação genética (ocorre durante a meiose) irá diminuir a associação dos alelos, diminuindo o ld. a compreensão da recombinação ao longo do genoma humano é a base para a interpretação de vários estudos de associação (procura de variantes para uma doença especifica) e caracterização de eventos de seleção. o objetivo deste projeto é estabelecer uma nova metodologia para estimar a proporção de recombinação no decurso do cromossoma utilizando um método filogenético. para isto ser realizado, cada cromossoma será dividido em janelas sobrepostas contendo 20/30 variantes. para cada janela de sobreposição uma rede filogenética irá ser construída usando o reduced-median algorithm. regiões com elevada recombinação irão mostrar um maior número de ciclos ou reticulações na rede. um mapa de linkage será construído para cada cromossoma usando esta nova metodologia, comparando resultados com outros métodos já existentes, regiões de baixa recombinação irão ser localizadas para uma futura análise filogenética e explorar algumas propriedades desta metodologia de modo a avaliar a seleção."
    ],
    0.06666666666666667
  ],
  [
    [
      "the amount of data in information systems is growing constantly and, as a consequence, the complexity of analytical processing is greater. there are several storage solutions to persist this information, with different architectures targeting different use cases. for analytical processing, storage solutions with a column-oriented format are particularly relevant due to the convenient placement of the data in persistent storage and the closer mapping to in-memory processing. the access to the database is typically remote and has overhead associated, mainly when it is necessary to obtain the same data multiple times. thus, it is desirable to have a cache on the processing side and there are solutions for this. the problem with the existing so lutions is the overhead introduced by network latency and memory-copy between logical layers. remote direct memory access (rdma) mechanisms have the potential to help min imize this overhead. furthermore, this type of mechanism is indicated for large amounts of data because zero-copy has more impact as the data volume increases. one of the problems associated with rdma mechanisms is the complexity of development. this complexity is induced by its different development paradigm when compared to other network commu nication protocols, for example, tcp. aiming to improve the efficiency of analytical processing, this dissertation presents a dis tributed cache that takes advantage of rdma mechanisms to improve analytical processing performance. the cache abstracts the intricacies of rdma mechanisms and is developed as a middleware making it transparent to take advantage of this technology. moreover, this technique could be used in other contexts where a distributed cache makes sense, such as a set of replicated web servers that access the same database.",
      "a quantidade de informação nos sistemas informáticos tem vindo a aumentar e consequentemente, a complexidade do processamento analítico torna-se maior. existem diversas soluções para o armazenamento de dados com diferentes arquiteturas e indicadas para determinados casos de uso. num contexto de processamento analítico, uma solução com o modelo de dados colunar e especialmente relevante devido à disposição conveniente dos dados em disco e a sua proximidade com o mapeamento em memória desses mesmos dados. muitas vezes, o acesso aos dados é feito remotamente e isso traz algum overhead, principalmente quando é necessário aceder aos mesmos dados mais do que uma vez. posto isto, é vantajoso fazer caching dos dados e já existem soluções para esse efeito. o overhead introduzido pela latência da rede e cópia de buffers entre camadas lógicas é o principal problema das soluções existentes. os mecanismos de acesso direto à memória remota (rdma - remote direct memory access) tem o potencial de melhorar o desempenho neste cenário. para além disso, este tipo de tecnologia faz sentido em sistemas com grandes quantidades de dados, nos quais o acesso direto pode ter um impacto ainda maior por ser zero-copy. um dos problemas associados com mecanismos rdma é a complexidade de desenvolvimento. esta complexidade é causada pelo paradigma de desenvolvimento completamente diferente de outros protocolos de comunicação, como por exemplo, tcp. tendo em vista melhorar a eficiência do processamento analítico, esta dissertação propõe uma solução de cache distribuída que tira partido de mecanismos de acesso direto a memoria remota (rdma). a cache abstrai as particularidades dos mecanismos rdma e é disponibilizada como middleware, tornando a utilização desta tecnologia completamente transparente. esta solução visa os sistemas de processamento analítico, mas poderá ser utilizada noutros contextos em que uma cache distribuída faça sentido, como por exemplo num conjunto de servidores web replicados que acedem a mesma base de dados."
    ],
    [
      "o desenvolvimento da internet, evidenciado nos últimos anos, proporcionou o aumento da comunicação entre pessoas e dispositivos. esta evolução, resultante na internet of things (iot), permitiu o desenvolvimento de novas tecnologias e o progresso de tecnologias já existentes, aplicadas aos mais diversos contextos, como é o caso das habitações. desta forma, surge a criação do conceito de casas inteligentes. casas inteligentes, permitem monitorizar e controlar remotamente os seus dispositivos iot. estas características possibilitam que os dispositivos sejam controlados através de mecanismos de automatização. este tipo de habitações permite a melhoria da qualidade de vida dos seus habitantes, bem como a redução dos recursos necessários ao seu funcionamento. geralmente, a programação de sistemas para casas inteligentes é realizada através da aplicação de técnicas de programação baseada em regras de ativação-condição-ação. este estilo de programação, em conjunto com as plataformas de automação, torna acessível a todos os utilizadores, automatizarem e coordenarem os seus dispositivos iot. assim, com a definição de um conjunto de regras do tipo “se uma condição se verificar, então executa-se uma ação”, é possível automatizar o conjunto de dispositivos. esta automatização permite adicionar inteligência às habitações. o aumento da disponibilidade dos dispositivos, bem como a sua simplicidade de programação, resulta no aumenta da adesão a estes sistemas. porém, a criação de sistemas complexos implica um conjunto de regras também complexo. à criação de um conjunto de regras complexo agrega-se a dificuldade de garantir que não ocorram conflitos entre todas as regras criadas. a ocorrência de conflitos neste tipo de sistemas pode resultar na execução de ações erradas que prejudicam a experiência do utilizador ou que comprometam a sua segurança. nesta dissertação, estuda-se a viabilidade da aplicação da ferramenta ivy workbench, na deteção de conflitos que ocorrem neste tipo de sistemas. esta ferramenta permite modelar um sistema, e verificar um conjunto de propriedades sobre ele expressas.",
      "the development of the internet in recent years has led to increased communication between people and between devices. this evolution, resulting in the internet of things (iot), has enabled the development of new technologies and the progress of existing technologies, in different contexts, such as the case of a home. this has led to the creation of the concept of smart homes. smart homes allow the remote monitoring and control of iot devices. these features allow devices to be controlled through automation mechanisms. this type of home improves the quality of life of its inhabitants and reduces the resources needed to run it. generally, the programming of smart home systems is carried out by applying programming tech niques based on activation-condition-action rules. this style of programming, together with automation platforms, makes it accessible for all users to automate and coordinate their iot devices. consequently, by defining a set of rules such as “if a condition is met, then an action is performed”, it is possible to automate a set of devices. this automation makes it possible to add intelligence to homes. the increased availability of the devices, as well as their simplicity of programming, has resulted in an increase in the adoption of these systems. however, the creation of complex systems implies an also complex set of rules. the creation of a complex set of rules is leads to the difficulty of ensuring that there are no conflicts between all the rules created. the occurrence of conflicts in this type of system can result in the execution of incorrect actions that compromise the experience of the user or their safety. this dissertation studies the viability of applying the ivy workbench tool to detect conflicts that occur in this type of system. this tool makes it possible to model a system and verify a set of properties expressed about it."
    ],
    0.3
  ],
  [
    [
      "a web está em constante evolução. a evolução tecnológica fez com que as aplicações web estivessem cada vez mais presentes no mercado e surgissem novos padrões arquiteturais, novos dispositivos e novas experiências de utilização das aplicações. tudo isto com o propósito de satisfazer as exigências que o mercado e os utilizadores finais impõem. toda esta evolução potenciou o aparecimento de uma nova versão do html, o html5, a qual está em constante progresso. não se pode esperar que o html5 esteja totalmente concluído para implementar interfaces com esta tecnologia, podendo assim ser considerado um living standard, porque já existem funcionalidades suficientemente maduras e que inclusivamente já são utilizadas em produtos no mercado. devido ao facto do html ser uma linguagem de marcação, torna-se indispensável associar o html ao javascript, de modo a poder disponibilizar dinamismo e funcionalidade às páginas web. existiu por isso, tal como o html, uma grande evolução relativa à linguagem, mais propriamente às frameworks javascript de desenvolvimento web existentes. devido ao progresso destas tecnologias, esta dissertação pretende analisar as várias funcionalidades disponibilizadas pela tecnologia html5 e as frameworks javascript existentes no mercado. para tal, estas tecnologias serão usadas, de forma experimental, num projeto de desenvolvimento de software na empresa onde decorre este trabalho. os resultados focam-se em identificar as funcionalidades html5 implementadas e apresentar uma comparação entre as frameworks javascript em estudo, segundo um conjunto específico de critérios. é do interesse da empresa onde este trabalho foi realizado aplicar as funcionalidades html5 e frameworks javascript, de modo a identificar as vantagens e desvantagens de cada tecnologia para, num futuro próximo, as aplicar em aplicações web. o objetivo deste trabalho é assim medir quantitativa e qualitativamente, segundo os critérios considerados para análise, o impacto da introdução do html5 e das frameworks javascript em produtos de software, caso estas substituam as que estão hoje em dia em utilização.",
      "the web is continuously evolving. this evolution increased the presence of web applications in the market, created new architectural patterns, new devices and new ways of user experience. all of this with the goal to meet the requirements that the market and the final users require. those developments have caused the creation of a new version of html, html5, which is in constant progress. no one can expect that html5 is a finished specification, to start creating interfaces only with this technology, but it can be considered a standard living because there are features mature enough to be developed in web applications available for the market. due to the fact html is a markup language, it is essential to link html to javascript, in order to provide dynamism and functionality to web pages. for this reason there was a major evolution on the language, more specifically in javascript frameworks existing for web development. the progress of these technologies has led this dissertation to analyze the multiple features provided by html5 technology and the existing javascript frameworks. for such, these technologies will be used, experimentally, in a software development project in the company where this work follows. the results are focused on identifying the features made with html5 and display a comparison of javascript frameworks studied, according to a specific set of criteria. it is the company's interest where this work was carried out, apply the html5 features and javascript frameworks, in order to identify the pros and cons of each technology, to in the near future, develop them in web applications. the goal of this work is to measure quantitatively and qualitatively, according to the criteria considered for analysis, the impact of the introduction of html5 and javascript frameworks in web software products, if they replace those in use today."
    ],
    [
      "constituído por três capítulos, este trabalho teve como objetivo oferecer um ponto de vista técnico jurídico sobre o quadro de normalização atual do exercício do direito de portabilidade de dados de saúde, em um contexto de dentro e fora da união europeia. para o efeito, foram considerados os processos e medidas em prestação de cuidados de saúde e as respetivas necessidades operacionais, extraindo das mesmas o que implicaria na necessidade de um intercâmbio de dados de saúde, dentro e fora da união europeia, considerando a proteção especial inerente aos mesmos e considerando alguns conceitos jurídicos diversos, mas com alguma intercessão, como o direito de acesso e de portabilidade. foram observadas as medidas de interoperabilidade a nível nacional, continental europeu e os desafios a nível transcontinental na saúde, tendo em conta a matéria de arquitetura de segurança das redes e sistemas de informação relativos a dados pessoais face ao exercício do direito de portabilidade e o caráter normalizador global que a união europeia tem assumido em matéria de interoperabilidade para o exercício do direito de portabilidade de dados ao abrigo do rgpd. foram estudadas e buscadas soluções recorrendo à lógica de soft-law, para alcançar uma normalização que não enfrentasse as limitações de aplicação geográfica aplicáveis às normas vinculadas a um ordenamento jurídico – hard-law no setor da saúde, buscando perceber se um standard de boas práticas para a portabilidade neste seguimento seria viável, aflorando ao final as soluções encontradas e as dificuldades pendentes de solução.",
      "consisting of three chapters, this work aimed to provide a technical legal viewpoint on the current standardization framework of the right to health data portability, in a context within and outside the european union. for this purpose, the processes and measures in health care provision and their respective operational needs were considered, extracting from them what would imply the need for an exchange of health data, inside and outside the european union, considering the special protection inherent to them and considering some diverse but somewhat interrelated legal concepts, such as the right of access and portability. interoperability measures at the national, continental european and transcontinental levels in health were observed, taking into account the matter of security architecture of networks and information systems regarding personal data in the face of exercising the right to portability and the global standardizing character that the european union has assumed in matters of interoperability for the exercise of the right to data portability under the gdpr. non-binding rules, as soft law, were studied in order to reach beyond the limitations of the scope of the regional legal system in the health sector, seeking to understand whether a standard of good practice for portability in this area would be feasible. at the end, the solutions found and the difficulties pending solution were highlighted."
    ],
    0.0
  ],
  [
    [
      "the demand for online games has risen over the years, expanding multiplayer support for new and different game genres. among them are massively multiplayer online games, one of the most popular and successful game types in the industry. nowadays, this industry is thriving, evolving alongside technological advancements and producing billions in revenue, making it an economic importance. however, as the complexity of these games grows, so do the challenges they face when constructing them. this dissertation aims to implement a distributed game, through a proof of concept or an existing game, using a distributed architecture to acquire knowledge in the construction of such complex systems and the effort involved in dealing with consistency, maintaining communication infrastructure, and managing data in a distributed way. it is also intended that this project implements multiple mechanisms capable of autonomously helping manage and maintain the correct state of the system. to evaluate the proposed solution, a detailed analysis is carried out with performance benchmark analysis, stress testing, followed by an examination of its security, scalability, and distribution’s resilience. overall, the present research work allowed for a greater understanding of the technologies and approaches used in constructing a gaming system, establishing a new set of development opportunities to be further investi gated upon the constructed solution.",
      "a procura por jogos online aumentou ao longo dos anos, expandindo o suporte multiplayer para novos e diferentes géneros. entre estes estão os jogos massively multiplayer online, um dos tipos de jogos mais populares e bem-sucedidos na indústria. atualmente, esta indústria está a prosperar, evoluindo com os avanços tecnológicos e gerando milhares de milhões em receita, tornando-se uma importância económica. porém, à medida que a complexidade destes jogos aumenta, também aumenta os problemas encontrados durante a sua construção. esta dissertação tem como objetivo implementar um jogo distribuído, através de uma prova de conceito ou um jogo existente, usando uma arquitetura distribuída a fim de adquirir conhecimento na construção destes sistemas complexos e o esforço envolvido em lidar com consistência, manter a infraestrutura de comunicação e gerir dados de maneira distribuída. para isto, é pretendido que este projeto também implemente vários mecanismos capazes de, forma autônoma, ajudar a gerir e manter o correto estado do sistema. para avaliar o solução proposta, uma análise detalhada é realizada sobre o desempenho, segurança, escalabilidade e resiliência da distribuição do sistema. de forma geral, o presente trabalho de pesquisa permitiu uma maior compreensão das tecnologias e abordagens utilizadas na construção de um sistema de jogos, estabelecendo um novo conjunto de oportunidades de desenvolvimento a serem investigadas sobre a solução construída."
    ],
    [
      "microbial communities have gained particular interest and have been used for practical applications such as biorefineries, and bioremediation. however, studying these communities has proven to be difficult due to the absence of experimental protocols and computational tools like the ones available for single organisms. in this work, we present genome-scale metabolic models both for methanospirillum hungatei strain jf1 and syntrophobacter fumaroxidans strain mpobt, together with a model that combines both into one community model. the genome-scale metabolic model reconstruction of s. fumaroxidans was performed in merlin whereas, the methane-producing archaeon m. hungatei was reconstructed in kbase’s environment and the model curation was performed in merlin. optflux and biocoiso, a tool implemented over cobrapy developed specifically for debugging model pathways, were used for curating and validating both models. the metabolism of each individual organism was assessed through its model reconstruction. in silico simulations demonstrated the production of various compounds of interest such as formate in m. hungatei and acetate in s. fumaroxidans. the meta-model representing the community composed by both organisms was assembled using framed, and it was able to describe the metabolic exchanges between the formate scavenger m. hungatei and the syntrophic partner s. fumaroxidans. the reconstructed models can be used to study further the metabolic interactions between these bacteria.",
      "as comunidades microbianas são de especial interesse e têm sido usadas para aplicações práticas como em biorrefinarias e biorremediação. no entanto, o estudo destas comunidades tem sido difícil devido há falta de protocolos experimentais e ferramentas computacionais, como os que existem para cada organismo individualmente. neste trabalho são apresentados os modelos metabólicos à escala genómica para estirpe jf1 de methanospirillum hungatei e a estirpe mpobt de syntrophobacter fumaroxidans, juntamente com um modelo que combina ambos os modelos criados num modelo de comunidade. a reconstrução do modelo metabólico à escala genómica de s. fumaroxidans foi realizada no merlin, enquanto que o modelo da bactéria produtora de metano m. hungatei foi reconstruído na kbase e a curação manual efetuada no merlin. optflux e biocolso, uma ferramenta implementada sobre o cobrapy, desenvolvida especificamente para a correção de vias do modelo, foram usadas para a curação e validação de ambos os modelos. o metabolismo de cada organismo foi acedido através das respetivas reconstruções realizadas para cada um. simulações in silico demonstraram a produção de vários compostos de interesse como o formato no caso de m. hungatei e acetato no caso de s. fumaroxidans. o meta-modelo criado que representa a comunidade formada por ambos os organismos foi criado a partir de uma ferramenta presente no framed, e este é capaz de descrever as trocas metabólicas entre m. hungatei e s. fumaroxidans. os modelos reconstruídos podem ser usados para estudar no futuro as interações metabólicas entre estas duas bactérias."
    ],
    0.075
  ],
  [
    [
      "a preservação de sistemas de informação é um dos maiores desafios da preservação digital. entre esses sistemas, encontram-se as bases de dados. estas servem de sustento à maior parte dos sistemas de gestão de informação, apresentando, assim, um grande interesse no que diz respeito à sua preservação. se por um lado existe a necessidade de migrar as bases de dados para outras mais atuais, que vão aparecendo com o evoluir da tecnologia, por outro, existe também a necessidade de preservar a informação nelas contida durante um longo período de tempo, quer seja por questões legais, quer seja por questões de arquivo. desta forma, essa informação deverá estar disponível independentemente do sistema de gestão da base de dados. nesta área, os produtos que existem para preservação de base de dados relacionais ainda são poucos - chronos e siard são os principais. o primeiro é muitas das vezes inacessível devido ao preço que apresenta. o segundo apenas suporta funcionalidades básicas. assim, existe a oportunidade de explorar as principais qualidades e fraquezas das ferramentas existentes, de forma a melhorar a ferramenta de preservação de base de dados db-preservation-toolkit, componente extraída do projeto roda. deste modo, esta ferramenta foi melhorada, quer pela adição de funcionalidades de forma a ser capaz de suportar um maior número de sistemas de gestão de bases de dados, quer pela adição do suporte ao formato siard, mas também pelo desenvolvimento de uma interface que possibilita a visualização e pesquisa de informação numa base de dados arquivada.",
      "the preservation of information systems is one of the biggest challenges of digital preservation. among those systems we can find databases. databases support the majority of the information management systems, showing themselves as a valuable resource to preserve. if in one hand there is a need to migrate databases to newer ones that appear with technological evolution, on the other hand there is also the need to preserve the information they hold for a long time period, due to legal duties but also due to archival issues. that being said, that information must be available no matter the database management system where the information came from. in this area, the existing products for relational database preservation are still scarce - chronos and siard are the main ones. the first one is, in most of the cases, unreachable due to the associated costs. the second one only supports basic features. therefore there is the urge to explore the main features and limitations of the existing products in order to improve db-preservation-toolkit, an extracted component from the roda project. therefore, this toolkit was improved by adding new features in order to support more database management systems, by adding support to the siard format, but also by the development of an interface that enables the possibility to visualize and search the information of an archived database."
    ],
    [
      "em todas as organizações, públicas ou privadas, as soluções de desmaterialização são cada vez mais usadas, começando a existir pressão para a assinatura eletrónica (em formato pades) dos documentos pdf. com a introdução de tecnologias de assinatura qualificada remota, cada vez menos se justifica que os documentos sejam assinados de forma manuscrita e depois digitalizados para serem integrados no sistema documental da organização, sendo natural a evolução para a integração de serviços de assinatura no próprio sistema documental da organização. neste sentido, esta dissertação de mestrado tem como objetivo desenvolver uma plataforma (que designaremos por “pades server signer”) que permita efetuar uma assinatura eletrónica (em formato pades) de documentos pdf, com base nos mecanismos de assinatura eletrónica: cmd (chave móvel digital) e cc (cartão de cidadão). a plataforma rege-se pelas rigorosas especificações do regulamento eidas da união europeia para assinaturas eletrónicas. perante o aumento da utilização das assinaturas realizadas remotamente, visa-se garantir que a plataforma esteja em conformidade com a especificação do cloud signature consurtium para a realização de assinatura baseadas em chave privada e hardware criptográfico remoto, utilizado sob controlo do titular da mesma. em paralelo, foi realizada uma aplicação web que realize a interação com o utilizador, permitindo que este possa assinar um documento pdf, através do pades server signer.",
      "in all public or private organizations, the dematerialization solutions are even more a requisition. pressure is beginning to exist for the electronic signatures for pdf documents (in pades format). with the introduction of qualified remote signature technologies, less and less reasons are required for documents to be signed by hand and then scanned to be integrated into the organization’s document system. in this master’s thesis was developed a platform (which we will call “pades server signer”) that allows the signature of pdf documents (in pades format), based on cmd (chave móvel digital), cc (cartão de cidadão). this signatures must complie with the strict specifications of the european union’s eidas regulation for electronic signatures. with the increase in the use of remote signatures, this platform is guided by the cloud signature consurtium standard for the creation of digital signatures where the signing key is held “in the cloud”. at the same time, was developed a web application for the user to sign a pdf document using the pades server signer application."
    ],
    0.06
  ],
  [
    [
      "muon tomography or muography is an imaging technique that allows non-invasive observation of the in terior of large and dense structures such as pyramids and volcanoes. muography resorts to cosmic-ray muons, subproduct particles of the interaction of cosmic rays with the earth’s atmosphere, and to muon telescopes, devices capable of detecting these particles and their trajectory. depending on the use given to this instrument, the technique subdivides into scattering muography and transmission muography. the latter works similarly to radiography and outputs two-dimensional projections of the structures in the field of view of the muon telescope. to obtain their 3d image it is necessary to apply image reconstruction algorithms to the 2d projections. in 2019, lip, the laboratory of instrumentation and experimental par ticle physics, constructed a muon telescope and initiated the first transmission muography experiment in portugal under the collaboration loumu. as a starting point, the telescope acquired data at its con struction site, the department of physics of the university of coimbra, and efforts are being made to use this same data to reconstruct three-dimensional images of the building. this dissertation arrives in that context, as the work presented here concerns how the 2d projections taken at the department of physics of the university of coimbra were obtained and how they are being used to derive 3d reconstructions of the building while resorting to image reconstruction algorithms. regarding the 2d results, it is explained how simulated and experimental images muography images were obtained. it is concluded that, although the two compare well on a coarse-grained scale, some disparities still need to be addressed in future analysis, namely to be able to perform 3d reconstruction. in that sense, the development and testing of image reconstruction algorithms to retrieve 3d images from the 2d projections was performed only in simulation. the applicability of two methods, the back-projection and the sart iterative algorithm, to the case study of the department of physics of the university of coimbra was analyzed and a third algorithm, analytical inversion, is currently being developed and tested under the same conditions. out of the first two approaches, only the iterative algorithm resulted in successful reconstructions while using the muon telescope of the loumu collaboration.",
      "a tomografia muónica ou muografia é uma técnica de imagiologia que permite a observação não-invasiva do interior de estruturas de elevadas dimensões e densidades. a muografia recorre a muões cósmicos, partículas secundárias da interação de raios cósmicos com a atmosfera terrestre, e a telescópios de muões, dispositivos capazes de detetar estas partículas e a sua trajetória. dependendo do uso dado a este intrumento, a técnica subdivide-se em muografia de scattering e muografia de transmissão. a segunda funciona de forma semelhante à radiografia e retorna projeções bidimensionais das estruturas no campo de visão do telescópio de muões. para obter imagens 3d é necessário aplicar algoritmos de reconstrução de imagem às projeções 2d. em 2029, o lip, laboratório de instrumentação e física experimental de partículas, construiu um telescópio de muões e iniciou a primeira experiência de muografia de transmissão em portugal sob a colaboração loumu. como ponto de partida, o telescópio adquiriu dados no seu local de construção, o departamento de física da universidade de coimbra, e estes têm sido analisados para obter images tridimensionais do edifício. esta dissertaçao surge nesse contexto, sendo o trabalho aqui apresentado relativo ao modo como as projeções 2d do departamento de física da universidade de coimbra foram obtidas e ao modo como estas estão a ser utilizadas para derivar recontruções 3d do edifício recorrendo a algoritmos de reconstrução de imagem. relativamente aos resultados 2d, é explicado como as imagens de muografia simuladas e experimentais foram obtidas. conclui-se que, embora estas sejam compatíveis a um nível médio, existem disparidades que precisarão de ser abordadas em análises futuras, nomeadamente para ser possível efetuar reconstrução 3d. nesse sentido, o desenvolvimento e avaliação de algoritmos de reconstrução de imagem para recuperação de imagens 3d a partir das projeções 2d foi efetuada somente em simulação. a aplicabilidade de dois métodos, a backprojection e o algoritmo iterativo sart, ao caso de estudo do departamento de física da universidade de coimbra foi analisada e um terceiro algoritmo, a inversão analítica, está atualmente a ser desenvolvido e testado sob as mesmas condições. das primeiras duas abordagens, apenas o método iterativo resultou em reconstruções bem-sucedidas com o telescópio da colaboração loumu."
    ],
    [
      "esta dissertação desenvolve-se em torno do problema da deteção de defeitos em couro. a deteção de defeitos em couro é um problema tradicionalmente resolvido manualmente, usando avaliadores ex perientes na inspeção do couro. no entanto, como esta tarefa é lenta e suscetível ao erro humano, ao longo dos últimos 20 anos tem-se procurado soluções que automatizem a tarefa. assim, surgiram várias soluções capazes de resolver o problema eficazmente utilizando técnicas de machine learning e visão por computador. no entanto, todas elas requerem um conjunto de dados de grande dimensão anotado e balanceado entre as várias categorias. assim, esta dissertação pretende automatizar o processo tradicio nal, usando técnicas de machine learning, mas sem recorrer a datasets anotados de grandes dimensões. para tal, são exploradas técnicas de novelty detection, as quais permitem resolver a tarefa de inspeção de defeitos utilizando um conjunto de dados não supervsionado, pequeno e não balanceado. nesta dis sertação foram analisadas e testadas as seguintes técnicas de novelty detection: mse autoencoder, ssim autoencoder, cflow, stfpm, reverse, and draem. estas técnicas foram treinadas e testadas com dois conjuntos de dados diferentes: mvtec e neadvance. as técnicas analisadas detectam e localizam a mai oria dos defeitos das imagens do mvtec. contudo, têm dificuldades em detetar os defeitos das imagens do dataset da neadvance. com base nos resultados obtidos, é proposta a melhor metodologia a usar para três diferentes cenários. no caso do poder computacional ser baixo, ssim autoencoder deve ser a técnica usada. no caso onde há poder computational suficiente e os exemplos a analisar são de uma só cor, draem deve ser a técnica escolhida. em qualquer outro caso, o stfpm deve ser a opção escolhida.",
      "this dissertation develops around the leather defects detection problem. the leather defects detec tion problem is traditionally manually solved, using experient assorters in the leather inspection. however, as this task is slow and prone to human error, over the last 20 years the searching for solutions that automatize this task has continued. in this way, several solutions capable to solve the problem effi ciently emerged using machine learning and computer vision techniques. nonetheless, they all require a high-dimension dataset labeled and balanced between all categories. thus, this dissertation pretends to automatize the traditional process, using the machine learning techniques without requiring a large dimensions labelled dataset. to this end, there will be explored novelty detection techniques, that in tend to solve the leather inspection task using an unsupervised small and non-balanced dataset. this dissertation analyzed and tested the following novelty detection techniques: mse autoencoder, ssim autoencoder, cflow, stfpm, reverse, and draem. these techniques are trained and tested in two distinct datasets: mvtec and neadvance. the analyzed techniques detect and localize most mvtec defects. however, they have difficulties in defect detection on neadvance samples. based on the ob tained results, it is proposed the best methodology to use for three distinct scenarios. in the case where the computational power available is low, ssim autoencoder should be the technique to use. in the case where there is enough computational power and the samples to inspect have the same color, draem should be the chosen technique. in any other case, the stfpm should be the chosen option."
    ],
    0.06666666666666667
  ],
  [
    [
      "in the portuguese public administration, there has been a recurring concern to modernize the archive of information generated and received by the different entities. it has provided the use of electronic document management systems, as well as the digitization of documents sent to be archived. these measures are intended to reduce the consumption of paper and printing consumables, as well as optimize processes and modernize administrative procedures. from these measures, the classification and evaluation of public information (clav) emer-ges, a national project, financed by simplex, which aims to simplify management public administration documents. clav aims to classify and evaluate all public administration documents, based on norms and rules coordinated by the directorate-general for books, archives and libraries (dglab). many portuguese public administration entities have their classification and assessment instruments published in the official gazette of portuguese republic in a document management ordinance (dim) or accumulated documentation assessment report (adar), which contains the said instrument, the selection tables (st). in the future, it is intended that the sd are created on the platform in an assisted manner. there is already a prototype of this functionality that is now necessary to finalize, as well as specify all the work flow necessary for the final approval of the instrument. it is also intended to develop, in the clav web application, the necessary interfaces that allow users to create the sts in an assisted form and to specify and define all the necessary web interfaces for the analysis, approval and removal workflow of a st.",
      "na administração pública (ap) portuguesa, tem havido uma recorrente preocupação em modernizar o arquivo que contém a informação gerada e recebida pelas diferentes entidades produtoras. este órgão (ap) tem proporcionado a utilização de sistemas de gestão documental eletrónicos, assim como a digitalização de documentos remetidos a serem arquivados. estas providências pretendem reduzir o consumo de papel e os consumíveis de impressão, bem como otimizar os processos e modernizar os procedimentos administrativos. destas necessidades surge então a classificação e avaliação da informação pública (clav), um projeto nacional, financiado pelo simplex, que visa simplificar a gestão documental da ap. a clav tem o intuito de classificar e avaliar todos os documentos da ap, tendo como base as normas e regras coordenadas pela direção-geral do livro, dos arquivos e das bibliotecas (dglab). muitas entidades da ap portuguesa têm os seus instrumentos de classificação e avaliação devidamente publicados em diário da república numa portaria de gestão de documentos (pgd) ou num relatório de avaliação de documentação acumulada (rada), que contém o instrumento, a tabela de seleção (ts). no futuro, pretende-se que as ts sejam criadas na plataforma de forma assistida. já existe um protótipo desta funcionalidade que agora é necessário finalizar, bem como especificar todo o fluxo de trabalho necessário à aprovação final do instrumento. nesta dissertação, pretende-se especificar formalmente uma ts e acrescentar à ontologia do clav os conceitos e as relações necessários para a sua descrição. pretende-se ainda finalizar, na aplicação web do clav, as interfaces necessárias que permitam aos utilizadores criar ou remover de forma assistida as ts, especificar e definir todas as interfaces web necessárias ao fluxo de trabalho de análise, aprovação e remoção de uma ts."
    ],
    [
      "as urgências hospitalares têm por objetivo responder a emergências que surjam, seja dentro ou fora de horas e, por ser um serviço onde qualquer tipo de caso pode surgir e, sendo cada caso um caso, é extremamente importante garantir a obtenção do melhor atendimento. isto aplica-se ainda mais quando se refere à ala de pediatria. estes por receberem crianças de todo o tipo de idades têm de conseguir responder às diferentes necessidades e dificuldades, mas grande parte desse trabalho passa pelos pais/encarregados de educação que têm a função de informar os enfermeiros, ou médicos, dos pré-cuidados que tenham sido administrados, ou sintomas anteriores. deste modo, esta dissertação apresenta a problemática, razões e solução encontrada, ao definir objetivos e ao estabelecer metas para lá chegar. é apresentado um breve estado de arte com as pesquisas e revisão de literatura mais relevantes na área do problema e identificação de soluções comuns. ainda são apresentadas as metodologias de investigação que serão utilizadas, são ela design science research e proof of concept, e as tecnologias a ser utilizadas, que passam por react para o frontend, node js para o backend e mysql para a base de dados. para terminar é demonstrado os artefactos criados, desde o levantamento de requisitos, passando pela arquitetura de software e terminando no desenho e desenvolvimento, todos estes foram ao longo da sua execução avaliados e feitos teste de usabilidade e viabilidade.",
      "hospital emergencies aim to respond to emergencies that arise, either within or outside hours and, because it is a service where any type of case can arise and, each case being a case, it is extremely important to ensure the best care. this applies even more when referring to pediatrics ward. and because they receive children of all ages have to be able to respond to different needs and difficulties, but much of that work goes through the parents/guardians who have the function of informing nurses or doctors of the pre-care that has been administered, or previous symptoms. thus, this dissertation presents the problem, the reasons and the solution found, when defining objectives and setting goals to get there. a brief state of the art is presented with the most relevant research and literature review of the problem and identification of common solutions. the research methodologies that will be used are still presented, it is design science research and proof of concept, and the technologies to be used, which go through react to the frontend, node js for the backend and mysql for the database. to finish it is demonstrated the artifacts created, from the requirements gathering, through the software architecture and ending in design and development, all these will be throughout its execution evaluated and made usability and feasibility test."
    ],
    0.3
  ],
  [
    [
      "nowadays, with the development of bigger and more complex applications, the architectural paradigm for application development is changing from a more traditional monolithic approach to an architectural style called microservices. in this more recent, and increasingly popular, style of developing applications, a tool that has also become increasingly more popular is api gateways. in this thesis i explored these and a few other concepts on various examples, recording my experience, with the intent to create a guide on how to more efficiently implement these tools on to your own projects, facilitating the usually long and arduous process of researching, learning, and implementing new technologies into your work.",
      "hoje em dia, com o desenvolvimento de aplicações maiores e mais complexas, o paradigma arquitetural para desenvolvimento de aplicações está a transacionar do estilo monolítico tradicional para um estilo de arquitetura chamado microserviços. neste mais moderno, e cada vez mais popular, estilo de desenvolvimento de aplicações, uma ferramenta que também se tem tornado cada vez mais popular tem o nome de api gateway. nesta tese eu explorei estes e outros conceitos em vários exemplos, documentando a minha experiência, com a intenção de criar um guia em como o leitor pode mais eficientemente implementar estas ferramentas nos seus próprios projectos, facilitando o normalmente longo e trabalhoso processo de pesquisa, aprendizagem, e implementação de novas tecnologias no próprio trabalho."
    ],
    [
      "com o crescimento exponencial dos serviços hospedados em ambientes digitais e consecutivamente com o aumento da criticidade dos mesmos, tem-se verificado uma procura constante por parte de desenvolvedores e gestores de projetos por plataformas que disponibilizem agilidade, flexibilidade e baixa complexidade para a disponibilização ao público das suas soluções o mais rápido possível com o mínimo de esforço por parte dos mesmos. este trabalho tem como objetivo precípuo o estudo e implementação de uma plataforma de iaas em cloud privada, com a finalidade de disponibilizar a várias entidades da área da saúde uma plataforma centralizada de gestão de ativos de computação e de rede, com o intuito de facilitar, perante o paradigma passado, a logística associada à criação e coordenação dos ativos por parte das mesmas. para tal o presente trabalho propõe um estudo de mercado, seguido de uma análise de formas e plataformas de automação de processos a serem implementados intrinsecamente e/ou extrinsecamente à plataforma de cloud privada, de modo a trabalharem em simbiose. são também apresentadas metodologias de desenho de scripting necessário para a realização dos casos de uso propostos pela entidade spms, assim como o processo utilizado para a integração da solução com plataformas e serviços terceiros. com este trabalho intenciona-se proporcionar à spms a otimização dos seus recursos computacionais e de rede, bem como diminuir drasticamente as horas humanas dedicadas a processos repetitivos e iterativos, canalizando-as para processos mais nobres.",
      "with the exponential growth of services hosted in digital environments and the increase in their criticality, it has created a constant demand on the part of developers and project managers for platforms that provide agility, flexibility and low complexity to make their services available to the public solutions as quickly as possible with minimal effort on their part. the main objective of this work is the study and implementation of an iaas platform in a private cloud with the purpose of providing several entities in the health area with a centralized platform for the management of computing and network assets, in order to facilitate, in the face of the past paradigm, the logistics associated with the creation and coordination of assets by them. with that in mind, the present work proposes a market study, followed by an analysis of processes automation forms and platforms to be implemented intrinsically and/or extrinsically to the private cloud platform, in order to work in symbiosis. methodologies for designing the scripting necessary to carry out the use cases proposed by the company are also presented, as well as the process used to integrate the solution with third-party platforms and services. with this work, the intention is to provide the company with the optimization of its computational and network resources, as well as drastically reducing the human hours dedicated to repetitive and iterative processes, channeling them to more noble processes."
    ],
    0.3
  ],
  [
    [
      "ontologies are a common approach used in nowadays for formal representation of concepts in a structured way. natural language processing, translation tasks, or building blocks for the new web 2.0 (social networks for example) are instances of areas where the adoption of this approach is emerging and quickly growing. ontologies are easy to store and can be easily build from other data structures. due to their structural nature, data processing can be automated into simple operations. also new knowledge can be quickly infered, many times based on simple mathematics properties. all these qualities brought together make ontologies a strong candidate for knowledge representation. to perform all of these tasks over ontologies most of the times custom made tools are developed, that can be hard to adapt for future uses. the purpose of the work presented in this dissertation is to study and implement tools that can be used to manipulate and maintain ontologies in a abstract and intuitive way. we specify a expressive and powerful, yet simple, domain specific language created to perform actions on ontologies. we will use this actions to manipulate knowledge in ontologies, infer new relations or concepts and also maintain the existing ones valid. we developed a set of tools and engines to implement this language in order to be able to use it. we illustrate the use of this technology with some simple case studies.",
      "ontologias são uma opção muito utilizada hoje em dia para representar formalmente conceitos de uma forma estruturada. processamento de linguagem natural, tarefas de tradução, ou componentes associados à web 2.0 (redes sociais por exemplo) são instâncias de ´áreas onde a adopção desta aproximação está a emergir e a crescer rapidamente. ontologias são fáceis de armazenar e podem ser facilmente construídas a partir de outras estruturas de dados. devido `a sua natureza estruturada, o processamento de dados pode ser automatizado em operações simples. além disso pode ser inferido novo conhecimento rapidamente, muitas vezes baseado em propriedades matemáticas simples. todas estas qualidades em conjunto fazem das ontologias fortes candidatas para a representação de conhecimento. na maior parte dos casos, para executar este tipo de operações, são desenvolvidas ferramentas customizadas à medida que podem ser difíceis de adaptar para uso futuro. o objectivo do trabalho apresentado nesta dissertação é estudar e implementar ferramentas que podem ser utilizadas para manipular e manter ontologias de uma forma abstracta e intuitiva. especificamos uma linguagem de domínio específico simples, no entanto expressiva e poderosa para efectuar operações sobre ontologias. vamos usar estas operações para manipular o conhecimento em ontologias, inferir novas relações ou conceitos e também para manter os existentes válidos. foram desenvolvidas um conjunto de ferramentas e motores que implementam esta linguagem de modo a que possamos utilizá-la. ilustramos o uso desta tecnologia com alguns casos de estudo simples."
    ],
    [
      "distributed systems and protocols are widely employed in the infrastructure that supports the internet and the services available online such as streaming services and social networks. at the same time, they are well known for usually being hard to implement correctly, even when this task is left to experienced programmers. consequently, distributed systems are prone to suffer from distributed concurrency bugs, which are a frequent source of significant service outages. thus, it is of the utmost importance to ensure that widely-used distributed systems are reliable and do not suffer from this kind of bugs. formal verification looks like a promising way to achieve this. however, we argue that the currently available techniques require too much of an investment in order to verify correctness of implementations of complex distributed systems. instead, we defend the usage of clever testing techniques and tools for all but the most critical of contexts. in this dissertation, we present one such tool – spider – designed to automatically detect data races from traced executions of distributed systems. data races originate when two memory accesses to the same memory location occur concurrently and they have been shown to be a major source of concurrency bugs in distributed systems. unfortunately, data races are often triggered by non-deterministic event orderings that are hard to detect when testing complex distributed systems. spider encodes the causal relations between the events in the trace as a symbolic constraint model, which is then fed into an smt solver to check for the presence of conflicting concurrent accesses. to reduce the constraint solving time, spider employs a pruning technique aimed at removing redundant portions of the trace. our experiments with multiple benchmarks show that spider is effective in detecting data races in distributed executions in a practical amount of time, providing evidence of its usefulness as a testing tool.",
      "os sistemas e protocolos distribuídos são amplamente utilizados na infraestrutura que suporta a internet e os serviços disponíveis online tais como, por exemplo, serviços de streaming e redes sociais. ao mesmo tempo, os sistemas distribuídos são reconhecidamente difíceis de implementar corretamente e tendem a sofrer de bugs de concorrência distribuída, mesmo quando são desenvolvidos por programadores experientes. este tipo de bugs é uma causa frequentemente de falhas nos serviços e, por esta razão, é da maior importância garantir que os sistemas distribuídos amplamente utilizados são confiáveis e não sofrem deste tipo de erros. a área de verificação formal fornece ferramentas poderosas que permitem evitar que estes bugs cheguem a código de produção. no entanto, consideramos que as técnicas do estado da arte disponíveis atualmente exigem grandes investimentos caso se pretenda verificar que uma implementação de um sistema distribuído está correta. em vez disso, defendemos o uso de técnicas e ferramentas sofisticadas de teste para assegurar a confiabilidade das implementações de sistemas distribuídos excepto nos contextos mais críticos, onde se devem empregar técnicas de verificação formal. nesta dissertação, apresentamos a spider, uma ferramenta de teste e debug de sistemas distribuídos desenvolvida para detectar automaticamente data races a partir de traces obtidos aquando da execução de sistemas distribuídos. as data races surgem quando dois acessos ao mesmo endereço de memoria ocorrem concorrentemente. a existência de data races é uma das principais causas de erros de concorrência em sistemas distribuídos. ao mesmo tempo, são extremamente difíceis de detetar uma vez que se manifestam raramente e de forma não determinística. a ferramenta spider codifica as relações causais entre os eventos no trace como um modelo de restrições, e, através de um smt solver, e capaz de inferir que pares de instruções podem levar a acessos concorrentes ao mesmo endereço de memória. para reduzir o tempo da análise, o spider emprega uma técnica de eliminação de eventos que remove partes redundantes do trace. com recurso a vários benchmarks, mostramos experimentalmente que o spider é eficaz a detetar data races a partir de traces de sistemas distribuídos e que tal é exequível em tempo útil, indiciando que a spider é útil como ferramenta de teste."
    ],
    0.3
  ],
  [
    [
      "com a chegada das metodologias agile, passou a ser possível a entrega e mudanças mais rápidas do software (1), aumentando o ritmo de desenvolvimento de toda a indústria de software, que trouxe como resultado inevitável o aumento do número de deployment de software desenvolvido de forma a satisfazer a necessidade de entregas ao cliente. estas metodologias provocaram também a mudança de paradigma de sistemas monolíticos para a utilização de microsserviços, pois o crescente ritmo de desenvolvimento tornou a gestão de sistemas monolíticos insustentável, sendo vantajosa a utilização de microsserviços pela sua manutenção, reusabilidade, escalabilidade e disponibilidade ser facilitada (2). no entanto, o potencial dos microsserviços é maximizado aquando da utilização de um sistema de orquestração que permita a simplificação e gestão dos deployment’s, especialmente em sistemas de alta complexidade (3) que necessitem de manter a alta disponibilidade, gerir a sua escalabilidade e reagir rapidamente a falhas (4). assim, esta dissertação pretende explorar diversas estratégias de deployment na tecnologia kubernetes (5), com o objetivo de verificar quais os seus impactos nos casos de estudo utilizados no que diz respeito à qualidade de software e prevenção de erros do software entregue, sejam eles, erros de disponibilidade durante e/ou após o deployment ou erros reportados pela monitorização posterior dos containers (6).",
      "with the arrival of agile methodologies, faster software delivery and changes became possible (1), increasing the pace of development throughout the software industry, which brought as an inevitable result the increasing number of deployments of the developed software in order to meet the need for customer deliverables. these methodologies have also caused the paradigm shift from monolithic systems to the use of microservices, as the increasing pace of development has made the management of monolithic systems unsustainable, and the use of microservices is advantageous due to its easier maintenance, reusability, scalability, and availability (2). however, the potential of microservices is maximized when using an orchestration system that allows the simplification and management of deployments, especially in highly complex systems (3) that need to maintain high availability, manage their scalability, and react quickly to failures (4). hence, this dissertation aims to explore multiple deployment strategies in kubernetes technology (5), with the purpose of verifying their impacts on the study cases used regarding software quality and error prevention of the delivered software, whether they be, availability errors during and/or after deployment or errors reported by post-deployment monitoring of the containers (6)."
    ],
    [
      "o desenvolvimento orientado pelo comportamento (behaviour-driven-development, bdd) é um paradigma de desenvolvimento de software que permite especificar as necessidades dos utilizadores e os seus critérios de aceitação. a especificação de um sistema é feita através da descrição de cenários de utilização que serão depois implementados. rocha silva (2022) propôs uma linguagem de especificação de cenários de utilização, para interfaces web, que utiliza os widgets da própria interface na especificação, permitindo não só especificar os requisitos do sistema, mas também referir como estes deverão ser implementados. no processo de bdd podem surgir na especificação cenários contraditórios que, passando despercebidos, podem levar a que a especificação tenha de ser revista na fase de implementação. todo este processo acarreta custos, pelo que surgiu a necessidade de criar um método de determinar se uma interface está especificada sem quaisquer contradições (isto é, se a especificação é ou não consistente na descrição do sistema). o objetivo deste trabalho é então definir um método que permite, para uma interface web especificada na linguagem proposta por rocha silva (2022), determinar se a especificação apresenta ou não contradições (ou seja, é inconsistente) nos seus cenários. para cumprir este objetivo será utilizada a ferramenta ivy workbench que permite analisar, de forma automática, modelos escritos em mal interactors do comportamento de sistemas interativos (permitindo verificar propriedades sobre estes). o primeiro passo do projeto será, então, desenvolver uma ferramenta (o modelador) capaz de traduzir uma interface web especificada na linguagem proposta por rocha silva (2022) para mal interactors, de modo a que esta possa ser analisada na ferramenta ivy workbench. depois, serão ainda apresentadas as propriedades ctl que serão verificadas na ferramenta ivy workbench para determinar se uma determinada especificação é consistente. por fim, será apresentado um método de, utilizando a ferramenta ivy workbench e o modelo em mal interactors gerado pelo modelador com as devidas propriedades ctl, determinar não só se a especificação é inconsistente mas, caso o seja, os cenários que dão origem a essa inconsistência.",
      "behaviour-driven-development, bdd is a software development paradigm that allows users needs and acceptance criteria to be specified. the specification of a system is done by describing usage scenarios that will then be implemented. rocha silva (2022) proposed a usage scenario specification language for web interfaces that uses the widgets of the interface itself in the specification, making it possible not only to specify the system’s requirements, but also how they should be implemented. in the bdd process, contradictory scenarios can appear in the specification which, unnoticed, can lead to the specification having to be revised in the implementation phase. this whole process has added costs, so the need arose to create a method of determining whether an interface is specified without any contradictions (that is, whether or not, the specification of the system is consistent). the goal of this project is to define a method that allows us to determine if a web interface, specified in the language proposed by rocha silva (2022), has contradictions or not in its scenarios (that is, whether it is consistent or not). to achieve this goal, the tool ivy workbench will be used. this tool allows us to automatically analyze models of the behaviour of interactive systems written in mal interactors (allowing properties to be verified on them). the first step of the project will then be to develop a tool (the modeler), capable of translating a web interface specified in the language proposed by rocha silva (2022) into mal interactors, so that it can be analyzed in the ivy workbench tool. the next step, will be to present the ctl properties that will be verified in the ivy workbench tool, to determine whether a given specification is consistent or not. finally, it will be presented a method that, using the ivy workbench tool and the model in mal interactors generated by the modeler with the appropriate ctl properties, will allow us to determine not only whether the specification is inconsistent but, in that case, the scenarios that give rise to this inconsistency."
    ],
    0.3
  ],
  [
    [
      "it is now possible to prove that technology has proven to be a strong ally in the most diverse areas, from economics and management to health or banking. education is therefore no exception. the insertion of technology and software that provide students with educational and motivational support for their learning has been a major challenge. it is based on this lack of support that a project called “leonardo” emerged at the university of minho. this project considers the development of educational software, which aims to provide students with a supervised learning method that will allow students to improve their knowledge and also observe their results. all of this will allow you to draw lessons that support your progress in a given area of study. this dissertation proposes, therefore, the development of a data warehousing system, which allows the collection of information about users and their interactions with the referred system. in this way, it will be possible to obtain a multidimensional data analysis platform, which allows to monitor the student’s state of knowledge over time, thus allowing him / her to evaluate his / her progress, in a given field of study, during the interaction with a student. the system.",
      "atualmente, é possível comprovar que a tecnologia tem-se demonstrado um forte aliado nas mais diversas áreas, desde a economia e gestão até à saúde ou à banca. a área da educação não é, pois, uma exceção. a inserção de tecnologia e de software que forneçam ao aluno um suporte de ensino e de apoio à sua aprendizagem, de forma educativa e motivadora, tem sido um grande desafio. é com base nesta carência de suporte, que surgiu na universidade do minho um projeto denominado “leonardo”. este projeto considera o desenvolvimento de software educacional, cujo objetivo é fornecer aos alunos um método de aprendizagem supervisionado, que permitirá aos alunos melhorarem o seu conhecimento e também observar os seus resultados. tudo isto permitirá retirar ilações que suportem o seu progresso numa dada área de estudo. esta dissertação propõe assim, o desenvolvimento de um sistema de data warehousing, que permita a recolha de informações acerca dos utilizadores e das suas interações com o sistema referido. desta forma, será possível obter uma plataforma de análise de dados, multidimensional, que permita acompanhar o estado do conhecimento de um aluno ao longo do tempo, permitindo desta forma avaliar o seu progresso, num determinado domínio de estudo, ao longo da sua interação com o sistema."
    ],
    [
      "à medida que as aplicações atingem uma maior quantidade de utilizadores, precisam de processar uma crescente quantidade de pedidos. para além disso, precisam de muitas vezes satisfazer pedidos de utilizadores de diferentes partes do globo, onde as latências de rede têm um impacto significativo no desempenho em instalações monolíticas. portanto, distribuição é uma solução muito procurada para melhorar a performance das camadas aplicacional e de dados. contudo, distribuir dados não é uma tarefa simples se pretendemos assegurar uma forte consistência. isto leva a que muitos sistemas de base de dados dependam de protocolos de sincronização pesados, como two-phase commit, consenso distribuído, bloqueamento distribuído, entre outros, enquanto que outros sistemas dependem em consistência fraca, não viável para alguns casos de uso. esta tese apresenta o design, implementação e avaliação de duas soluções que têm como objetivo reduzir o impacto de assegurar garantias de forte consistência em sistemas de base de dados, especialmente aqueles distribuídos pelo globo. a primeira é o primary semi-primary, uma arquitetura de base de dados distribuída com total replicação que permite que as réplicas evoluam independentemente, para evitar que os clientes precisem de esperar que escritas precedentes que não geram conflitos sejam propagadas. apesar das réplicas poderem processar tanto leituras como escritas, melhorando a escalabilidade, o sistema continua a oferecer garantias de consistência forte, através do envio da certificação de transações para um nó central. o seu design é independente de modelos de dados, mas a sua implementação pode tirar partido do controlo de concorrência nativo oferecido por algumas base de dados, como é mostrado na implementação usando postgresql e o seu snapshot isolation. os resultados apresentam várias vantagens tanto em ambientes locais como globais. a segunda solução são os multi-record values, uma técnica que particiona dinâmicamente valores numéricos em múltiplos registros, permitindo que escritas concorrentes possam executar com uma baixa probabilidade de colisão, reduzindo a taxa de abortos e/ou contenção na adquirição de locks. garantias de limites inferiores, exigido por objetos como saldos bancários ou inventários, são assegurados por esta estratégia, ao contrário de muitas outras alternativas. o seu design é também indiferente do modelo de dados, sendo que as suas vantagens podem ser encontradas em sistemas sql e nosql, bem como distribuídos ou centralizados, tal como apresentado na secção de avaliação.",
      "as applications reach an wider audience that ever before, they must process larger and larger amounts of requests. in addition, they often must be able to serve users all over the globe, where network latencies have a significant negative impact on monolithic deployments. therefore, distribution is a well sought-after solution to improve performance of both applicational and database layers. however, distributing data is not an easy task if we want to ensure strong consistency guarantees. this leads many databases systems to rely on expensive synchronization controls protocols such as two-phase commit, distributed consensus, distributed locking, among others, while other systems rely on weak consistency, unfeasible for some use cases. this thesis presents the design, implementation and evaluation of two solutions aimed at reducing the impact of ensuring strong consistency guarantees on database systems, especially geo-distributed ones. the first is the primary semi-primary, a full replication distributed database architecture that allows different replicas to evolve independently, to avoid that clients wait for preceding non-conflicting updates. al though replicas can process both reads and writes, improving scalability, the system still ensures strong consistency guarantees, by relaying transactions’ certifications to a central node. its design is independent of the underlying data model, but its implementation can take advantage of the native concurrency control offered by some systems, as is exemplified by an implementation using postgresql and its snapshot isolation. the results present several advantages in both throughput and response time, when comparing to other alternative architectures, in both local and geo-distributed environments. the second solution is the multi-record values, a technique that dynami cally partitions numeric values into multiple records, allowing concurrent writes to execute with low conflict probability, reducing abort rate and/or locking contention. lower limit guarantees, required by objects such as balances or stocks, are ensure by this strategy, unlike many other similar alternatives. its design is also data model agnostic, given its advantages can be found in both sql and nosql systems, as well as both centralized and distributed database, as presented in the evaluation section."
    ],
    0.3
  ],
  [
    [
      "ensuring the good usability and user experience of software systems is invaluable, and for that, following a standardized usability engineering process is fundamental. prototyping plays a crucial role in this process, enabling the proper validation of the usability guidelines before reaching the actual implementation phase. this dissertation focuses on the construction of a javascript widgets library to ease the process of prototyping user interfaces. this library will later be incorporated in the prototyping software tool pvsio-web.",
      "assegurar a boa usabilidade e experiência por parte dos utilizadores de um sistema de software é algo de valor inestimável, e, para isso, seguir um processo estabilizado de engenharia de usabilidade é fundamental. a prototipagem desempenha um papel crucial neste processo, ao dar a possibilidade de validar a usabilidade de um sistema antes de se iniciar a fase de implementação. esta dissertação foca-se na construção de uma biblioteca javascript de widgets para facilitar o processo de desenvolvimento de prototipagem de interfaces. esta biblioteca será posteriormente incorporada na ferramenta de prototipagem pvsio-web."
    ],
    [
      "realistic simulation and rendering of water in real-time is a challenge within the field of computer graphics, as it is very computationally demanding. a common simulation approach is to reduce the problem from 3d to 2d by treating the water surface as a 2d heightfield. when simulating 2d fluids, the shallow water equations (swe) are often employed, which work under the assumption that the water’s horizontal scale is much greater than it’s vertical scale. there are several methods that have been developed or adapted to model the swe, each with its own advantages and disadvantages. a common solution is to use grid-based methods where there is the classic approach of solving the equations in a grid, but also the lattice-boltzmann method (lbm) which originated from the field of statistical physics. particle based methods have also been used for modeling the swe, namely as a variation of the popular smoothed-particle hydrodynamics (sph) method. this thesis presents an implementation for real-time simulation and rendering of a heightfield surface water volume. the water’s behavior is modeled by a grid-based swe scheme with an efficient single kernel compute shader implementation. when it comes to visualizing the water volume created by the simulation, there are a variety of effects that can contribute to its realism and provide visual cues for its motion. in particular, when considering shallow water, there are certain features that can be highlighted, such as the refraction of the ground below and corresponding light attenuation, and the caustics patterns projected on it. using the state produced by the simulation, a water surface mesh is rendered, where set of visual effects are explored. first, the water’s color is defined as a combination of reflected and transmitted light, while using a cook- torrance bidirectional reflectance distribution function (brdf) to describe the sun’s reflection. these results are then enhanced by data from a separate pass which provides caustics patterns and improved attenuation computations. lastly, small-scale details are added to the surface by applying a normal map generated using noise. as part of the work, a thorough evaluation of the developed application is performed, providing a showcase of the results, insight into some of the parameters and options, and performance benchmarks.",
      "simulação e renderização realista de água em tempo real é um desafio dentro do campo de computação gráfica, visto que é muito computacionalmente exigente. uma abordagem comum de simulação é de reduzir o problema de 3d para 2d ao tratar a superfície da água como um campo de alturas 2d. ao simular fluidos em 2d, é frequente usar as equações de águas rasas, que funcionam sobre o pressuposto de que a escala horizontal da água é muito maior que a sua escala vertical. há vários métodos que foram desenvolvidos ou adaptados para modelar as equações de águas rasas, cada uma com as suas vantagens e desvantagens. uma solução comum é utilizar métodos baseados em grelhas onde existe a abordagem clássica de resolver as equações numa grelha, mas também existe o método de lattice boltzmann que originou do campo de física estatística. métodos baseados em partículas também já foram usados para modelar as equações de águas rasas, nomeadamente como uma variação do popular método de sph. esta tese apresenta uma implementação para simulação e renderização em tempo real de um volume de água com uma superfície de campo de alturas. o comportamento da água é modelado por um esquema de equações de águas rasas baseado na grelha com uma implementação eficiente de um único kernel de compute shader. no que toca a visualizar o volume de água criado pela simulação, existe uma variedade de efeitos que podem contribuir para o seu realismo e fornecer dicas visuais sobre o seu movimento. ao considerar águas rasas, existem certas características que podem ser destacadas, como a refração do terreno por baixo e correspondente atenuação da luz, e padrões de cáusticas projetados nele. usando o estado produzido pela simulação, uma malha da superfície da água é renderizada, onde um conjunto de efeitos visuais são explorados. em primeiro lugar, a cor da água é definida como uma combinação de luz refletida e transmitida, sendo que uma brdf de cook-torrance é usada para descrever a reflexão do sol. estes resultados são depois complementados com dados gerados num passo separado que fornece padrões de cáusticas e melhora as computações de atenuação. por fim, detalhes de pequena escala são adicionados à superfície ao aplicar um mapa de normais gerado com ruído. como parte do trabalho desenvolvido, é feita uma avaliação detalhada da aplicação desenvolvida, onde é apresentada uma demonstração dos resultados, comentários sobre alguns dos parâmetros e opções, e referências de desempenho."
    ],
    0.06666666666666667
  ],
  [
    [
      "multiple sclerosis (ms) is an autoimmune demyelinating disorder that affects the central nervous system by damaging myelin and axons, but the exact cause of ms remains unclear. it is known that the immune system destroys oligodendrocytes (ols), the myelinating cells of the cns, and regardless of the efforts, it remains challenging to induce replacement of the lost ols. the choroid plexus (cp) is fundamental for brain homeostasis as it secretes the cerebrospinal fluid. it is also a key modulator of neurogenesis and constitutes a site of neuroinflammation. since ms is an inflammatory disorder, studying the relation between the cp and ventricular-subventricular zone (v-svz) is of interest. in this dissertation we explored the role of the cp in the modulation of neural stem cells and ols progenitor cells (opcs), located in the svz, in order to assess its potential to induce ol (re)generation. for that we used single cell rna sequencing (scrna-seq) data from both control and an ms model generated by our team. here we explored the cell types, rna velocities and cell-cell communication (ccc) inference. we have characterized the cell populations present in the v-svz of controls and ms model, and found differences in the clusters identified. rna velocity revealed two central cores from where most of progenitor cells seamed to arise from, being one located at the neuronal intermediate progenitor cells cluster and the other one located in the astrocytes cluster. further analysis has to be performed in order to confirm these results. concerning ccc inference, we found evidence of signalling pathways between endothelial cells and pericytes with opcs, which corroborates previous studies where this communication was reported. together with literature, our data indicates that endothelial cells and pericytes regulate opcs proliferation through pdgf signalling. when analyzing the results from ccc inference in the cp-svz integrated data we found evidence of a communication from mesenchymal cells at the cp and ependymal cells through wnt signalling. another interaction was between opcs and ependymal cells through tenascin signalling. nevertheless, the existence of this interactions in vivo needs to be further validated.",
      "a esclerose múltipla (em) é uma desordem autoimune desmielinizante que afeta o sistema nervoso central (snc) ao danificar a mielina e os axónios, mas a causa exata da em permanece pouco clara. sabe-se que o sistema imunitário destrói oligodendrócitos (ols), as células mielinizantes do snc, e independentemente dos esforços, continua a ser um desafio induzir a substituição das ols perdidas. o plexo coroide (pc) é fundamental para a homeostase cerebral, uma vez que segrega o líquido cefalorraquidiano (lcr). é também um modulador chave da neurogénese e constitui um local de neuroinflamação. uma vez que a em é uma doença inflamatória, o estudo da relação entre o pc e a zona ventrículo-subventricular (z-vsv) é de interesse. nesta dissertação exploramos o papel do pc na modulação das células estaminais neurais e células progenitoras de ols (cpo), localizadas na svz, a fim de avaliar o seu potencial para induzir a (re)geração de ol. para isso, utilizamos dados de sequenciamento de rna de célula única (scrna-seq), tanto de controlo como de um modelo ms gerado pela nossa equipa. aqui exploramos os tipos celulares, as velocidades de arn e a inferência de comunicação célula-célula (ccc). caracterizámos as populações celulares presentes na v-svz dos controlos e modelo ms, e encontramos diferenças nos clusters identificados. a velocidade do rna revelou dois núcleos centrais de onde surgiu a maioria das células progenitoras, estando um localizado no aglomerado de células progenitoras neuronais intermédias e o outro localizado no aglomerado de astrócitos. é necessário efetuar uma análise mais aprofundada a fim de confirmar estes resultados. em relação à inferência ccc, encontramos provas de vias de sinalização entre células endoteliais e pericitos com opc, o que corrobora estudos anteriores onde esta comunicação foi relatada. juntamente com a literatura, os nossos dados indicam que as células endoteliais e os pericitos regulam a proliferação de cpos através da sinaçlização pdgf. ao analisar os resultados da inferência de ccc nos dados integrados do cp-svz, encontrámos provas de uma comunicação de células mesenquimais no cp e células ependimárias através da sinalização wnt. outra interação foi entre opcs e células ependimárias através da sinalização de tenascin. no entanto, a existência destas interações in vivo precisa de ser mais validada."
    ],
    [
      "databases are one of the main technologies supporting organizations’ information assets, and very often these databases contain information that is irreplaceable or prohibitively expensive to reacquire. the digital preservation field attempts to maintain this kind of information accessible and authentic for multiple decades, but the complexity commonly found in databases and the incompatibilities between database systems make it difficult to preserve this kind of digital object. the database preservation toolkit is a software that automates the migration of relational databases to the second version of the software independent archiving of relational databases format. furthermore, this flexible tool that supports the current most popular relational database management systems can also convert a preserved database back to a database management system, allowing for some special usage scenarios in an archival context. the conversion of databases between different formats, whilst retaining the databases’ significant properties, poses a number of interesting issues, which are described in this document, along with their current solutions. to complement the conversion software, the database visualization toolkit is introduced, a software tool that provides access to preserved databases, enabling a consumer to quickly search and explore a database without knowing any query language. the viewer is capable of handling big databases as well, promptly presenting results of searching and filtering operations on millions of records. this work covers the challenges of relational database preservation, and the development of a format and tools that play an important role in successfully preserving this kind of information.",
      "as bases de dados são uma das principais tecnologias para armazenamento e gestão de informação digital de uma organização e, caso se perdesse, esta informação poderia ser de muito difícil ou dispendiosa recuperação. para evitar este tipo de situações e despesas, a área da preservação digital tenta encontrar formas de manter a informação acessível e autêntica durante várias décadas, no entanto, devido à complexidade presente nas bases de dados e às incompatibilidades entre diferentes sistemas de base de dados, preservar bases de dados não é uma tarefa trivial. o database preservation toolkit é uma aplicação que automatiza a conversão de bases de dados relacionais para um formato especialmente desenhado para a sua preservação, o software independent archiving of relational databases. esta ferramenta é capaz de exportar bases de dados dos sistemas de gestão de base de dados mais populares, e também recuperar a base de dados para um sistema de gestão de base de dados, potencialmente diferente do seu sistema original. esta flexibilidade na escolha dos formatos ou sistemas de entrada e saída faz com que a ferramenta possa ser usada para solucionar vários problemas no contexto da preservação de bases de dados. para complementar a ferramenta de conversão foi criada uma plataforma de visualização de bases de dados que estejam num formato de preservação, o database visualization toolkit. esta plataforma permite analisar os meta-dados da base de dados e pesquisar o seu conteúdo, sem requerer conhecimento especializado na área das bases de dados. a ferramenta foi desenhada para providenciar acesso a bases de dados de grandes dimensões, para ter a capacidade de apresentar rapidamente os resultados de pesquisas em milhões de registos. o presente documento foca-se na preservação de bases de dados relacionais, abordando as principais dificuldades dessa atividade, assim como o desenvolvimento de um formato específico para preservação desses objetos, e descreve o desenvolvimento e funcionamento de ferramentas que têm um papel crucial na preservação deste tipo de objetos digitais."
    ],
    0.3
  ],
  [
    [
      "um biofilme é uma comunidade de microrganismos envoltos por uma matriz extracelular produzida pelos próprios, que lhes garante proteção. os biofilmes representam um problema para a saúde pública pois facilmente encontram-se em dispositivos médicos, podendo causar problemas graves para os pacientes. estudos prévios indicam algumas alterações observáveis do aspeto físico e bioquímico das comunidades microbianas na resposta à resistência e à virulência. assim a morfologia da comunidade pode ser um indicativo da reação regulatória associada com fenómenos de patogenicidade microbiana. o objetivo deste trabalho é por um lado a criação de um novo sistema de classificação de morfologia de colonia com medidas extraídas de softwares de imagem por outro lado, o estudo da classificação morfológica existente e do novo sistema de classificação, através de técnicas de mineração de dados com o objetivo de ajudar nestas classificações. apresentamos vários softwares como solução que vão desde a caraterização da estrutura dos biofilmes até a caraterização de morfologia de colonia",
      "biofilms are communities of microorganisms embedded in a self-produced extracellular matrix, adherent to an inanimate, biotic surface that provides them with protection. biofilms are a healthcare problem since they can be found in several medical devices and end up causing problems for the patients. previous studies have reported observable physical and biochemical changes of microbial communities associated with resistance and virulence response. this suggests that the morphology of a biofilm is a marker for regulatory interplays associated with the microbial phenomenon of pathogenicity. the aims of this work are on the one hand, create a novel system of colony morphological classification with measurements extract from image software on the other hand study the current manual morphological classification and the novel one through data mining techniques. here we present several software solutions to facilitate the process, from the determination of biofilm structure to the characterisation of colony morphology."
    ],
    [
      "com o surgimento da computação em cloud, tem havido uma crescente adoção da containerisação (containerisation) e orquestração de containers para o desenvolvimento de software. as empresas que aderem a práticas de continuousintegration/continuousdelivery(ci/cd)e microserviços beneficiam muito da adoção destas tecnologias, pois os containerstem permitido o aprovisionamento mais rápido e automatizado de aplicações, melhorando a sua escalabilidade e capacidade de tolerância a faltas. a feedzai é uma empresa que usa algoritmos de machine learning para combater a fraude financeira, usando um sistema distribuído complexo constituído por múltiplas tecnologias. escrever configurações para ambientes de teste nestas condições é frequentemente um desafio para o engenheiro de testes, especialmente se feito manualmente, resultando num maior custo em horas-humano necessárias para desempenhar esta tarefa. quando se trata de ambientes de teste, estas configurações dependem muito da topologia requerida pelo teste, o que resulta num potencialmente grande e crescente número de ficheiros de configuração para gerir. é obrigatório resolver este problema cedo, de forma a antecipar uma stack tecnológica difícil de gerir à medida que os casos de teste que terão de ser cobertos crescem. este problema foi alvo de várias tentativas de solução por parte de engenheiros na feedzai, mas as soluções resultantes provaram, com o passar do tempo, ser insuficientes, resolvendo apenas parte do problema. esta dissertação apresenta a arquitetura e principais decisões de implementação do programmableenvironmentsforquickorchestrationofdeployments(pequod), uma framework que se propõe a resolver o problema supramencionado ao permitir ao programador lançar um ambiente composto por uma stack tecnológica arbitrária usando uma qualquer tecnologia de containerisação/orquestração escolhida pelo mesmo. com esta ferramenta, o programador apenas escolhe quais os componentes que serão lançados e descreve as dependências entre os mesmos; a lógica de configurar estes componentes usando a tecnologia escolhida é executada pelo pequod, sem que o programador tenha de ficar familiarizado com esta. o desenho da domain-specificlanguage(dsl)que permite ao programador definir o ambiente de forma transparente é também discutido aqui. o presente documento apresenta também uma avaliação das capacidades desta framework usando dois produtos distintos da feedzai. os resultados desta avaliação revelaram que esta nova framework está em conformidade com os objetivos delineados de início, resolvendo os problemas que as soluções antecessoras não resolviam.",
      "there has been an ever growing adoption of containerisation and container orchestration for software development. the enterprises that adhere to ci/cd and microservices practices benefit a lot from the adoption of these technologies, since containers allow for faster and automated provisioning of applications, as well as improved scalability and fault tolerance. feedzai is a company that uses machine learning algorithms for fighting financial fraud, using a complex distributed system consisting of multiple technologies. writing test environment configurations in these conditions is often a challenge for the testing engineer, especially if done manually, resulting in a high cost in human hours needed. when it comes to testing environments, these configurations depend a lot on the topology required by the test, which results in a potentially large and growing number of configuration files to manage. it is essential to tackle this problem early on, anticipating an unmanageable technological stack as the growing number of test cases will need to be covered. solving this problem has been attempted by engineers at feedzai multiple times in the past, but these solutions proved to be insufficient over time, solving only part of the bigger problem. this dissertation presents the architecture and main implementation decisions of pequod, a framework that proposes to solve the aforementioned problem by allowing the developer to launch an environment composed of an arbitrary technological stack using a given containerisation/orchestration technology chosen by the developer. with this tool, the developer only has to choose the components that are to be launched and the dependencies between them; the logic of configuring these components using the technology chosen is carried out by the framework itself, without the developer having to become familiar with it. the design of the dsl that allows the developer to define the environment transparently is also discussed here. this document also presents the evaluation of the capabilities of this framework with two distinct feedzai products. the outcomes of this evaluation revealed that the new framework conforms with the objectives initially outlined, solving the problems that its predecessor solutions were unable to."
    ],
    0.0
  ],
  [
    "tanto na literatura científica como na indústria, especificamente na área da computação disseminada e espaços inteligentes (pervasive computing and smart spaces), são encontradas muitas aplicações e sistemas baseados em interações (implícitas e explícitas) com recursos físicos no ambiente, tais como pontos wi-fi, recetores gps, componentes bluetooth, leitores rfid, telefones móveis ou câmaras. bluetooth é uma tecnologia sem fios de curto alcance, que não requer configurações e que está presente num elevado número de dispositivos móveis. tornou-se, assim, numa poderosa ferramenta para interação com ambientes físicos, sendo consecutivamente adotada por diferentes aplicações e sistemas como uma tecnologia privilegiada de interação. atualmente, estas aplicações e sistemas implementam os seus próprios componentes bluetooth, capazes de executar tarefas especificamente relacionadas com as aplicações em causa, tais como a obtenção de informação sobre os dispositivos dos utilizadores ou trocas de ficheiros com estes. nesta dissertação argumenta-se que os componentes bluetooth podem ser tratados como recursos de interação do espaço físico, com a possibilidade de serem partilhados e reutilizados por diferentes aplicações e sistemas. desta forma liberta-se os seus programadores de questões relacionados com a implementação e gestão específicas da tecnologia bluetooth, incentivando-os a focarem-se nos objetivos da aplicação. além disso, a nossa abordagem poderá também contribuir para a sustentação da indústria da computação disseminada sob uma perspetiva ambiental, dado que irá permitir a partilha e reutilização de recursos físicos, reduzindo o número de dispositivos computacionais. neste trabalho estudam-se projetos e aplicações relevantes para a indústria da computação disseminada e espaços inteligentes que recorrem à tecnologia bluetooth como uma forma de interação com os seus utilizadores. são identificadas as características comuns destes projetos, tratando-se de um passo importante para a sistematização destas interações. este trabalho constitui a base do desenho de um novo recurso bluetooth. foram desenvolvidos e instalados protótipos em vários cenários reais, a fim de se validar não apenas a viabilidade de tal componente em espaços inteligentes mas, também, o modelo de integração com as aplicações. tal validação é apresentada neste documento, funcionando como uma prova de conceito desta componente.",
    [
      "over the last few years, there has been a development of information technologies (it) and its applicability has had repercussions in the most varied domains. the health sector has been no exception, with significant repercussions in terms of improving the quality and effectiveness of healthcare provided by the various organizations, the security of data maintenance and transmission, and the increased interoperability between the various hospital information systems (his). the present dissertation project comes in the context of the need for maintenance and updating of university hospital center of porto computer services. in this sense, the objective was to develop a new web application, called portal pedidos, through which will be managed the whole process of requesting complementary diagnostic and therapeutic means. with reference to users’ perceptions of the current state of the platform and the needs identified by them, the developments to be implemented should ensure the optimization of the existing essential functionality and the introduction of gains that enable earnings in terms of usability, adaptation to devices and increased data structuring performance. the application developed also allows the availability of information, previously monolithic and exclusive of the application in use, with other services within the hospital unit through the implementation oriented to a web service. for the purpose of application development, the prior exploration and selection of the technologies to be adopted was crucial in order to guarantee an appropriate option to achieve the established objectives. the strategy adopted was the use of innovative web technologies, specifically using the reactjs for frontend and node.js for backend. the adoption of these technologies enabled the implementation of the pre-established requirements and the incorporation of the contributions that emerged throughout the process.",
      "ao longo dos últimos anos tem-se assistido a um desenvolvimento das tecnologias da informação (ti) e a sua aplicabilidade tem vindo a ter repercussões nos mais variados domínios. o setor da saúde não tem sido exceção, constatando-se reflexos significativos no que concerne à melhoria da qualidade e eficácia dos cuidados de saúde prestados pelas diversas organizações, ao nível da segurança na manutenção e transmissão dos dados e ao aumento da interoperabilidade entre os diversos sistemas de informação hospitalar (sih). o presente projeto de dissertação surge num contexto de necessidade de manutenção e de atualização dos serviços informáticos do centro hospitalar universitário do porto (chup). neste sentido, definiu-se como objetivo o desenvolvimento de uma nova aplicação web, denominada portal pedidos, através da qual se processará a gestão de todo o processo de solicitação de meios complementares de diagnóstico e te-rapêutica (mcdt). tendo por referência as perceções dos utilizadores relativamente ao atual estado da plataforma e as necessidades por eles identificadas, os desenvolvimen-tos a operar deverão garantir a otimização das funcionalidades essenciais atualmente existentes e a introdução de mais valias que viabilizem ganhos em termos de usabili-dade, de adaptação a vários dispositivos e ainda o desempenho acrescido ao nível da estruturação de dados. a aplicação desenvolvida também permite a disponibilização da informação, anteriormente monolítica e exclusiva da aplicação em uso, com outros serviços dentro da unidade hospitalar através da implementação orientada a um web service. para efeito de desenvolvimento da aplicação revelou-se crucial a prévia exploração e seleção das tecnologias a adotar, por forma a garantir uma opção adequada à concreti-zação dos objetivos estabelecidos. definiu-se como estratégia a utilização de tecno-logias web inovadoras, recorrendo-se concretamente ao reactjs, para o frontend, e o node.js para o backend. a adoção destas tecnologias viabilizou a implementação dos requisitos pré-estabelecidos e a incorporação dos contributos que foram surgindo ao longo do processo."
    ],
    0.0
  ],
  [
    [
      "the use of deep learning techniques in medical image analysis has been a subject of growing interest in recent years. one of the most important applications of these techniques is the detection and segmentation of tumors in histological images. this dissertation focused on investigating the use of deep learning models to segment tumors, with the aim of providing medical specialists with a tool that can help them make more precise diagnoses. tumor growth patterns are an important histological characteristic that can provide information about the aggressiveness and degree of malignancy of a tumor. specifically, the epithelial-mesenchymal transition on the tumor front is a pattern that has been shown to confer high aggressiveness and a great capacity to invade tissues and cause metastases, leading to a poor prognosis regarding the evolution of the tumor. therefore, detecting and segmenting tumors in histological images can be a critical step in the diagnosis and treatment of tumors. the research process involved several steps, including preprocessing the images to prepare them for deep learning models. this step involved developing methods to enhance the quality of the images and make them suitable for training deep learning models. two types of deep learning architectures, the u-net and tiramisu, were trained in a supervised way, and different types of loss functions were experimented with to measure their efficiency in controlling the training process. additionally, different types of hyperparameters were tried, and the best value was chosen for each hyperparameter. finally, the effectiveness of the models was evaluated and compared both qualitatively and quantitatively based on their performance in image segmentation. the results obtained show that deep learning models surpassed the initially predicted values and reached a value above 94% based on the training data. for the interception over the union metric. this result demonstrates the potential of deep learning techniques to detect and segment tumors in histological images and reinforces the importance of continuing to investigate this topic. the best results of the present work were achieved with total loss, as explained on page 89.",
      "a aplicação de técnicas de aprendizagem profunda na análise de imagens médicas tem sido alvo de um interesse crescente nos últimos anos. uma das aplicações mais importantes destas técnicas é a deteção e segmentação de tumores em imagens histológicas. a presente dissertação fucou-se na investigação sobre a utilização de modelos de aprendizagem profunda para segmentar tumores, com o objetivo de fornecer aos especialistas médicos uma ferramenta que ajude a efetuar diagnósticos mais corretos. os padrões de crescimento tumoral são uma característica histológica importante, que pode fornecer informação sobre a agressividade e grau de malignidade dum tumor. especificamente, a transição epitelial-mesenquimal na frente do tumor é um padrão que confere alta agressividade e grande capacidade de invadir tecidos e causar metástases, conduzindo a um mau prognóstico sobre a evolução do tumor. portanto, a detecção e segmentação de tumores em imagens histológicas é um passo crítico no diagnóstico e tratamento dos tumores. o trabalho desenvolvido decorreu em várias etapas, incluindo o pré-processamento das imagens para prepará-las para treinar os modelos de aprendizagem profunda. essa etapa envolveu o desenvolvimento de métodos para melhorar a qualidade das imagens e torná-las adequadas para o treino de modelos de aprendizagem profunda. dois tipos de modelo de aprendizagem profunda, u-net e tiramisu, foram treinados de forma supervisionada, e experimentaram-se diferentes tipos de função de perda para medir a sua eficácia no controlo do processo de treino. adicionalmente, testaram-se diferentes tipos de hiperparâmetros e escolheu-se o melhor valor para cada hiperparâmetro a utilizar em futuras experiências. finalmente, a eficácia dos modelos foi avaliada e comparada tanto qualitativamente como quantitativamente com base no seu desempenho na segmentação de imagens. os resultados obtidos mostram que os modelos de aprendizagem profunda ultrapassaram os valores inicialmente previstos e alcançaram um valor acima de 94% com base nos dados de treinamento para a métrica de intercepção sobre união. este resultado demonstra o potencial das técnicas de aprendizagem profunda para detetar e segmentar tumores em imagens histológicas e reforça a importância de continuar a investigar este tópico. os melhores resultados deste trabalho foram obtidos com a função de perda total, como se mostra na página 89."
    ],
    [
      "causal consistency is gaining importance in modern geo-replicated distributed services: it is the strongest consistency model that does not sacrifice availability under high latency and network partitions. however, traditional causal delivery middleware, while ensuring a de livery order consistent with causality, does not provide client applications with knowledge about the end-to-end (as seen by each client process) happens-before relation. an end-to-end happens-before is essential to modern applications, namely for the semantics of operation based crdts, but also for traditional applications, in which its absence may cause incorrect behavior when using traditional causal delivery middleware. this thesis designs and im plements a tagged causal multicast middleware service as a rust library. rust was chosen because it is a safe concurrent and fast programming language supporting both functional and imperative paradigms. this allows an efficient implementation where the use of com plex data structures does not decrease the performance as would be the case of using functional languages like erlang. finally, an empirical evaluation of the performance of this middleware service is made, comparing the novel graph-based implementation against a more traditional one based on vector clocks.",
      "a coerência causal está a ganhar importância nos serviços distribuídos geo-replicados modernos: é o modelo de coerência mais forte que não sacrifica a disponibilidade face a partições de rede ou elevada latência. contudo, middleware tradicional de entrega causal, apesar de garantir uma ordem de entrega consistente com a causalidade, não oferece às apli cações cliente o conhecimento sobre a relação happens-before do ponto de vista de cada pro cesso cliente. um end-to-end happens-before é essencial para aplicações modernas, nomeada mente para a semântica de operações baseadas em crdts, mas também para aplicações tradicionais, na qual a sua ausência pode causar um comportamento incorreto quando se utiliza um middleware de entrega causal tradicional. esta tese desenha e implementa um serviço de tagged causal multicast, enquanto middleware, como uma biblioteca de rust. foi escolhida rust por ser uma linguagem de programação segura e com bom suporte a con corrência, multi-paradigma, e apropriada para programação de sistemas. isto permite uma implementação eficiente em que o uso de estruturas de dados complexas não diminui o desempenho, como seria o caso numa linguagem funcional como erlang. finalmente, é re alizada uma avaliação empírica do desempenho deste serviço de middleware, comparando a nova abordagem baseada em grafo com uma implementação mais tradicional baseada em relógios vectoriais."
    ],
    0.3
  ],
  [
    [
      "with the widespread availability of high-throughput technologies, it is now possible to study the behavior of dozens or even hundreds of gene/proteins through a single experiment. still, these experiments provide only the gene/protein expression values, telling nothing about their interactions with each other. to understand these interactions, network inference methods need to be applied. by understanding such interactions, new light can be shed into biological processes and, in particular, into disease’s mechanisms of action, providing new insights for drug design: which genes/proteins should be targeted in order to cure/prevent a specific disease. in this thesis, we developed and tested two alternative extensions for a previously developed model based on linear programming. such model infers signal transduction networks from perturbation steady-state data. the extensions now developed take advantage of perturbation time-series data, which further improves the resolution of causal relationships between genes/proteins. in a first phase, we use artificial networks with simulated data to test the performance of both extensions in different conditions. additionally, we compare their performance to the original model and to a state-of-the-art model for perturbation timeseries data, ddepn. overall, our second extension exhibits a better performance, and significantly higher sensitivity. this extension assumes a given gene/protein can only influence its targets if it is in an active form. in a second phase, we use two experimental datasets related to erbb signaling and evaluate the resulting networks: 1) by finding literature support for the inferred edges, and 2) by using a network assembled with ingenuity ipa as true network to do a quantitative assessment. our results are further compared to ddepn and the original model in a quantitative way. quantitatively, our second model extension is shown to perform better than both the original model and ddepn. qualitatively, we find literature support for most of the inferred edges in both datasets, while also inferring a few plausible edges for which no literature evidence was found.",
      "com o uso generalizado de tecnologias de alto rendimento como os microarrays de adn, torna-se comum estudar dezenas ou mesmo centenas de genes/proteínas numa única experiência. contudo, estas experiências apenas nos permitem determinar a expressão dos genes/proteínas e nada nos dizem sobre as interações entre os mesmos. assim, torna-se necessário o uso de métodos de inferência de redes, de modo a estudar as interações entre genes/proteínas. ao perceber estas interações, não só é possível perceber melhor os processos biológicos em geral, como também o modo como actuam as doenças, de forma a desenvolver novos medicamentos. nesta tese de mestrado, desenvolvemos e testámos duas extensões para um modelo baseado em programação linear. este modelo infere redes de transdução de sinal a partir de experiências de rnai em que as medidas são feitas após a perturbação, quando a rede se encontra em estado estacionário. com as extensões desenvolvidas nesta tese é possível tirar partido de séries temporais de dados provenientes de experiências de rnai, o que permite distinguir relações de causalidade entre proteínas. numa primeira fase, usamos redes artificiais e dados simulados para testar a performance de ambas as extensões em diferentes condições. além disso, comparamo-las com o modelo original e com um modelo recente, ddepn, que usa séries temporais de dados de experiências em que a rede a inferir é perturbada. em geral, a nossa segunda extensão obtém melhores resultados, principalmente em termos de sensibilidade. esta extensão assume que só proteínas activas podem influenciar outras proteínas. numa segunda fase, usamos dois conjuntos de dados experimentais e avaliamos os resultados obtidos: 1) procurando referências na literatura para as ligações inferidas, e 2) usando uma rede de referência para fazer uma avaliação quantitativa e estabelecer comparações com o modelo original e o ddepn. quantitativamente, a nossa segunda extensão obtém melhores resultados do que o modelo original e o ddepn. qualitativamente, encontrámos suporte na literatura para a maioria das ligações inferidas pela segunda extensão. inferimos ainda algumas ligações bastante plausíveis, embora não tenhamos encontrado suporte para estas."
    ],
    [
      "currently, advanced driver assistance systems (adas) have been gradually increasing their presence in everyday life, thanks in part to its ability to recognize several distinct types of objects in the road, namely, traffic signs. these systems employ convolutional neural networks (cnns), a type of classification algorithms that relies on an enormous amount of data in order to be effective. current traffic sign datasets suffer from a scarcity of samples due to the necessity of compiling and labeling them manually. such task is highly resource and time consuming. thus, researches resort to other mechanisms to deal with this problem, such as increasing the architectural complexity of the neural networks or performing data augmentation. this work addresses the data shortage issue by exploring the feasibility of developing a synthetic dataset. such set would not require gathering and labelling manually thousands of real word traffic sign images, requiring only easily collectable information and no human intervention. the only data required is a set of templates for each sign given that a particular sign may have more than one template. this is required to cope with outdated pictograms that are still present in streets and roads. we apply several colour and geometric processing methods to the templates aiming to achieve a look similar to real signs, from the cnn point of view. one of such methods is the usage of perlin noise to both simulate shadows and avoid the clean and homogeneous look that templates have. two use cases for synthetic data usage are presented: considering the synthetic dataset as a standalone training set, and merging synthetic data with real samples when real data is available. the first option provided results that not only clearly surpass any previous attempt on using synthetic data for traffic sign recognition, but are also encouragingly placing the accuracies obtained close to state-of-the-art results, with much simpler networks. the second approach provided results on three distinct test datasets that consistently beat state-of-the-art results, either in accuracy or in simplicity of the network.",
      "atualmente, sistemas avançados de assistência ao condutor têm vindo a aumentar gradualmente a sua presença no quotidiano graças, em parte, à sua capacidade de reconhecer vários objetos distintos na estrada, nomeadamente, sinais de trânsito. estes sistemas empregam redes neuronais convolucionais (cnns), um tipo de algoritmos de classificação que dependem de unia enorme quantidade de dados de forma a serem eficientes. os conjuntos de dados de sinais de trânsito atuais sofrem de escassez de amostras devido à necessidade de as compilar e rotular manualmente. tal tarefa consome imenso tempo e recursos. por conseguinte, investigadores recorrem a outros mecanismos para serem capazes de lidar com esse problema, tais como, aumentar a complexidade arquitetural das redes neuronais ou efetuar data augmentation. desta forma, este trabalho aborda a questão da escassez de dados, explorando a viabilidade do desenvolvimento de um conjunto de dados sintéticos. tal conjunto não exigiria recolher e rotular manualmente milhares de imagens de sinais de trânsito, necessitando apenas de informação facilmente recolhida sem intervenção humana. os únicos dados necessários são um conjunto de modelos para cada sinal uma vez que um sinal particular pode apresentar mais que um modelo. tal é necessário para lidar com pictogramas desatualizados que ainda se encontram nas ruas e estradas. aplicamos vários métodos de processamento de cor e geometria aos templates visando obter uma aparência semelhante a sinais reais, do ponto de vista da cnn. um desses métodos é a utilização do ruído de perlin para simular sombras e evitar a aparência limpa e homogênea que os modelos apresentam. dois casos de uso com dados sintéticos são apresentados: considerar o conjunto de dados sintético como um conjunto de treino independente, e unir dados sintéticos com amostras reais sempre que estas estiverem disponíveis. a primeira opção forneceu resultados que, não apenas superam claramente qualquer tentativa anterior de usar dados sintéticos para reconhecimento de sinais de trânsito, como também colocam as precisões obtidas próximas dos resultados do estado da arte, com redes muito mais simples. a segunda abordagem forneceu resultados em três conjuntos de dados de teste distintos que superam consistentemente os resultados do estado da arte, tanto na precisão quanto na simplicidade da rede."
    ],
    0.0
  ],
  [
    [
      "social networks are currently one of the main sources of fake-news dissemination. on the other hand, fact-checking agencies, which emerged with the aim of solving the problem of fake-news, find it difficult to spread their content on social media. the engagement of fake-news is considerably greater than that of fact-checking. this dissertation contributes to defining a set of heuristics applicable to fact-checking posts in mi-croblog environments, in order to increase their reach and engagement with the reader. the systematic review, inspired by the guidelines defined by kitchenham, made it possible to identify the main strategies used in the dissemination of fake-news and fact-checkings on social media. the results showed that the dissemination and viralization of posts depends on two aspects: the dissemination strategy and the engagement strategy created with the reader. the first is outside the scope of study in this document. regarding the second, the systematic review showed that the inclusion of emotions and personality in social media posts is an efficient strategy to improve reader engagement. furthermore, engagement seems to depend on the type of language and elements that make up the post. since we are working in the context of fake-news in which ethical limits are frequently tested and extrapolated, it is important to reinforce that the defined approach is ethically valid. in this sense, the course of the dissertation continued with the design and development of a post generator algorithm based on emotions and personality capable of increasing the engagement of fact-checking posts. the algorithm was tested in an experiment carried out with twenty participants, where patterns were searched between the posts obtained by the algorithm and those created by the fact-checking journal snopes and the interactions achieved in each of them. this experiment, carried out in a lab environment, proved the hypothesis that emotions and personality in microblog posts increase user engagement with fact-checking posts.",
      "as redes sociais são, atualmente, uma das principais fontes de difusão de fake-news. por outro lado, as agências fact-checking, que surgiram com o objetivo de colmatar o problema das fake-news, têm dificuldade em difundir o seu conteúdo nas redes sociais. o alcance das fake-news é consideravelmente superior ao dos fact-checkings. esta dissertação contribui para definir um conjunto de heurísticas aplicáveis aos posts de fact-checking em ambientes microblog, com o objetivo de aumentar o seu alcance e envolvimento com o leitor. a revisão sistemática, inspirada nas guidelines definidas por kitchenham, permitiu identificar as principais estratégias utilizadas na difusão de notícias falsas ou de verificação de conteúdo nas redes sociais. os resultados mostraram que a difusão e viralização de posts depende de duas vertentes: a estratégia de disseminação e a estratégia de envolvimento criada com o leitor. a primeira não faz parte do âmbito de estudo deste documento. em relação à segunda, a revisão sistemática mostrou que a inclusão de emoções e personalidade em posts de redes sociais é uma estratégia eficiente para melhorar o envolvimento com o leitor. além disso, o envolvimento parece depender do tipo de linguagem e elementos que constituem o post. uma vez que estamos a trabalhar no contexto das fake-news em que limites éticos são testados e extrapolados com frequência é importante reforçar que a abordagem definida seja eticamente válida. neste sentido, o curso da dissertação seguiu com a conceção e desenvolvimento de um algoritmo de geração de posts baseado em emoções e personalidade capaz de aumentar o engajamento de posts de fact-checking. o algoritmo foi testado num experimento realizado com vinte participantes, onde foram pesquisados padrões entre os posts obtidos pelo algoritmo desenvolvido e os criados pelo jornal de fact-checking snopes e as interações alcançadas em cada um deles. este experimento, realizado num ambiente de laboratório, comprovou a hipótese de que emoções e personalidade em posts de microblogs aumentam o envolvimento do utilizador com os posts de fact-checking."
    ],
    [
      "two agents want to securely communicate on a insecure channel in the presence of an adversary. for that they agree in a strong cryptographic key based on a weak-source of randomness stemming from the physical network characteristics where these agents communicate. in this dissertation we evaluate the tradeo s between two protocols: an information theoretically-secure authenticated key agreement (aka) [7] that was speci cally designed for this scenario; and a password- authenticated key exchange (pake) protocol [11] whose security guarantees are based on computational arguments. to this end, we carry out an analysis of the concrete security of both protocols, considering in both cases that the goal is to agree on a fresh 128-bit secret key.",
      "duas entidades desejam comunicar de forma segura através de um canal inseguro na presença de um adversário. para isso acordam uma chave criptográfica forte a partir de uma fonte fraca de aleatoriedade, extra da com base nas características físicas da rede onde comunicam. nesta dissertação avaliamos as contrapartidas entre dois protocolos: um onde as garantias de segurança se baseiam em noções de teoria da informação protocolo authenticated key agreement (aka) proposto em [7] e que foi especialmente criado para este cenário e outro que baseia a sua segurança em argumentos computacionais, o protocolo password-authenticated key exchange (pake) proposto em [11]. para este efeito efetuamos uma análise detalhada da segurança concreta de ambos os protocolos, considerando que em ambos os casos se pretende acordar uma nova chave de 128 bits."
    ],
    0.0
  ],
  [
    [
      "the monitoring of different physical and cognitive functions of the human being has been the subject of numerous studies in recent years. however, most of these monitor-ing systems use invasive and very expensive techniques, which complicate its use in research projects and real scenarios alike. some studies are trying to obtain relevant data from common devices like personal computers or smartphones, but this area has not been properly explored yet. in this project, it was proposed to perform an analysis of the interaction of the users with a computer using the mouse and the keyboard in order to obtain relevant conclu-sions about the effects of stress on the individual. the hypothesis presented here is in-teresting due to the use of non-invasive techniques to retrieve data as well as the use of common and inexpensive hardware instead of specific and expensive one.",
      "a monitorização das diferentes funções físicas e cognitivas do ser humano tem sido al-vo de numerosas ações ao longo dos últimos anos. acontece que grande parte desses sistemas de monitorização utiliza técnicas invasivas e bastante dispendiosas. existem já alguns estudos que têm tentado obter dados relevantes a partir de dispositivos comuns como por exemplo computadores, ou smartphones, mas trata-se de uma área que ainda não se encontra bem explorada. neste projeto de investigação procedeu-se a uma análise da interação dos utilizadores com o computador através do uso do rato e teclado de forma a obter conclusões relevan-tes sobre os efeitos do stress. a hipótese aqui apresentada revela-se interessante em vir-tude de utilizar técnicas de recolha de dados não invasivas, assim como o facto de não requerer a utilização de qualquer hardware adicional, e ter um custo de produção baixo."
    ],
    [
      "the graphical user interface (gui) is crucial for the success of an interactive system. incorrect operation of the gui may inhibit the proper functioning of the system. ensuring the quality of a system is essential to its success. a practical way to ensure it is through software testing. model- based testing (mbt) is a black-box technique that compares the behaviour of the system under test against the behaviour of an oracle (the system's model). these tests are intended to verify that the implementation of a software meets its speci cation. applying mbt tools to guis enables the systematic testing of the system by automatically simulating user actions on the user interface. however, applying the mbt process to guis creates several challenges such as the mapping between the actions in the model and the actions in the application interface. this dissertation will focus on the model-based testing of graphical user interfaces. the main objective is to further the development of the tom framework. the tom framework supports the process of mbt applied to guis, in particular, web-based guis, enabling the creation and execution of user interfaces tests and thus increasing the probability of error detection in the tested interfaces.",
      "a interface gráfica do utilizador é imprescindível para o sucesso de um sistema interativo. o incorreto funcionamento da interface gráfica pode inibir o bom funcionamento da aplicação e, consequentemente, do sistema de software. assegurar a qualidade de um sistema é essencial para o seu sucesso. uma maneira prática de o garantir é através dos testes de software. os testes baseados em modelos são uma técnica de caixa-preta que compara o comportamento do sistema sob teste com o comportamento de um oráculo (o modelo do sistema). estes testes destinam-se a verificar se a implementação do software cumpre as suas especificações. a aplicação de ferramentas que suportem testes baseados em modelos em interfaces gráficas do utilizador permite o teste sistemático do sistema, simulando automaticamente ações realizadas pelo utilizador na interface disponibilizada. no entanto, a aplicação do processo dos testes baseados em modelos em interfaces gráficas cria vários desafios, como por exemplo, o mapeamento entre as ações no modelo e as ações na interface da aplicação. esta dissertação incidirá sobre os testes baseados em modelos e a sua aplicação a interfaces gráficas do utilizador. o principal objectivo é continuar o desenvolvimento da framework tom. a framework tom suporta a aplicação do processo de testes baseados em modelos a interfaces gráficas do utilizador, permitindo a criação e execução de testes em interfaces e aumentando a probabilidade de detecção de erros nas interfaces testadas."
    ],
    0.0
  ],
  [
    [
      "de forma a uniformizar o mercado europeu e conseguir mais confiança nas transações eletrónicas (sic considerando 2º do regulamento eidas (2014)), a união europeia publicou o regulamento ue nº 910/2014 (regulamento eidas (2014)), também conhecido como regulamento eletronic identification, authentication and trust services (eidas). este normativo legal pretende regular as assinaturas e selos electrónicos, a identificação eletrónica e os serviços de confiança dentro do espaço europeu. o objetivo deste regulamento é permitir transações seguras e eficazes entre negócios, pessoas e as autoridades públicas. para atingir o seu objetivo, o regulamento eidas introduziu o conceito de serviços de confiança qualificados. os serviços de confiança qualificados permitem às assinaturas eletrónicas o efeito legal equivalente a uma assinatura manuscrita, quando baseadas num certificado qualificado de assinatura eletrónica emitido por uma entidade que está integrada na lista de confiança de um determinado estado membro. estas assinaturas são intituladas de assinaturas eletrónicas qualificadas e são reconhecidas nos restantes estados membros. tribunais (ou outros órgãos encarregados de procedimentos legais) não podem descartá-las como prova apenas porque são eletrónicas, têm de avaliá-las da mesma forma que fariam com o seu equivalente em papel. (sic artigo 25º do regulamento eidas (2014)) a necessidade de preservação de longo prazo de assinaturas eletrónicas é reconhecida no seio da união europeia (ue). no regulamento eidas, entre os serviços de confiança qualificados introduzidos, encontra-se o serviço de preservação qualificado. um serviço de preservação qualificado tem como objectivo preservar o estado de validade de uma assinatura eletrónica qualificada ao longo do tempo. esta dissertação tem o seu foco no desenvolvimento de uma prova de conceito do serviço de confiança qualificado de preservação de assinaturas e selos eletrónicos qualificados, que se antecipa que comece a ser utilizado massivamente nos próximos anos.",
      "in order to standardise the european market and achieve greater confidence in electronic transactions (sic recital 2º eidas (2014), the european union has developed eu regulation no 910/2014, also known as eidas. this regulation aims to regulate electronic signatures and seals, electronic identification and trust services in europe. the aim of this regulation is to enable secure and efficient transactions between businesses, individuals and public authorities. so as to achieve its objective, the eidas regulation introduced the concept of qualified trust services. qualified trust services allow subscribers and electronic signatures the legal effect equivalent to a handwritten signature when based on an electronic signature qualified certificate issued by an entity which is part of the trust list of a given member state. these signatures are entitled qualified electronic signatures and are recognised in the other member states. courts (or other bodies in charge of legal proceedings) cannot discard them as evidence just because they are electronic, they have to assess them in the same way as they would for their paper equivalent. (sic article 25º of the regulation eidas (2014)) the need for long-term preservation of electronic signatures is recognised within the ue. in the regulation eidas, among the qualified services of trust introduced, is the qualified preservation service. a qualified preservation service aims to preserve the validity status of a qualified electronic signature over time. this dissertation focuses on the development of a proof of concept for the qualified electronic signature and seal preservation trust service, which is expected to start to be used massively in the coming years."
    ],
    [
      "the use of computer vision for identification and recognition of coins is well studied and of renowned interest. however the focus of research has consistently been on modern coins and the used algorithms present quite disappointing results when applied to ancient coins. this discrepancy is explained by the nature of ancient coins that are manually minted, having plenty variances, failures, ripples and centuries of degradation which further deform the characteristic patterns, making their identification a hard task even for humans. another noteworthy factor in almost all similar studies is the controlled environments and uniform illumination of all images of the datasets. though it makes sense to focus on the more problematic variables, this is an impossible premise to find outside the researchers’ laboratory, therefore a problematic that must be approached. this dissertation focuses on medieval and ancient coin recognition in uncontrolled “real world” images, thus trying to pave way to the use of vast repositories of coin images all over the internet that could be used to make our algorithms more robust. the first part of the dissertation proposes a fast and automatic method to segment ancient coins over complex backgrounds using a histogram backprojection approach combined with edge detection methods. results are compared against an automation of grabcut algorithm. the proposed method achieves a good or acceptable rate on 76% of the images, taking an average of 0.29s per image, against 49% in 19.58s for grabcut. although this work is oriented to ancient coin segmentation, the method can also be used in other contexts presenting thin objects with uniform colors. in the second part, several state of the art machine learning algorithms are compared in the search for the most promising approach to classify these challenging coins. the best results are achieved using dense sift descriptors organized into bags of visual words, and using support vector machine or naïve bayes as machine learning strategies.",
      "o uso de visão por computador para identificação e reconhecimento de moedas é bastante estudado e de reconhecido interesse. no entanto o foco da investigação tem sido sistematicamente sobre as moedas modernas e os algoritmos usados apresentam resultados bastante desapontantes quando aplicados a moedas antigas. esta discrepância é justificada pela natureza das moedas antigas que, sendo cunhadas à mão, apresentam bastantes variações, falhas e séculos de degradação que deformam os padrões característicos, tornando a sua identificação dificil mesmo para o ser humano. adicionalmente, a quase totalidade dos estudos usa ambientes controlados e iluminação uniformizada entre todas as imagens dos datasets. embora faça sentido focar-se nas variáveis mais problemáticas, esta é uma premissa impossível de encontrar fora do laboratório do investigador e portanto uma problemática que tem que ser estudada. esta dissertação foca-se no reconhecimento de moedas medievais e clássicas em imagens não controladas, tentando assim abrir caminho ao uso de vastos repositórios de imagens de moedas disponíveis na internet, que poderiam ser usados para tornar os nossos algoritmos mais robustos. na primeira parte é proposto um método rápido e automático para segmentar moedas antigas sobre fundos complexos, numa abordagem que envolve histogram backprojection combinado com deteção de arestas. os resultados são comparados com uma automação do algoritmo grabcut. o método proposto obtém uma classificação de bom ou aceitável em 76% das imagens, demorando uma média de 0.29s por imagem, contra 49% em 19,58s do grabcut. não obstante o foco em segmentação de moedas antigas, este método pode ser usado noutros contextos que incluam objetos planos de cor uniforme. na segunda parte, o estado da arte de machine learning é testado e comparado em busca da abordagem mais promissora para classificar estas moedas. os melhores resultados são alcançados usando descritores dense sift, organizados em bags of visual words e usando support vector machine ou naive bayes como estratégias de machine learning."
    ],
    0.0
  ],
  [
    [
      "as necessidades e expectativas dos clientes acompanham o avanço tecnológico que ocorre de forma exponencial, o que gera um ambiente muito competitivo entre as instituições que precisam de manter os clientes habituais e chamar novos. deste modo, para que as instituições consigam permanecer e evoluir têm de conseguir um elevado grau de qualidade que só é possível caso se mantenham atentos às constantes mudanças sociais e tecnológicas. na área da saúde este facto não é exceção e, adicionalmente, o fator qualidade é ainda mais importante, já que se lida com a vida dos pacientes. a saúde do utente não está dependente apenas do topo das tecnologias relacionadas com os cuidados de saúde diretos, portanto todas as tecnologias que permitem uma melhor organização dos dados do paciente, bem como um aumento na eficiência de todos os processos envolventes devem ser consideradas. diversas instituições de saúde encontraram nos sistemas self-service a solução para tornar o processo de atendimento ao utente mais eficiente. no entanto, nas horas de maior afluência estes sistemas não são suficientes e as filas de espera persistem, causando interferências no bem-estar dos pacientes e no desenrolar de outros processos. tirando proveito das oportunidades que esta quarta revolução industrial revela, problemas deste tipo podem ser minimizados, a experiência do paciente pode tornar-se mais rica e a instituição de saúde pode tomar um lugar de notoriedade. é neste conceito que surge esta dissertação, onde se conceptualiza uma solução para integrar um sistema de quiosques self-service, já existente em algumas unidades de saúde, não só para otimizar o atendimento dos utentes, mas também para modernizar a instituição. é desenvolvido também um possível protótipo desta solução. para o seu desenvolvimento foi imperativo a utilização de conceitos de tecnologias promissoras, destacando-se a tecnologia mobile, realidade aumentada e wifi, de modo a que a solução final agregue todas as funcionalidades dos quiosques com todas as vantagens das tecnologias utilizadas.",
      "the costumer’s needs and expectations go along with the technological advances that occur exponentially, which creates a very competitive environment among the institutions that need to keep customers and call new ones. in this way, for institutions can stay and evolve, they must achieve a high degree of quality that is only possible if is maintained attention to the constant social and technological changes. in the health care field this fact is no exception and, additionally, the quality factor is even more important, from the moment that the life of the patients is concerned. the health of the patients is not only dependent on the top technologies directly related with the health care, so all the technologies that allow better organization of patient data, as well as an increase in the efficiency of all the surrounding processes must be considered. several healthcare institutions have found a solution in self-service systems to make the process of patient service more efficient. however, at peak times these systems are not enough and waiting lines persist, causing interference in patient's well-being and in the unfolding of other processes. by taking advantage of the opportunities that this fourth industrial revolution reveals, problems of this type can be minimized, the patient's experience can become richer, and the health institution can take a place of notoriety. it is in this concept that this dissertation arises, where a solution to integrate a system of self-service kiosks, already existing in some health units, is conceptualized, not only to optimize the service of the users, but also to modernize the institution. a possible prototype of this solution is also developed. for its development it was imperative to use promising technology concepts, especially mobile technology, augmented reality and wifi, so that the final solution brings together all the functionalities of the kiosks with all the advantages of the technologies used."
    ],
    [
      "over the past decade, there has been an increase in the number of surgeries for reconstruction of cranial defects with implants. computer-aided design (cad) software enables the design of patient-specific cranial implants, therefore increasing the reliability of the reconstruction, but there is often a lack of appropriate software. either the software is expensive to acquire which limits its availability or it is not user-friendly for clinicians and therefore very time-consuming to use. this thesis proposes a deep learning (dl) approach towards the automated cad of cranial implants, allowing the design process to be less user-dependent and even less time-consuming. the problem of reconstructing a cranial defect, which is essentially filling in a hole in a skull, was posed as a 3d shape completion task and, to solve it, a volumetric convolutional denoising autoencoder (dae) was implemented using the open-source dl framework pytorch. in order to train the autoencoder, a large amount of 3d skull models was required, and these were obtained by processing an open-access dataset of magnetic resonance imaging (mri) brain scans. the 3d skull models were represented as binary voxel occupancy grids and experiments were carried out for different voxel resolutions (303, 603 and 1203). for each experiment, the autoencoder was evaluated in terms of quantitative and qualitative 3d shape completion performance. the obtained results showed that the implemented volumetric dae is able to perform shape completion on 3d models of defected skulls, allowing for an efficient and automatic reconstruction of cranial defects with a single forward pass of the trained model. even though the current computational resources impose limitations in the resolution of the 3d skull models, the results presented in this thesis make it possible to conclude that dl can be considered a promising approach towards the automated reconstruction of cranial defects.",
      "o número de cirurgias de reconstrução de defeitos cranianos com recurso a implantes tem vindo a aumentar nos últimos anos. software de desenho assistido por computador permite planear implantes cranianos que sejam específicos para cada paciente, tornando o processo de reconstrução mais fiável, mas tem-se vindo a verificar que há falta de software adequado. por um lado, o software existente é caro e portanto não está facilmente disponível. por outro lado, não é específico para uso médico e consequentemente a sua utilização acaba por ser bastante demorada. esta tese tem como objetivo propor uma abordagem deep learning como forma de automatizar o desenho assistido por computador de implantes cranianos, de modo a que este processo seja menos dependente do utilizador e menos demorado. o problema da reconstrução de um defeito craniano consiste essencialmente em preencher uma lacuna no crânio, pelo que para tal, foi implementado um volumetric convolutional denoising autoencoder com recurso à ferramenta open-source pytorch. contudo, para treinar o autoencoder, foi necessária uma grande quantidade de modelos 3d de crânios. estes foram obtidos através do processamento de um dataset de imagens de ressonância magnética do crânio. para os modelos 3d, foi adotada uma representação binária de ocupação dos vóxeis e foram experimentadas diferentes resoluções (303, 603 e 1203). para cada experiencia, a performance do autoencoder foi quantitativa e qualitativamente avaliada. os resultados obtidos demonstraram que o volumetric denoising autoencoder implementado é capaz de preencher as lacunas em modelos 3d de crânios defeituosos, permitindo assim uma reconstrução automática e eficiente de defeitos cranianos através de um único forward pass pelo modelo treinado. apesar de a resolução dos modelos 3d de crânios estar limitada pelos recursos computacionais disponíveis, os resultados obtidos nesta tese permitem concluir que uma abordagem deep learning pode ser considerada promissora no sentido de automatizar a reconstrução de defeitos cranianos."
    ],
    0.3
  ],
  [
    [
      "alloy is a declarative specification language which describes rules and complex structural behaviors. alloy analyzer is used to analyze this specifications, this tool generates concrete instances from the invariants specified in a model, it simulates sequences of defined operations and verifies whether properties introduced are valid or not. currently, the tool is available as a runnable .jar and it contains a trivial gui to interact with it. being such, it requires java installed. it’s in the best interest of the community to achieve and easier access to this tool through a web platform that shall support it in real time and also allow sharing models developed in it by users. formal methods of software development are growing and they would also benefit from the constructive feedback obtained through this platform regarding the alloy language/tool.",
      "alloy é uma linguagem de especificação declarativa que descreve regras e comportamentos de estruturas complexas. para analisar estas especificações, utiliza-se o alloy analyzer, uma ferramenta que gera instâncias concretas a partir dos invariantes especificados num modelo, simula sequências de operações definidas e verifica se as propriedades introduzidas são satisfeitas. atualmente, a ferramenta está disponível sob a forma de um .jar executável e contém uma gui trivial para a sua utilização, requerendo assim o respetivo download e instalação do software adicional java até estar operacional. é do interesse da comunidade conseguir um acesso facilitado a esta ferramenta através de uma plataforma web que a suporta em tempo real e permite a partilha simplificada de trabalhos elaborados. também seria uma mais valia para os métodos formais na produção de software, que se encontram em constante crescimento, a extração de informação estatística acerca da utilização desta plataforma. daqui poderia obter-se um feedback positivo face aos prós e contras da linguagem/ferramenta."
    ],
    [
      "recycling stands as one of the most effective contemporary practices for pollution prevention. through the process of recycling, a reduction in our reliance on finite natural resources is achieved, concurrently leading to energy conservation, decreased carbon dioxide emissions, and economic savings. in the context of the european union, it is noteworthy that portugal currently registers one of the lowest recycling rates. consequently, it becomes imperative for the nation to commit towards accomplish the european objective of recycling all single use packaging materials. a significant strategy to boost these recycling rates involves the widespread deployment of small, medium or large capacity waste containers, typically ranging from 120 liters to 360 liters, across municipalities. however, the efficient management of these containers necessitates a consistent and meticulous approach by waste collection entities. presently, the methodology employed in this regard is antiquated, characterized by waste collection teams manually inspecting each container within their designated areas to check their fill status. this labor-intensive process poses inherent inefficiencies and challenges. the primary objective of this master’s project involves the development of a system capable of detecting and classifying urban waste containers. this goal holds promising applications in the domain of waste management, potentially facilitating the generation of daily collection routes in the future. images for this study were sourced from individual contributors, from street view feature in google maps and a project known as tidy city, which gathers various items, including containers, from a designated municipality. subsequently, a model was constructed with the ability to discern and categorize a specific container based on the type of waste it accommodates, the configuration of the container (e.g., 4 wheels, 2 wheels), and the condition of its lid (open, closed, or full). additionally, the model demonstrates proficiency in identifying and classifying waste materials in close proximity to the container.",
      "a reciclagem é uma das práticas contemporâneas mais eficazes para a prevenção da poluição. através do processo de reciclagem, consegue-se uma redução da nossa dependência de re cursos naturais finitos, conduzindo simultaneamente à conservação de energia, à diminuição das emissões de dióxido de carbono e a poupanças económicas. no contexto da união europeia, é de salientar que portugal regista atualmente uma das mais baixas taxas de reciclagem. consequentemente, torna-se imperativo que o país se empenhe em cumprir o objetivo europeu de reciclar todos os materiais de embalagem de utilização única. uma estratégia significativa para aumentar estas taxas de reciclagem envolve a implantação gener alizada de contentores de resíduos de pequena, média ou grande capacidade, normalmente entre 120 litros e 360 litros, em todos os municípios. no entanto, a gestão eficiente destes con tentores exige uma abordagem consistente e meticulosa por parte das entidades de recolha de resíduos. atualmente, a metodologia utilizada nesta matéria é antiquada, caracterizada por equipas de recolha de resíduos que inspecionam manualmente cada contentor dentro das suas áreas designadas para verificar o seu estado de enchimento. este processo de trabalho intensivo apresenta ineficiências e desafios inerentes. o objetivo principal deste projeto de mestrado é o desenvolvimento de um sistema capaz de detetar e classificar contentores de lixo urbano. este objetivo tem aplicações promissoras no domínio da gestão de resíduos, facilitando potencialmente a criação de rotas de recolha diárias no futuro. as imagens para este estudo foram obtidas de colaboradores individuais, através da funcionalidade de street view do google maps e de um projeto conhecido como tidy city, que recolhe vários objetos, incluindo contentores de resíduos sólido urbanos, de um município designado. posteriormente, foi construído um modelo com a capacidade de distinguir e categorizar um contentor específico com base no tipo de resíduos que acomoda, na configuração do contentor (por exemplo, 4 rodas, 2 rodas), e no estado da sua tampa (aberta, fechada ou cheia). para além disso, o modelo demonstra proficiência na identificação e classificação de materiais residuais nas proximidades do contentor."
    ],
    0.06666666666666667
  ],
  [
    [
      "o recurso a sistemas de videovigilância tem-se tornado cada vez mais popular. no entanto, cada fabricante deste tipo de equipamentos desenvolvia os seus próprios protocolos de comunicação, não existindo compatibilidade entre diversos sistemas de videovigilância. este cenário era economicamente prejudicial para os consumidores, e dificultava o desenvolvimento de sistemas que integrem equipamentos diferentes ou de diferentes fabricantes. foi então necessário criar um protocolo comum a todos eles. nesse contexto surgiu o open network video interface forum (onvif), uma organiza- ção sem fins lucrativos composta pelas principais companhias deste ramo que tem como objetivo desenvolver normas para estes dispositivos. a norma onvif baseia-se em serviços web simple object access control (soap) e também em protocolos que já estão padronizados como o hypertext transfer protocol (http) ou real-time transfer protocol (rtp). os dispositivos onvif são divididos em network video transmitter (nvt), network video display (nvd), network video storage (nvs) e network video analytics (nva). o hypertext markup language (html) era, inicialmente, utilizado para definir a estrutura de documentos. no entanto, devido à sua baixa complexidade de utilização tornou-se rapidamente a linguagem de marcação mais utilizada para a construção de páginas web. hoje em dia, a mesma está na quinta versão a qual permite maior flexibilidade na utiliza- ção de conteúdo multimédia. estas páginas juntamente com plugins ou com a linguagem de programação javascript são capazes de constituir as rich internet application (ria), aplicações que são executadas em ambiente web. devido à falta de segurança e instabilidade causados pelos plugins, hoje em dia começa a ser utilizado apenas o javascript. desta forma, foi desenvolvida uma aplicação web que consiste num cliente que faz a comunicação com um web service (ws) representational state transfer (rest). este por sua vez, encontra-se alojado num servidor hypertext transfer protocol (http) apache e está implementado como um fast common gateway interface (fastcgi). este fastcgi utiliza a biblioteca umoc para transferir dados com dispositivos nvt (câmaras internet protocol (ip)). o objetivo deste projeto é aumentar o desempenho desta aplicação existente, tanto no servidor como no cliente e ainda implementar novas funcionalidades do onvif. foram desenvolvidas soluções para o lado do cliente que permitem que a aplicação seja executada com maior velocidade e com menor consumo de recursos e foram também implementadas novas funcionalidades. de entre as contribuições técnicas destacam-se a utiliza- ção da web storage em vez da indexed db, a transformação da application programming interface (api) de comunicação com as câmaras mais percetível e mais eficaz e a apresentação dos dados de forma dinâmica. em termos de funcionalidades, foi adicionado o suporte à receção dos eventos da câmara utilizando server sent events (sse). no que toca ao lado do servidor, foi realizado o estudo experimental dos servidores http mais conhecidos pela sua eficiência e implementação do ws-notification através da ferramenta gsoap.",
      "the usage of video surveillance systems has become increasingly widespread. each manufacturer has developed its own communication protocol, not existing compatibility between multiple video surveillance systems. this scenario was economically harmful for the consumers and it makes more difficult to integrate several devices from different manufacturers. so the need to create a common protocol became fundamental. onvif was born in that context. it is a non-profit organization consisting of the main manufacturers of the field, and its main goal is to build standards for the abovementioned devices. onvif standard is based on soap web services and in already existing protocols like http or rtp. onvif devices are split into nvt, nvd, nvs and nva. in the beginning html was used to define documents structure. however, due to the low complexity, it quickly became the most used markup language for building web pages. nowadays, it is in the fifth version which allows more flexibility to manipulate multimedia content. web pages developed with plugins or with javascript language are used to build rias, applications which run in web environment. on account of security issues and instability induced by plugins, currently only javascript starts being used. in this way, a web application was developed that consist of a client which communicates with a rest ws. this in turn is hosted in a apache http server as a fastcgi, and it takes advantage of umoc library to transfer data with nvt devices (cameras ip). at present exists a wide range of http servers which are different in the way how they work and can be distinguished in two groups: those which are used to serve static responses and the others which serve dynamic content. the very first can serve dynamic responses if they integrate some type of technologies like fastcgi or scgi. the goal of this project is to increase the performance of this exist application, both in server and client and implement new onvif features. solutions were developed for the client side which enable the application to run faster with less resources consumption and new functionalities were implemented. technical contributions that stand out are the use of web storage instead of indexeddb, a renewed camera communication api which is more efficient and more perceptible and, also, a dynamic way of presenting data. functionalities that were added include the communication of events using sse. at the server side, an experimental performance study was made of http servers known for their efficiency and an implementation of ws-notification was added using gsoap tool."
    ],
    [
      "reinforcement learning (rl) consists of designing agents that make intelligent decisions without human supervision. when used alongside function approximators such as neural networks (nns), rl is capable of solving extremely complex problems. deep q-learning, a rl algorithm that uses deep nns, even achieved super-human performance in some specific tasks. nonetheless, it is also possible to use variational quantum circuits (vqcs) as function approximators in rl algorithms. this work empirically studies the performance and trainability of such vqc-based deep q-learning models in openai’s gym cartpole-v0 and acrobot-v1 environments. more specifically, we research how data re-uploading affects both these metrics. we show that the magnitude and the variance of the gradients of these models remain substantial throughout training due to the moving targets of deep q-learning. moreover, we show that increasing the number of qubits does not lead to a decrease in the magnitude and variance of the gradients, unlike what was expected due to the barren plateau phenomenon. this hints at the possibility of vqcs being specially adequate for being used as function approximators in such a context. we also use the universal quantum classifier as a function approximator in vqc-based deep q-learning and implement vqc-based models capable of achieving considerable performance in the acrobot-v1 environment, a previously untapped environment for vqcs.",
      "reinforcement learning (rl) consiste em projetar agentes que tomam decisões inteligentes sem super visão humana. quando usado em conjunto com aproximadores de funções, como redes neuronais (rns), rl é capaz de resolver problemas extremamente complexos. deep q-learning é um algoritmo de rl que usa rns profundas e que alcançou um desempenho super-humano em algumas tarefas específicas. no entanto, também é possível utilizar circuitos variacionais quânticos (vqcs) como aproximadores de funções em algoritmos de rl. este trabalho estuda empiricamente o desempenho e a treinabilidade de tais modelos de deep q-learning baseados em vqc nos ambientes cartpole-v0 e acrobot-v1 do ope nai gym. mais especificamente, investigamos como o data re-uploading afeta ambas estas métricas. demonstramos que a magnitude e a variância dos gradientes destes modelos permanecem substanciais ao longo do treino devido aos alvos móveis do deep q-learning. além disso, mostramos que aumentar o número de qubits não leva a uma diminuição na magnitude e variância dos gradientes, contrariamente ao que era esperado devido ao barren plateau phenomenon. isto sugere a possibilidade dos vqcs serem especialmente adequados para serem usados como aproximadores de funções neste contexto. também utilizamos o universal quantum classifier como um aproximador de funções em deep q-learning e implementamos modelos baseados em vqc capazes de alcançar um desempenho considerável no ambiente acrobot-v1, um ambiente anteriormente inexplorado para vqcs."
    ],
    0.3
  ],
  [
    [
      "este projeto tem o objetivo principal de melhorar a qualidade de vida de pessoas idosas, com deficiências físicas ou psicológicas. quando ficam em casa sozinhas são privadas de uma vida autónoma e ativa o que leva à necessidade de uma monitorização continua. tendo em consideração a crescente disponibilidade de dispositivos interativos num ambiente doméstico abre-se uma porta para necessidade de sistemas de integração e fusão de sensores. uma plataforma inteligente de monitorização capaz de comunicar com os vários dispositivos prova a sua utilidade ao ser capaz de tornar as pessoas mais autónomas proporcionando aos familiares e amigos mecanismos de monitorização ajustados conforme o perfil do utilizador em caso de ausência. pretende-se desenvolver um sistema que possa ser implementado num dispositivo central que estabeleça a comunicação entre os diferentes dispositivos e sistemas electrónicos, seja capaz de integrar serviços em tempo real e registe o seu estado em determinado momento. tem o intuito de aproveitar todos os aparelhos e serviços que as pessoas já possuem, evitando assim um gasto monetário exagerado tendo em conta o estado económico existente e a capacidade monetária dos utilizadores. estes sistemas electrónicos ou dispositivos podem ser, por exemplo, um sistema de ar condicionado, capaz de adequar a temperatura à preferível pelo utilizador, ou um sistema de iluminação com capacidade de regular a intensidade da luz e assim reduzir nas despesas. estes sistemas implicam uma melhoria em termos de qualidade de vida do utilizador ao providenciar automatismos inteligentes no seu dia-a-dia.",
      "this project has the main goal of improving the quality of live of elderly people with physical or psychological disabilities. when they stay at home alone they are deprived of an independent and active life and which leads to the need for continuous monitoring. taking into account the increasing availability of interactive devices in a home environment, opens a door for the need of integration systems and sensor fusion. an intelligent monitoring platform capable of communicating with multiple devices proves its utility in being able to make people more autonomous, providing to the family and friends monitoring mechanisms adjusted as the user's profile in case of absence. the aim is to develop a system that can be implemented in a central device to establish communication between different electronic devices and systems, capable of integrating services in realtime and record their status. aims to take advantage of all the devices and services that people already have, thus avoiding spending money exaggeratedly, given the existing economic and monetary capacity of users. these electronic systems or devices can be, for example, an air conditioning system capable of adjusting the temperature preferred by the user and lighting system capable of regulating the light intensity and thus reduce the costs. these systems imply an improvement in quality of life of the user by providing intelligent automation in their day-to-day."
    ],
    [
      "esta dissertação apresenta um estudo compreensivo e extensivo de todo o processo de planeamento, desenvolvimento e implementação de um protótipo de uma consola de operador com o objetivo de auxiliar os seus utilizadores a gerir um elevado número de chamadas, de forma a ir de encontro às necessidades de várias organizações, face aos dias de hoje. devido ao ritmo acelerado associado à evolução do software, é do interesse destas organizações melhorarem a eficiência dos locais de trabalho, algo que o protótipo desenvolvido pretende alcançar, agilizando e otimizando as várias operações de rececionistas e operadores. este documento inicia com um estudo aprofundado de várias soluções de consolas de operador presentes no mercado, com o intuito de analisar o que estas oferecem, incluindo a solução da altice labs, com o objetivo final de estabelecer uma comparação entre estas, e analisar possíveis aspetos a melhorar desta última, assim como a sua posição e papel no mercado atual. a fase de planeamento inicia com um processo de tomada de decisão relativo às diferentes abordagens possíveis face ao trabalho a desenvolver. após uma longa e rigorosa ponderação, que inclui uma fase de testes englobando outras soluções oferecidas por outras organizações, a abordagem relativa ao desenvolvimento de um protótipo de raiz, focado num cliente web, foi selecionada, uma vez que foi a única, na altura, viável. por este motivo, a fase de planeamento foi bastante extensa, envolvendo uma seleção de funcionalidades e requisitos com uma abordagem centrada ao utilizador, assegurando uma compreensão das suas necessidades e preferências, tendo sempre em mente as operações suportadas pelos serviços a integrar, que se revelaram como um quanto restritivos, passando também pelo desenho de uma interface modular e intuitiva, com foco nas várias funcionalidades planeadas. a fase de desenvolvimento focou-se principalmente na implementação daquilo planeado na fase anterior, esclarecendo os motivos que levaram à escolha das tecnologias escolhidas e tendo sido tomado o cuidado de abordar a arquitetura e o funcionamento da aplicação, explicando os motivos que levaram às decisões tomadas. para terminar esta fase, foi também exposto o resultado final e as várias alterações sofridas face ao planeamento, na tentativa de melhorar o produto final. para terminar o documento, a eficácia da plataforma desenvolvida é avaliada através de um teste de usabilidade, com o objetivo de analisar o comportamento dos utilizadores na utilização da aplicação e coletar a opinião dos participantes, contribuindo para o levantamento de aspetos a melhorar e para tirar conclusões relativas ao produto final. os resultados obtidos neste teste de usabilidade foram positivos, indicando que a plataforma desenvolvida desempenhou um bom papel ao auxiliar os utilizadores a lidarem com um elevado fluxo de chamadas, aumentando a sua produtividade e eficiência, mas também servindo como fundação para trabalhos futuros na área.",
      "this thesis presents a comprehensive and extensive study regarding all the planning, developing and implementation process of an attendant console prototype, with the goal of aiding its users in handling a higher volume of calls, in order to meet daily workplace demands. due to the high pace of software evolution, its in the organization’s best interest to improve workplace efficiency, which is what the proposed prototype aims to achieve, enhancing and optimizing daily receptionist’s and operator’s tasks. this dissertation begins with an in-depth study of existing attendant console solutions available in the market, aiming to analyze what they offer, including altice labs’ own solution, culminating in a comparison among all the studied platforms, analyzing possible improvements and its role in the current market. the planning phase starts with a decision-making process regarding the multiple available approaches to the problem at hand. after careful consideration, including a testing phase, where other solutions were tested, it was decided that the only viable approach referred to the development of a prototype from scratch, with focus on the web client. for this reason, there was a long planning phase regarding the selection of all the requirements, th rough a user-centric approach, ensuring a broad understanding of their needs and preferences, with the integrated services supported tasks always in mind, which later revealed to be quite restrictive, including the design of an intuitive and modular user interface. the development phase was mainly implemented according to the previous planning phase, expanding on the rationale used when considering the techno logy utilized and explaining thoroughly all the platform’s architecture and behaviour, as well as the thought process behind the decisions. to finish this chapter, the final product was also displayed, discussing the changes applied from the planning state. the effectiveness of the developed software was also evaluated through a usability test, aiming to gather user feedback and to observe user behaviour while conducting the test, contributing for collecting various points to further improve the platform, as well as to draw conclusions regarding the final product. the obtained results from the usability test were mainly positive, indicating the developed platform was able to successfully fulfill its role in helping its users in handling a higher influx of phone calls, increasing their productivity and efficiency, but also in becoming a foundation for future work and improvement in the area."
    ],
    0.0
  ],
  [
    [
      "this document, in context of second year of integrated master of informatics engineering, reports the development of a project that intends to teach computational thinking to kids with special educational needs, in this case blindness. the aim of this research is to characterize both subjects, computational thinking and blindness, and identify what are the current most used and best practises to teach this different way of thinking to kids with special needs. to achieve this, melodic was created. this is a system composed by a software and a hardware where the user must create sequences with the tactile blocks (the hardware) and then read them with the mobile application (the software), that converts the sequence created into sound. with this, the user can easily hear the differences that the changes in the blocks sequence can make. this can be compared to the computational thinking teaching through the use of robots, because in that case, users can see the result of their instructions in the robot movement and with melodic, the user can hear the result of their instruction with the musical note sequence played by the app. in this document more technical aspects such as the architecture of the application that is proposed to accomplish the goal of the present master’s project, will also be discussed. after this, the project development process that lead to the creation of melodic is described as well as all the decisions taken. a description of all functionalities of this system can also be seen in this document. to prove the research hypothesis initially stated, some exercises were created and described. the referred exercises were designed to access if melodic actually develops computational thinking.",
      "este documento, no contexto do segundo ano do mestrado integrado em engenharia informática, relata o desenvolvimento de um projecto que pretende ensinar pensamento computacional a crianças com necessidades educativas especiais, neste caso a cegueira. o objetivo desta investigação é caraterizar tanto o pensamento computacional como a cegueira, e identificar quais são atualmente as práticas mais usadas para ensinar esta forma diferente de pensar a crianças com necessidades especiais. para o conseguir, foi criado o melodic. este é um sistema composto por software e hardware onde o utilizador deve criar sequências com os blocos tácteis (o hardware) e depois lê-los com a aplicação móvel (o software), que converte a sequência criada em som. com isto, o utilizador pode facilmente ouvir as diferenças que as alterações na sequência dos blocos podem fazer. isto pode ser comparado com o ensino pensamento computacional através do uso de robôs, sendo que nesse caso os utilizadores podem ver o resultado das suas instruções no movimento do robô e com melodic, os utilizadores podem ouvir o resultado das suas instruções com a sequência de notas musicais tocadas pela aplicação. este é um sistema composto por um software e um hardware onde o utilizador deve criar sequências com os blocos tácteis (o hardware) e depois lê-los com a aplicação móvel (o software), que converte a sequência criada em som. neste documento serão também discutidos aspetos mais técnicos, tais como a arquitetura da aplicação que é proposta para atingir o objectivo deste projecto de mestrado. depois disto, descreve-se o processo de desenvolvimento do projecto que levou à criação da melodic, bem como todas as decisões tomadas. uma descrição de todas as funcionalidades deste sistema também pode ser vista neste documento. para comprovar a hipótese de investigação inicialmente referida, foram criados e descritos alguns exercícios. os referidos exercícios foram concebidos para verificar se o melodic realmente se treina o pensamento computacional."
    ],
    [
      "with the advance of technology surrounding the automobile industry, we are starting to see a shift in the need a personal vehicle, opting more often for other options like rental cars and car-sharing services. with this shift these services face more problems and more specific damage to the vehicles in the fleet. in order to help these services keep track of their fleet state and to help detect impacts if they happen, a multi-sensor fusion for impact detection in vehicles is proposed. the main focus of this thesis is to implement a multi-sensor fusion approach to detect impacts in vehicles. a comparative study of the previously implemented solution is carried out to help develop and implement the suggested approach. one of the sub-objectives of this work is to find which of the two implemented fusion methods better improves the system performance. the sensors that compose the detection strutcure, are a inertial measurement unit (imu) and a mi crophone, which are located inside the vehicle in different positions. note that the structure used differs for each dataset. the fusion works by combining the information of all the accelerometers placed in the vehicle. two sensor fusion methods applied to the two datasets in this thesis are as follows: a complementary filter which is part of the data fusion level and the second consists in a feature fusion approach by fusion features from vairous sensor combinations.",
      "com o avanço da tecnologia no ramo da industria automóvel, estamos a começar a ver uma mudança na necessidade de compra e posse de um veiculo pessoal, optando por outras opções como serviços de aluguer de carros e car-sharing. com esta mudança, é esperado que estes serviços encontrem mais problemas, mais em especifico, danos nos veículos presentes na frota. de maneira a facilitar o acompanho do estado das suas frotas e a ajudar na deteção de impactos nos veículos quando acontece, é proposto um sistema de fusão sensurial com o objetivo de detetar impactos em veículos. o maior foco desta tese é implementar um sistema de fusão sensorial para detetar impactos em veículos. um estudo comparativo da solução anteriormente implementada vai ser realizado para ajudar a desenvolver e implementar a nova proposta. um dos sub objetivos deste trabalho é encontrar qual dos dois metodos de fusão consegue melhorar o desempenho geral do sistema. os sensores que constituem o sistema de deteção são uma imu e um microfone, que em sua vez são instalados no veiculo em diferentes posições. neste caso a fusão ocorre ao combinar a informação proveniente de todos os acelerometros dentro do veiculo. dois métodos de fusão sensorial vão ser aplicados para fundir a informação medida de cada sensor. isto em troca nos vai permitir uma melhor e mais robusta compreensão do ambiente em que se insere. uma pesquisa detalhada sobre fusão sensorial foi realizada para completar este objetivo."
    ],
    0.3
  ],
  [
    [
      "nos dias de hoje os dispositivos iot fazem parte das nossas vidas, nomeadamente na integração das nossas casas, automação industrial ou monitorização de vários tipos de ambiente. thread é um protocolo de redes mesh wireless que tem uma enorme eficiência energética, segurança, alcance, interoperabilidade, descentralização e com capacidade de regeneração, sendo por isso interessante para integrar em sistemas iot. dado isto, a validação e verificação dos requisitos do protocolo é extremamente importante e, por isso, nesta dissertação são propostas várias formalizações deste protocolo em alloy. posteriormente foram realizadas várias verificações das suas propriedades mais relevantes, uma vez que, são estas que garantem o correto funcionamento do algoritmo. todas foram verificadas sem nenhum contra-exemplo encontrado. para além disso, esta dissertação também explora a relação entre o openthread network simulator, desenvolvido pela google, e o modelo produzido em alloy, com o objetivo de validar este último. foram realizadas várias simulações e convertidas em instâncias executáveis nesse modelo. isto permitiu detetar algumas incongruências entre ambos e corrigir o modelo por forma a ser capaz de executar todas as simulações realizadas.",
      "nowadays, iot devices are part of our lives, namely in the integration on our homes, industrial automation or monitoring of various types of environment. thread is a wireless mesh protocol that has tremendous energy efficiency, security, range, interoperability, and no single point of failure including self-healing, making it interesting to integrate into iot systems. given this, validation and verification of the protocol requirements is extremely important and therefore, several formalizations of this protocol in alloy are proposed in this dissertation. subsequently, several verifications were performed of the most relevant properties, since these are what guarantees the correct behaviour of the algorithm. all were verified with no counterexamples found. in addition, this dissertation also explores the connection between the openthread network simulator, developed by google, and the model produced in alloy, with the goal of validating the latter. several simulations were performed and converted into executable instances in that model. this enabled us to detect some inconsistencies between both, and fix the model to be capable of interpreting all the performed simulations."
    ],
    [
      "the field of artificial intelligence has lately witnessed extraordinary results. the ability to design a system capable of beating the world champion of go, an ancient chinese game known as the holy grail of ai, caused a spark worldwide, making people believe that some thing revolutionary is about to happen. a different flavor of learning called reinforcement learning is at the core of this revolution. in parallel, we are witnessing the emergence of a new field, that of quantum machine learning which has already shown promising results in supervised/unsupervised learning. in this dissertation, we reach for the interplay between quantum computing and reinforcement learning. this learning by interaction was made possible in the quantum setting using the con cept of oraculization of task environments suggested by dunjko in 2015. in this dissertation, we extended the oracular instances previously suggested to work in more general stochastic environments. on top of this quantum agent-environment paradigm we developed a novel quantum algorithm for near-optimal decision-making based on the reinforcement learn ing paradigm known as sparse sampling, obtaining a quantum speedup compared to the classical counterpart. the achievement was a quantum algorithm that exhibits a complexity independent on the number of states of the environment. this independence guarantees its suitability for dealing with large state spaces where planning may be inapplicable. the most important open questions remain whether it is possible to improve the orac ular instances of task environments to deal with even more general environments, especially the ability to represent negative rewards as a natural mechanism for negative feedback instead of some normalization of the reward and the extension of the algorithm to perform an informed tree-based search instead of the uninformed search proposed. improvements on this result would allow the comparison between the algorithm and more recent classical reinforcement learning algorithms.",
      "o campo da inteligência artificial tem tido resultados extraordinários ultimamente, a capacidade de projetar um sistema capaz de vencer o campeão mundial de go, um antigo jogo de origem chinesa, conhecido como o santo graal da ia, causou uma faísca em todo o mundo, fazendo as pessoas acreditarem em que algo revolucionário estar a para acontecer. um tipo diferente de aprendizagem, chamada aprendizagem por reforço está no cerne dessa revolução. em paralelo surge também um novo campo, o da aprendizagem máquina quântica, que já vem apresentando resultados promissores na aprendizagem supervisionada/não, supervisionada. nesta dissertação, procuramos invés a interação entre computação quântica e a aprendizagem por reforço. esta interação entre agente e ambiente foi possível no cenário quântico usando o conceito de oraculização de ambientes sugerido por dunjko em 2015. neste trabalho, estendemos as instâncias oraculares sugeridas anteriormente para trabalhar em ambientes estocásticos generalizados. tendo em conta este paradigma quântico agente-ambiente, desenvolvemos um novo algoritmo quântico para tomada de decisão aproximadamente ótima com base no paradigma da aprendizagem por reforço conhecido como amostragem esparsa, obtendo uma aceleração quântica em comparação com o caso clássico que possibilitou a obtenção de um algoritmo quântico que exibe uma complexidade independente do número de estados do ambiente. esta independência garante a sua adaptação para ambientes com um grande espaço de estados em que o planeamento pode ser intratável. as questões mais pertinentes que se colocam é se é possível melhorar as instâncias oraculares de ambientes para lidar com ambientes ainda mais gerais, especialmente a capacidade de exprimir recompensas negativas como um mecanismo natural para feedback negativo em vez de alguma normalização da recompensa. além disso, a extensão do algoritmo para realizar uma procura em árvore informada ao invés da procura não informada proposta. melhorias neste resultado permitiriam a comparação entre o algoritmo quântico e os algoritmos clássicos mais recentes da aprendizagem por reforço."
    ],
    0.3
  ],
  [
    [
      "water scarcity and pollution are two main ecological focus nowadays. knowledge of wastewater composition, regarding microorganisms and pollutants, is of great importance to improve the capacities of the effluent treatment plants (etp). advances in next-generation sequencing (ngs) methodologies allowed for faster, cheaper and more accurate study of microbial communities. besides being an extremely powerful analysis resource, whole shotgun metagenomic analysis comprises many challenging aspects, regarding the processing and analysis. in the present work a shotgun metagenomic bioinformatics analysis was performed comprising three samples from common etps (cetp) and four samples from a petrochemical complex etps (wastewaters with low and high salts collected in two distinct timepoints). the samples were sequenced with illumina® hiseq, generating paired-end reads with 2x150bp length. the main goals of this project were to evaluate currently available tools, establish a customized bioinformatics pipeline and to extract relevant biological information from the sequenced datasets. there were generated simulated datasets representative of the target data, in order to evaluate the performance of the available bioinformatics tools. datasets were generated with three coverage levels and were used to test pre-processing, assembly and taxonomic tools. the target datasets, both with and without coverage split, were then subjected to processing and analysis using the pre-defined pipeline. a preliminary functional study was also performed using mg-rast and mgx. results from the evaluation of the performance of the bioinformatics tools showed that different tools behave differently in distinct datasets. the pipeline was defined using bayeshammer and fastq-mcf as pre-processing tools, spades for assembly and metaphlan v2.0 for the taxonomical analysis. the assembly results for the target datasets showed a higher contiguity for high coverage levels and a lower contiguity for low coverage levels, highlighting the differences in microorganisms’ abundance and diversity and its impact during analysis. taxonomical composition suggests the presence of putative pathogenic and opportunistic microorganisms on two of the cetp datasets (a2 and akr12). it also suggests a more hostile environment in petrochemical complex etps datasets, which is concordant with a higher abundance of defence mechanisms on this datasets. the present results must be accounted to the effluent treatment processes.",
      "a escassez de água e a poluição são dois dos principais problemas ecológicos atualmente. o conhecimento da composição das águas residuais, referente a microrganismos e poluentes, é de grande importância para melhorar as capacidades das estações de tratamento de águas residuais (etar). os avanços nos métodos de sequenciação de nova geração permitiram o estudo mais rápido, barato e preciso de comunidades microbianas. apesar de ser um meio de análise altamente poderoso, a análise metagenómica por whole shotgun compreende muitos aspetos desafiadores, no que respeita o processamento e a análise. no presente trabalho, uma análise bioinformática de dados metagenómicos de shotgun foi efetuada incluindo três amostras de etars comuns e quatro amostras de etars de um complexo petroquímico (águas residuais com baixos e altos teores de sais, colhidas em dois momentos distintos). as amostras foram sequenciadas com illumina® hiseq, gerando paired-end reads com comprimento igual a 2x150pb. os principais objetivos deste projeto foram avaliar ferramentas disponíveis atualmente, estabelecer uma pipeline bioinformática personalizada e extrair informação biológica relevante dos datasets sequenciados. foram gerados datasets simulados representativos dos dados a analisar, de forma a avaliar a performance das ferramentas bioinformáticas disponíveis. os datasets foram gerados com três níveis de coverage e foram usados para testar ferramentas de pré-processamento, assembly e taxonomia. os datasets alvo, com e sem divisão por coverage, foram então sujeitos a processamento e análise usando a pipeline pré-definida. um estudo funcional preliminar foi realizado com mg-rast e mgx. os resultados da avaliação da performance das ferramentas bioinformáticas mostraram que diferentes ferramentas comportam-se de forma diferente em datasets distintos. a pipeline foi definida usando bayeshammer e fastq-mcf como ferramentas de pré-processamento, spades para assembly e metaphlan v2.0 para a análise taxonómica. os resultados de assembly para os datasets alvo mostraram uma grande contiguidade para altos níveis de coverage e baixa contiguidade para baixos níveis de coverage, realçando as diferenças de abundância e diversidade dos microrganismos e o seu impacto durante a análise. a composição taxonómica sugere a presença de microrganismos potencialmente patogénicos e oportunistas nos dois datasets de etars comuns (a2 e akr12). sugere também um ambiente mais hostil nos datasets das etars do complexo petroquímico, o que é concordante com uma maior abundância de mecanismos de defesa nestes datasets. os presentes resultados devem ser tidos em conta nos processos de tratamento de águas residuais. palavras-chave: metagenómica de whole shotgun, sequenciação de nova geração, estação de tratamento de águas residuais."
    ],
    [
      "query optimizers are considered one of the most relevant and sophisticated components in a database management system. however, despite currently producing nearly optimal results, optimizers rely on statistical estimates and heuristics to reduce the search space of alternative execution plans for a single query. as a result, for more complex queries, errors may grow exponentially, often translating into sub-optimal plans resulting in less than ideal performance. recent advances in machine learning techniques have opened new opportunities for many of the existing problems related to system optimization. this document proposes a solution built on top of postgresql that learns to select the most efficient set of optimizer strategy settings for a particular query. instead of depending entirely on the optimizer’s estimates to compare different plans under different configurations, it relies on a greedy selection algorithm that supports several types of predictive modeling techniques, from more traditional modeling techniques to a deep learning approach. the system is evaluated experimentally with the standard tpc-h and join order ing benchmark workloads to measure the cost and benefits of adding machine learning capabilities to traditional query optimizers.",
      "os otimizadores de queries são considerados um dos componentes de maior relevância e complexidade num sistema de gestão de bases de dados. no entanto, apesar de atualmente produzirem resultados quase ótimos, os otimizadores dependem do uso de estimativas estatísticas e de heurísticas para reduzir o espaço de procura de planos de execução alternativos para uma determinada query. como resultado, para queries mais complexas, os erros podem crescer exponencialmente, o que geralmente se traduz em planos sub-ótimos, resultando num desempenho inferior ao ideal. os recentes avanços nas técnicas de aprendizagem automática abriram novas oportunidades para muitos dos problemas existentes relacionados com otimização de sistemas. este documento propõe uma solução construída sobre o postgresql que aprende a selecionar o conjunto mais eficiente de configurações do otimizador para uma determinada query. em vez de depender inteiramente de estimativas do otimizador para comparar planos de configurações diferentes, a solução baseia-se num algoritmo de seleção greedy que suporta vários tipos de técnicas de modelagem preditiva, desde técnicas mais tradicionais a uma abordagem de deep learning. o sistema é avaliado experimentalmente com os workloads tpc-h e join ordering benchmark para medir o custo e os benefícios de adicionar aprendizagem automática a otimizadores de queries tradicionais."
    ],
    0.06666666666666667
  ],
  [
    [
      "na literatura do domínio do processamento analítico de dados facilmente se podem encontrar métodos e soluções que respondem ao problema de seleção de vistas multidimensionais no processo de implementação de um cubo olap. uma forma que se evidencia como sendo extremamente vantajosa, é a de fazer a seleção baseada em critérios que se apoiem essencialmente nos conteúdos que são consultados sobre o cubo de dados ao longo das sessões de consulta olap. as principais vantagens que advêm desta monitorização, está relacionada com a possibilidade de efetuar correspondências rigorosas com a informação em que os agentes de decisão mais se apoiam para efetuar as suas tomadas de decisão. ao ser feita a identificação da informação que se evidencia como sendo a mais relevante, ou pelo menos a mais frequentemente consultada, várias ilações se podem retirar, como, por exemplo, a definição de perfis de utilização, a expressão de preferências, a identificação de metodologias de trabalho, ou então a definição de processos que procurem construir cubos iceberg com forte probabilidade de explorações futuras sobre o cubo. este último aspeto constitui, basicamente, o trabalho desta dissertação. ao se efetuar a materialização dos conteúdos mais pesquisados no servidor olap, obtém-se um melhor desempenho ao nível do servidor, uma vez que o preparamos antecipadamente com os dados que mais vezes são solicitados, reduzindo assim o número de vezes que seria necessário recorrer ao data warehouse para retornar os resultados pretendidos por uma dada query multidimensional. em termos gerais, neste trabalho de dissertação, desenvolveu-se um estudo detalhado acerca das ideias e práticas que levam ao desenvolvimento de um dado método de seleção, que seja capaz de indicar de forma precisa as partes de um cubo que são mais utilizadas, sugerindo com base nessa informação uma nova estrutura para o cubo em questão que utilize menos recursos computacionais, nomeadamente espaço em disco e tempo de processamento.",
      "in the literature, we can easily find methods and solutions that solve the problem of pick a set of multidimensional views in the implementation of a data cube process. one way that is shown to be extremely advantageous, is to make this selection based on criteria directly related with the contents that are searched on the data cube, along the olap query sessions. the main advantage that becomes with this monitoring process is the ability to make accurate matches with the information that decision-makers really are interested. with the identification of the information that is characterized as being the most researched, several conclusions and utilities can be made, such as setting profile users, find expressions of preferences, identify methods of work, or defining processes that build iceberg cubes with a strong probability of further explorations, subject that is discussed in this master thesis. better performance can be developed in the server if we materialize only the most researched content on the olap server. the server is prepared with the data that is more times requested and then the number of times that is needed to exploit the data warehouse is reduced. in this master thesis is produced a study who combines ideas and practices that lead to the development of a selection method that makes a very precise indication of the contents to be selected, making a constant control of the multidimensional queries made on the data cube. then we can identify which parts of the cube have the priority to be materialized. with this resolution, we can provide a more effective utilization for the community of users."
    ],
    [
      "linear programming is a mathematical optimization technique used in numerous fields including mathematics, economics, and computer science, with numerous industrial contexts, including solving optimization problems such as planning routes, allocating resources, and creating schedules. as a result of its wide breadth of applications, a considerable amount of its user base lacks programming knowledge and experience and thus often resorts to using graphical software such as microsoft excel. however, despite its popularity amongst less technical users, the methodologies used by these tools are often ad-hoc and prone to errors. block-based languages have been successfully used to aid novice programmers and even children in programming. thus, we created a block-based programming language termed lpblocks that allows users to create linear programming models using data contained in spreadsheets. this language guides the users to write syntactically and semantically correct programs and thus aids them in a way that current languages do not. we have also implemented a web application where users can define linear programming models, reactively see their mathematical representation and execute them to obtain the optimization values for the variables defined by the users. to assess the applicability of lpblocks we used it to successfully express numerous and varied linear programming problems. furthermore, we designed and ran a qualitative empirical study to understand the experience our tool and language brings to users from various backgrounds. although we see differences amongst the users, most of them were able to model several problems using lpblocks.",
      "programação linear é um conjunto de técnicas de otimização matemática utilizada em várias áreas estas incluem matemática, economia, ciências da computação e usos em contextos industriais, incluindo planear rotas, alocar recursos e planear horários. como resulta das suas aplicações variadas uma grande quantidade dos seus utilizadores não possuem conhecimentos de programação e por isso utilizam software gráfico como o microsoft excel. apesar da sua popularidade este software utiliza metodologias ad-hoc e propicias a erros. as linguagem de programação por blocos tem surgido nos últimos anos com o intuito de ajudar programadores iniciantes, tendo mesmo aplicações no ensino de crianças. sendo assim nos criamos uma linguagem de programação pro blocos que utiliza dados contidos em folhas de calculo para criar modelos de programação linear chamada lpblocks. esta linguagem guia utilizadores na criação de modelos semanticamente e sintaticamente corretos. para avaliar a validade de lpblocks nos implementamos vários problemas utilizando a mesma. posteriormente implementamos esta linguagem e utilizamo-la num estudo com utilizadores de vários níveis de experiência. depois utilizamos a informação recolhida durante o estudo para avaliar lpblocks e propor melhorias."
    ],
    0.0
  ],
  [
    [
      "the contemporary landscape of problem-solving requires individuals to possess robust computational thinking (ct) skills. acquiring these skills is contingent on the availability of adequate training resources. this scarcity is particularly pronounced for visually impaired individuals, as the majority of existing ct training materials are inaccessible due to their reliance on visual elements.to remedy this situation, pathit is introduced, offering a non visual ct resource tailored to provide full accessibility to visual impaired individuals. pathit encompasses a physical component that provides a tactile interface for users interaction and a software platform offering a range of ct challenges with both visual and auditory outputs, thereby catering to a diverse audience, including those with and without visual impairments. this master’s dissertation encompasses the design, development, assessment, and pre ceding research that led to the creation of the pathit system. the efficacy of pathit as a ct training resource is rigorously evaluated across a spectrum of visual abilities and age groups, showcasing its potential as a versatile tool for nurturing ct skills and underscoring its adaptability and inclusivity.",
      "o cenário contemporâneo de resolução de problemas exige que os indivíduos possuam habilidades sólidas de pensamento computacional (pc). adquirir essas habilidades depende da disponibilidade de recursos de treino adequados. essa escassez é particularmente pronunciada para pessoas com deficiência visual, uma vez que a maioria dos recursos existentes depende de elementos visuais. para culmatar esta situação, apresenta-se o pathit, um recurso de pc não visual projetado para fornecer total acessibilidade a pessoas com deficiência visual. o pathit engloba um componente físico que fornece uma interface tátil para a interação dos utilizadores e uma plataforma de software que oferece uma variedade de desafios de pc com saídas visuais e auditivas, atendendo assim a um público diversificado, incluindo aqueles com e sem deficiência visual. esta dissertação de mestrado abrange o design, o desenvolvimento, a avaliação e a pesquisa anterior que levou à criação do sistema pathit. a eficácia do pathit como recurso de treino de pc é rigorosamente avaliada em pessoas com diferentes habilidades visuais e faixas etárias, demonstrando seu potencial como uma ferramenta versátil para nutrir habilidades de pc e realçando a sua adaptabilidade e inclusão."
    ],
    [
      "primavera has invested a significant and costly man power in developing business-specific software solutions. such solutions share a significant part of boilerplate code, namely the user interface. to minimize costs and, thus, improving it's software engineers productivity, primavera bss has invested many resources developing a framework that allows for the next family of primavera products to be generated. the developed tool allows the primavera software factory to easily adopt software development processes based on agile methodologies. the goal of this dissertation is to add a new software component to this framework, a test automation component, that allows automated execution of tests to be performed on products modelled on the framework.",
      "a primavera bss investiu muitos recursos na criação de uma framework que permitisse gerar a próxima família de produtos primavera. a ferramenta desenvolvida permite à primavera software factory adoptar, mais facilmente, um processo de desenvolvimento baseado em metodologias agile. o objectivo desta dissertação é o de adicionar uma nova componente a esta framework, uma componente de testes automáticos, que permita executar, de forma automática, testes aos produtos resultado da modelação efectuada na framework e se consiga assim que as soluções de teste do departamento de qualidade acompanhem processos de desenvolvimento ágeis."
    ],
    0.0
  ],
  [
    [
      "cloud orchestration systems, as kubernetes, allow us to dynamically manage aspects such as location of components. this makes traditional resource-oriented monitoring systems inadequate. they also make it desirable that control mechanisms act directly on the orchestrator and not on individual components. this project aims to design, develop and test an application for monitoring and control distributed database systems, solving the challenges posed by this new environment. this dissertation motivated by project h2020 clouddb appliance.",
      "os sistemas de orquestração na nuvem, como o kubernetes, permitem gerir dinamicamente aspetos como a (co-)localização de componentes e o número de instâncias de cada um. isto faz com que os sistemas tradicionais de monitorização, orientados aos recursos físicos, sejam desadequados. fazem também com que seja desejável que os mecanismos de controlo ajam diretamente sobre o orquestrador e não sobre os componentes individualmente. este projeto tem como objetivo projetar, desenvolver e testar uma aplicação para monitorização e controlo de sistemas de bases de dados distribuídas resolvendo os desafios colocados por este novo ambiente. esta proposta de dissertação enquadra-se no projeto h2020 clouddb appliance."
    ],
    [
      "as técnicas de endoscopia digestiva alta e de colonoscopia são fundamentais na prestação de cuidados de saúde primários, pois permitem ao profissional de saúde validar o diagnóstico e prescrever o tratamento mais adequando. estas técnicas geram vários tipos de resultados, dos quais se destacam os vídeos endoscópicos, uma vez que desempenham um papel preponderante no rastreio de patologias ou de lesões que possam estar presentes no trato digestivo. as tarefas de visualização e análise dos vídeos endoscópicos, subsequentes à realização dos exames, podem variar entre os 2 a 32 minutos para a endoscopia digestiva alta e entre os 20 minutos a 1 hora para a colonoscopia, para cada vídeo. tal implica uma demora significativa na interpretação dos dados, com consequências ao nível da fadiga e de diagnósticos erróneos por parte do profissional de saúde. para além deste problema, identifica-se um outro, relacionado com falta qualidade da imagem captada durante os exames. esta pode muitas vezes encontrar-se desfocada, podendo obstar a presença de uma dada patologia ou lesão. é com base no panorama descrito previamente que se justifica o desenvolvimento de soluções inovadoras que permitam colmatar os problemas acima identificados, particularmente, o processamento e a análise de vídeos endoscópicos de longa duração e identificação de informação não relevante para o diagnóstico. uma das soluções engloba a eliminação de frames capturados fora do trato digestivo e permitiu obter vídeos endoscópicos reduzidos e, consequentemente, uma poupança de tempo utilizado nas tarefas de visualização e análise dos mesmos, na ordem dos 45,6 %, para o caso das endoscopias digestivas altas e de 56 %, para as colonoscopias. a solução referente à eliminação de frames desfocados permitiu não só ter ganhos de tempo, 4,6 %, para endoscopias digestivas altas, e 4,8 %, para colonoscopias, como de tamanho de armazenamento dos vídeos endoscópicos reduzidos, de 4,1 %, para endoscopias digestivas altas, e de 4 %, para colonoscopias. em ambas as soluções foi identificado um fator limitativo, o aumento do bit rate, no entanto os valores obtidos não vão influenciar o diagnóstico por parte do profissional de saúde.",
      "both techniques of upper gastrointestinal endoscopy and colonoscopy are key providers of primary health care, they allow health professionals to validate the diagnosis and prescribe the appropriate treatment. these techniques generate various types of results, among which are the endoscopic videos, as they play an important role in tracking diseases or injuries that may be present in the digestive tract. the visualization and analysis tasks of endoscopic videos, following the exams, can vary between 2 to 32 minutes to upper gastrointestinal endoscopy and between 20 minutes to 1 hour for the colonoscopy, for each video. this implies a significant delay in data interpretation, with consequences in terms of fatigue and misdiagnosis by the health care professional. in addition to this problem, is identified another one related to image quality lack captured during the exams. the image is often blurred, and may hide the presence of a given disease or injury. it is based on the scenario described previously that justifies the development of innovative solutions to address the problems identified above, particularly, processing and analysis of endoscopic videos with long duration and the identification of non-relevant information for diagnosis. one solution involves the removal of captured frames outside the digestive tract and allowed to obtain reduced endoscopic videos and, therefore, time savings in visualization and analysis tasks in the order of 45.6 % for the upper gastrointestinal endoscopy cases and 56 %, for colonoscopies. the solution regarding the removal of unfocused frames allowed not only have time savings, 4.6 %, for upper gastrointestinal endoscopy, and 4.8 %, for colonoscopies, as storage size of the reduced endoscopic videos, 4.1 %, for upper gastrointestinal endoscopies, and 4 %, for colonoscopies. in both solutions was identified as a limiting factor, the higher the bit rate, however the values obtained will not influence the diagnosis from the health care professional."
    ],
    0.09999999999999999
  ],
  [
    [
      "there is nowadays an increasing need for database replication, as the construction of high performance, highly available, and large-scale applications depends on it to maintain data synchronized across multiple servers and to achieve fault tolerance. a particularly popular approach, is the mysql open source database management system and its built-in asynchronous replication mechanism. the limitations imposed by mysql on replication topologies mean that data has to go through a number of hops or each server has to handle a large number of slaves. this is particularly worrisome when updates are accepted by multiple replicas and in large systems. noting the most common topologies and taking into account the asynchrony referred, a problem arises, the freshness of the data, i.e. the fact that the replicas do not have just the most recently written data. this problem contrasts with the state of the art in group communication. in this context, the work presented in this master’s thesis is the result of an evaluation of the models and mechanisms for group communication, as well as the practical advantages of group-based replication. the proposed solution extends the mysql proxy tool with plugins combined with the spread group communication system offering, transparently, active and passive replication. finally, to evaluate the proposed and implemented solution we used the reference workload defined by the tpc-c benchmark, widely used to measure the performance of commercial databases. under this specification, we have evaluated our proposal on different scenarios and configurations",
      "existe nos dias de hoje uma necessidade crescente da utilização de replicação em bases de dados, sendo que a construção de aplicações de alta performance, disponibilidade e em grande escala dependem desta para manter os dados sincronizados entre servidores e para obter tolerância a faltas. uma abordagem particularmente popular, é o sistema código aberto de gestão de bases de dados mysql e seu mecanismo interno de replicação assíncrona. as limitações impostas pelo mysql nas topologias de replicação significam que os dados tem que passar por uma série de saltos ou que cada servidor tem de lidar com um grande número de réplicas. isto é particularmente preocupante quando as actualizações são aceites por várias réplicas e em sistemas de grande escala. observando as topologias mais comuns e tendo em conta a assincronia referida, surge um problema, o da frescura dos dados. ou seja, o facto das réplicas não possuírem imediatamente os dados escritos mais recentemente. este problema vai de encontro ao estado da arte em comunicação em grupo. neste contexto, o trabalho apresentado nesta dissertação de mestrado resulta de uma avaliação dos modelos e mecanismos de comunicação em grupo, assim como as vantagens práticas da replicação baseada nestes. a solução proposta estende a ferramenta mysql proxy com plugins aliados ao sistema de comunicação em grupo spread oferecendo a possibilidade de realizar, de forma transparente, replicação activa e passiva. finalmente, para avaliar a solução proposta e implementada utilizamos o modelo de carga de referência definido pelo tpc-c, largamente utilizado para medir o desempenho de bases de dados comerciais. sob essa especificação, avaliamos assim a nossa proposta em diferentes cenários e configurações"
    ],
    [
      "massive data processing tools for distributed environments such as spark or dask allow programmers to process massive amounts of data in data centers. a large portion of the operation costs of these infrastructures corresponds to the energy consumption resulting in performing these operations. current tools use simple algorithms for efficient scheduling of data processing jobs in distributed computing, relying on heuristics without considering the workload characteristics. recent work explores efficient scheduling of data processing jobs in distributed computing, especially in heterogeneous environ ments, despite these infrastructures being typically homogeneous. this dissertation makes an analysis of job executions in spark and proposes easahum a new al gorithm for job scheduling in massive data processing tools with energy efficiency concerns using the conclusions drawn. the implementation and evaluation in a simulator using real and synthetic execution traces in spark demonstrate that the algorithm can reduce energy consumption by up to 16% and reduce job execution time by up to 12.25% without significant impact on the scheduling time.",
      "as ferramentas de processamento de dados massivos em ambientes distribuídos como o spark ou dask permitem aos programadores processar grandes quantidades de dados em centros de dados. uma grande fatia dos custos de operação destas infraestruturas corresponde ao consumo energético resultante de processar estes dados. as ferramentas atuais utilizam algoritmos simples para o agendamento eficiente de trabalhos de processamento de dados em computação distribuída, recorrendo a heurísticas sem ter em conta as características da carga de trabalho. trabalho recente explora o agendamento eficiente de trabalhos de processamento de dados em computação distribuída, especialmente em ambientes heterogéneos, sendo que estas infraestruturas são tipicamente homogéneas. esta dissetação faz uma analise de execuções de trabalhos em spark e propõem easahum um novo algoritmo para o agendamento de trabalhos para ferramentas de processamento de dados massivos com preocupações de eficiência energética com as conclusões tiradas. a implementação num simulador e avaliação usando traces de execuções reais e sintéticas em spark, demonstram que o algoritmo consegue reduzir o consumo energético em até 16%, além de conseguir reduzir o tempo de execução dos trabalhos em até 12.25%, sem grande impacto no tempo gasto no agendamento."
    ],
    0.0
  ],
  [
    [
      "nos últimos anos, cada vez mais pessoas que anteriormente viviam em zonas rurais migram para centros urbanos à procura de novas oportunidades. face a este movimento, vários problemas e adversidades foram-se agravando, nomeadamente, o aumento do fluxo rodoviário, que cria problemas de trânsito, o aumento dos níveis de poluição, o acesso à saúde, entre outros. desta forma, torna-se imperativo gerir de forma eficaz e sustentável os recursos, com a finalidade de melhorar a qualidade de vida dos habitantes destas cidades. neste contexto, juntamente com os avanços tecnológicos que se tem observado, surge o conceito de cidades inteligentes, que recorrendo a redes de sensores recolhem todos os dados necessários para ”virtualizar” as cidades. desse modo, a informação coletada está centralizada, para que assim seja possível gerir os recursos disponíveis de forma informada, responsável e eficiente, para que seja possível responder às necessidades da população. com este trabalho, pretende-se estudar dois problemas concretos no âmbito das cidades inteligentes, nomeadamente na área do transporte inteligente, recorrendo à simulação de redes de sensores, constituídas por sensores de aceleração instalados na rede de transporte públicos da cidade, a partir da qual vão ser recolhidos dados. o primeiro problema que se tenciona solucionar está relacionado com a monitorização do estado do pavimento. com os dados provenientes dos acelerómetros, espera-se ser possível estimar o estado de conservação das vias rodoviárias e, desta forma, as entidades responsáveis passam a ser capazes de realizar decisões informadas e apropriadas face ao estado de determinada estrada, procedendo assim à sua restauração caso necessário. uma segunda vertente que se pretende explorar foca a monitorização da congestão das vias rodoviárias em que, com base na mesma rede de transportes, se projeta ser possível determinar os níveis de fluxo rodoviário. por fim, é ainda expectável que beneficiando dos transportes públicos dos quais já se está a tirar proveito, seja plausível medir os níveis de poluição aérea.",
      "in recent years, we have been witnessing more and more people who previously lived in rural areas are migrating to urban centers in search of new opportunities. faced with this movement, several problems and adversities have worsened, namely, the increase in the flow of traffic that creates traffic problems, increased levels of pollution, and access to health, among others. thus, it becomes imperative to manage the resources effectively and sustainably, in order to improve the quality of life of the inhabitants in these cities. in this context, and with the technological advances that have been observed, the concept of smart cities emerges, by using sensor networks, it’s possible to collect all the data needed to ”virtualize”the cities. in this way, the information collected is centralized to manage the available resources in an informed, responsible, and efficient way to meet the population’s needs. this work aims to study two specific problems in the scope of smart cities, namely in the area of intelligent transportation, using a simulation of a network of sensors, based on accelerometers, installed in the city’s public transportation network, from which data will be collected. the first problem that is intended to be solved is related to sidewalk condition monitoring. with the data from the accelerometers, it is expected that it will be possible to estimate the condition of the roads, and in this way, the responsible entities will be able to make informed and appropriate decisions regarding the condition of a given road, and thus, proceed with its restoration, if necessary. the second aspect that’s intended to be explored focuses on monitoring the congestion of roads, which based on the same transport network is projected to be possible to determine the levels of road flow. finally, if possible, it is also expected that the public transport vehicles, that are already being taken advantage of, can be used to measure the levels of air pollution."
    ],
    [
      "o ancestors notebook é uma ferramenta de apoio à gestão e organização de documentos e informações sobre a história e herança familiar. tem como intuito oferecer diferentes potencialidades que facilitem todo o processo de registo e construção de um legado relativamente a uma ou mais genealogias específicas. o ancestors notebook funciona sobre o file-system linux utilizando um conjunto de convenções, comandos e domain specific languages (dsls) para nomear e organizar diretorias e com um foco especial em documentos com um formato específico - dgu - criados especialmente este toolkit. o seu propósito é trazer um controlo organizacional personalizado ao utilizador, contribuindo assim para um fluxo coerente de ideias sempre correlacionado com o aglomerar de dados de cariz genealógico. a organização dos dados passa pela definição de entidades representativas de vários elementos, para aglutinar distintos formatos num só, de maneira a ter um maior segmento organizacional no sistema de ficheiros. definiu-se, também, a geração de templates genéricos para uma visualização mais agradável e familiar, exportável em formato pdf, denominados caderno de antepassados. estes consideram uma funcionalidade de agregação e organização de documentos por entidades, contribuindo assim para um maior leque de alternativas na definição dinâmica de opções de visualização. o ancestors notebook toolkit dispõe de um sistema de controlo de versões, cujo funcionamento está dependente de um sistema de representação de conhecimento sob forma de ontologia e um projection editor que permite visualizar e manipular a estrutura genealógica como está representada no sistema de ficheiros. o toolkit é definido na linguagem de programação python com definição de comandos disponíveis no sistema de ficheiros. utilização de diferentes módulos python para a definição de views para o utilizador. a criação de templates é feita usando o motor de geração de templatesjinja2. o toolkit é definido como package instalável através do pip.",
      "the ancestors notebook is a tool designed to support the management and organization of documents and information related to family history and heritage. its purpose is to offer various features to facilitate the process of recording and building a legacy related to one or more specific genealogies. the ancestors notebook operates on the linux file system using a set of conventions, commands, and domain specific languages (dsls) to name and organize directories. it has a particular focus on documents in a specific format called dgu, specially created for this toolkit. its goal is to provide users with customized organizational control, thus contributing to a coherent flow of ideas always correlated with the accumulation of genealogical data. data organization involves defining representative entities for various elements to aggregate different formats into one, creating a more structured organizational system within the file system. additionally, the toolkit includes the generation of generic templates for a more pleasant and familiar view that can be exported in pdf format, known as the ancestors notebook. these templates consider the functionality of aggregating and organizing documents by entities, offering a wide range of options for dynamic visualization. the ancestors notebook toolkit incorporates a version control system, which relies on a knowledge representation system in the form of an ontology and a projection editor that allows users to view and manipulate the genealogical structure as it is represented in the file system. this toolkit is defined in the python programming language, with commands available for use within the file system. it utilizes various python modules to define views for the user, and the creation of templates is done using the jinja2 template generation engine. the toolkit is packaged and installable via pip."
    ],
    0.3
  ],
  [
    [
      "the http protocol is a stateless protocol, that means, each request made by the user is an independent request, there is no notion of state. so, to add it to the applications we need an additional tool to implement this notion of state. for this, cookies are used, allowing the websites to identify the authenticated users. a cookie is a file stored in the customer’s browser and sent together with http requests, allowing the website to recognize the customer and send a response corresponding to the request made. this dissertation aims to strengthen the protection of data associated with authentication sessions through the identification and analysis of authentication cookies using machine learning techniques. if web applications are vulnerable to malicious attacks, such as broken authentication or xss (cross-site scripting), attackers can gain access to the information stored in the cookie. using this information they can steal the user’s session, being able to authenticate themselves in the web application to obtain access to data/services. using machine learning techniques, we can identify within a set composed of several types of cookies, which cookies are associated with authentication. the objective is the recognition of this type of cookies, since this is the one that needs greater security, taking care in case the attacker even gaining access to this file, there is no possibility of deciphering the information that puts the users session at risk. in addition to the classification of cookies, the detection and analysis of the encoding used will be carried out. the tool will then be integrated into the security testing software, burp suite, working as an extension in order to facilitate and reduce the time necessary for a qa analyst to spend checking cookies.",
      "o protocolo http é um protocolo stateless, ou seja, cada pedido efetuado pelo utilizador é um pedido independente, não existe uma noção de estado. logo para adicionarmos isto nas aplicações web precisamos de uma ferramenta adicional para implementar esta noção de estado. para tal, são utilizados os cookies, permitindo aos websites identificar os utilizadores autenticados. o cookie é um ficheiro armazenado no navegador do cliente e que é enviado juntamente com os pedidos http, permitindo ao website reconhecer o cliente e enviar a resposta correspondente ao pedido efetuado. esta dissertação tem como objetivo reforçar a proteção dos dados associados às sessões de autenticação através da identificação e análise dos cookies de autenticação recorrendo a técnicas de machine learning. caso as aplicações web estejam vulneráveis a ataques maliciosos, como por exemplo broken authentication ou xss(cross-site scripting), os atacantes podem conseguir acesso à informação armazenada no cookie. utilizando esta informação podem roubar a sessão do utilizador, conseguindo autenticar-se na aplicação web para obter acesso a dados/serviços. recorrendo a técnicas de machine learning podemos identificar dentro de um conjunto composto por vários tipos de cookies, quais os cookies associados à autenticação. o objetivo é o reconhecimento deste tipo de cookies, dado que este é o que necessita de uma maior segurança, precavendo-se para o caso do atacante mesmo conseguindo acesso a este ficheiro, não haja a possibilidade de decifrar a informação que coloca em risco a sessão dos utilizadores. para além da classificação de cookies, será realizada a deteção e análise do encoding utilizado. a ferramenta será depois integrada no software de realização de testes de segurança, burp suite, funcionando como uma extensão do mesmo, de forma a facilitar e reduzir o tempo necessário que um analista de qa(quality assurance) irá dispender com a verificação dos cookies."
    ],
    [
      "the term e-health has been increasingly used in research projects in the health industry. this concept encompasses the development and application of software and hardware solutions for the collection, storage, manipulation, and communication of data in an efficient way, with an objective of continuously improving the provision of health care. the development of health information systems has increasingly lacked the establishment of high levels of interoperability in its semantic, syntactic, and technical aspects. the development of systems that allow promoting interoperability between his within the same institution or even between his from different institutions is on the global agenda as a common concern for all countries. the use of globally recognized standards has been increasingly common, ensuring that both the information structure and its meaning remain intact, regardless of the flow they follow. thus arises the motivation to develop a system that contributes to the continuous improvement of interoperability in health, through the use of the openehr standard. the following dissertation presents a novel way to handle clinical data by creating an artifact enabling the conversion of openehr standardized data into a json object. the web application showcases a new way for a user to check for openehr data while the api can be utilized by developers to work with openehr data in a more accessible and supported manner with other programming tools. to carry out the work a thorough examination of web development tools to build both the backend and frontend of the app was essential, as well as coming up with the most accurate regex expressions that are able to extract data from openehr files. the research and engineering effort put through the project was successful in showcasing this novel approach implementing yet another tool to help out healthcare professionals and biomedical software engineers.",
      "o termo e-health tem sido cada vez mais utilizado em projetos de pesquisa na área da saúde. este conceito engloba o desenvolvimento e aplicação de soluções de software e hardware para recolha, armazenamento, manipulação e comunicação de dados de forma eficiente, com o objetivo de melhorar continuamente a prestação de cuidados de saúde. o desenvolvimento dos sistemas de informação em saúde (sis) tem falta de estabelecimento de elevados níveis de interoperabilidade nas suas vertentes semântica, sintática e técnica. o desenvolvimento de sistemas que permitam promover a interoperabilidade entre sis dentro da mesma instituição ou mesmo entre sis de diferentes instituições é uma preocupação comum a todos os países. a utilização de padrões mundialmente reconhecidos tem sido cada vez mais comum, garantindo que tanto a estrutura da informação assim como o seu significado permaneçam intactos, independentemente do fluxo que seguem. surge assim a motivação para desenvolver um sistema que contribua para a melhoria contínua da interoperabilidade na saúde, através da utilização da norma openehr. esta dissertação apresenta uma nova forma de tratar dados clínicos criando um artefato que permite a conversão de dados padronizados openehr em um objeto json. a aplicação web apresenta uma nova maneira de um usuário verificar dados openehr enquanto a api pode ser utilizada por desenvolvedores para trabalhar com dados openehr de uma maneira mais acessível e compatível com outras ferramentas de programação. para realizar o trabalho, foi essencial uma análise das ferramentas de desenvolvimento web para construir o backend e o frontend do aplicativo, além de encontrar as expressões regex mais precisas que são capazes de extrair dados de arquivos openehr. o esforço de pesquisa e engenharia realizado no projeto foi bem-sucedido em mostrar esta nova abordagem, implementando mais uma ferramenta para ajudar profissionais de saúde e engenheiros de software biomédico."
    ],
    0.3
  ],
  [
    [
      "this document describes the development of high performance web applications using django framework. initially, the operation and usage mode of django are introduced, as well as several web applications’ latency reduction techniques. the work carried out fo cused on the design, implementation and performance optimization of a web application, which consists of an article sharing system. the development process followed the scrum methodology. during development, several technologies were explored, such as memcached, celery and varnish, which enabled the implementation of certain performance optimi zation strategies. the latency of several operations was measured, before and after the application of optimization techniques, in order to ensure that one was moving in the right direction. the optimization of the application’s performance was performed at various le vels, including the transfer of content across the network and the backend services. http caching, data compression and minification tecniques, as well as static content replication using content delivery networks, were used. partial update of the application’s pages on the front-end and asynchronous processing techniques were applied. the database utili zation was optimized by creating indexes and by taking advantage of a nosql solution. memory caching strategies, with distinct granularities, were implemented to store templa tes and application objects. furthermore, asynchronous task queues were used to perform some costly operations. all of the aforementioned techniques favorably contributed to the web application’s latency decrease. django only supports the application of some of these techniques, because it operates on the back-end. since performance must be optimized at various levels, it was necessary to use other tools besides django.",
      "este documento descreve o desenvolvimento de aplicações web de elevado desempenho com a framework django. inicialmente, apresenta-se o funcionamento e o modo de utiliza ção do django, bem como diversas técnicas de diminuição da latência das aplicações web. o trabalho realizado focou-se na conceção, implementação e otimização do desempenho de uma aplicação web, que consiste num sistema de partilha de artigos. o processo de desenvolvimento seguiu a metodologia scrum. durante o desenvolvimento foram explo radas diversas tecnologias, tais como memcached, celery e varnish, que possibilitaram a implementação de determinadas estratégias de otimização do desempenho. realizaram-se medições da latência de diversas operações, antes e após a aplicação das estratégias de oti mização, para garantir que se caminhava no sentido correto. a otimização do desempenho da aplicação ocorreu a vários níveis, incluindo a transferência do conteúdo pela rede e os serviços de back-end. utilizaram-se técnicas como o caching http, bem como a compressão e minificação de informação e, ainda, a replicação de conteúdo estático utilizando content delivery networks. aplicaram-se técnicas de processamento assíncrono e atualização parcial das páginas da aplicação no front-end. otimizou-se a utilização da base de dados, criando índices e tirando partido de uma solução nosql. implementaram-se estratégias de caching em memória com granularidades distintas, para armazenar templates e objetos gerados pela aplicação. recorreu-se ainda a filas assíncronas de tarefas para a realização de algumas operações custosas. todas as técnicas mencionadas contribuíram favoravelmente para a di minuição da latência da aplicação web. o django apenas suporta a aplicação de algumas destas técnicas, já que opera no back-end. como o desempenho deve ser otimizado a vários níveis, foi necessário recorrer a outras ferramentas para além do django."
    ],
    [
      "a utilização do posicionamento, no âmbito das aplicações fornecidas aos utilizadores, tem vindo a aumentar exponencialmente. os trabalhos desenvolvidos na área do posicionamento indoor têm vindo a aumentar com o aumento da mobilidade dos utilizadores. as possibilidades de uso destas tecnologias são imensas: aumentar a experiência do utilizador e a lealdade, aumentar as vendas através de marketing de proximidade, ajudar a movimentação de utilizadores em locais públicos, o uso de geofencing para encontrar pessoas, etc. de forma a reaproveitar as infraestruturas já existentes nos edifícios, a tecnologia de wi-fi fingerprinting tem sido uma escolha frequente por parte das equipas de investigadores e programadores. o principal objetivo desta dissertação é desenvolver uma aplicação para computadores pessoais usando um sistema de posicionamento baseado em wi-fi fingerprinting, integrando-a no sistema where@um. a solução apresentada detalhadamente na dissertação implementa funcionalidades já disponibilizadas na aplicação android. foram também desenvolvidas novas respostas a problemas já existentes e integrados novos módulos na arquitetura, como a integração com as redes sociais e o suporte multiplataforma, tendo especial cuidado em manter alguma homogeneidade no ambiente aplicacional where@um, através do uso de interfaces de utilizador similares.",
      "the positioning usage in desktop and mobile applications have been increasing exponentially. new applications and research in the indoor positioning field area are increasing proportionally with the intensification of the user’s mobility. the use of positioning creates endless possibilities for example: increase user experience and loyalty, increase sales through proximity marketing, navigation to location inside public buildings, geofencing to find friends in public spaces, etc. buildings existent infrastructures can be used, if the positioning technology is wi-fi fingerprinting. this has been a frequent choice by teams and developers worldwide. the main goal of this thesis is to develop a desktop application using a positioning system based in wi-fi fingerprinting, the application will be integrated with the where@um system. the detailed solution that will be presented implements functionalities already provided by the android application. of course that new solutions answering problems were, also, implemented with the creation of new modules in the architecture such as: social networks integration and multiplatform support. during the implementation it was one objective to create homogeneity in the where@um environment, using similar user interfaces."
    ],
    0.3
  ],
  [
    [
      "transcription factors (tfs) are proteins that mediate the cellular response to the changes of the surrounding environment. studying their functional domains and protein structure is fundamental in order to gain insight of the way they are triggered and how they shape genetic transcription. the current work aimed for classifying both tfs and functional domains, understanding which features can be related to the different functions of the tfs. by using uniprotjapi, a java library that allows remote access to uniprot, the information of 200 escherichia coli’s (e. coli) tfs has been retrieved. this data was manually curated, in order to remove domain duplicates and other excess information, and to add missing domains. the obtained functional domains were classified according to their molecular function, while the tfs were classified according to their regulatory function. tfs that exclusively induce gene expression were classified as activators, while tfs that only perform gene repression were classified as repressors. on the other hand, tfs that perform both the activation and repression of transcription were classified as duals. the information was then analysed altogether in order to understand what relationships between the tfs’ function and functional domains could exist. several analysis were performed, which include statistical tests and clustering methods. along with the analysis of the full list of tfs, tfs that are part of two-component signal transduction systems and global tfs were given special focus, due to their important role in cellular function. the results showed that there is a relationship between the functional domains and the regulatory function of the different tfs. this may be related to the evolutionary relationships between repressors and activators. it is also understandable that dual regulators are closely related to activators and repressors than what activators and repressors are to each other. moreover, tfs of two-component signal transduction systems are similar to each other, given that they perform similar functions. their domain architectures are also predictable and do not vary from what was expected of these tfs. however, in global tfs the results are opposite of the ones obtained for two-component system tfs: their structures are very different from each other and each tf is specific. the amount of different domains is high when comparing to the full sample of tfs, since the number of domains exceeds the number of tfs. domains of all classification types are present in their structure and the domain architectures are varied, which reflects their different activities within the cell.",
      "os factores de transcrição (tfs) são proteínas que mediam resposta celular perante alterações do meio em que se inserem. estudar os seus domínios funcionais e estrutura proteica é fundamental para compreender a forma como as suas funções são desencadeadas e como moldam a regulação da transcrição. este trabalho teve como objectivos a classificação dos tfs de acordo com a sua função, assim como a classificação dos domínios funcionais. através do uso da uniprotjapi, uma biblioteca de java que permite o acesso remoto à uniprot, foi recolhida informação de 200 tfs da escherichia coli (e. coli). estes dados foram curados manualmente, com o objectivo de remover domínios duplicados e outra informação em excesso, assim como de adicionar domínios em falta. os domínios funcionais obtidos foram classificados de acordo com a sua função molecular, enquanto que os tfs foram classificados de acordo com a sua função regulatória. tfs que exclusivamente induzem a expressão genética foram classificados como activadores, enquanto que tfs que apenas reprimem a expressão genética foram classificados como repressores. por usa vez, tfs que tanto induzem como reprimem a expressão genética foram classificados como duais. a informação dos domínios e dos tfs foi considerada como um todo de forma a compreender quais as possíveis relações entre a função regulatória dos tfs e os domínios funcionais. várias análises foram efectuadas, das quais testes estatísticos e métodos de clustering. para além da análise de todos os tfs, foi também feita uma análise de tfs que fazem parte de two-component transduction systems e tfs globais, devido à sua importância na actividade celular. os resultados demonstram que existe uma relação entre os domínios funcionais e a função regulatória dos tfs. esta pode ter a ver com as relações evolucionárias dos activadores e repressores. é, também, perceptível que os reguladores duais relacionam-se com mais proximidade dos activadores e dos repressores do que os activadores e os repressores se relacionam entre si. para além disso, tfs de two-component transduction systems têm estruturas semelhantes , uma vez que desempenham funções idênticas. as duas arquitecturas de domínios também são previsíveis e não variam do que era esperado. contudo, para os tfs globais, os resultados são antagónicos: as suas estruturas são diferentes umas das outras e cada tf é específico. a quantidade de domínios diferentes é elevada em comparação com a amostra completa de tfs, uma vez que o número de domínios excede o número de tfs. domínios de todas as classificações estão presentes na estrutura dos tfs globais e as arquitecturas de domínios são variadas, o que reflecte as suas actividades específicas na célula."
    ],
    [
      "cada vez mais as necessidades e os requisitos do paciente acompanham os desenvolvimentos tecnológicos na área da cirurgia médica. tal acontece para que estes obtenham uma intervenção o mais eficiente e seguro possível por parte dos serviços de saúde e dos seus profissionais. no entanto, hoje em dia ainda é difícil implementar e operar diferentes tipos de tecnologias em ambientes médicos devido às desvantagens que estes podem trazer para os seus utilizadores e por todo o processo de aprendizagem que estas requerem. numa primeira abordagem, este trabalho tem como objetivo esclarecer conceitos e reunir algumas soluções existentes para a resolução destes problemas, assim como as respetivas tecnologias utilizadas pelas mesmas. posteriormente é elaborado e apresentado um conceito e protótipo de uma aplicação móvel de planeamento cirúrgico ortopédico que implementa tecnologias de realidade aumentada. a solução proposta pretende ajudar o cirurgião desde a fase de planeamento até a própria fase de intervenção cirúrgica. para além de alguns exemplos e da apresentação do trabalho realizado para a solução, é também descrito o processo de implementação e a arquitetura do sistema. tendo em conta o protótipo desenvolvido, são discutidas as vantagens da sua utilização num contexto cirúrgico e levantados alguns pontos de interesse futuro a serem estudados e implementados para a sua melhoria.",
      "patient needs and requirements are increasingly following technological developments in the area of medical surgery. this happens, so they can obtain the most efficient and safe intervention possible from the health services and their professionals. however, today it is still difficult to implement and operate different types of technologies in medical environments due to the disadvantages that they can bring to their users and the entire learning process they require. in a first approach, this work aims to clarify concepts and gather some existing solutions to solve these problems, as well as the respective technologies used by them. subsequently, a concept and prototype of a mobile orthopaedic surgical planning application that implements augmented reality technologies is developed and presented. the proposed solution aims to help the surgeon from the planning stage to the surgical intervention phase itself. in addition to some examples and the presentation of the work developed for the solution, the implementation process and the system architecture are also described. taking into account the developed prototype, the advantages of its use in a surgical context are discussed and are raised some future points of interest to be studied and implemented for its improvement."
    ],
    0.0857142857142857
  ],
  [
    [
      "gait function can be affected by neurological disorders such as spinal cord injury (sci), stroke, or traumatic brain injury (tbi). these limitations have significant negative effects on the affected people’s independence and quality of life. brain-computer interfaces (bcis) have the potencial to create solutions that may overcome irreversible disabilities. several studies in recent years have shown that electroencephalographic (eeg) signals can be used to develop bcis for the rehabilitation of human limbs through lower-limbs robotic devices and exoskeletons. therefore, their effectiveness and safety depend on how successfully they can detect and react to movement. this dissertation aims at developing and validating an eeg-based motor intent decoding framework to accurately classify human intent regarding five daily performed locomotor tasks. this framework will contribute on the developing of bci to recover the mobility of neurologically impaired subjects. for this, a provided multi-channel dataset will be used. the implementation of this solution was divided into two phases. the first is about how signals are processed to obtain the features that best characterize each of the locomotion modes under analysis. as a result, three distinct studies that differ in the number of channels used were created. through the application of the ica method, it has been determined that the more channels are used in a study, the more likely it is that these channels may be corrupted, affecting the ica method’s effectiveness. the second section discusses the classification methodology. three different deep learning algorithms, cnn, lstm, and their combination, c-lstm, were studied here. additionally, three different features used as the input for the models were compared for each of them and for each of the studies. the features that were selected showed a higher impact on the results than the actual classification algorithm, with erps being the features that produced the best results. on the other hand, across classifiers, all three provided high performance, demonstrating reduced differences between them. the study with higher accuracy as the study 3 with the most reliable channel selection.",
      "o movimento humano da marcha pode ser afetado por distúrbios neurológicos, tais como lesão na medula espinhal, acidente vascular cerebral (avc), ou traumatismo craniano. estas limitações têm efeitos negativos significativos tanto a nível de independência, como na qualidade de vida das pessoas afetadas. interfaces cérebro-computador (bcis) mostram ter potencial para fornecer soluções para o tratamento de distúrbios cerebrais. nos últimos anos, vários estudos mostraram que sinais eletroencefalográficos (eeg) podem ser usados no desenvolvimento de bcis para a reabilitação de membros humanos através de equipamentos robóticos de membros inferiores e exoesqueletos. a sua eficiência e segurança dependem do sucesso com que conseguem detetar e reagir ao movimento. esta dissertação tem como objetivo desenvolver e validar um framework de decodificação de intenção motora baseada em eeg para classificar com precisão a intenção humana segundo cinco tipos de locomoção presentes no dia-a-dia. este framework irá contribuir para o desenvolvimento de um bci com a finalidade de recuperar a mobilidade de sujeitos com deficiência neurológica. para isso, será utilizado um dataset multicanal já existente. a implementação desta solução foi dividida em duas fases. a primeira refere-se ao processamento dos sinais para obter as features que melhor identificam cada um dos modos de locomoção em análise. como resultado, foram criados três estudos distintos que diferem no número de canais utilizados. através da aplicação do método ica, foi concluido que quanto mais canais forem utilizados num estudo, maior é a probabilidade de existirem canais corrompidos, afectando a eficácia dos resultados. a segunda secção discute a metodologia de classificação. três diferentes algoritmos de deep learning, cnn, lstm, e a sua combinação, c-lstm, foram estudados. além disso, foram comparadas três features diferentes utilizadas como input para cada um dos modelos e para cada um dos estudos. as features que foram seleccionadas mostraram um maior impacto nos resultados do que o próprio algoritmo de classificação, sendo os erps as features que obtiveram os melhores resultados. por outro lado, todos os modelos apresentaram um bom desempenho, não havendo diferenças significativas entre eles. o estudo que obteve maior precisão foi o estudo 3."
    ],
    [
      "this thesis works on reinforcement learning tree search and attempts to find the best possible sequence of actions the agent needs to execute to get the most reward while using less computational effort than by just applying a quantum maximum finding algorithm. to achieve this we will use the property that makes it possible to limit our search space to the elements that were marked by the oracle in grover’s algorithm, by marking a fourth of the search space and following it with a quantum maximum finding subroutine. from this, one of the marked elements is obtained and the information encoded in it is used to update a probabilistic distribution stored in a classical memory. the goal is to encounter the minimum amount of iterations of this process and compare the results, i.e., percentage of success which is measured as the number of times the algorithm produces a solution (element with maximum reward) and the number of queries used - with a traditional quantum maximum finding procedure. if this is observed, it is also hypothe sized that the algorithm could be used to observe a step further into the future compared to the traditional procedure, i.e., use the same or fewer queries to evaluate a larger number of sequences fruit of increasing the horizon of the episodes. the last hypothesis tests the depth of the circuits, more specifically the number of gates used. if the algorithm evaluates shallower circuits than the quantum maximum finding, the approach can be applied on the current quantum machines (nisq) because the shallower circuits produces more error-proof measurements. the results show that the proposed algorithm has no advantages compared to a traditional quantum maximum finding procedure due to using more queries to achieve the same rate of success which, consequently, invalidates the first and second hypothesis. for the third hypothesis, the gate complexity was not directly measured. instead, was opted to measure the number of queries used by circuit which might not be sufficient to conclude that the algorithm uses shallower circuits.",
      "esta tese trabalha com a busca em árvores utilizando a aprendizagem por reforço para encontrar a melhor sequência possível de ações que o agente terá de executar de forma a obter o maior prémio possível, isto enquanto usa menos esforço computacional em comparação com utilizar apenas um algoritmo de procura quântica pelo máximo. para atingir estes objectivos, usaremos a propriedade que possibilita limitar o espaço de procura para os elementos marcados pelo oráclo no algoritmo de grover, marcando exatamente um quarto do espaço de procura, procedendo com uma procura quântica. disto resulta um dos elementos marcado e a informação codificada nele será usada para atualizar uma distribuição probabilística guardada em memoria clássica. o objectivo é encontrar o mínimo de iterações deste processo necessário para obter uma percentagem de sucesso - número de vezes que o algoritmo retornou um elemento que é solução do problema e o número de queries usado - e comparar estes resultados com um procedimento tradicional de procura quântica. caso isto se observe, é colocada a hipótese de se usar este algoritmo como forma de observar ações futuras em comparação com os algoritmos tradicionais, isto é, usar o mesmo ou menos queries para avaliar um maior número de sequências fruto do aumento do horizonte dos episódios a avaliar. a última hipótese testa se a profundidade dos circuitos, mais concretamente o número de gates usadas. caso o algoritmo proposto utilize circuitos menos profundos que o algoritmo quantum maximum finding, este poderá ser utilizado nas máquinas quânticas atuais pois estes circuitos produzem medições mais resistentes a erros. os resultados mostraram que o algoritmo proposto não possui qualquer vantagem comparado ao quantum maximum finding por usar mais queries para atingir a mesma percentagem de sucesso o que, consequentemente, invalida a primeira e segunda hipótese. quanto à terceira hipótese, o número de gates usadas por circuito não foi medido diretamente. em vez disso, optou-se por medir o número de queris por circuito o que poderá não ser suficiente para obter conclusões quanto à profundidade dos circuitos medidos."
    ],
    0.3
  ],
  [
    [
      "devops presents a mix of agile methodologies that allow an application’s release cycle to be shortened. this translates into a faster delivery of value to the stakeholders. however, the value creation chain does not finish at the end of that cycle. it is necessary to monitor the artifacts produced at a system level, and at the application level, in order to ensure the compliance of the functional and non functional requirements. today, there seems to be a clear separation between the monitoring process and the application development process. as the development and operations processes have merged in devops, this dissertation pretends to investigate how to integrate several aspects of monitoring into the regular lifecycle of an application’s development. the inclusion of external services further emphasizes the need to include an observability component into an infrastructure. the main goal of this dissertation is to develop a solution for the deployment of an infrastructure using stateof- the-art technologies and frameworks, while also providing observability to the system and to the applications running on it. to do so, it required the investigation of the methodologies and concepts that are the base of the software development lifecycle, focusing on the latter stages of that process: the deployment, and monitoring phases. these methodologies and concepts were complemented with the study of state-of-the-art technologies and frameworks that aim to ease the burden of setting up an infrastructure quickly and with the necessary tools to evolve it after the initial setup and with each new software release. furthermore, it also involved the research of tools that enable the collection of metrics from applications, as well as processing such data and displaying it in useful ways for operators and stakeholders. in this context, this dissertation aims to provide a solution for the deployment of mobileid applications at inesc tec, using the mobile driving licence as the primary case study. the proposed design and implementation with a container orchestration framework and ci/cd pipelines, enables faster development of different mobileid applications, while also providing continuous monitoring to the deployments. with this implementation, it was possible to assess how container orchestration frameworks provide greater flexibility to applications, and how this observability can be augmented with the use of dedicated monitoring systems.",
      "devops baseia-se na utilização de um conjunto de metolodogias ágeis que permitem encurtar o ciclo de desenvolvimento de uma aplicação de forma a que as alterações efetuadas pelos programadores se traduzam no valor desejado pelas partes interessadas. no entanto, a criação de valor não termina na parte final desse ciclo. é necessário monitorizar os artefactos produzidos tanto a nível de sistema, como a nível aplicacional, de forma a garantir o cumprimento de requisitos funcionais e não funcionais. todavia, parece existir uma separação entre o processo de monitorização e o processo de desenvolvimento de aplicações. tal como os processos de desenvolvimento e de operações se uniram no conceito de devops, pretende-se também investigar como será possível integrar vários aspetos de monitorização no ciclo normal de desenvolvimento de uma aplicação. o principal objetivo desta dissertação é desenvolver uma solução de operacionalização de infraestruturas de suporte a aplicações com recurso às tecnologias e ferramentas mais adequadas. esta solução deverá ser acompanhada, em paralelo, por mecanismos de observabilidade dessa infraestrutura e das aplicações que nela são executadas. para isso, foi necessária a investigação de metodologias e conceitos que formam a base do processo de desenvolvimento de software. o foco esteve nas partes finais do processo: a fase de deployment e a de monitorização. estas metodolodogias e conceitos foram complementados com o estudo de tecnologias e ferramentas que pretendem facilitar o processo de montar uma infraestrutura rapidamente, bem como permitir a evolução da arquitetura inicial consoante os subsequentes lançamentos de aplicações. para além disso, também envolveu a pesquisa de ferramentas que permitem extrair e armazenar métricas de aplicações, bem como processar essa informação e disponibilizá-la em formato útil quer para operadores, quer para outras partes interessadas. neste contexto, esta dissertação pretende desenvolver uma solução que permita efetuar o deployment de aplicações de identidade digital no inesc tec, utilizando a carta de condução móvel como caso de estudo. a arquitetura proposta, e a respetiva implementação com recurso a um orquestrador de containers e pipelines de ci/cd, permite o desenvolvimento mais ágil de novas aplicações de identidade digital, e proporciona monitorização contínua a cada iteração do desenvolvimento. a partir do resultado prático obtido, foi possível aferir de que forma os orquestradores de containers permitem melhorar a observabilidade de aplicações, e de que forma ela pode ser aumentada com recurso a sistemas dedicados de monitorização contínua."
    ],
    [
      "desde os anos 70, a teoria do funcional da densidade tem sido uma das técnicas mais utilizadas em física quântica para resolver a equação de schrödinger, para determinar as propriedades eletrónicas dos materiais, usando funções de onda eletrónica e a energia de cada eletrão. o método de resolução do campo autoconsistente (sigla em inglês scf) é um processo iterativo que calcula a densidade dos eletrões a partir de funções de onda. estes cálculos com a equação de schrödinger são realizados múltiplas vezes de forma sucessiva até se atingir a convergência autoconsistente. o scf neste processo iterativo é atualmente calculado em pacotes de software dedicado, como o quantum espresso (qe), um produto open-source em fortran 90, para cálculo da estrutura eletrónica dos materiais. sendo estes cálculos computacionalmente intensivos, a sua execução paralela permite melhorar o desempenho do processo de cálculo. o quantum espresso (qe) suporta paralelismo em ambiente de memória distribuída, com message passing interface (mpi) e, mais recentemente, em memória partilhada, com openmp. a presente dissertação apresenta várias propostas de instalação e configuração desta fer ramenta. estas propostas sugerem diferentes estratégias de paralelismo tendo em vista obter melhorias de desempenho deste tipo de simulações, comparativamente a um estudo anterior realizado nas mesmas condições de experimentação. para o presente estudo foram utilizados processadores multicore e many-core do cluster search. estes testes exploraram o impacto de versões multiprocesso com múltiplos fios de execução por processo, introduzi das em versões mais recentes do qe com desenvolvimento de paralelizações híbridas. através de diferentes casos de teste, diferentes instalações e parâmetros configuráveis (número de pools) este trabalho explorou e procurou obter um ambiente de execução que melhor favorecia o desempenho de simulações do tungsten diselenide (wse2) no cluster search. os resultados obtidos nestes testes, onde se aconselham certas configurações e se desaconselham outras, destinam-se a ajudar as comunidades de física a encontrar um ambiente de execução afinado em termos de desempenho, para o caso concreto deste tipo de simulações.",
      "since the 70’s, density functional theory (dft) has been one of the most used techniques in quantum physics to solve the schrödinger equation. the resolution of this equation assumes a prominent role in the characterization of the electronic properties of the materials, with the use of wave functions and the energy of each electron. the computation method follows an iterative process, known as self consistent field (scf), to compute the electrons density from an initial set of wave functions. this iterative process successively recurs to the schrödinger equation until it reaches a self-consistence convergence. the scf computation uses qe, an open-source software package written in fortran 90, to determine the electronic structure of materials. this calculation is computationally very intensive, requiring an adequate support for parallelism to improve the computation per formance to reach the convergence point. the qe already has message passing interface (mpi) support for distributed memory systems and recently introduced support for shared memory parallelization with openmp. this dissertation presents alternative approaches to adequately install and configure qe in a compute cluster with distributed memory nodes, where each node contains one or more multicore devices sharing the same memory address space. these approaches suggest different parallelism strategies that will reflect performance improvements on simulations, when compared to an earlier study, conducted under the same experimental conditions. the testbed uses multicore and many-core processors from the search cluster to mea sure the impact of multi-process simulations with multiple threads per process, recently introduced in qe with hybrid parallelizations. using different case studies and through different installations and parameters config urations (number of pools), this work explored and aimed to reach an efficient execution environment for the simulations of tungsten diselenide (wse2) in the search cluster. the obtained results in the experimental tests aim to help physics communities to find the best performance environment for this type of simulations."
    ],
    0.3
  ],
  [
    [
      "the benefits of using mobile identification applications as substitutes for physical documents are obvious, whether these are university student cards, company employee identification cards, the citizen card or driving license. however, as these applications grow in popularity and complexity, new requirements and needs arise that need to be addressed without disturbing the normal behavior of the application. often the data needed to provide an authentication service is spread across multiple servers, which need to be integrated. this becomes more complicated and complex when an application provides more than one form of authentication (a driving license and a student card require data provided by different services). in this dissertation we are going to look for solutions that allow to develop an architecture that is prepared to integrate new services at runtime and allows the management of the system, maintaining its dynamic and independence from third parties, regardless of the technology and form of communication used by them. so, this dissertation presents the state of the art regarding the integration of multiple service providers and the design and implementation a proposed solution, using the wso2 products to do so. this process is performed in the context of the mobile id, that is a implementation of a mobile driving license based on the iso/iec 18013-5:2021.",
      "são cada vez mais evidentes os benefícios do uso de aplicações de identificação móvel como substitutos aos documentos físicos, sejam estes cartões de estudantes universitários, cartões de identificação de funcionários de empresas, o cartão de cidadão ou a carta de condução. no entanto, à medida que estas aplicações se tornam mais populares e mais complexas, surgem novas ex igências e necessidades que precisam de ser colmatadas sem perturbar o normal funcionamento da aplicação. muitas vezes os atributos necessários para fornecer um serviço de identificação encontram-se distribuídos por múltiplos servidores, que necessitam de ser integrados. isto torna-se mais complicado e complexo quando uma aplicação disponibiliza mais de uma forma de identificação (uma carta de condução e um cartão de estudante requerem dados fornecidos por multiplos e diferentes serviços). nesta dissertação vamos procurar soluções que permitam desenvolver uma arquitetura que esteja preparada para integrar novos serviços em runtime e permitir toda a gestão do sistema, mantendo a aplicação dinâmica e independente de entidades terceiras, independentemente da tecnologia e forma de comunicação usada pelo serviço. assim, nesta dissertação é apresentado o estado da arte relativamente à integração de múltiplos fornece dores de serviço e o design e implementação da solução proposta, utilizando os produtos do wso2 para fazê lo. todo este processo é realizado no contexto do mobile id, que é uma implementação da carta de condução digital baseada na iso/iec 18013-5:2021."
    ],
    [
      "in the last decades, the scientific community has produced huge amounts of publications about the most varied biomedical topics, making the search for relevant information a really difficult task for every researcher. some approaches have been followed to develop tools that can facilitate this process. for instance, pubmed implemented in 2017 a machine learning model to sort documents by their relevance. nevertheless, even the authors consider that their system would benefit from the implementation of a deep learning model, which for now needs more studies. in this context, a package called biotmpy1 was developed in this work, to perform document classification of biomedical literature using the python programming language. the package is divided into different modules to provide to the user functions to read documents in different formats, perform preprocessing and data analysis and to train, optimize and evaluate machine and deep learning models. our package also provides intuitive pipelines that can be easily adapted for the user needs, illustrating how to implement complex deep learning models. the developed package was applied to a dataset from a challenge of the biocreative forum, from 2019, about protein-protein interactions altered by mutations, an important topic for the advances related to precision medicine. using this dataset, it was possible to observe a slightly better performance of biowordvec pre-trained embeddings over glove, ”pubmed pmc” and ”pubmed ncbi” embeddings. also, with the evaluation of the developed models on the test set, we managed to overcome the challenge’s best submission, by using a model with biobert and a bidirectional lstm on top, resulting in a difference of 7.25% for average precision, 3.22% for precision, 2.99% for recall and 3.15% for the f1-score. also, a web server was developed to provide access to the best deep learning model trained in this work. the overall pipeline here developed can be applied to other case studies in different topics, provided there is a set of documents annotated as relevant and non-relevant, allowing to train the models.",
      "nas últimas décadas, a comunidade científica tem produzido uma enorme quantidade de publicações sobre os mais variados tópicos biomédicos, tornando a procura de informação relevante num processo complicado para qualquer investigador. alguma abordagem tem sido seguidas para desenvolver ferramentas que possam facilitar este processo. por exemplo, o pubmed implementou em 2017 um modelo de aprendizagem máquina para ordenar documentos pela sua relevância. contudo, os autores consideram que o seu sistema pode beneficiar com a implementação de um modelo de deep learning, o que para já necessita de mais estudos. neste projeto, foi desenvolvida um package chamado biotmpy para classificar documentos da literatura biomédica através da linguagem de programação python. este package é dividido em diferentes módulos para fornecer ao utilizador funções para ler documentos de formatos diferentes, realizar pré-processamento e análise de dados, e para treinar, otimizar e avaliar modelos de aprendizagem máquina. a plataforma também fornece pipelines intuitivas que podem ser facilmente adaptadas de acordo com as necessidades do utilizador, demonstrando como implementar modelos complexos de deep learning. o package desenvolvido foi aplicado a um conjunto de dados de um desafio do fórum biocreative, de 2019, acerca de interações proteína-proteína alteradas por mutações, um tópico importante para a área da medicina de precisão. usando este conjunto de dados, consegue-se observar um melhor desempenho dos biowordvec embeddings pré-treinados em relação a embeddings como glove, ”pubmed pmc” e ”pubmed ncbi”. com os modelos desenvolvidos, foi possível ultrapassar a melhor submissão do challenge, usando um modelo com biobert e uma lstm bidirecional acima, obtendo-se diferenças de 7.25% na precisão média, 3.22% na precisão, 2.99% no recall e 3.15% para o f1 -score. foi ainda desenvolvido um servidor web de forma a fornecer acesso ao nosso melhor modelo. a plataforma desenvolvida neste trabalho poderá ser aplicável a outros casos de estudo em diferentes tópicos, desde que exista um conjunto de documentos anotado como relevante ou não relevante, que permita treinar os modelos."
    ],
    0.02727272727272727
  ],
  [
    [
      "o tema principal abordado nesta dissertação é a classificação de imagens através de redes neuronais. o crescimento do estudo desta área da inteligência artificial permite a que, atualmente, os sistemas sejam mais eficientes. a forma mais ancestral da gestão dos parques de estacionamento passa pela colocação de sensores de movimento ou de presença nos lugares de estacionamento. tal organização leva a grandes despesas na aquisição e manutenção do material. a implementação de um sistema de gestão centralizado e com recurso a métodos inteligentes, além de diminuir as despesas dos materiais, uma vez que uma câmara vem substituir os vários sensores, também leva a um maior conforto por parte dos utilizadores ao nível da simplicidade em encontrar uma vaga no parque de estacionamento. a metodologia desta dissertação passa por implementar várias redes neuronais e decidir qual é a que obtém um maior número de previsões realizadas com sucesso. foram utilizados dois tipos de redes neuronais: as multi-layer preceptron (mlp) e as convolution neural networks (cnn). todas as redes em estudo foram alimentadas com o dataset criado na plataforma coppeliasim, onde são criadas simulações de imagens captadas por uma câmara num parque de estacionamento. os resultados dos testes realizados mostram que tanto as redes mlp como as redes cnn obtêm bons resultados no projeto implementado, comprovando que as imagens podem ser observadas como uma sequência de pixéis e, dessa forma, padronizadas. o presente trabalho constitui uma forte contribuição na crescente área de estudo das redes neuronais pois demonstra que, selecionando os parâmetros de rede adequados, é possível aumentar a sua eficiência. mais ainda, as redes mlp conseguem apresentar melhores resultados comparativamente às redes cnn, preparadas e desenhadas para a interpretação de imagens.",
      "the main topic addressed in this dissertation is image classification through neural networks. the growth of research in this area of artificial intelligence allows current systems to be more efficient. the most ancestral way of parking lot management involves the placement of motion or presence sensors in parking slots. such an organization leads to significant expenses in the acquisition and maintenance of equipment. the implementation of a centralized management system using intelligent methods not only reduces material costs, as a single camera replaces multiple sensors, but also provides greater comfort for users in term of simplicity in finding a parking slot. the methodology of this dissertation involves implementing several neural networks and determining which one achieves the highest number of successfully made predictions. two types of neural networks were used: multi-layer perceptron (mlp) and convolutional neural network (cnn). all the networks under study were fed with the dataset created in the coppeliasim platform, where the images simulated camera captured images in a parking lot. the results of the conducted tests demonstrated that both mlp and cnn networks yield favorable outcomes in the implemented project, confirming that images can be viewed as a sequence of pixels and then, associated with patterns can be associated with them. the present work constitutes a strong contribution to the field as these results, show that different neural networks can perform better when appropriate parameters are selected. moreover, mlp seem to be more effective when compared to image interpretation networks, like cnn."
    ],
    [
      "document archiving as a fundamental resource for operational efficiency and a resource for organizational and social memory has several inherent implications. in addition to the high number of documents produced and the consequent accumulation of paper, the greatest difficulty of this process is in the classification and determination of the final destination of the documents. the analysis based on the semantics of the document is a complicated process, leading to the elimination of important evidences and case-by-case unnecessary conservation. the portuguese public administration with the intent of bridging semantic interoperability has implemented policies that allows uniform representations, recognized by all parties involved in any process, regardless of the respective area of intervention. in conjunction with technological development, the clav initiative appears, promoting measures for de-materialization of processes and the consequent adoption of a electronic document management system. this project was born out of an initiative by dglab. with the objective of taking advantage of the standardization of public entities functions, creating a link between the entities and the archivist, aiming to classify and evaluate documentation efficiently. this dissertation has as main objectives the reformulation of the user interface, in a clear, concise, familiar and efficient way, both in use and development, and the creation of a notification system that allows different types of users to monitor the status of the document classification and evaluation process.",
      "o arquivo de documentos enquanto recursos fundamentais na eficiência operacional e recurso de memória organizacional e social, tem várias implicações inerentes. para além do elevado número de documentos produzidos e consequente acumular de papel, a maior dificuldade deste processo está na classificação e determinação do destino final dos documentos. a análise baseada na semântica do documento é um processo complicado, dá origem à eliminação de evidências importantes e à conservação casuística e desnecessária. a administração pública portuguesa com a pretensão de colmatar a interoperabilidade semântica têm implementado políticas que permitam representações, uniformemente reconhecidas por todas as partes intervenientes num qualquer processo, independentemente da respetiva área de intervenção. em conjunto com o desenvolvimento tecnológico as medidas de promoção para desmaterialização de processos e consequente adoção de sistemas de gestão documental electrónicos, surge a iniciativa clav. este projeto nasce de uma iniciativa da dglab. com o objetivo de tirar partido da uniformização das funções das entidades públicas e criar uma plataforma que funciona como elo de ligação entre as entidades e o arquivista, este projeto pretende classificar e avaliar a informação eficientemente. esta dissertação tem como principais objetivos a reformulação da interface do utilizador, de uma forma clara, concisa, familiar e eficiente tanto na de utilização como no seu desenvolvimento, e a criação de um sistema de notificações que permita aos diferentes tipos de utilizadores acompanharem o estado do processo de classificação e avaliação da documentação."
    ],
    0.3
  ],
  [
    [
      "sleep represents a fundamental role to our well-being and today, as sleep disorders become more and more common, there is a growing necessity to monitor our sleep quality daily. unobtrusive automatic sleep stage classification has made a tremendous breakthrough in this subject allowing regular users to monitor their sleep with day-to-day wearables, such as fitbit charge 2 tracker, contrary to the traditional manual sleep scoring based on polysomnography (psg). using cardiorespiratory signals to sleep stage has attracted increased attention as these signals can be obtained through unobtrusive techniques and have potential for continuous daily application. therefore, in this thesis, deep learning frameworks based on long-short-memory networks (lstms) and convolutional neural networks (cnns) are used to sleep stage classify, either just using respiratory effort signals, for example obtained from respiratory inductance plethysmography (rip), or using the combination of respiratory and cardiac features, often based on heart rate variability (hrv) calculated from electrocardiogram (ecg). the dataset used was the siesta dataset that contains a total of 294 subjects (588 psg recordings) of which 197 are healthy subjects, 51 suffer from obstructive sleep apnea syndrome (osa), and the remaining from a variety of sleep or sleep related disorders. the classification problem was divided in a three-class and four-class sleep stage classification problem. as for the results, it was obtained with respiratory data for three stages classification (wake, rapid eye-movement (rem) and non-rem stages) a cohen’s kappa (𝜅) of 0.46 for the overall pool of subjects (all), 0.50 for healthy subjects and 0.34 for osa subjects. for four stages classification (wake, rem, light sleep (n1/n2) and deep sleep (n3/n4) stages) it was obtained a cohen’s kappa (𝜅) of 0.40 for the subject pool containing all subjects (all), 0.44 for healthy subjects and 0.31 for osa. with cardiorespiratory data, for four stages classification, it was obtained a 𝜅 of 0.40 for the overall subject pool (all), 0.44 for healthy subjects and 0.30 for osa subjects. with three stages, a 𝜅 of 0.46 for all subjects, 0.51 for healthy and 0.32 for osa subjects. these results demonstrate that, with the developed frameworks, it is possible to achieve fairly good results as they are similar, in some cases moderately higher, to the current state-of-the-art but fail to generalize well, as significant differences can be found between subject types (all, healthy and osa).",
      "o sono representa um papel fundamental no nosso bem-estar. com o aumento de disturbios relacionados com o sono, mutio devido ao progresso técnlogico e à constante utilização de aparelhos eletronicos, existe a necessidade constante de monitorização da qualidade do mesmo. a classificação automática de estágios de sono de forma não intrusiva tem tido bastante impacto nesta matéria por permitir que utilizadores monitorizem de forma regular o seu sono através da utilização diária de “wearables”, como a pulseira fitbit charge 2, contrariamente ao método standard de classificação de estágios de sono baseado em polissomnografia (psg). a utilização de sinais cardiorrespiratórios para classificação de estágios de sono tem ganho muito relevo devido à fácil obtenção dos mesmos através de técnicas não-invasivas e com extenso potencial para utilização continua diária. portanto, nesta tese, modelos deep learning baseados em long-short-term-memory (lstms) e redes neuronais convolucionárias (cnns) serão utilizados para classificação de estágios de sono, com sinais respiratórios, obtidos, por exemplo, com pletismografia respiratória por indutância (rip) ou através da combinação de sinais respiratórios com sinais cardíacos baseados em variabilidade da frequência cardíaca (hrv) calculados através do eletrocardiograma (ecg). o dataset utilizado foi o siesta que contém 294 sujeitos (588 gravações psg), dos quais 51 sofrem de sindrome de apneia obstrutiva do sono (osa) e 94 são sujeitos saudáveis. o problema de classificação foi dividido em três classes e quatro classes de estágios de sono. foram obtidos, para o problema de classificação com três classes um kappa de cohen (𝜅) de 0.46 para o dataset all que contém todos os sujeitos, 0.50 para o dataset que contém apenas sujeitos saudáveis (healthy) e 0.34 para o dataset que contém sujeitos com osa. para o problema de classificação de quatro classes, foi obtido um 𝜅 de 0.40 com o dataset que contém todos os sujeitos (all), 0.44 com o dataset que contém apenas sujeitos saudáveis e 0.31 com o dataset que contém sujeitos com osa. quanto à classificação de estágios de sono com sinais cardiorrespiratórios, com quatro classes foi obtido um 𝜅 de 0.40 com o dataset que continha todos os sujeitos (all), 0.44 com o dataset que apenas continha sujeitos saudáveis e 0.30 com o dataset que apenas continha sujeitos com osa. para três classes, a classificação obtida com sinais cardiorrespiratórios foi um 𝜅 igual a 0.46 com o dataset que continha todos os sujeitos (all), 0.51 com o dataset que continha apenas sujeitos saudáveis (healthy) e 0.32 com o dataset que apenas continha pacientes com osa. estes resultados demonstram que, com os modelos desenvolvidos, é possível atingir resultados moderademente satisfatórios, e, em alguns casos, ligeiramente superiores aos apresentados no estado-da-arte. no entanto, este modelos não generalizam muito bem sendo possível observar diferenças significativas entre os tipos de sujeito (all, healthy, osa)"
    ],
    [
      "distributed agreement is a well known and researched problem, one whose solutions have vast application in distributed systems, as reaching agreement over a certain value or over the order of received messages is extremely important in many multi-agent contexts. approximate agreement has long been relegated to the sidelines compared to exact agreement, with its most notable application being clock synchronisation. other proposed applications stemming from control theory target multi-agent consensus, namely for sensor stabilisation, coordination in robotics, and trust estimation. several proposals for approximate agreement follow the mean-subsequence-reduce approach, simply applying different functions at each phase. however, taking clock synchronisation as an example, applications do not fit neatly into each generic algorithm’s definition: instead they require adapting their internals. our contribution is three-fold. first, we conduct a survey on approximate agreement and related algorithms, delineating their characteristics thoroughly. second, we identify additional configuration points, establishing a more general template of msr approximate agreement algorithms. we then show how this allows us to implement not only generic algorithms but also those tailored for specific purposes. finally, we propose a toolkit for making approximate agreement practical, providing classical implementations as well as allow these to be configured for specific purposes. we validate the implementation with classical algorithms and clock synchronisation.",
      "acordo distribuído é um problema bem conhecido e estudado, com soluções de vasta aplicabilidade em sistemas distribuídos, uma vez que atingir acordo sobre um certo valor ou sobre a ordem de mensagens recebidas é extremamente importante em muitos contextos multi-agente. acordo aproximado tem sido relegado a segundo plano em comparação a acordo exato, sendo a sua aplicação mais notória sincronização de relógios. outras aplicações propostas, provenientes da teoria de controlo, visam o consenso multi-agente, nomeadamente para a estabilização de sensores, coordenação robótica e estimativa de confiança. várias propostas de algoritmos de acordo aproximado seguem a abordagem mean-subsequence-reduce, simplesmente aplicando várias funções de aproximação em cada fase. no entanto, utilizando sincronização de relógios como exemplo, aplicações não encaixam perfeitamente na definição dos algoritmos genéricos: é necessário adaptá-los. a nossa contribuição é constituída por três partes. primeiramente, realizamos uma sondagem sobre acordo aproximado e algoritmos relacionados, delineando minuciosamente as suas características. em segundo lugar, identificamos pontos de configuração adicionais, estabelecendo um esqueleto de algoritmos de acordo aproximado msr. demonstramos então como é que isto nos permite implementar não só algoritmos genéricos, como também aqueles adaptados a propósitos específicos. finalmente, propomos um conjunto de ferramentas para passar acordo aproximado à prática, incluindo implementações de algoritmos clássicos, bem como a possibilidade de implementar mais algoritmos para propósitos específicos. validamos então a implementação com algoritmos clássicos e com sincronização de relógios."
    ],
    0.12857142857142856
  ],
  [
    [
      "digital portfolios (also known as e-portfolios) can be described as digital collections of artifacts, being both a product (a digital collection of artifacts) and a process (reflecting on those artifacts and what they represent). it is an extension of the traditional curriculum vitae, which tells the educational and professional milestones of someone, while the portfolio proves and qualifies them (e.g.: annually thousands of students finish a master degree on informatics, but only one has built vue, twitter or facebook – the portfolio goes beyond the cv milestones by specifying the person’s output throughout life and distinguishing them). e-portfolios augment this by introducing new digital representations and workflows, exposed to a community, being both a product and a process. this approach can be useful for individual self-reflection, education or even job markets, where companies seek talented individuals, because it expands the traditional cv concept and empowers individual merit. there have been many studies, theories, and methodologies related with e-portfolios, but transpositions to web applications have been unsuccessful, untuitive and too complex (in opposition to the cv format, which had success in various applications, for example linkedin). this project aims to study new approaches and develop an exploratory web/mobile application of this method ology, by exploring the potential of social networks to promote them, augmented by emergent web technologies. its main output is the prototype of a new product (a social network of e-portfolio) and its design decisions, with new theoretical approaches applied to web development. by the end of this project, we will have idealized a web infrastructure for interacting with networks of users, their skills, and communities seeking them. the approach to the development of this platform will be to integrate emerging technologies like webassembly and rust in its development cycle and document our findings. at the end of this project, in addition to the prototype of a new product, we hope to have contributed to the state of the art of web engineering and to be able to answer questions regarding new emerging web development ecosystems.",
      "os portfólios digitais (também conhecidos como e-portfolios) podem ser descritos como coleções digitais de artefatos, sendo tanto um produto (uma coleção digital de artefatos) quanto um processo (refletindo sobre esses artefatos e o que eles representam). é uma extensão do tradicional curriculum vitae, onde o primeiro conta os marcos educacionais e profissionais de alguém, enquanto que o segundo, o portfólio, comprova-os e qualifica-os (e.g.: anualmente milhares de alunos concluem graduações em informática, no entanto apenas um consebeu o vue, o twitter ou o facebook - o portfólio vai além dos indicadores quantitativos do cv, especificando e qualificando a produção da pessoa ao longo da vida e distinguindo-a). os e-portfolios expandem este conceito com a introdução de novas representações digitais e fluxos de trabalho, expostos a uma comunidade, sendo tanto um produto como um processo. esta abordagem pode ser útil para a autorreflexão individual, educação ou mesmo mercados de trabalho, onde as empresas procuram indivíduos talentosos, porque expande o conceito tradicional de cv e potencializa o mérito individual. existem muitos estudos, teorias e metodologias relacionadas com os e-portfolios, mas as transposições para aplicações web têm sido mal sucedidas, pouco intuitivas e muito complexas (em oposição ao formato cv, que tem tido sucesso em várias aplicações, por exemplo no linkedin). este projeto visa estudar novas abordagens neste domínio e desenvolver uma aplicação exploratória web/mobile que melhor exprima os e-portfolios, explorando o potencial das redes sociais para os promover em conjunto com tecnologias web emergentes. as principais produções esperadadas deste trabalho são um protótipo de um novo produto (uma rede social de e-portfolio) e documentar novas abordagens teóricas aplicadas ao desenvolvimento web. no final deste projeto, teremos idealizado uma infraestrutura web para interagir com redes de utilizadores, as suas competências e comunidades que os procurem. a abordagem ao desenvolvimento desta plataforma será integrar tecnologias emergentes como webassembly e rust no seu ciclo de desenvolvimento e documentar as nossas descobertas e decisões. no final deste projeto, para além do protótipo de uma plataforma, esperamos ter contribuido para o estado da arte da engenharia web e responder a questões sobre novos ecossistemas emergentes de desenvolvimento web."
    ],
    [
      "a sociedade atual está bastante dependente de sistemas de navegação por satélite. estes sistemas utilizam satélites para fazer a geolocalização de um dispositivo. um dos exemplos mais conhecidos de um sistema deste tipo é o famoso global positioning system (gps). devido à atenuação de sinais causada por materiais de construção, um sistema de posicionamento por satélites está limitado a espaços exteriores. um sistema de posicionamento interior tenta responder a este problema e usa um conjunto de dispositivos que permitem fazer o posicionamento de pessoas ou objetos em espaços interiores. esta área de estudo tem sido alvo de várias pesquisas nos últimos anos e, recentemente têm sido implementados em vários setores. por exemplo, na monitorização de idosos que vivem sozinhos, na gestão de material hospitalar, no seguimento de pessoas para fins de segurança e para uma melhor gestão de recursos em grandes armazéns. embora os sistemas de posicionamento interiores tenham evoluído significativamente nos últimos anos, existem poucos dispositivos móveis (tags) disponíveis para integração com os sistemas. além disto, as capacidades das tags que existem são limitadas, especialmente no que toca à sua comunicação com sistemas não proprietários. esta dissertação procura desenvolver e propor uma tag que possa responder a estes problemas.",
      "today’s society relies heavily on global navigation satellite systems (gnss). gnss are systems that use satellites to provide geo-spatial positioning. one example of such a system is the well-known gps. satellite-based positioning systems are limited to outdoor use due to the signal attenuation caused by construction materials and other physical objects inside buildings. this makes gnss unsuitable for locat ing entities in indoor or underground locations. an indoor positioning system (ips) may include a device (or set of devices) used to locate persons or objects in an indoor environment. the development of this technology has been the subject of years of research and development. in the past decade, positioning systems have been deployed in various fields, including monitoring individuals living alone, managing med ical equipment in hospitals, tracking people for security purposes, and better management of resources in large warehouses. even though indoor positioning technology has evolved significantly in recent years, only a few mobile positioning devices (tags) are available for integration. in addition, the capabilities of existing tags are limited, especially in communicating with open systems. this work aims to develop and propose a tag to address some of these issues."
    ],
    0.3
  ],
  [
    [
      "the worldwide web has dramatically evolved in recent years. web pages are dynamic, expressed by pro grams written in common programming languages given rise to sophisticated web applications. thus, web browsers are almost operating systems, having to interpret/compile such programs and execute them. although javascript is widely used to express dynamic web pages, it has several shortcomings and performance inefficiencies. to overcome such limitations, major it powerhouses are developing a new portable and size/load efficient language: webassembly. in this dissertation, we conduct the first systematic study on the energy and run-time performance of webassembly and javascript on the web. we used micro-benchmarks and real applications to have more realistic results. the results show that webassembly, while still in its infancy, is starting to already outperform javascript, with much more room to grow. a statistical analysis indicates that webassembly produces significant performance differences compared to javascript. however, these differences differ between micro-benchmarks and real-world benchmarks. our results also show that webassembly improved energy efficiency by 30%, on average, and show how different webassembly behaviour is among three popular web browsers: google chrome, microsoft edge, and mozilla firefox. our findings indicate that webassembly is faster than javascript and even more energy-efficient. our benchmarking framework is also available to allow further research and replication.",
      "a web evoluiu dramaticamente em todo o mundo nos últimos anos. as páginas web são dinâmicas, expressas por programas escritos em linguagens de programação comuns, dando origem a aplicativos web sofisticados. assim, os navegadores web são quase como sistemas operacionais, tendo que interpre tar/compilar tais programas e executá-los. embora o javascript seja amplamente usado para expressar páginas web dinâmicas, ele tem várias deficiências e ineficiências de desempenho. para superar tais limitações, as principais potências de ti estão a desenvolver uma nova linguagem portátil e eficiente em tamanho/carregamento: webassembly. nesta dissertação, conduzimos o primeiro estudo sistemático sobre o desempenho da energia e do tempo de execução do webassembly e javascript na web. usamos micro-benchmarks e aplicações reais para obter resultados mais realistas. os resultados mostram que webassembly, embora ainda esteja na sua infância, já está começa a superar o javascript, com muito mais espaço para crescer. uma análise estatística indica que webassembly produz diferenças de desempenho significativas em relação ao javascript. no entanto, essas diferenças diferem entre micro-benchmarks e benchmarks de aplicações reais. os nossos resultados também mostram que o webassembly melhorou a eficiência energética em 30%, em média, e mostram como o comportamento do webassembly é diferente entre três navegadores web populares: google chrome, microsoft edge e mozilla firefox. as nossas descobertas indicam que o webassembly é mais rápido que o javascript e ainda mais eficiente em termos de energia. a nossa benchmarking framework está disponível para permitir pesquisas adicionais e replicação."
    ],
    [
      "o serviço de urgência é uma das áreas hospitalares com maior afluência, onde a procura e o grau de complexidade são elevados e imprevisíveis. para além disso, o acesso é irrestrito e as exigências são crescentes, assim como a necessidade de gestão de recursos para evitar o colapso das instituições e diminuir os tempos de espera excessivos, que são das consequências mais preocupantes na área da saúde. este projeto surge da oportunidade de estágio e proposta de tema de dissertação/projeto académico, na empresa de consultoria tecnológica para a área da saúde, glintt healthcare solutions, sa. o principal objetivo, é o estudo teórico e desenvolvimento de um protótipo funcional de um sistema, tendo em vista a realização de recomendações/sugestões aos profissionais de saúde a nível hospitalar, mais concreta mente nos serviços de urgência. este sistema irá ter em conta, informação de utentes, como historial médico e estado de saúde (p.e. doenças crónicas, medicação, informações recolhidas na triagem, alergias conhecidas, etc.). de forma mais específica, a solução tecnológica proposta para dar resposta ao problema e contexto acima referido, contém várias etapas. num primeiro momento, é realizada uma investigação relativa aos softwares de urgência já existentes nos hospitais em diferentes contextos(nacional e internacional). seguidamente, perceber, no panorama português, onde se encontram os dados relevantes em contexto de urgência, para que possam ser recolhidos e posteriormente utilizados. numa etapa mais intermédia e após a definição dos requisitos essenciais, pretende-se a definição e conceção de uma arquitetura interoperável, que englobe os mesmos. é nesta fase que se procede à análise das abordagens a integrar na arquitetura, bem como ao estudo do funcionamento do motor de inferência, responsável por gerar as recomendações e sugestões destinadas a apoiar a tomada de decisão por parte do utilizador. a ideia, numa fase final, é que a arquitetura definida, seja implementada e posteriormente sujeita a avaliação e validação, por parte dos profissionais ligados à empresa. deste modo, este projeto visa otimizar a gestão de recursos na saúde, agilizando o atendimento e melhorando a eficiência, beneficiando tanto pacientes, como profissionais de saúde.",
      "the emergency service is one of the hospital areas with the highest affluence, where demand and the degree of complexity are high and unpredictable. in addition, access is unrestricted and demands are growing, as well as the need for resource management to avoid the collapse of institutions and reduce excessive waiting times, which are one of the most worrying consequences in the health area. this project arose from an internship opportunity and a proposal for a dissertation/academic project topic at glintt healthcare solutions, sa, a health technology consultancy company. the main objective is the theoretical study and development of a functional prototype system, with a view to providing recom mendations/suggestions to healthcare professionals in a hospital setting, specifically in the emergency services. this system will take into account patient information, such as medical history and health status (e.g., chronic illnesses, medication, information gathered during triage, known allergies, etc.). the technological solution proposed to respond to the problem and context mentioned above has seve ral stages. initially, research is conducted regarding existing emergency department software in hospitals in different contexts (national and international). subsequently, the goal is to identify where relevant data in the portuguese context is located for collection and later use. in an intermediate stage, after defining the essential requirements, the aim is to design an interoperable architecture that encompasses them. this phase involves analyzing the approaches to be integrated into the architecture, as well as studying the functioning of the inference engine responsible for generating recommendations and suggestions to support user decision-making. the ultimate idea is that, in the final stage, the defined architecture will be implemented and subsequently subject to evaluation and validation by professionals associated with the company. thus, this project aims to optimize healthcare resource management, streamline patient care, and enhance efficiency, benefiting both patients and healthcare professionals."
    ],
    0.3
  ],
  [
    [
      "based on the similarity between the categorial derivation of classical programs from their specification and the category theory approach to quantum physics, this dissertation aims at extending the laws of classical program algebra to quantum programming. in this context, the principles of the algebra of classical programs are applied to quantum programming, in order to verify the feasibility of creating correct-by-construction quantum circuits that can run on quantum devices available in the ibm q experience. the reversibility restrictions of quantum circuits are ensured by minimal complements. moreover, measurements are postponed to the end of recursive computations called “quantamorphisms” to avoid the collapse of quantum states. quantamorphisms are classical catamorphisms extended to ensure quantum reversibility. the derived quantamorphisms implement quantum cycles (vulg. for-loops) and quantum folds on lists. by kleisli correspondence, quantamorphisms can be written as monadic functional programs with quantum parameters. this enables the use of haskell, a monadic functional programming language, to perform the experimental work. the examples of the calculated quantum programs are simulated in haskell, quipper and qiskit and run on the quantum computers of the ibm q experience. the main conclusions of this work are that, while all the simulations produced correspond to the predicted results, running these programs on real quantum devices results in a significant amount of errors. as quantum devices are constantly evolving, it is likely that in the near future these devices will increase their reliability, allowing programs to run more accurately. the extension of the quantamorphism concept to more general input structures, such as finite trees, remains a challenge that is left for future work. also relevant will be the study of conditional quantum control without measurements, which will extend the scope of quantamorphisms as quantum circuit specifications.",
      "tendo como base a similaridade entre a matemática categorial para derivar programas a partir da sua especificação e a teoria categorial usada na física quântica, esta dissertação pretende estender as leis da álgebra de programas clássicos à programação quântica. nesse contexto, a dissertação trata de explorar o significado desses princípios e suas construções na programação quântica e verificar a viabilidade de as aplicar à criação de programas quânticos correctos que possam correr em dispositivos quânticos disponíveis no ibm q experience. as restrições de reversibilidades exigidas pela programação quântica são asseguradas por complemento mínimo, e para evitar o colapso dos estados quânticos a medição é adiada através de “quantamorfismos”, nome dado à extensão reversível do conceito clássico de catamorfismo. os quantamorfismos que se implementaram permitem correr ciclos-for quânticos e folds quânticos sobre listas. com base na correspondência de kleisli é possível escrevê-los como programas funcionais monádicos com parâmetros quânticos. para esse efeito recorre-se à linguagem de programação funcional haskell como base para o trabalho experimental. os exemplos dos programas quânticos calculados foram simulados em haskell, quipper e qiskit e correram nos computadores quânticos da ibm q experience. constata-se que, enquanto todas as simulações produzidas correspondem ao previsto, correr estes programas em máquinas reais resulta numa quantidade significativa de erros. como os dispositivos quânticos estão em constante evolução, é provável que num futuro próximo estes dispositivos aumentem a sua fiabilidade, permitindo que os programas corram de forma mais precisa. entre as questões que esta tese levanta inclui-se a extensão dos seus resultados a estruturas de entrada mais gerais, como por exemplo árvores, e estruturas de controlo condicionais que não efectuem medidas e que assim possam estender o âmbito do quantamorfismo como veículo de especificação de circuitos quânticos."
    ],
    [
      "testing graphic user interfaces (gui) involves, mainly, lengthy and expensive processes involving user testing. finding simpler and easier alternatives to use than these processes becomes an exciting proposal. this project presents an alternative to existing processes through the use of model-based testing - mbt. the mbt technique takes advantage of models that describe the correct operation of the system (for this project task models). the use of mbt may thus become a new approach to testing gui's, since the implemented gui is tested against the model that specifies it the correct behavior. all inconsistencies found during the tests will be treated as potential errors that must be corrected. this report describes the development of a prototype for an environment able to generate and execute test cases applying mbt to gui's.",
      "a realização de testes a interfaces gráficas (gui) envolve, maioritariamente, processos morosos e dispendiosos. encontrar alternativas mais simples e fáceis de utilizar do que estes processos torna-se uma proposta aliciante. este projeto apresenta uma solução alternativa aos processos já existentes através da utilização de casos de teste baseados em modelos (modelbased testing - mbt). esta técnica tira partido de modelos que descrevem o correto funcionamento do sistema (no caso particular do projecto modelos de tarefas). a utilização do mbt pode assim transformarse numa nova abordagem aos testes realizados sobre gui’s, uma vez que a gui implementada será testada contra o modelo especificado que contém o funcionamento correto desta. as incoerências encontradas nos testes apontam para potenciais erros que deverão ser corrigidos. este relatório descreve o desenvolvimento de um protótipo para um ambiente capaz de gerar, mutar e executar casos de teste para gui’s aplicando o mbt."
    ],
    0.0
  ],
  [
    [
      "a virtualização de funções de rede (nfv, network function virtualization) é uma das principais tecnologias impulsionadoras da quinta geração de redes móveis 5g, cujo objetivo é separar as funções de rede (nfs, network functions) do hardware. as nfs são virtualizadas sobre hardware comum, o que traz mais flexibilidade e escalabilidade às redes. esse paradigma tem como principal desafio a integração de tecnologias emergentes e o apoio aos casos de uso que o 5g deverá suportar. neste trabalho, fez-se a implantação e avaliação do desempenho do open source mano (osm) nas versões 7, 8 e 9. em termos de métricas funcionais, foram avaliadas as percentagens de resource footprint da máquina virtual onde o osm foi instalado, bem como as utilizadas pelas funções de rede virtual (vnfs, virtual network functions) no openstack, o gestor de rede virtual (vim, virtual infrastructure manager), utilizado. quanto às métricas operacionais, foi medido o atraso no processo de on-boarding (opd, on-boarding process delay), ou seja, o tempo necessário para que a imagem de uma vnf inicialize, e o atraso no processo de implantação (dpd, deployment process delay), que é o tempo necessário para que uma vnf seja configurada. além disso, foi feita uma comparação do osm com outras plataformas de gestão e orquestração (mano, management and orchestration) de codigo aberto em relação às suas especificações técnicas. para a implantação da plataforma osm foi criado um ambiente virtualizado adequado à realização do experimento, onde foram instaladas as três versões do osm (7, 8 e 9) e o openstack, versão microstack, sendo que cada versão do osm foi integrada com o openstack para realização do experimento operacional. foi possível realizar os experimentos para todas as versões. dos experimentos para a medição das métricas operacionais opd e dpd, verificou-se, de forma geral, que as versões 7 e 8 do osm apresentam desempenho semelhante e melhor que o da versão 9. também se verificou que o valor do opd tem uma tendência crescente com a complexidade davnf e com o incremento sucessivo de vnfs na infraestrutura de virtualização de redes (nfvi, network functions virtualization infrastructure).",
      "network function virtualization (nfv) is one of the main technologies driving the fifth generation of mobile networks (5g, 5th generation), whose objective is to separate the network functions (nfs, network functions) from the hardware. nfs are virtualized on common hardware, which brings more flexibility and scalability to networks. this paradigm has as its main challenge the integration of emerging technologies and support for the use cases that 5g will have to support. in this work, the implementation and performance evaluation of open source mano (osm) in versions 7, 8 and 9 was carried out. in terms of functional metrics, the percentages of resource footprint of the virtual machine where osm was installed were evaluated, as well as as used by the virtual network functions (vnfs, virtual network functions) in openstack, the virtual network manager (vim, virtual infrastructure manager), used. as for the operational metrics, the delay in the on boarding process (opd, on-boarding process delay) was measured, that is, the time required for the image of a vnf to initialize, and the delay in the deployment process (dpd, deployment process delay), which is the time required for a vnf to be configured. in addition, a comparison was made between osm and other open source management and orchestration platforms (mano, management and orchestration) in terms of their technical specifications. for the implementation of the osm platform, a virtualized environment suitable for carrying out the experiment was created, where the three versions of osm (7, 8 and 9) and openstack, microstack version were installed, with each version of osm being integrated with openstack for carrying out the operational experiment. it was possible to carry out the experiments for all versions. version 8 proved to be more stable because once it was operational there was no need to restart its services. from the experiments to measure the operational metrics opd and dpd, it was found, in general, that versions 7 and 8 of osm present similar and better performance than version 9. it was also verified that the value of the opd has a growing trend with the complexity of vnf and the successive increment of vnfs in the network virtualization infrastructure (nfvi, network functions virtualization infrastructure)."
    ],
    [
      "nos dias que correm, é cada vez mais frequente o recurso à tecnologia para a resolução de problemas nas mais diversas atividades, nos mais variados setores. o setor da educação não é, pois, uma exceção. nesse setor, um dos aspetos que tem vindo a obter alguma relevância aborda a temática dos its — intelligent tutoring systems, apesar da timidez das diversas aproximações realizadas, em especial ao nível do ensino de nível académico. atualmente, está em curso na universidade do minho o desenvolvimento de um its, denominado leonardo, cujo objetivo principal é o auxílio dos alunos no seu processo de aprendizagem e formação. este sistema pretende fornecer um acompanhamento personalizado ao utilizador, tanto em termos do seu processo de formação, como, posteriormente, em termos do processo de avaliação correspondente. nesta dissertação apresenta-se o desenvolvimento de um sistema de profiling para apoio ao processo de avaliação, que possibilita a construção do perfil do utilizador (do estudante) à medida que este interage com o sistema ao longo do tempo. este sistema de profiling representa um dos módulos do sistema leonardo e, através das devidas interações com os restantes módulos, serve de base de apoio a todas as decisões que são tomadas no sistema, uma vez que estabelece o perfil do utilizador pelo qual o sistema se deve reger ao adaptar os conteúdos a disponibilizar.",
      "nowadays, technology is increasingly used to solve problems in a wide range of activities, in a wide range of sectors. the education sector is therefore not an exception. in this sector, one of the aspects that has come to some relevance is the subject of its — intelligent tutoring systems, in spite of the timidity of the different approaches made, especially at the level of academic level teaching. currently, the university of minho is developing an its, called leonardo, whose main objective is to help students in their learning and training process. this system intends to provide a personalized follow-up to the user, both in terms of their training process and, subsequently, in terms of the corresponding evaluation process. this dissertation presents the development of a profiling system to support the evaluation process, which allows the construction of the user's (student) profile as it interacts with the system over time. this profiling system represents one of the modules of the leonardo system and, through appropriate interactions with the other modules, serves as the basis for all the decisions that are made in the system, as it establishes the user profile by which the system should govern itself to adapt the content to be made available."
    ],
    0.3
  ],
  [
    [
      "o classificação e avaliação de processos da administração pública (clav) é um projeto nacio nal financiado pelo simplex e que visa a classificação e a avaliação de toda a documentação circulante na administração pública (ap) portuguesa. a ontologia atual que suporta o clav foi desenvolvida de uma forma incremental (patchwork) e a certa altura não houve o cuidado e o formalismo necessário. este projeto teve duas componentes. uma teórica onde se especificou uma nova ontologia web ontology language (owl) seguindo os padrões de nomenclatura findability, accessibility, interoperability, and reusability (fair), e em inglês. e uma prática, que consistiu em criar de raiz uma application programming interface (api) de dados sobre esta ontologia, onde se implementou cerca de oitenta rotas que no seu todo constituem um serviço autónomo (houve um reaproveitamento da api atual mas foi necessário recodificar algumas rotas, criar novas e alterar o output de outras). foi criada também a documentação da api em swagger de modo a haver uma fácil compreensão da mesma. ainda no contexto deste projeto, foi desenvolvido um novo migrador que transfere a informação das folhas de recolha de dados usadas atualmente para a nova ontologia especificada.",
      "clav is a national project financed by simplex that aims to classify and evaluate all the current documentation in the portuguese public administration (ap). the current ontology that supports clav was developed in an incremental way (patchwork) and at some point there was no care and formalism necessary. this project had two components. a theoretical one, where it was specified a new owl ontology, using the fair naming patterns, and in english. and a practical one, that consisted in creating from scratch an api about this ontology, implementing about eighty routes which as a whole will constitute an autonomous service (the current api was reused, however recoding some routes and creating new ones and changing others output was necessary). the api documentation was also created, using swagger, so that it can be easily understood. also in the context of this project, it was developed a new migrator that transfers the information from the currently used data collection sheets to the new specified ontology."
    ],
    [
      "globalmente, 310 milhões de cirurgias são realizadas a cada ano e existe uma probabilidade de 2% a 5% de infeções do local cirúrgico. problemas relacionados com a esterilização de equipamentos são uma das razões. estas infeções impactam negativamente a saúde física e mental do paciente comprometendo a sua qualidade de vida. as cirurgias assistidas por computador estão a ajudar os cirurgiões a realizar operações mais seguras e a permitir aos pacientes menos tempo de recuperação. no entanto, este meio de interação geralmente depende de dispositivos de contato físico, como rato e teclado, que expõem a sala de cirúrgica a condições assépticas. o leap motion ultrapassa o problema dos dispositivos físicos uma vez que não precisa de nenhum tipo de interação física. esta dissertação tem como objetivo conceber, desenvolver e validar uma abordagem de interação homem-computador intuitiva e sem contacto, baseada no reconhecimento automático de gestos manuais através do leap motion, seguindo uma conceção centrado no utilizador. para tal, foi primeiramente realizado um protocolo junto dos utilizadores finais para determinar que gestos eram mais intuitivos. posteriormente, foram criados dois grandes datasets (um de imagens da mão e um com características da mão) para alimentar modelos de inteligência artificial que pudessem reconhecer os gestos manuais de qualquer pessoa. o melhor modelo desenvolvido, com 96.25% de precisão nos dados de teste, foi baseado no algoritmo support vector machine e foi, de seguida, integrado na ferramenta de reconhecimento de gestos manuais que através das previsões do modelo, executa a respetiva ação no ecrã, removendo a necessidade de periféricos com contacto físico. a partir de uma validação preliminar realizada junto de voluntários da universidade do minho e uma validação clínica realizada junto de cirurgiões do hospital trofa saúde braga centro, verificou-se que os utilizadores demoram mais tempo a realizar o mesmo conjunto de tarefas com a ferramenta de deteção de gestos manuais do que com o uso tradicional do rato. contudo, foi possível observar que há uma curva de aprendizagem da ferramenta e que estes tempos diminuem com a experiência. por fim, o system usability scale, que é um teste padronizado de avaliação de usabilidade, revelou que a aplicação desenvolvida atinge um resultado de 76.67 ± 9.86, porém há uma perceção de usabilidade maior na validação preliminar do que na validação clínica (67.5 ± 6.37). através de uma última questão aberta pôde-se ainda perceber que a sensibilidade do cursor é o que precisa de mais atenção e constitui o ponto principal do trabalho futuro, juntamente com melhorias na interface gráfica.",
      "globally, 310 million major surgeries are performed each year, and there is a 2% to 5% chance of surgical site infections. problems related to equipment sterilization are one of the reasons. these infections negatively affect the patient’s physical and mental health, comprising their quality of life. computer-assisted surgeries are helping surgeons perform safer surgical interventions and allowing patients to have shorter recovery times. however, this means of interaction usually relies on physical contact devices, such as a mouse and a keyboard, which exposes the operating room to aseptic conditions. the leap motion overcomes the problem of physical devices as it does not require any kind of physical interaction. this dissertation aims to design, develop, and validate an intuitive, touch-free human-computer interaction approach based on the automatic recognition of hand gestures through leap motion, following an end-user-centred design. to achieve this, a protocol was carried out with end-users to determine which gestures were most intuitive. subsequently, two large datasets were created (one with hand images and one with hand features) to feed artificial intelligence models that could later recognise anyone's hand gestures. the best model developed, with 96.25% accuracy on the test data, was based on the support vector machine algorithm and was then integrated into the hand gesture recognition tool, which using the model's predictions, performs the corresponding action on the screen, removing the need for peripherals with physical contact. preliminary validation with volunteers from the university of minho and clinical validation with surgeons from the trofa saúde braga centro hospital showed that users take longer to perform the same set of tasks with the hand gesture recognition tool than with the traditional mouse control. however, it was possible to observe that there is a learning curve for the tool and that these times decrease with experience. finally, the system usability scale, which is a standardised usability evaluation test, revealed that the application developed achieves a score of 76.67 ± 9.86, but there is a greater perception of usability in the preliminary validation than in the clinical validation (67.5 ± 6.37). a final open question also revealed that the cursor sensitivity is what needs more attention and is the focus of future work, along with improvements to the graphic interface."
    ],
    0.06666666666666667
  ],
  [
    [
      "atualmente, os sistemas críticos estão cada vez mais presentes no nosso dia-a-dia, fazendo aumentar a necessidade de os assegurar cada vez mais e reduzindo o risco de acidente ou falha. a industria espacial e automóvel são exemplos de indústrias que usam esses sistemas e que necessitam de os ver assegurados. consequentemente, têm de ser tomadas medidas para garantir a segurança de um sistema ao nível de software e hardware. a injeção de falhas é uma das respostas a esse problema, fazendo uso das suas diferentes técnicas para poder avaliar e validar sistemas críticos. a injeção de falhas pode ser considerada uma técnica de teste ao software, onde as falhas podem ser injetadas ao nível do software ou hardware e cujos resultados podem ser monitorizados de forma a avaliar como é que o sistema reagiu a tais falhas. scan-chain implemented fault injection é a técnica de injeção de falhas que proporciona uma maior acessibilidade, observabilidade e controlabilidade. com esta técnica, os níveis de hardware e de integração de sistemas podem ser validados. o csxception® é um ambiente de injeção de falhas automatizado desenvolvido pela critical software s.a para avaliar e validar sistemas críticos. a sua arquitetura é dinâmica e baseada em plug-ins de injeção de falhas. devido à crescente presença dos microcontroladores arm® cortex-m3 na industria automóvel, surgiu a necessidade de criar um novo plug-in de injeção de falhas para o csxception®. assim, o objectivo principal desta dissertação de mestrado é o desenvolvimento de um novo plug-in de injeção de falhas para o csxception®, que permita injetar falhas em microcontroladores arm® cortex-m3, contextualizar o novo plug-in com a norma iso-26262 e utilizar um caso de estudo para mostrar alguns dos resultados obtidos.",
      "nowadays, critical systems are much more present in our daily life, increasing the need to ensure that these systems are becoming safer and thus reducing the risk of accident or failure. the space and automotive industry are examples of industries who use these systems and need to see them insured. therefore, actions need to be taken to guarantee the safety of a system, both at software and hardware levels. fault injection is one of the answers to that specific problem, making use of its different techniques in order to respond to the critical system validation and evaluation. fault injection can be considered as a testing technique, where faults are injected in the hardware or software levels and whose results are monitored in order to evaluate how the system handles such faults. scan-chain implemented fault injection is a fault injection technique that provides more reachability, observability and controllability. with this technique, the hardware-level and system-integration validation can be guaranteed. csxception® is an automated fault injection environment that validates and evaluates critical systems. developed by critical software, s.a., the csxception®'s architecture is dynamic and based on fault injection plug-ins. with the increasing presence of cortex-m3 microcontrollers on the automotive industry, a new plug-in for csxception® needs to be developed. thus, the main goal of this master dissertation is the development of a new fault injection plug-in for csxception® that allows the user to inject faults into arm® cortex-m3 microcontrollers, to contextualize the new plug-in with the iso-26262 safety standards and to use a case study to show some of the obtained results."
    ],
    [
      "throughout the years, deep learning has proven to be an excellent technology to solve problems that would otherwise be too complex. furthermore, it has shown great success in the area of medical imaging, especially when applied to segmentation of brain tissues. as such, this dissertation explores a possible new approach, using deep artificial neural networks to perform spatial normalization on brain mri studies as well as classify using brain mri studies regarding their state of brain atrophy. spatial normalization of magnetic resonance images by tools like the fsl, or spm turned out to be inefficient for researches as they need too many resources to achieve good results. these resources include, for example, wasted human and computer time when executing the commands to normalize and waiting for the process to finish, this can take up to several hours just for one study. therefore, a new approach was needed, a faster and easier way to normalize the mri studies. to do so, deep artificial neural networks were used by creating a python program to deal with said studies in much less time. this program should free the researchers’ time for other more relevant tasks and help reach conclusions faster in their studies when trying to find patterns between the analysed brains. several architectures were tried, having better results with u-net based architecture as well as gan architecture. at the end, the model couldn’t learn correctly all the brain features to be changed in any of the approaches but showed great potential. even though the final model did achieve the correct shape it could not yet achieve the final normalization. with some more time invested in perfecting the models, these could, in the future, learn to correctly perform the final normalization and allow the researchers to perform it in less than 10 seconds per exam instead of hours. regarding the brain atrophy models, the models showed some potential too as the predictions were partially correct. with more data, and less unbalanced, the model could probably learn correctly and output the expected results for all classes.",
      "ao longo dos anos, abordagens deep learning têm provado ser uma excelente tecnologia para resolver problemas que seriam complexos demais. além disso, demonstrou grande sucesso na área da imagem médica, principalmente quando aplicada em segmentação de imagens. como tal, esta dissertação explora uma possível nova abordagem, usando as redes neurais artificiais profundas para realizar a normalização espacial em estudos de rm do cérebro, bem como classificá las usando estudos cerebrais de rm em relação ao seu estado de atrofia cerebral. a normalização espacial dos estudos de ressonância magnética através de ferramentas como a biblioteca fsl acabou sendo pouco eficiente para uso na investigação, pois estas ferramentas precisam de muitos recursos para obter bons resultados. esses recursos incluem, por exemplo, desperdício de tempo humano e de computador ao executar os comandos para normalizar e aguardar a conclusão do processo; o que pode demorar várias horas, apenas para um estudo. portanto, uma nova abordagem é necessária, uma maneira mais rápida e fácil de normalizar os estudos de rm. para isso, foram utilizadas redes neurais artificiais profundas, criando um programa em python para lidar com os estudos em muito menos tempo. esse programa deve liberar o tempo dos investigadores para outras tarefas mais exigentes e ajudar a chegar a conclusões mais rapidamente nos seus estudos, ao tentar encontrar padrões entre os cérebros analisados. várias arquiteturas para o modelo foram testadas, obtendo melhores resultados com a arquitetura baseada em u-net e com a arquitetura gan. no final, o modelo não conseguiu aprender corretamente todos os detalhes do cérebro a serem alterados em nenhuma das abordagens, mas mostrou grande potencial. apesar de o modelo final ter atingido a forma correta, ainda não conseguiu a normalização final. com mais tempo investido no aperfeiçoamento dos modelos, estes poderiam, no futuro, aprender a executar corretamente a normalização final e permitir que os pesquisadores realizassem este processo em menos de 10 segundos por exame, em vez de horas. em relação aos modelos de atrofia cerebral, estes também mostraram algum potencial, pois as previsões estavam parcialmente corretas. com mais dados e menos desequilíbrio nos mesmos, o modelo provavelmente poderia aprender corretamente e gerar os resultados esperados para todas as classes."
    ],
    0.0
  ],
  [
    [
      "o processo de identificação geralmente é usado para facilitar transações comerciais e governamentais. apesar da existência de formas de identificação, maioritariamente presenciais, estas são pouco úteis para realização de negócios online. de forma a encarar este problema, vários governos de diferentes países estão a criar sistemas nacionais de identificação eletrónica (eid), isto é, uma coleção de tecnologias e políticas que permitem aos cidadãos provarem eletronicamente a sua identidade, ou um atributo da sua identidade para um sistema de informação. com o aparecimento da eid, um dos principais problemas que surgiu foi a insuficiente interoperabilidade entre os sistemas de identificação dos diferentes países adotantes, especialmente devido à falta de uma base jurídica comum. ao longo dos últimos anos surgiram soluções que, direta ou indiretamente, solucionam o problema de interoperabilidade, como é o caso do eidas. no entanto, o eidas é um regulamento seguido apenas pela união europeia, existindo também a necessidade de readaptar o sistema de identificação eletrónica dos diferentes estados membros para criar uma ligação entre os diferentes sistemas. como alternativa às soluções já existentes, o comité iso/iec jtc 1 estabeleceu uma norma, aplicada especificamente para a carta (ou licença) de condução, mas que pode ser adaptada para outros documentos de identificação. esta norma tem sido bem recebida pela maioria dos países e promete ser uma alternativa bastante viável para a implementação de um sistema de identificação eletrónica. assim, o principal foco desta dissertação é a definição de uma arquitetura aplicacional genérica, baseada na norma técnica iso/iec dis 18013-5. com base na arquitetura definida, como prova de conceito pretende-se criar de um sistema de identificação eletrónica configurável, que permita ao utilizador final implementar o seu sistema de identificação eletrónica, conforme as suas necessidades.",
      "identification is commonly used to help facilitate commercial and government transactions. while there are available traditional forms of face-to-face transactions, these forms of identification are less useful for conducting online business. to face this challenge, many governments are creating national electronic identification (eid) systems, a collection of technologies and policies that enable individuals to electronically prove their identity or an attribute about their identity to an information system. with the emergence of the eid, one of the main problems that came up was the insufficient cross-border interoperability between eid systems from different states/countries, especially due to a lack of common legal basis. during the last few years, some solutions had been presented that solves these issues. the eidas regulation is an example. however, this is a standard adopted by the european union and there is also a necessity to re-adapt the eid system from the different member states in order to create a link between the different systems. as an alternative to these solutions, the iso/iec jtc 1 committee created a standard, specifically applied to the driving license, but it can also be adapted to other identity documents. it has been welcomed by the majority of countries, and it promises to revolutionize the electronic identity sector. so, the main goal of this dissertation is to define an application architecture, based on the iso/iec dis 18013- 5 standard. as a proof of concept (poc), the architecture will be implemented to create a configurable electronic identification system, allowing the end-user to implement their own electronic identification system and adapt them, according to their requirements."
    ],
    [
      "spreadsheets are used for a diverse number of objectives, that range from simple applications to complete information systems. in all of these cases, they are frequently used as data repositories that can grow tremendously in size, and as the amount of the data grows, the frustration and challenge to withdraw information out of them also grows. this thesis project focuses on the problem of spreadsheet querying. speci cally, the objective is to meticulously and carefully study competing query languages, and proposing our very own expressive and composable query language to be used in spreadsheets, where intuitive queries can be de ned. this approach builds on a model-driven spreadsheet development environment, and queries are expressed referencing classsheet model entities instead of the actual data. furthermore, this language shall be integrated into the mdsheet framework, taking into account evolution mechanisms, auto-generation of models for query results, and shall rely on google's query function for spreadsheets.",
      "as folhas de c alculo s~ao utilizadas para diversos ns, desde aplica c~oes simples at e sistemas de informa c~ao completos. entre todos estes casos, s~ao frequentemente utilizadas para armazenar grandes volumes de dados, sendo que, a medida que o reposit orio cresce, a frusta c~ao e o desa o de recolher informa c~ao tamb em aumenta. o projeto desta disserta c~ao foca-se no problema da consulta e interroga c~ao de folhas de c alculo. especi camente, o objetivo e estudar de forma cuidada e meticulosa diversas linguagens de interroga c~ao existentes, e prop^or a nossa pr opria linguagem para ser utilizada em folhas de c alculo, que se caracteriza por ser uma linguagem expressiva, que possibilita a composi c~ao de interroga c~oes e a de ni c~ao das mesmas de forma intuitiva. a abordagem a utilizar passa pela utiliza c~ao de folhas de c alculo dirigidas por modelos, sendo as interroga c~oes expressas atrav es de entidades do modelo classsheet em vez de dados em concreto. al em disto, a linguagem desenvolvida ser a integrada no framework mdsheet, considerando diversos mecanismos de evolu c~ao, gera c~ao autom atica de modelos para os resultados de uma interroga c~ao, e ser a baseada na fun c~ao query desenvolvida pela google para a interroga c~ao de folhas de c alculo."
    ],
    0.0
  ],
  [
    [
      "when one comes across a new problem that needs to be solved, by abstracting from its associated details in a simple and concise way through the use of formal methods, one is able to better understand the matter at hand. alloy (jackson, 2012), a declarative specification language based on relational logic, is an example of an effective modelling tool, allowing high-level specification of potentially very complex systems. however, along with the irrelevant information, measurable data of the system is often lost in the abstraction as well, making it not as adequate for certain situations. the alloy analyzer represents the relations under analysis by boolean matrices. by extending this type of structure to: • numeric matrices, over n0 , one is able to work with multirelations, i.e. relations whose arcs are weighted; each tuple is thus associated with a natural number, which allows reasoning in a similar fashion as in optimization problems and integer programming techniques; • left-stochastic matrices, one is able to model faulty behaviour and other forms of quantitative information about software systems in a probabilistic way; in particular, this introduces the notion of a probabilistic contract in software design. such an increase in alloy’s capabilities strengthens its position in the area of formal methods for software design, in particular towards becoming a quantitative formal method. this dissertation explores the motivation and importance behind quantitative analysis by studying and establishing theoretical foundations through categorial approaches to accomplish such reasoning in alloy. this starts by reviewing the required tools to support such groundwork and proceeds to the design and implementation of such a quantitative alloy extension. this project aims to promote the evolution of quantitative formal methods by successfully achieving quantitative abstractions in alloy, extending its support to these concepts and implementing them in the alloy analyzer.",
      "quando se depara com um novo problema que precisa de ser resolvido, ao abstrair dos seus detalhes associados de forma simples e concisa recorrendo a métodos formais, é possível compreender melhor o assunto em questão. alloy (jackson, 2012), uma linguagem de especificação declarativa baseada em lógica relacional, é um exemplo de uma ferramenta de modelação eficaz, possibilitando especificações de alto-nível de sistemas potencialmente bastante complexos. contudo, em conjunto com a informação irrelevante, os dados mensuráveis são muitas vezes também perdidos na abstração, tornando-a não tão adequada para certas situações. o alloy analyzer representa as relações sujeitas a análise através de matrizes booleanas. ao estender este tipo de estrutura para: • matrizes numéricas, em n0 , é possível lidar com multirelações, i.e., relações cujos arcos são pesados; cada tuplo é consequentemente associado a um número natural, o que proporciona uma linha de raciocínio semelhante à de técnicas de problemas de otimização e de programação inteira; • matrizes estocásticas, permitindo a modelação de comportamento defeituoso e de outros tipos de informação quantitativa de sistemas de software probabilisticamente; em particular, é introduzida a noção de contrato probabilístico em design de software. tal aumento às capacidades do alloy, fortalece a sua posição na área de métodos formais para design de software, em particular, a caminho de se tornar um método formal quantitativo. esta dissertação explora a motivação e a importância subjacente à análise quantitativa, a partir do estudo e consolidação dos fundamentos teóricos através de abordagens categóricas de forma a conseguir suportar esse tipo de raciocínio em alloy. inicialmente, as ferramentas imprescindíveis para assegurar tal base são analisadas, passando de seguida ao planeamento e posterior implementação de tal extensão quantitativa do alloy. este projecto pretende promover a evolução dos métodos formais quantitativos através da concretização de abstracção quantitativa em alloy, estendendo a sua base para suportar estes conceitos e assim implementá los no alloy analyzer."
    ],
    [
      "the current exponential growth of data demands new strategies for processing and analyzing information. increased internet usage, as well as the everyday appearance of new sources of data, is generating data volumes to be processed by cloud applications that are growing much faster than available cloud computing power. these issues, combined with the appearance of new devices with relatively low computational power (such as smartphones), have pushed for the development of new applications able to make use of this power as a complement to the cloud, pushing the frontier of computing applications, data storage and services to the edge of the network. however, the environment in edge computing is very unstable. it requires leveraging resources that may not be continuously connected to a network and device failure is a certainty. the system has to be aware of the processing capabilities of each node to achieve proper task distribution as it may exist a high level of heterogeneity between the system devices. a recent approach for developing applications in the cloud, named function as a service (faas), proposes a way to enable data processing in these environments. faas services adhere to the principles of serverless architectures, providing stateless computing containers that allow users to run code without provisioning or managing servers. in this dissertation we present openflasks, a new approach to the management and processing of data in a decentralized manner across cloud and edge. we build upon these types of architectures and other data storage tools and combine them in a novel way to create a flexible system capable of balancing data storage and data analytics needs in both environments. in addition, we call for a new approach to provide task execution both in edge and cloud environments that is able to handle high churn and heterogeneity of the system. our evaluation shows an increase in the percentage of task execution success under high churn environments of up to 18%withopenflasks relatively to other faas systems. in addition, it denotes improvements in load balancing and average resource usage in the system for the execution of simple analytics at the edge.",
      "o atual crescimento exponencial de dados exige novas estratégias para processar e analisar informação. o aumento do uso da internet, assim como o aparecimento diário de novas fontes de dados, produz volumes de dados a ser processados por aplicações cloud que crescem a umamaior velocidade do que o poder de computação aí disponível. este problema, combinado com o surgir de novos dispositivos com poder computacional relativamente baixo (como smartphones), tem motivado o desenvolvimento de novas aplicações capazes de usar esse poder como complemento a cloud computing, expandindo a fronteira dos serviços de processamento e armazenamento de dados atuais para o limite da rede (edge). no entanto, o ambiente de edge computing é muito instável. requer a gestão de recursos que podem não estar continuamente conectados à rede e a falha de dispositivos é uma certeza. o sistema deve estar ciente das capacidades de processamento de cada dispositivo para obter uma distribuição de tarefas adequada, dado que pode existir um alto nível de heterogeneidade entre os dispositivos do sistema. uma abordagem recente para o desenvolvimento de aplicações de cloud computing, denominada function as a service (faas), propõe uma forma de permitir o processamento de dados neste tipo de ambientes. os serviços faas aderem aos princípios de arquiteturas serverless, fornecendo containers de computação que nãomantêmestado e que permitemaos utilizadores executar código sem a necessidade de instanciar e gerir servidores. nesta dissertação apresentamos openflasks, uma nova abordagem para a gestão e processamento de dados de forma descentralizada em ambientes cloud e edge. baseamo-nos neste tipo de arquiteturas, assimcomo outros serviços atuais de armazenamento de dados e combinamo-los de forma a criar um sistema flexível, capaz de equilibrar o armazenamento e as necessidades de análise de dados em ambos ambientes. além disso, propomos uma nova abordagem para possibilitar a execução de tarefas tanto em ambientes de edge como de cloud, capaz de lidar com o elevado dinamismo e heterogeneidade do sistema. a nossa avaliação mostra um aumento na percentagem de sucesso da execução de tarefas sob ambientes de elevado dinamismo de até 18% relativamente a outros sistemas faas. além disso, denotamelhorias na distribuição de carga e no uso médio de recursos do sistema para a execução de data analytics simples em ambientes edge."
    ],
    0.3
  ],
  [
    [
      "o ritmo da evolução tecnológica e a sua relação com o ser humano tem aumentado significativamente ao longo dos tempos, sendo que um dos ramos que mais impacto está a causar no quotidiano das pessoas é a inteligência artificial. a grande ascensão desta área é um fenómeno transversal a praticamente todos os setores da sociedade. esta dissertação enquadra-se no setor da justiça. ao longo dos anos, um dos principais problemas nos sistemas judiciais por todo o mundo é a mo rosidade na resolução dos processos judiciais. tendo por base esta problemática, as entidades governa mentais adotam cada vez mais reformas na área da justiça com recurso à tecnologia, desejando sistemas judiciais cada vez mais eficientes. neste sentido, esta dissertação tem como objetivo o desenvolvimento de uma solução capaz de extrair conhecimento a partir de dados jurídicos portugueses. esta solução é caracterizada por um conjunto de mecanismos, com recurso a técnicas de inteligência artificial, que vai desde a extração de dados até à realização de duas grandes experiências: análise de sentimentos e previsão da decisão. estes mecanismos permitiram gerar diversas informações e conhecimento. por um lado evidenciou-se a pouca relação entre a carga emocional dos textos dos juízes e a decisão, por outro destaca-se o desenvolvimento de modelos inteligentes capazes de prever a decisão com precisões de média de 76%, recorrendo ao conteúdo textual. além disso, ao longo de todo o processo, também foram extraídas outras informações, como por exemplo, as palavras relacionadas com a procedência e improcedência de acórdãos, a legislação que é geralmente citada em conjunto, entre outras. em paralelo desenvolveu-se um protótipo de uma dashboard com a apresentação de informações e conhecimento de alto nível sobre os dados.",
      "the pace of technological evolution and its relationship with the human being has increased over time. one of the branches causing the most significant impact on people’s daily lives is artificial intelligence. the remarkable rise in this area is a phenomenon that affects practically all sectors of society, but this project fits in the area of justice. over the years, one of the significant problems in all countries’ judicial systems is the delay in resolving judicial processes. based on this problem, governmental entities have been adopting more and more reforms in justice linked to technology, with a view to increasingly efficient judicial systems. this master’s dissertation aims to build up a solution capable of extract knowledge from portuguese juridical data. this solution is characterized by a group of mechanisms that use artificial intelligence tech niques that go from the data extraction until the implementation of two experiences: sentimental analysis and predict judicial decisions. these mechanisms allow generating information and knowledge. on the one hand, there was a lack of connection between the emotional side of the text from the judges and the decision of the verdict. on the other hand, an important matter was developing the innovative models capable of predicting with a precision of 76%, using textual content extracted. besides that, throughout the entire process, another type of information was removed, for example, the words relate to the court decisions, the legislation that was most of the time quoted together, among others. at the same time, was developed a dashboard prototype with the presentation of the high-level information and the knowledge of the data."
    ],
    [
      "organ transplantation is the best and often the only treatment for patients with end-stage organ failure. however, the universal shortage of deceased donors and the international variation in donation and transplantation activities result in a worrying situation that must be addressed. as in most countries, portugal has implemented donation programs to answer the increasing need for transplants with the objective to identify all the possible and potential donors admitted to hospitals. these donors constitute the largest share of organ donors in portugal, but identifying a patient that may progress to brain death could be a complex task and cadaveric organs must be transplanted in a short period of time in order to achieve satisfactory results. therefore, the urgent need of intelligent solutions that are able to support the decision-making process is crucial in critical areas as the organ transplantation is. the aim of this dissertation is firstly the knowledge acquisition on the potential organ donor criteria for further detection and secondly the design and implementation of a software platform to assist the inefficient process of identification of potential organ donors. this will result in an increase of control of the screening method and consequently optimize the workflow of the pre-transplantation process. accordingly, and after several meetings with the transplant team, a prior identification pattern was structured and used to characterize the development of the proposed solution, named organite. organite is defined as a system to support the transplantation process, based on business intelligence technologies. it is responsible for the collection, management, storage, and signaling of potential organ donors using information from the disparate health information systems to provide real-time tracking of patients and optimize the transplant team’s workflow. the developed platform is currently implemented at centro hospitalar do porto, hospital de santo antónio, epe and displays a steady and competent behavior providing consequently a way to have more control of the information needed for the decision-making process. as a result, the number of transplantation records at centro hospitalar do porto, hospital de santo antónio, epe are expected to show more profitable outcomes.",
      "o transplante de órgãos é a melhor e muitas vezes a única forma de tratamento para pacientes com casos de insuficiência terminal de órgãos. no entanto, a escassez universal de dadores falecidos e a variação internacional nas atividades de doação e transplantação resultam numa situação preocupante que deve ser abordada. tal como em muitos países, portugal tem implementado programas de doação para responder à crescente necessidade de transplantes com o objetivo de identificar todos os possíveis e potenciais dadores em morte cerebral internados em hospitais. estes dadores em morte cerebral constituem a maior parcela dos dadores de órgãos em portugal. contudo, identificar um paciente que pode evoluir para morte cerebral pode ser uma tarefa complexa e a transplantação de órgãos de cadáveres deve ocorrer num curto período de tempo a fim de alcançar resultados satisfatórios. portanto, a necessidade urgente de soluções inteligentes que sejam capazes de apoiar o processo de tomada de decisões é crucial em áreas tão críticas como a transplantação de órgãos é. o objetivo desta dissertação é, em primeiro lugar, a aquisição de conhecimento sobre os critérios de decisão de um potencial dador de órgãos para posterior deteção e, em segundo lugar, a conceção e implementação de uma plataforma de software para auxiliar o processo até agora ineficiente de identificação de potenciais dadores de órgãos. isto deverá permitir ter mais controlo sobre o método de deteção e, consequentemente, otimizar o fluxo de trabalho do processo de pré-transplantação. por conseguinte, e após várias reuniões com a equipa do gabinete coordenador de colheita e transplantação, um padrão de identificação prévio foi estruturado e utilizado para caracterizar o desenvolvimento da solução proposta, chamada organite. a plataforma organite é definida como um sistema para suportar o processo de transplantação, baseado em tecnologias de business intelligence. é, assim, responsável pela recolha, gestão, armazenamento e sinalização de potenciais dadores recorrendo a dados provenientes de vários sistemas de informação hospitalar para oferecer acompanhamento dos pacientes em tempo real e otimizar o fluxo de trabalho da equipa do gabinete coordenador de colheita e transplantação. a plataforma assim desenvolvida está implementada atualmente no centro hospitalar do porto, hospital de santo antónio, epe e apresenta um comportamento estável e eficiente proporcionando, consequentemente, uma forma de ter mais controlo sobre a informação necessária ao processo de tomada de decisão. como resultado, espera-se que o número de transplantações efetuadas no centro hospitalar do porto, hospital de santo antónio, epe apresente valores mais rentáveis."
    ],
    0.06666666666666667
  ],
  [
    [
      "this master’s thesis explores the advantages of using a quantum-based distance metric in a machine learning (ml) algorithm. it compares the performance of such a hybrid algorithm with an entirely classical algorithm. quantum machine learning (qml) has been growing in recent years. some studies suggest that qml may even provide a polynomial speed-up for data categorization compared to traditional ml. however, analyzing the benefits is not straightforward, as qml algorithms often rely on abstract, oracle (black-box) models that frequently rely on quantum random access memory (qram). furthermore, loading classical data onto quantum registers limits the applicability of qml, imposing a bottleneck. we used the swap test to measure the overlap between two quantum states to achieve our objective. then we replaced the classical distance metric in a distance-based machine learning algorithm with the quantum-based distance metric. our research showed that the swap test could be used as a distance metric in classical algorithms, despite the fact that the results obtained are not better than the classical metrics. in the final discussion, we present some ways that can improve the obtained results.",
      "esta dissertação de mestrado visa explorar as potenciais vantagens de usar uma métrica de distância baseada em quantum num algoritmo de machine learning (ml) e comparar o desempenho de um algo ritmo híbrido com o de um algoritmo totalmente clássico. quantum machine learning (qml) tem vindo a crescer nos últimos anos. alguns estudos sugerem que o qml pode avir a contribuir para uma aceler ação polinomial na categorização de dados em comparação com o ml tradicional landman [2021]. no entanto, analisar os benefícios não é direto, pois os algoritmos qml geralmente dependem de modelos abstratos baseados em oráculos (caixa preta) que frequentemente dependem de quantum random ac cess memory (qram). a aplicabilidade pode ser limitada devido á dificuldade imposta em carregar dados clássicos para registos quânticos. para atingir o nosso objetivo, usamos o swap test para medir a so breposição entre dois estados quânticos e, em seguida, substituímos a métrica de distância clássica num algoritmo de machine learning por uma métrica de distância baseada em quantum. a nossa pesquisa mostrou que o swap test pode ser usado como métrica de distância, em algoritmos clássicos, apesar de os resultados obtidos não serem melhores que as métricas clássicas. na discussão final, apresentamos algumas formas que podem melhorar os resultados obtidos."
    ],
    [
      "semantic web multilingual information has been growing in last years. consequently we assist to the need of creating multilingual ontologies from monolingual ones in order to allow for the semantic interoperability between applications in different natural languages. cross lingual enrichment is however often based in human interaction, which is a cumbersome and error-prone task. this document presents a study of already existing multilingual ontology creation meth ods in order to gain knowledge on what techniques exist, pointing the advantages and disadvantages of each one. from the knowledge obtained with the bibliographic research, a new web-based method was proposed and implemented. the detailed description of this development process is the main content of the present master’s dissertation. the multilingual ontology creation tool produces multilingual ontologies in two different ways. the first is based on translating certain elements of an input ontology and the other is done through a triangulation process between two existing multilingual ontologies. in the first approach, that involves translation, a logfile is also generated offering to the user the possibility to download it for future examination. this logfile contains detailed information concerning translation choices the system has made that can affect the quality of the translations. the tool introduced and discussed in this dissertation was tested throughout its the development. lastly, a web application was developed to host the tool in order to facilitate its access and use for the users.",
      "a informação multilingue da web semântica tem vindo a aumentar nos últimos anos. consequentemente, assistimos à necessidade de criar ontologias multilingues a partir de ontologias monolingues, a fim de permitir a interoperabilidade semântica entre aplicações em diferentes línguas naturais. no entanto, o enriquecimento multilingue baseia-se fre quentemente na interação humana, o que é uma tarefa complicada e propensa a erros. este documento apresenta um estudo dos métodos de criação de ontologias multilingues já existentes, com o objetivo de conhecer as técnicas existentes, apontando as vantagens e desvantagens de cada uma. a partir do conhecimento obtido com a pesquisa bibliográfica, foi proposto e implementado um novo método baseado na web. a descrição detalhada deste processo de desenvolvimento é o conteúdo principal da presente dissertação de mestrado. a ferramenta de criação de ontologias multilingues produz ontologias multilingues de duas formas diferentes. a primeira baseia-se na tradução de certos elementos de uma ontologia de entrada e a outra é feita através de um processo de triangulação entre duas ontologias multilingues existentes. na primeira abordagem, que envolve a tradução, é também gerado um logfile que oferece ao utilizador a possibilidade de o descarregar para análise futura. este logfile contém informação detalhada sobre as escolhas de tradução que o sistema fez e que podem afetar a qualidade das traduções. a ferramenta apresentada e discutida nesta dissertação foi testada ao longo do seu desenvolvimento. por fim, foi desenvolvida uma aplicação web para alojar a ferramenta, de modo a facilitar o seu acesso e utilização por parte dos utilizadores."
    ],
    0.3
  ],
  [
    [
      "esta dissertação surge no contexto das análises de dados gerados pelo lhc (large hadron collider), do esperado crescimento do volume de dados produzidos depois da atualização de 2013-2014 e do atual paradigma pseudo-paralelo destas aplicações no lip-minho (laboratório de instrumentação e física experimental de partículas, delegação minho). o trabalho surgiu como um estudo da utilização do proof (parallel root facilities) como plataforma para habilitar a extração automática de paralelismo nas aplicações de análises de dados do lip-minho. na consideração que as análises em estudo têm uma estrutura semelhante que é susceptível de ser paralelizada, partimos de um caso de estudo para a familiarização e experimentação do ambiente proof. face às dificuldades de adaptação da aplicação para utilização do sistema proof, desenvolvemos e testamos uma nova estrutura de classes, chamada event, que pode eliminar uma série de problemas na fase de desenvolvimento. esta proposta é suportada por um gerador de código esqueleto de aplicações deste tipo, o makeevent. os testes efetuados comprovam a possibilidade de usar a estrutura event como alternativa à api tselector, sem perda de desempenho e com a possibilidade de alcançar speedups superlineares no ambiente de cluster utilizado. no caso de códigos de análise de dados com alguma dimensão e complexidade, o processo de adaptação para um modelo compatível com o sistema proof pode ser uma tarefa morosa e exigente que pode não ser trivial. por este motivo, propomos como trabalho futuro a criação de uma biblioteca que trate das tarefas habituais no processo de análise dos dados. prevê-se também que a aplicação makeevent permita a seleção apenas dos branches utilizados na classe event, reduzindo significativamente o tempo de execução de análises de dados que carregam desnecessariamente todos os branches de uma tree. a conclusão a que chegamos é a da viabilidade da utilização da estrutura event, e consequentemente do makeevent, como uma alternativa possível para a extração de paralelismo automático das análises de dados em estudo, recorrendo à plataforma proof.",
      "this dissertation comes in the context of the analysis of data generated by the lhc (large hadron collider), the expected growth of the produced data volume after the machine upgrade in 2013-2014 and the current pseudo-parallel paradigm of these applications at lip-minho (laboratório de instrumentação e física experimental de partículas, minho delegation). the work came as a study of the use of proof (parallel root facilities) as a platform to enable automatic extraction of parallelism in data analysis applications at lip-minho. knowing that the analysis in study have a similar structure that is capable of being parallelized, we start from a case study for familiarization and testing the proof environment. given the difficulties of porting the application to use the proof system, we developed and tested a new class structure, named event, which can eliminate a series of problems in the development phase. this proposal was supported by a generator of skeleton code of applications of this type, makeevent. the tests performed show the possibility of using the event structure as an alternative to tselector api without loss of performance and with the possibility of reaching superlinear speedups in the cluster environment used. in the case of data analysis with considerable size and complexity, the process of adaptation to a level compatible with the proof system can be a time consuming and demanding task, most likely non trivial. for this reason, we propose as future work to create a library that handles the common tasks in the data analysis process. it is also envisaged that the makeevent application will allow one to select only the branches used in the event class, significantly reducing the execution time data analysis that needlessly load all the branches of a tree. the conclusion we reached is the viability of using the event structure, and consequently the makeevent, as a possible alternative for the automatic extraction of parallelism of data analysis in study, using the proof platform."
    ],
    [
      "não podendo deixar de aproveitar os benefícios da era tecnológica que vivemos a administração pública portuguesa caminha a passos largos para a digitalização de todo o seu processo organizacional. tal se explica através de diversos fatores que, apesar de díspares, se complementam e interligam entre si. assim, primeiramente se pensarmos no fator da proteção ambiental, a digitalização vai permitir uma redução da utilização de papel e simultaneamente uma redução de custos com o mesmo. por outro lado, esta garante uma maior agilização e otimização dos processos administrativos, assegurando, ainda, uma maior longevidade aos documentos. de forma a atingir estes objetivos nasceu a plataforma classificação e avaliação da informação pública (clav), plataforma essa que tem vindo a crescer ao longo dos últimos anos, que conta com vários colaboradores do departamento de informática da universidade do minho, sendo financiado pelo simplex, visando a classificação e avaliação de toda a documentação presente na administração pública portuguesa. como referido esta plataforma já está bem madura e como tal já conta com diversas funcionalidades para criação e manutenção dos instrumentos de classificação e avaliação, esta dissertação pretende acrescentar uma nova componente ao clav que permita não só a criar como também gerir os planos de preservação digital. para isso, foi necessário definir um modelo a seguir, tendo em conta todos os seus requisitos e invariantes, e adicionar as interfaces necessárias ao clav, com todas as funcionalidades e métodos necessários para a criação, importação e manutenção dos planos de preservação digital.",
      "being able to take advantage of the benefits of the technological age that we are experiencing, the portuguese public administration is making great strides towards the digitization of its entire organizational process.this is explained by several factors that, although disparate, complement and interconnect with each other.so, first, if we think about the environmental protection factor, digitization will allow a reduction in the use of paper and simultaneously a cost reduction with the same.on the other hand, it ensures greater speed and optimization of administrative processes, while also ensuring greater longevity to documents.to achieve these objectives, the clav platform was born, a platform that has been growing over the past few years, with several employees from the it department of the university of minho, being financed by simplex, aiming at the classification and evaluation of all the documentation present in the portuguese public administration.as mentioned, this platform is already very mature and as such it already has several functionalities for creating and maintaining the classification and evaluation instruments, this dissertation intends to add a new component to the clav that allows not only to create but also to manage the digital preservation plans.therefore, it was necessary to define a model to follow, bearing in mind all its requirements and invariants, and add the necessary interfaces to the clav, with all the functionalities and methods necessary for the creation, import, and maintenance of the digital preservation plans."
    ],
    0.3
  ],
  [
    [
      "cloud computing is a widely adopted model for the management of computing resources. elasticity, an important characteristic of cloud computing, is the ability to allocate and release computing resources according to demand. an elasticity controller has the responsibility of performing decisions regarding when resources are provisioned or released and which types of resource are necessary. developers have expectations of allocating the least amount of resources required and reaching optimal states using the least number of costly actions. therefore, the task of controlling elasticity is challenging, especially in cases where the elastic application is composed of components with complex dependencies among themselves. in this documentation, we introduce cecontroller, an elasticity controller for applications orchestrated with cloudify. cecontroller introduces a novel elasticity strategy that takes into account dependencies between components and differences between metric dimensions. cecontroller is evaluated in an environment created to test the controller, which includes the adaptation of a previously used application, and use of a load generation tool. finally, we discuss the results obtained using cecontroller in a web application and discuss the results.",
      "cloud computing é um modelo de gestão de recursos computacionais amplamente utilizado. elasticidade, uma importante propriedade de cloud computing, é a capacidade de alocar e libertar recursos computacionais segundo a procura. um controlador elástico tem a responsabilidade de realizar decisões acerca de quando recursos deverão ser provisionados ou libertados assim como averiguar os tipos de recursos que são necessários. programadores têm como objetivo a alocação da menor quantidade de recursos possível, assim como chegar a estados eficientes com o menor custo possível em ações efetuadas. desta maneira, realizar o controlo de elasticidade é uma tarefa complicada, especialmente em casos onde a aplicação elástica é composta por diversos componentes com dependências complexas entre estes. neste documento, introduzimos cecontroller, um controlador elástico para aplicações orquestradas com cloudify. cecontroller introduz uma nova estratégia de controle elástico, que leva em conta dependências entre componentes assim como diferentes dimensões de métricas. de seguida, apresentamos o processo de criação de um ambiente de testes para o controllador. incluindo a adaptação de uma aplicação previamente utilizada, e o uso de uma ferramenta para geração de carga. por fim, apresentaremos os resultados obtidos ao utilizar cecontroller em uma aplicação web e discutiremos os resultados."
    ],
    [
      "serviços web como facebook e reddit, entre outros, lidam com exigentes cargas de trabalho, processando milhões de pedidos por segundo. como tal, estes dependem do armazenamento em memória (cache) em quase todas as camadas da infraestrutura para fornecer alta responsividade, reduzir a carga na rede e no armazenamento, e diminuir os custos operacionais. no entanto, à medida que a complexidade das implementações de cache, a heterogeneidade das cargas de trabalho, e o número de inquilinos aumentam, também aumenta a dificuldade em gerir eficientemente estes sistemas de cache. nomeadamente, os atuais sistemas de armazenamento em cache como a cachelib, a poderosa biblioteca de cache da meta, gerem os recursos de forma rígida e monolítica, o que é adverso aos seus inquilinos e cargas de trabalho altamente dinâmicas. por um lado, as políticas manuais, rígidas, e propensas a erros, mesmo que ótimas pontualmente, acabam por condicionar a eficácia das caches à medida que a carga de trabalho varia. por outro lado, as implementações típicas unificam controlo e dados, e à medida que a sua base de código cresce, é progressivamente mais difícil de as manter e desenvolver. por último, a investigação tipicamente propõe caches com novas políticas de gestão que continuam a sofrer dos mesmos problemas devido ao seu desenho tradicional de cache e dos seus pressupostos. assim, esta dissertação propõe o holpaca, um novo desenho de cache programável e adaptável, para permitir uma configuração polivalente dos recursos, como partições e despromoção, entre outros, e ajustar o sistema em função do comportamento dos inquilinos e das cargas de trabalho. contrariamente aos trabalho anteriores, este desenho segue uma abordagem desacoplada que separa a lógica de controlo dos mecanismos de cache, para promover o desenvolvimento independente e mais fácil das políticas de gestão sem comprometer a responsividade percebida pelo utilizador, nem reiniciar o sistema todo. para demonstrar a aplicabilidade da nossa solução, prototipámos o holpaca como uma extensão da cachelib, combinando o seu desenho programável com o nosso desenho de armazenamento em cache adaptável com control desacoplado e garantias multi-inquilino. a nossa avaliação mostra ganhos promissores ao gerir dinamicamente os recursos de memória, nomeadamente para maximizar a taxa de sucesso global e respeitar múltiplas garantias de qualidade de serviço em ambientes multi-inquilino heterogéneos, tudo isto sem sobrecarga percetível.",
      "web services such as facebook and reddit, among others, handle demanding workloads, processing millions of requests per second. as such, they rely on caching at almost every layer of the infrastructure to provide high responsiveness, reduce the load on the network and storage, and lower operating costs. however, as the complexity of cache implementations, the heterogeneity of workloads, and the number of tenants increases, so does the difficulty of efficiently managing these caching systems. in particular, current caching systems such as cachelib, meta’s powerful caching library, manage resources in a rigid and monolithic way, which is adverse to their tenants and highly dynamic workloads. on the one hand, manual, rigid, and error-prone policies, even if optimal at times, end up conditioning the effectiveness of caches as the workload varies. on the other hand, typical implementations unify control and data, and as their code base grows, they become progressively more difficult to maintain and develop. finally, research typically proposes caches with new management policies that still suffer from the same problems due to the traditional design of caches and their assumptions. as such, this dissertation proposes holpaca, a new programmable and adaptable cache design, to allow a general-purpose configuration of resources, such as partitions and eviction, among others, and adjust the system according to the behavior of tenants and workloads. contrary to prior work, this design follows a decoupled approach that separates the control logic from the current caching mechanisms, to promote the independent and easier development of management policies without compromising the user-perceived responsiveness or restarting the entire system. to demonstrate the applicability of our solution, we prototyped holpaca as an extension of cachelib, combining its programmable design with our adaptable caching design with decoupled control and multi tenant guarantees. our evaluation shows promising gains by dynamically managing memory resources, namely to maximize the overall hit rate and respect multiple quality of service guarantees in heterogeneous multi-tenant environments, all without noticeable overhead."
    ],
    0.06666666666666667
  ],
  [
    [
      "road accidents are estimated to be the cause of millions of deaths and tens of millions of injuries every year. for this reason, any measure that reduces accidents' probability or severity will save lives. speeding, driving under the influence of psychotropic substances and distraction are leading causes of road accidents. causes that can be classified as human since they all come from driver errors. autonomous driving is a potential solution to this problem as it can reduce road accidents by removing human error from the task of driving. this dissertation aims to study artificial intelligence techniques and edge computing networks to explore solutions for autonomous driving. to this end, artificial intelligence models for detecting and tracking objects based on machine learning and computer vision, and edge computing networks for vehicles were explored. the yolov5 model was studied for object detection, in which different training parameters and data pre-processing techniques were applied. for object tracking, the strongsort model was chosen, for which its performance was evaluated for different combinations of its components. finally, the simu5g simulation tool was studied in order to simulate an edge computing network, and the viability of this type of network to aid autonomous driving was analysed.",
      "é estimado que os acidentes rodoviários sejam a causa de milhões de mortes e dezenas de milhões de lesões todos os anos. por esta razão, qualquer medida que diminua a probabilidade de acidentes ou que diminua a sua gravidade acabará por salvar vidas. excesso de velocidade, condução sob influência de substâncias psicotrópicas e distração no ato da condução são algumas das principais causas de acidentes rodoviários. causas essas que podem ser classificadas como humanas visto que são oriundas de um erro do condutor. a condução autónoma surge como solução para este problema. esta tem o potencial de diminuir acidentes rodoviários removendo o erro humano da tarefa da condução. esta dissertação teve como objetivo o estudo de técnicas inteligência artificial e redes computação de borda de forma a explorar soluções para a condução autónoma. para tal foram estuados modelos inteligência artificial de deteção e rastreamento de objetos com base nas áreas de aprendizagem máquina e visão por computador e redes de computação de borda para veículos. para a deteção de objetos foi estudado o modelo yolov5, no qual diferentes combinações de parâmetros de treino e técnicas de pré-processamento de dados foram aplicadas. para o rastreamento de objetos foi escolhido o modelo strongsort, para o qual foi avaliada a sua performance para diferentes combinações das suas componentes. por fim, foi estudada a ferramenta de simulação simu5g, de forma a simular uma rede de computação de borda, e foi feita uma análise sobre a viabilidade deste tipo de redes no auxílio à condução autónoma."
    ],
    [
      "nos últimos anos assistiu-se a uma mudança drástica na forma como o software é desenvolvido. os projectos de software de grande escala estão a ser construídos através de uma composição flexível de pequenos componentes possivelmente escritos em diferentes linguagens de programação e com os processos de deploy independentes – as chamadas aplicações baseadas em microsserviços. isto tem sido motivado pelos desafios associados ao desenvolvimento, manutenção e evolução de grandes sistemas de software, mas também pelo aparecimento da cloud e pela facilidade que trouxe em termos de escalabilidade horizontal, reutilização e flexibilidade na propriedade e no deploy. o crescimento dramático da popularidade dos microsserviços levou várias empresas a aplicar grandes refactorings aos seus sistemas de software. contudo, esta é uma tarefa desafiante que pode demorar vários meses ou mesmo anos. esta dissertação propõe uma metodologia capaz de transformar automaticamente aplicações desenvolvidas em java sob uma arquitetura monolítica em aplicações baseadas em microserviços. a metodologia proposta é direccionada para as aplicações que tiram partido da técnica orm para relacionar classes com as entidades da base de dados, através de anotações no código fonte. a nossa abordagem recebe como input o código fonte e uma proposta de microsserviços, e aplica técnicas de refactoring às classes para tornar cada microsserviço independente. esta metodologia cria uma api para cada chamada de métodos de classes que se encontram noutros serviços, e as entidades da base de dados também sofrem refactoring para serem incluídas no serviço correspondente. a metodologia proposta foi implementada através da construção de uma ferramenta que suporta o refactoring de aplicações desenvolvidas em java spring e que utilizam as anotações da jpa para o mapeamento entre as classes e as entidades. realizou-se um análise quantitativa e qualitativa em 120 projetos open-source aleatoriamente recolhidos do github. na avaliação quantitativa procurou-se perceber a aplicabilidade da metodologia e na analise qualitativa, através da execução de testes unitários, procurou-se avaliar se aplicação original e a aplicação baseada em microserviços gerada são funcionalmente equivalentes. os resultados são promissores sendo a metodologia capaz de realizar o refactoring em 69% dos projetos, sendo o resultado da execução dos testes unitários igual em ambas as versões dos projetos.",
      "in the last few years we have been seeing a drastic change in the way software is developed. largescale software projects are being assembled by a flexible composition of many (small) components possibly written in different programming languages and with independent deploy processes – the so-called microservice-based applications. this has been motivated by the challenges associated with the development, maintenance, and evolution of large software systems, but also by the appearance of the cloud and the ease it brought in terms of horizontal scaling, reusability and flexibility in ownership and deployment. the dramatic growth in popularity of microservice-based applications has pushed several companies to apply major refactorings to their software systems. however, this is a challenging task that may take several months or even years. this dissertation proposes a methodology to automatically evolve a java monolithic application into a microservice-based one. the proposed methodology is directed to the applications that take advantage of the orm technique to relate java classes to database entities, through annotations in the source code. the methodology receives as input the java source code and a proposition of microservices and refactors the original classes to make each microservice independent. the proposed methodology creates an api for each method call to classes that are in other services. the database entities are also refactored to be included in the corresponding service. the proposed methodology was implemented by building a tool that supports the refactoring of java spring applications that use jpa annotations for mapping between java classes and database entities. we performed a quantitative and qualitative analysis on 120 open-source projects randomly selected from github. in the quantitative evaluation we tried to understand the applicability of the methodology and in the qualitative analysis, through the execution of unit tests, we tried to evaluate if the original application and the application based on micro-services generated are functionally equivalent. the results are promising, with the methodology being able to refactor 69% of the projects, with the unit test results being the same in both versions of the projects."
    ],
    0.3
  ],
  [
    [
      "segundo um estudo realizado pela empresa international data corporation (idc) (adrian bridgwater, 2009), o mercado dos data warehouses tem tido um grande crescimento. cada vez mais as empresas procuram guardar todos os dados relacionados com o seu negócio, de forma a obter o máximo de conhecimento possível, podendo, assim, tomar melhor decisões relacionadas com o seu negócio. os data warehouses aparecem como uma ferramenta útil para suporte a processos de tomada de decisão. a capacidade dos data warehouses guardarem grandes quantidades de dados relativos ao negócio da empresa e permitirem aos agentes de decisão acederem de forma simples e fácil a esses dados, fazem deles uma ferramenta de eleição para o processo de tomada de decisão. a partir dos dados presentes num data warehouse pode-se efetuar relatórios e análises detalhadas. apesar de serem ferramentas muito poderosas, os data warehouses ditos convencionais ainda contêm limitações relativamente à capacidade de guardar e analisar dados com características geográficas. estas ferramentas capazes de lidar com este tipo de características são largamente utilizadas pelas empresas em muitos domínios de aplicação, como as telecomunicações ou a segurança, que com auxilio desta ferramenta conseguem descobrir qual o melhor local onde instalar antenas ou, então, por entidades governamentais, de forma a descobrir as zonas do seu país, com a maior criminalidade. ao longo desta dissertação, pretende-se entender o processo de construção de um data warehouse espacial desde a sua fase de levantamentos e análises de requisitos até à sua fase de implementação, sendo, por fim, transformado um data warehouse convencional num data warehouse espacial recorrendo a toda a informação obtida ao longo do processo.",
      "according to a study by international data corporation ( idc ) ( adrian bridgwater , 2009) , the market for data warehouses suffered a great growth. more and more companies are looking to save all data related to your business in order to get much knowledge as possible, being able to take better decisions related to your business. data warehouses appear as a useful tool to support decision-making processes. the capacity of data warehouses keep large amounts of data relating to the company business and enable decision-makers to access simple and easy way to such data, make them a tool of preference for decision-making process. from the data presented in a data warehouse one can make detailed reports and analysis. although they are very powerful tools, said conventional data warehouses have limitations regarding the ability to store and analyze data with geographical characteristics. these tools capable of dealing with this kind of features are widely used by companies in many application domains, such as telecommunications or security, with the help of this tool you can find out what the best place where to install antennas or else by government entities, in order to discover the zones of their country, with the highest criminality. during this dissertation it is intended to understand the process of building a spatial data warehouse from its phase of survey and analysis of requirements until its implementation phase, and finally turned a conventional data warehouse in a spatial data warehouse using all the information obtained during the process."
    ],
    [
      "a monitorização de redes é uma tarefa essencial na gestão e engenharia das redes de comunicações atuais. a conﬁguração de um sistema de monitorização de rede deve considerar as exigências da tarefa de rede a medir e os parâmetros de medição correspondentes, compartilhando algumas necessidades comuns na medição de infraestruturas, mas com especiﬁcidades de acordo com o tipo de serviço prestado e recursos envolvidos. por exemplo, medir o volume de tráfego da rede tem requisitos de medição distintos da deteção de ataques distributed denial-of-service. da mesma forma, os parâmetros e as exigências temporais para medir serviços de dados e vídeo são diferentes. assim, é essencial ter uma visão global dos distintos aspetos relacionados com a monitorização de serviços para uma melhor compreensão dos pontos-chave e promover a qualidade dos serviços prestados. a consciência do contexto é, portanto, um aspeto importante nos sistemas de monitorização de serviços auto-conﬁguráveis. o tema desta dissertação enquadra-se no contexto anteriormente descrito na medida em que vai ser estudado e deﬁnido um modelo de monitorização de serviços baseados em contexto. o objetivo principal prende-se com a identiﬁcação dos requisitos e arquitetura necessários num sistema de monitorização baseado em contexto. por conseguinte, pretende- se identiﬁcar e avaliar tecnologias apropriadas para lidar com o desenvolvimento de uma ontologia que visa apoiar um sistema de monitorização baseado em amostragem. por ﬁm, após o estudo dos métodos e tecnologias existentes, um sistema ontológico é desenvolvido utilizando a tecnologia selecionada que forma a base de um sistema de monitorização baseado em amostragem e contexto.",
      "network monitoring is an essential task in the management and engineering of current communications networks. the configuration of a network monitoring system should consider the measurement requirements of a network task and the corresponding measurement parameters. this may share common needs of the infrastructure measurement, but with specific characteristics according to the type of service provided and resources involved. for example, measuring the volume of network traffic has different measurement requirements from distributed denial-of-service attacks detection. similarly, the parameters and temporal requirements for measuring data and video services are different. thus, it is essential to have a global view of the distinct aspects related to service monitoring for a better understanding of its key points, and promote the quality of services provided. contextawareness is therefore an important aspect in the monitoring of self-configurable network services. the theme of this work falls within the context described above as it will be studied and defined a monitoring model of services based on context. the main objective is related to the identification of the requirements and architecture for a context-aware monitoring system. therefore, it is intended to identify and evaluate appropriate technologies to deal with the development of an ontology which aims at assisting a sampling-based monitoring system. finally, after studying existing technologies and methods, an ontological system is developed using the selected technology that forms the basis of a monitoring system based on sampling and context."
    ],
    0.0
  ],
  [
    [
      "nunca como hoje as opiniões e os sentimentos de cada ser humano desempenharam um papel tão fundamental no quotidiano da sociedade. a mineração de opiniões e de sentimentos providencia um grau de conhecimento bastante bom sobre o nível de satisfação que um dado negócio, um acontecimento, ou uma decisão estratégica pode gerar. estar na posse de um índice de satisfação sobre um determinado bem, serviço, ou acontecimento é dispor de condições que nos permitem alcançar situações de sucesso que podem ser usadas para melhorar o planeamento ou a estruturação de uma decisão ou mesmo de um negócio, face ao feedback que usualmente provocam. o conhecimento recolhido por este tipo de processo de mineração e, consequentemente, acumulado sobre a experiência que um certo cliente detém num determinado negócio, ou sobre a opinião (ou sentimento) que um certo indivíduo possui relativamente a um acontecimento de grande relevância político ou social permite definir novas estratégias de abordagem em determinados mercados. neste trabalho de dissertação realizámos a implementação de um sistema de mineração de opiniões e de sentimentos tendo como finalidade de definir e estabelecer um dado índice de satisfação, de forma a que seja possível angariar conhecimento útil sobre aspetos que potenciem a geração de opiniões e de sentimentos, potenciando novas oportunidades de negócio",
      "never like today, opinions and feelings of each human being, play a fundamental role on society's quotidian. attending to the development state at what we arrived on the informations technology level, the individual opinion assumes an importance that erstwhile never had. knowing what an individual thinks, feels, and desires, and consequently, decides, on the most variety of all life dimensions, is fundamental in a global context. knowing these feelings that could influence, directly and decisively, the most various behaves, is, on these days, of capital importance on the economics plan. with the right available tools, we can increase the knowledge about behaves that could and should contribute to the improvement of economic structures performance. opinion and feeling mining provides a knowledge level about the satisfaction level that a business, an event, or a strategical decision generates. being on possession of a satisfaction level about a certain good, service, or event is arranging the conditions that aim to the success and that could be used to better plan a business or a decision, against the feedback that these ones provoke. the collected knowledge, and, consequently, accumulated about the experience that a certain client has on a determined business, or about the opinion/feeling that a certain individual possesses relatively to a certain event of enormous relevance at the political and social levels, enable the acquirement of specific strategies of approach on determined markets. on the present dissertation we propose the implementation of an opinion and feeling mining system, generating like that the establishment of satisfaction indexes, so that we could obtain knowledge about the various aspects on cause, that generate the opinions and feelings, enabling by that way, the act of potentiate possible opportunities of business/businesses."
    ],
    [
      "nowadays, the high percentage of mobile devices equipped with gps and bluetooth, as well as the growing number of public wi-fi networks, makes the public environment surrounding us immensely rich in information. the advertising industry sees this as an opportunity to transform the way advertising is done in public by adopting increasingly pervasive and intelligent digital signage screen systems. the objective of this work is to develop an ad serving solution for a digital signage network that explores user profile information, as well as, audience screen activities to deliver targeted ads. the architecture of an ad server was conceptualized and developed with its own recommender system, offering a solution covering all different aspects of the ad serving process, including a web platform for campaign managing.",
      "hoje em dia, a elevada percentagem de equipamentos móveis equipados com gps e bluetooth, bem como o aumento de redes wi-fi, torna o ambiente que nos rodeia extremamente rico em informação. a industria de publicidade vê isto como uma oportunidade para inovar a maneira como a publicidade em espaços públicos é feita, adoptando sistemas de ecrâs publicos cada vez mais pervasivos. é neste contexto que surge o projecto instant places. o objectivo deste trabalho é o desenvolvimento de uma solução que permita servir anuncios na rede de ecrãs deste projeto, usando a informação disponibilizada por esta rede, sobre os seus utilizadores e ambiente em torno dos ecrãs, para direcionar os anuncios servidos à audiencia correta, com vista a obter um aumento na taxa de sucesso das campanhas de anúncios servidos pela rede."
    ],
    0.0
  ],
  [
    [
      "secure multiparty computation (abrv. mpc) is a group of techniques that enable multiple entities to compute some function on their private inputs. moreformally,itenablesasetofplayers{p1,...,pn},eachofthemholding private inputs xi, where i is the index of the player, to evaluate a function f(x1,...,xn) = (y1,...,yn) so that every player pi learns yi. multiparty computation has many application scenarios such as private auctioning, computation outsourcing, or private information retrieval. these and other applications make mpc a very appealing object of study, since it may be a solution to problems in different ﬁelds. the objective of this thesis is twofold. first, we aim to provide a comprehensive guide to the current state of multiparty computation protocols. wedosobyinspectingthetwocategorieswheremostgeneralfunctionality secure function evaluation protocols are inserted - boolean or arithmetic circuit evaluation. moreover, we show how to implement mpc protocols with current constructions, and what problems are inherently connected to the chosen representations. second, we document some of the design choices behind a partial implementation of a concrete secure arithmetic circuit evaluation protocol - the spdz protocol by damgård et al. [10]. we explain the protocol in general terms, and then go into the details of some subprotocols, namely those that were implemented.",
      "secure multiparty computation (abrv. mpc) é uma área que engloba um conjunto de técnicas que permitem que diferentes entidades avaliem uma função sem revelar os seus valores privados. formalmente, esta técnica permite a um conjunto de participantes {p1,...,pn}, que possuem inputs privados xi, onde i é o índice de cada participante,avaliaremafunção f(x1,...,xn) = (y1,...,yn) deformaaque cada participante pi obtenha unica e exclusivamente yi. multiparty computation destaca-se pelo facto de ser aplicável em diversas áreas como private auctioning, computation outsourcing ou private information retrieval. estas aplicações tornam mpc um tópico de bastante interesse, pois engloba a solução para problemas de diferentes ramos. esta dissertação é composta por duas partes. primeiro, disponibilizamos um manual de introdução ao tópico, com o objectivo de descrever o estado actual da tecnologia. com esse ﬁm, introduzimos os dois grandes ramos onde todos os protocolos de avaliação genérica se enquadram - avaliação de circuitos booleanos ou avaliação de circuitos aritméticos. a nossa descrição engloba também aspectos práticos, como os recursos existentes em termos de frameworks para desenvolvimento de protocolos multiparty. por ﬁm, descrevemos os problemas que estão associados à escolha de uma representação em vez de outra. na segunda parte, descrevemos algumas das decisões que foram tomadas aquando da implementação parcial do protocolo spdz de damgård et al. [10]. com esse objetivo, começamos por descrever o protocolo em termos gerais,e posteriormente explicamos o funcionamento em detalhe de alguns subprotocolos, nomeadamente os que foram implementados."
    ],
    [
      "in this project, we design and implement an iot streaming data ingestion pipeline for monetdb, using the distributed message queueing platform apache kafka. our objective is to leverage monetdb’s analytical power for iot data, expanding its ingestion capabilities to improve reliability and performance. the ingestion pipeline is put to the test with the real world maritime tracking system ais. we also evaluate monetdb’s current iot processing engine and compare it to other state-of-the-art engines, to appraise its functionalities and identify possible future improvements.",
      "neste projecto, conceptualizamos e implementamos uma pipeline para ingestão de dados streaming para o sistema de base de dados monetdb, utilizando o apache kafka, uma plataforma para message queueing distribuído. o nosso objectivo é utilizar o poder analítico do monetdb para dados iot, expandindo as suas capacidades e melhorando a confiabilidade e desempenho na ingestão de dados streaming. a pipeline é posta à prova com o sistema de tracking marítimo ais, demonstrando a sua aplicabilidade no mundo real. as funcionalidades de processamento de dados iot do monetdb são avaliadas e comparadas com outras plataformas de última geração para identificar desenvolvimentos futuros."
    ],
    0.3
  ],
  [
    [
      "in the past decade, machine learning has been heavily applied to automobile industry solutions, the most promising being development of autonomous vehicles. new mobility services are available today as alternatives to owning a car, like ride hailing and carsharing. high costs associated with the maintenance of the vehicle and the reduced rate of vehicle use throughout the day are some of the factors for the popularity of these services. car-sharing is self-service mode of transport that provides its members with access to a fleet of vehicles parked in various locations throughout a city. damages are expected to happen when vehicles are used and the required repair implies costs to fleet operators. systems able to detect these damages will promote a better use of these vehicles by vehicle users. vehicle damages result from impacts with other objects, for instance, other vehicles or structures of any kind and these impacts inflict deformations to the vehicle exterior structure. most of these impacts can be perceived or detected by the forces involved as result of the impact. anomaly detection is a technique applicable in a variety of domains, such as intrusion detection, fraud detection, event detection in sensor network or detection ecosystem disturbances. the objective of this thesis is the study and development of a semi-supervised intelligent system for detection and classification of vehicle impacts with an anomaly detection approach, using the accelerometer data, and following a strategy that would allow exploring a machine learning cycle. this thesis was developed under an internship in the company bosch car multimedia s.a, located in braga.",
      "na última década, machine learning tem sido extensamente utilizado em soluções na indústria automóvel, o mais promissor sendo o desenvolvimento de veículos com condução autônoma. novos serviços de mobilidade estão disponíveis hoje como alternativas à posse de um carro, como ride hailing ou car-sharing. os elevados custos associados à manutenção do veículo e a sua reduzida taxa de utilização ao longo do dia são alguns dos fatores que contribuem para a popularidade destes serviços. car-sharing é um modo de transporte self-service que fornece aos seus membros acesso a uma frota de veículos estacionados em vários locais duma cidade. danos são espectáveis de ocorrer quando os veículos são usados e a reparação necessária implica custos para os operadores da frota. sistemas capazes de detectar esses danos irão promover um melhor aproveitamento desses veículos pelos utilizadores dos veículos. os danos de veículos resultam de impactos com outros objetos como, por exemplo, outros veículos ou estruturas e esses impactos provocam deformações na estrutura externa do veículo. a maioria desses impactos podem ser compreendidos ou detetados pelas forças envolvidas do resultado do impacto. anomaly detection é uma técnica aplicável em uma variedade de domínios, como deteção de intrusões, deteção de fraude, deteção de eventos numa rede de sensores ou deteção de distúrbios no ecossistema. o objetivo desta dissertação foi o estudo e desenvolvimento de um sistema inteligente semi-supervisionado para detecção e classificação de impactos de veículos a partir de uma abordagem de anomaly detection, utilizando os dados de acelerómetro, e seguindo uma estratégia que permitisse explorar um ciclo de machine learning. esta dissertação foi desenvolvida no âmbito de um estágio na empresa bosch car multimedia s.a, situada em braga."
    ],
    [
      "a transformação digital é cada vez mais visível na nossa sociedade, e as transações realizadas entre diferentes entidades, sejam eles algum tipo de empresa ou organização, ou até mesmo cidadãos comuns, não escapam a esta nova era digital. para viabilizar este novo paradigma de identificação digital é necessário estabelecer uma base de confiança, de modo a que todos se sintam confortáveis para migrarem, com alguma segurança, para esta nova realidade. é em seguimento desta premissa que surge o regulamento eidas. este regulamento emitido pelo parlamento europeu estabelece as condições e conceitos que configuram as bases de confiança necessárias para concretização prática dos serviços de identificação eletrónica reconhecidos por todos os diferentes estados membros. com vista à definição de normas sob uma perspetiva mais técnica, surge o organismo etsi, o qual apresenta normas relativas a todos os elementos e serviços de confiança que integram e servem de fundamento para todo este ecossistema de identificação digital, como é o caso das assinaturas e selos eletrónicos qualificados(as). no que diz respeito à validação, foram emitidos standards com vista à normalização dos mais variados elementos, desde as próprias assinatura eletrónicas (ades), e alternativas de implementação prática para as mesmas (cades, xades, etc.), aos mais ínfimos detalhes tais como, o próprio procedimento a aplicar por parte da entidade prestadora do serviço de validação, assim como o relatório de validação emitido, protocolos de comunicação, políticas para validação, entre outros. no contexto desta dissertação, é efetuada uma implementação prática de uma prova de conceito do serviço qualificado de validação de assinaturas e selos eletrónicos qualificados, a par de uma aplicação cliente capaz de demonstrar as suas funcionalidades para alguns casos de uso. essa implementação foi precedida por um levantamento de requisitos a serem cumpridos pelo serviço de validação a implementar, tendo por base a documentação emitida pelo etsi nesse sentido. a justificação para esse processo de seleção prende-se com a existência de uma quantidade considerável de requisitos que incidem sobre questões fora do âmbito de uma prova de conceito (p.e. deploy, etc.) e que por isso, não lhe são aplicáveis. adicionalmente, foram implementados alguns serviços e funcionalidades secundárias com o intuito de proporcionar uma melhor experiência de utilização da aplicação, como é o caso do serviço de gestão de políticas de validação, que viabiliza a criação, pesquisa e obtenção de políticas de validação aplicáveis, em qualquer momento, à validação, e ainda a possibilidade de personalização da base de certificados de confiança a serem usados pelo processo de validação.",
      "the digital transformation is becoming increasingly visible in our society, and transactions carried out between different entities, be they some form of company or organization, or even ordinary citizens, are not exempt from this new digital era. to enable this new paradigm of digital identification, it is necessary to establish a foundation of trust so that everyone feels comfortable migrating to this new reality with some degree of security. it is in line with this premise that the eidas regulation emerges. this regulation issued by the european parliament establishes the conditions and concepts that form the necessary trust foundations for the practical implementation of electronic identification services recognized by all the different member states. with a view to defining standards from a more technical perspective, the etsi organization emerges, which presents standards relating to all the elements and trust services that make up and serve as the foundation for this entire digital identification ecosystem, such as qualified electronic signatures and seals. as for validation, standards have been issued to standardize various elements, from electronic signatures themselves (ades), and practical implementation alternatives for them (cades, xades, etc.), down to the smallest details, such as the procedure to be applied by the validation service provider, as well as the validation report issued, communication protocols, validation policies, among others. in the context of this dissertation, a practical implementation of a proof of concept for the qualified validation service of qualified electronic signatures and seals is carried out, alongside a client application capable of demonstrating its functionalities for some use cases. this implementation was preceded by a requirement analysis to be met by the validation service to be implemented, based on the documentation issued by etsi for this purpose. the justification for this selection process is due to the existence of a significant number of requirements that pertain to issues beyond the scope of a proof of concept (e.g. deploy, etc.), and are therefore not applicable to it. additionally, some secondary services and functionalities were implemented to enhance the user experience of the application; this includes the validation policy management service, which enables the creation, search, and retrieval of applicable validation policies at any time, as well as the option to customize the trusted certificate base (i.e. trust anchor) to be used in the validation process."
    ],
    0.06666666666666667
  ],
  [
    [
      "dispositivos móveis como smartphones e tablets estão a conquistar meritoriamente um espaço de destaque no nosso dia-a-dia, tanto a nível pessoal como profissional. simples atividades como ler um livro ou um jornal tornaram-se num ato muito mais cómodo a partir do momento que podemos recorrer a um destes dispositivos, tornandonos cada vez mais dependentes das facilidades que a tecnologia nos proporciona. na prática obstétrica, esta interação humano-computador faculta aos profissionais de saúde o acesso de forma ubíqua a diversas informações sobre os pacientes, fornecendolhes ferramentas que facilitam o acompanhamento profissional aos utentes, podendo revelar-se muito útil para o bom funcionamento do serviço. existem já algumas aplicações desenvolvidas no âmbito do auxílio à prática obstétrica, e apesar de algumas oferecerem funcionalidades interessantes, nenhuma oferece um conjunto de ferramentas robusto o suficiente para ser essencial no dia-a-dia de qualquer profissional de saúde da área. dada a especificidade aplicação, a impressão global é que ainda não existe uma aplicação que se apresente ao obstetra como uma ferramenta completa na prática clínica, sendo portanto o seu desenvolvimento uma mais-valia. antes de desenvolver uma aplicação para dispositivos móveis, é necessário dominar algumas linguagens de programação específicas – caso se pretenda desenvolver uma aplicação para a plataforma ios, é necessário dominar objetive c; caso se pretenda desenvolver uma aplicação para a plataforma android, é necessário dominar java. ao ser desenvolvido um produto para posterior distribuição, tem de ser definida qual a plataforma alvo. no entanto, a definição de qual a plataforma a eleger como alvo é um assunto bastante delicado dado que não é benéfico do ponto de vista comercial atender apenas um conjunto de potenciais interessados no produto. mas, por outro lado, proceder ao desenvolvimento para diferentes plataformas implica um custo enorme ao nível da produtividade pelo facto de ser necessário desenvolver versões do produto em linguagens diferentes de forma a serem compatíveis com as diferentes plataformas móveis. portanto, é bastante benéfico enveredar por uma abordagem multiplataforma que permita desenvolver apenas um produto e distribui-lo por várias plataformas.",
      "mobile devices such as smartphones and tablets are gaining meritoriously a prominent space in our day-to-day life, both in personal and professional context. simple activities such as reading a book or a newspaper became an act much more comfortable since the moment that we are able to use one of these devices, making us increasingly dependent on the facilities that technology provides us. in obstetric practice, a system that aids the pregnancies management process facilitates the access to varied ubiquitous information about patients, offering tools that facilitate a professional monitoring service to pregnant women and may prove to be very useful for the proper functioning of the service. there are already some applications developed to support obstetric practice, and although some offer interesting features, none offers a robust enough toolset to be essential to any health professional in the área on a daily basis. given the specificity of the application, the overall impression is that there is not yet an application that presents to the obstetrician as a complete tool in clinical practice, and therefore its development would be extremely valuable. before developing an application for mobile devices, it is necessary to master some specific programming languages - if we want to develop an application for the ios platform, it is necessary to master objetive c; if we want to develop an application for the android platform, it is necessary to master java. when developing a product for posterior distribution, it must be defined the target platform. however, the definition of which target platform to choose is a very delicate matter because it is not beneficial to serve only a set of potentially interested users in the product, from a commercial point of view. on the other hand, developing applications for different platforms is very expensive in terms of productivity because it is necessary to develop different product versions in different programming languages to make the product compatible with different mobile platforms. therefore, it is quite beneficial to engage in a multiplatform approach that allows developing only one product and distributing it to various platforms."
    ],
    [
      "invasive fungal infections caused by candida are associated with high mortality and morbidity rates in hospitalized patients. iron plays a major role in these infections, as they are exacerbated under iron overload conditions. in this context, it is important to understand the association between iron levels and invasive fungal infections, as it can serve as an indicator of the severity of the disease, and eventually it can help establish measures to improve treatment efficacy. nowadays, manually inferring these associations from biomedical documents is a time consuming task, due to the high amount of available scientific text data. as such, these tasks naturally benefit from the biomedical text mining field, which includes a wide variety of methods for automatic extraction of high-quality information from biomedical text documents. in this work, relevant documents related to iron overload and fungal infections were retrieved from pubmed to build a corpus. then, both named entity recognition and relation extraction processes were executed using the @note text mining tool. finally, relevant sentences were manually extracted and a curated dataset with documents containing those sentences was created. since the number of publications obtained about candida and iron overload was very low, the analysis was made taking into account all fungi. a total of 15 publications were considered relevant and 168 relevant associations were extracted. although associations of iron levels with both severity of infection and treatment efficacy were not extracted, it was possible to conclude that, in many cases, iron overload is a predictor for fungal infections, and patients’ iron levels highly affect treatment efficacy. the biomedical text mining process described in the present thesis enabled the creation of a dataset of relevant biomedical publications containing interesting associations between fungal infections, drugs and associated diseases in a clinical context of iron overload, although in the future this process could be improved, especially regarding dictionaries, in order to obtain a higher number of relevant publications.",
      "as infeções fúngicas invasivas causadas por candida estão associadas a elevadas taxas de mortalidade e morbilidade em doentes hospitalizados. o ferro tem um papel importante neste tipo de infeções, visto que estas são exacerbadas em condições de excesso de ferro. neste contexto, é extremamente importante compreender a associação entre os níveis de ferro e infeções fúngicas invasivas, pois pode servir como indicador da severidade da doença e, eventualmente, ajudar a estabelecer medidas para melhorar a eficácia de tratamento. atualmente, inferir manualmente este tipo de associações de documentos biomédicos revela-se uma tarefa bastante demorada, devido ao elevado volume de dados de texto científico disponíveis. como tal, estas tarefas beneficiam claramente da área da mineração de textos biomédicos, que inclui uma ampla variedade de métodos para extração de informação de alta qualidade de documentos de texto biomédicos. no presente trabalho, foram identificados, inicialmente, documentos relevantes que associam o ferro com infeções fúngicas invasivas para construir um corpus. de seguida, os processos de reconhecimento de entidades nomeadas e extração de relações foram realizados usando a ferramenta de mineração de textos @note. finalmente, as frases mais relevantes foram extraídas e foi criado um corpus curado de documentos contendo essas mesmas frases. visto que o número de publicações obtidas relacionadas com candida e excesso de ferro foi muito baixo, a análise foi feita tendo em conta todos os fungos. um total de 15 publicações foram consideradas relevantes e 168 associações foram extraídas. embora não tivesse sido possível extrair associações entre níveis de ferro e a eficácia do tratamento/severidade da infeção, foi possível concluir que o excesso de ferro prevê o surgimento de infeções fúngicas em muitos casos, e que os níveis de ferro dos pacientes afetam fortemente a eficácia do tratamento. o processo de mineração de textos biomédicos no presente trabalho possibilitou a criação de um corpus de publicações biomédicas relevantes contendo associações interessantes entre infeções fúngicas, fármacos e doenças associadas, no contexto clínico de excesso de ferro, embora este processo pudesse ser melhorado no futuro, especialmente no que diz respeito aos dicionários, para que seja possível a obtenção de um maior número de publicações relevantes."
    ],
    0.0
  ],
  [
    [
      "many organizations have developed open software components for 5g (fifth generation) networks and recognize the importance of new technologies based on virtualization and softwarization. with these solutions, it is possible to implement a 5g virtualized network without having access to a mobile network, which has many restrictions. implementing a 5g testbed is essential because it allows the creation of a framework that can enable the development and research of new solutions related to 5g. this dissertation proposes a solution that uses open-source software to emulate the access network and deploys software modules that implement core network functionalities. moreover, network capabilities, as well as interoperability, are described.",
      "muitas organizações têm desenvolvido soluções de software aberto para componentes da rede 5g (fifth generation) e reconhecido a importância de novas tecnologias baseadas em virtualização e em princípios de software. é possível implementar uma rede virtualizada 5g com base nestas soluções, sem necessidade de ter acesso a uma rede móvel, o que possui muitas restrições. a criação de um ambiente de testes 5g é importante, uma vez que permite criar uma estrutura que pode possibilitar o desenvolvimento e o estudo de novas soluções relacionadas com o 5g. nesta dissertação, é proposta uma solução emulando a rede de acesso e implementando um core recorrendo a software de código aberto. as capacidades da rede são descritas, bem como a interoperabilidade entre as diferentes soluções."
    ],
    [
      "a tecnologia blockchain tem evoluído a um ritmo incrível desde da criação da bitcoin em 2008, por satoshi nakamoto, fazendo com que esta tecnologia seja um dos assuntos mais falados aquando a escrita desta dissertação. são já imensas as aplicações e industrias em que a tecnologia blockchain é usada: nfts (non-fungible tokens), banking, secure data sharing, music royalties, iot, aml tracking, voting, real estate, supply chain, insurance, energy, cross-border payments (payment gateways) - funcionalidade em que se focará esta dissertação - entre outras funcionalidades. o termo payment gateway (pg) é utilizado para descrever um sistema de pagamentos que pode ser encontrado no checkout das lojas online dos comerciantes, este tipo de sistemas tem como objetivo facilitar os pagamentos para compradores e comerciantes. assim sendo, crypto payment gateway (cpg) é o termo utilizado para sistemas que, ao contrário dos anteriores, se focam nos pagamentos em criptomoeda, conseguindo assim remover intermediários no processo. são já várias as soluções de payment gateways existentes, no entanto, estes sistemas partilham um grande problema, não possuem uma solução que permita aos compradores o uso das suas criptomoedas para efetuar compras ou proceder a pagamentos. é então que os cpgs entram para resolver essa questão, contudo também estes possuem limitações, entre as quais se destaca a centralização. assim sendo, quer pg quer cpg são de alguma forma controlados por uma entidade centralizada, o que pode ser visto como uma desvantagem para o utilizador, pela falta de transparência nos processos. por forma a resolver os problemas mencionados, foi criado um sistema de pagamentos descentralizado. para tal foram desenvolvidos smart contracts e criadas interfaces para facilitar a interação com os mesmos, foi ainda implementado um sistema para obter as informações dos eventos dos smart contracts. posteriormente, foi desenvolvido um protótipo de uma loja online de um comerciante, de forma a demonstrar como o projeto funcionaria se implementado em contexto real.",
      "blockchain technology has evolved at an incredible rate since the creation of bitcoin in 2008 by satoshi nakamoto, making this technology one of the most talked about topics at the time of writing this dissertation. there are already many applications and industries in which blockchain technology is used: nfts (non fungible tokens), banking, secure data sharing, music royalties, iot, aml tracking, voting, real estate, supply chain, insurance, energy, cross-border payments (payment gateways) - feature that this dissertation will focus on - among other features. the term payment gateway (pg) is used to describe a payment system that can be found at the checkout of merchants’ online stores, this type of systems aims to facilitate payments for buyers and merchants. therefore, crypto payment gateway (cpg) is the term used for systems that, unlike the previous ones, focus on payments in cryptocurrency, thus managing to remove middlemans in the process. there are already several existing payment gateway solutions, however, these systems share a major problem, they do not have a solution that allows buyers to use their cryptocurrencies to make purchases or make payments. that’s when the cpgs come in to solve this issue, however they also have limitations, among which centralization stands out. therefore, both pg and current cpg are somehow controlled by a centralized entity, which can be seen as a disadvantage for the user, due to the lack of transparency in the processes. in order to solve the mentioned problems, a decentralized payment system was created. for this purpose, smart contracts were developed and interfaces were created to facilitate the interaction with them, and a system was also implemented to obtain information on smart contract events. posteriorly, a prototype of a merchant’s online store was developed, in order to demonstrate how the project would work if implemented in a real context."
    ],
    0.3
  ],
  [
    [
      "dado o aumento da preocupação relativamente às alterações climáticas, e políticas de redução dos efeitos das alterações climáticas, verifica-se cada vez mais um aumento na procura por veículos elétricos. esta procura, implica alterações na rede de abastecimento de veículos com a necessidade de incluir estações de carregamento. estas alterações, criaram condições para que novas empresas pudessem surgir, como é o caso da we can charge. porém, este aumento, implica também novos desafios na monitorização, manutenção e especificamente na deteção de anomalias nas mesmas. atualmente, a we can charge utiliza o open charge point protocol (ocpp). este é um protocolo e standard de código aberto, que permite a comunicação entre os diversos componentes de uma rede de carregamento de veículos elétricos, permitindo obter informação relevante sobre os mesmos pela análise da informação enviada nos pacotes de comunicação. através de uma ferramenta dedicada, é possível à we can charge, detetar a ocorrência de anomalias. apesar disto, a deteção das anomalias não é imediata e usa uma quantidade limitada da informação fornecida pelo protocolo, não utilizando a informação do estado dos conectores e transações de carregamento. assim, o principal objetivo desta dissertação é analisar e utilizar os dados recolhidos sobre o estado dos seus conectores e transações de carregamento, aplicando-os a algoritmos de machine learning. para tal, efetuamos a criação de vários modelos, um modelo de deteção em tempo real, onde obtivemos os melhores resultados utilizando o algoritmo isolation forest, e dois modelos de previsão baseados em redes lstms, um relativo ao estado e outro ao número de erros reportados nos dados relativos ao estado dos conectores. combinando estes modelos, foi possível a criação da connectors forecasting network (cnf), que nos permite a previsão de anomalias futuras nas estações de carregamento.",
      "in recent years the concern for environmental changes and the increasing measures imposed reduce climate change effects led to an increase in demand for electric vehicles. the growing number of these vehicles, in turn, calls for changes in the re-fuelling network introducing the need to include and manage charging stations where these can charge. this change formed an opportunity for new companies such as we can charge to emerge. however, the increase in the size of the network by increasing the number of charging stations comes with an uprising challenge when it comes to managing and ensuring that any anomalies are detected and resolved in the shortest time possible, allowing the continued use of their services. the open charge point protocol (ocpp) is an open-source standard used in enabling the communica tion between a charging network’s components. currently, we can charge uses a dedicated and custom tool that analyses ocpp packets containing information sent by each charging station. however this tool only uses part of the data that ocpp provides, not making use of important information like connector status and charging transaction information. therefore, this study aims to utilise this data, applying several machine learning algorithms and com paring the obtained results in order to determine which model performs best in the real-time anomaly detection task, but also in forecasting these anomalies. to accomplish that task, we created a real time detection model, obtaining the best results with the isolation forest algorithm, enabling us to classify connector status data, and two lstm forecasting models one for the status and another for the error counts obtained from connector status data. these results enabled us to combine these models in a sin gle pipeline creating the connector forecasting network (cfn), which in turn allowed us to predict future anomalies in the charging stations of the network."
    ],
    [
      "e-commerce is continuously expanding which increases market competitiveness. with the increase of plat-forms arrives a need to stand out from the competition, thus creating the necessity to improve marketing strate-gies. marketing strategies, such as creating personalized birthday emails or registration welcome-emails cannot be done in the traditional way. this idea of creating custom services like sending user-specific emails creates the need for a marketing automation solution. following this need, its proposed the development of a marketing automation platform with integration with a machine learning engine. this system will be hosted on a cloud and will automate marketing campaigns and provide dents with results from machine learning models.",
      "o comércio electrónico está em constante expansão originando maior competitividade no mercado. com o aumento do número destas plataformas surge a necessidade de se diferenciar da concorrência, criando assim a necessidade de melhorar as estratégias de marketing. as estratégias de marketing como a criação de um email personalizado para aniversários ou emails de boas-vindas após o registo não podem ser feitas da forma tradicional essa ideia de criar serviços personalizados, como o envio de emails customizados para os utilizadores, cria a necessidade de uma solução de automatização o marketing. seguindo esta necessidade, proprõe-se o desenvolvimento de uma plataforma de marketing automático com integração com um motor de machine learning. este sistema será hospedado numa nuvem e terá capacidade de automatizar campanhas de marketing e fornecer aos clientes resultados dos modelos de machine learning."
    ],
    0.3
  ],
  [
    [
      "prognosis and patient stratification for brain tumors is an important and clinically relevant task and a precise treatment outcome prediction would allow to choose an adequate therapy strategy and schedule the most appropriate follow-up examinations. magnetic resonance imaging (mri) is an already know imaging technique to assess these tumors. next to medical imaging, other clinical information is important for patient management, e.g. genetic markers like o6-methyl-guanine-methyl-transferase (mgmt) methylation is a well-known prognostic marker in glioblastoma (gbm) tumors. therefore, the main goal of this thesis was to study deep learning (dl) approaches to combine mri with non-image clinical data in two different classification scenarios: brain tumor segmentation and patient outcome prediction. there are studies that combine these two types of data, however, in two steps: extracting mri features and then combining them with relevant non-image data. here, end-to-end dl architectures with two input layers are presented, as well as an infrastructure that allows the easy development of future machine learning (ml) /dl models that consumes these two types of data in a clinical context. in this way, the classification in both scenarios is done in a single step, where convolution layers perform the feature extraction in mri input. in brain tumor segmentation, the model with combined data achieved a slightly better dice similarity coefficient (dsc) (0.894 ± 0.025) over image only model (0.882 ± 0.025). as for patient outcome prediction, when trying to predict the progression-free survival (pfs) class (“bad”,” medium” and “good” outcomes), the combined model didn't improve when compared with the model where only mri was used. both models, however, outperformed models where only non-image data was used. the segmentation results point to a positive influence when adding the clinical information to mri. nevertheless, there is a lot more to investigate in this field, not only in the model architecture, but also in selecting relevant clinical information. in same way, more tests should be run for patient outcome prediction, especially using overall survival (os) information.",
      "o prognóstico de pacientes com tumores cerebrais é uma tarefa importante e clinicamente relevante. uma previsão precisa do resultado do(s) tratamento(s) permitiria escolher uma estratégia de terapia adequada e agendar os exames de seguimento mais apropriados. a ressonância magnética (rm) é uma técnica de imagem já conhecida na avaliação deste tipo de tumores. outras informações clínicas como o marcador tumoral mgmt, são também relevantes no prognóstico de tumores de glioblastoma (gbm). neste sentido, o objetivo principal desta tese foi estudar modelos de deep learning (dl) para combinar imagens de rm com informação clínica e aplicá-los em dois cenários de classificação: segmentação de tumores cerebrais e previsão do prognóstico do paciente. existem estudos que combinam estes dois tipos de dados, porém, em duas fases: extraindo atributos das imagens de rm e combinando-as posteriormente com informação clínica relevante. contrariamente, aqui são apresentadas arquiteturas end-to-end de dl com duas camadas de entrada, assim como uma infraestrutura que permite um fácil desenvolvimento de modelos de machine learning (ml) / dl capazes de consumir estes dois tipos de dados num ambiente hospitalar. desta forma, a classificação em ambos os cenários é feita em um único passo, onde camadas de convolução realizam a extração de características das imagens na entrada de mri. no cenário de segmentação de tumores, o modelo que utilizou os dados combinados obteve um dsc (dice similarity coefficient) de 0.894 ± 0.025, superando ligeiramente o modelo que usou apenas imagens de rm (0.882 ± 0.025). quanto à previsão do prognóstico do paciente, classificando com a medida de progression-free survival (pfs) (“bad”,” medium” e “good”) o modelo combinado não melhorou quando comparado com o modelo que apenas utilizou imagens de rm. ambos os modelos, no entanto, superaram modelos que utilizaram apenas os dados clínicos. os resultados da segmentação apontam para uma influência positiva na adição de informação clínica às imagens de rm. no entanto, ainda há muito a investigar neste campo, não apenas na arquitetura do modelo, mas também na seleção de informações clínicas relevantes. da mesma forma, mais testes deverão ser executados para a previsão do prognóstico do paciente, especialmente usando a medida de overall survival (os)."
    ],
    [
      "spreadsheets are among the most used programming languages today. the easy to use and the intuitive nature of the visual interface makes them a preferred programming tool for any kind of individual or organization. the flexibility they provide to organize data as users need to is one of the reasons that makes them so popular. however, this flexibility also makes them very error-prone. in order to improve spreadsheet quality and reduce the number of errors, software engineering practices were introduced, namely object oriented and model-driven techniques. these techniques enabled the specification of the spreadsheet business logic, which offers the possibility to better structure data, while at the same time narrowing the range of types of errors made by user input. while these developments had a huge impact, spreadsheet evolution is still an inherently human process, which is in itself error-prone. in many real world applications of spreadsheets, they are used to store and disseminate data between different systems. different systems can use different data formats, this leads to the need to change and adapt the data produced by a source system so that it complies to the data format consumed by a target system. usually in these cases, both the initial and final data models are known in advance. the objective of this thesis is to present techniques that enable data evolution to be made automatically, using model-driven spreadsheets.",
      "folhas de cálculo são um dos paradigmas de programação mais utilizados actualmente. a sua facilidade de utilização e reduzida curva de aprendizagem torna-as numa das ferramentas de programação mais utilizadas diariamente por milhões de indivíduos e organizações. a flexibilidade concedida pelas folhas de cálculo para organizar dados consoante a preferência dos utilizadores é uma das razões que as torna tão populares. esta flexibilidade tem, contudo, uma grande desvantagem: torna-as muito propícias a erros. de forma a elevar a qualidade, e reduzir o número de erros em folhas de cálculo, foram introduzidas práticas já estabelecidas em engenharia de software, nomeadamente técnicas de desenvolvimento orientado a objectos e desenvolvimento dirigido por modelos. com estas técnicas passou a ser possível especificar a lógica de negócio de folhas de cálculo, o que proporciona a estruturação dos dados nelas contidos e, ao mesmo tempo, limita o tipo de erros passíveis de serem cometidos pelos utilizadores. embora estes desenvolvimentos tenham tido um grande impacto, a evolução de folhas de cálculo continua a ser um processo inerentemente humano, o que pode, ainda assim, originar erros. em muitos casos reais de utilização de folhas de cálculo, elas são utilizadas para armazenar e disseminar informação entre diferentes sistemas. diferentes sistemas podem utilizar diferentes formatos de dados, isto leva à necessidade de adaptar os dados produzidos por um sistema para que sejam compatíveis com um determinado sistema de destino. normalmente nestes casos, ambos os modelos de dados são conhecidos à partida. o objectivo desta tese é apresentar um conjunto de técnicas que permitam fazer esta evolução de forma totalmente automática, utilizando para isso folhas de cálculo dirgidas por modelos."
    ],
    0.0
  ],
  [
    [
      "the internet emerged in the late sixties in a scenario marked by the race of world hegemony between usa and ussr. besides military applications, it was also initially used by researchers, academics, and college students, enabling file transfer between hosts. after the nineties the internet reached the general public. it was then focused on other purposes, such as access to hypermedia, social networks, advertising and even products sale. given the diversification of these accesses, the adoption of protocols for safe browsing has be come essential to protect user’s information. combined with the classification of encrypted traffic, using appropriate techniques for this purpose, this paper aims to analyze the use of https pro tocol in various browsing scenarios once considered safe. through testing scenarios, this research intends to verify changes and impacts that this protocol promotes regarding the data collection from the users during the internet access experience.",
      "a internet surgiu no final da década de sessenta em um cenário marcado pela disputa da hegemonia mundial entre eua e urss. além de aplicações militares, ela foi utilizada inicial mente por pesquisadores, académicos e estudante universitários, possibilitando a transferência de arquivos entre hospedeiros. a partir da década de noventa a internet chegou ao grande público. passou, então, a ser utilizada para outros propósitos, como o acesso a hipermídias, redes sociais, publicidade e até venda de produtos. diante da diversificação desses acessos, a adoção de protocolos para navegação segura tornou se essencial para proteção das informações dos utilizadores. aliado à classificação de tráfego encriptado, utilizando técnicas apropriadas para o efeito, este trabalho tem por objetivo analisar o uso do protocolo https em vários cenários de navegação considerados seguros. através de cenários de teste, pretende-se verificar mudanças e impactos que este protocolo repercute quanto à exposição de dados na experiência de acesso à internet de um utilizador final."
    ],
    [
      "the existence of helpdesk teams is a common occurrence in companies nowadays. these teams are an expensive resource and can serve a limited number of users, which evidences the importance of helpdesk teams operating as efficiently as possible. one common occurrence is the existence of a set of repeated actions that could be automated to allow a helpdesk team to focus on other tasks, thus allowing to increase its productivity, as well as serving a larger quantity of users with a lower response time. in this context, vilt, an international software company, came up with a master’s thesis proposal to automatize the answering of technical queries from its programmers and the triggering of some usual routines and procedures needed to help the referred collaborators. the proposal of a chatbot to solve this problem is an interesting approach given the ease of use and popularity of chatbots in the industry. a chatbot is capable of answering a variety of questions with a lower response time compared to a human counterpart and is capable of delivering these answers at any given time. this master’s dissertation in the area of informatics engineering describes the proposal and develop ment of triton, a chatbot built to answer vilt’s demands referred to above. triton provides an interface with which users may interact, being capable of solving a predetermined set of common problems that were previously handled by a member of the helpdesk team. throughout the development of this chatbot, the users were included in different processes so that the final solution may be as adequate as possible to their needs. a comprehensive evaluation of the implemented solution is presented in this dissertation, and the results show that triton is indeed capable of helping vilt’s employees with common problems and questions, therefore assisting the helpdesk team.",
      "a existência de equipas de suporte é uma ocorrência comum em empresas atualmente. estas equipas são um recurso dispendioso e apenas atendem um número limitado de utilizadores. este cenário evidencia a importância de estas equipas operarem o mais eficientemente possível. uma ocorrência comum é a existência de ações repetidas que poderiam ser automatizadas, algo que permite a uma equipa de suporte focar-se em outras tarefas, permitindo um aumento de produtividade, bem como a possibilidade de atender um maior número de utilizadores com um tempo de resposta menor. com este objetivo, a vilt, uma multinacional do setor da tecnologia, propôs uma dissertação de mestrado com o intuito de automatizar a resposta de questões técnicas dos seus desenvolvedores, e o acionamento de rotinas e procedimentos necessários ao auxílio dos seus colaboradores. a proposta de um chatbot para resolver este problema é uma abordagem interessante dado a facilidade de utilização e popularidade associada a chatbots atualmente. um chatbot é capaz de responder a uma variedade de perguntas com um tempo de resposta menor comparativamente a um humano, e é capaz de responder a utilizadores a qualquer momento. esta dissertação de mestrado na área da engenharia informática descreve a proposta e desenvolvi mento do triton, um chatbot construído para responder às necessidades da vilt. esta solução fornece uma interface com a qual os utilizadores podem interagir, sendo capaz de resolver um conjunto predeter minado de problemas comuns que eram previamente resolvidos por um membro da equipa de suporte. ao longo do desenvolvimento deste chatbot, os futuros utilizadores foram incluídos em diversos processos de maneira a que a solução final seja o mais adequada possível às suas necessidades. uma avaliação detalhada da solução implementada é apresentada nesta dissertação, e os resultados indicam que o chatbot desenvolvido é de facto capaz de auxiliar os colaboradores da vilt com problemas e questões comuns, sendo assim um auxiliar da equipa de suporte."
    ],
    0.0
  ],
  [
    "with this work it is intended to create / identify user profiles through their actions on social networks. this identification is to determine, in a specific way, which profile each user has, linking between the following dimensions and their sets of variables: sociodemographic characteristics (gender, age, education, situation before the economic activity indicator and occupational class) the specific type of aggregate practices conducted over the internet (study, work, services, search for information, communication and entertainment), the context of use of social networks (home, school, workplace or other), frequency of use (daily, frequent or sporadic) and, finally, the range of motivations of users of social networks (professional, informational, recreational or other). after a careful analysis of these dimensions, we are able to separate the different types of users use only analyzing their sets of variables that are associated with each other. this analysis also allows to deepen knowledge about the various uses of social networks, and may also be useful to the market in that it provides substantive information concerning the forms of articulation between the social characteristics of users and their activities, schemes and contexts of use.",
    [
      "nowadays, we have the ability to trace everything, to extract valuable data from wherever we want, all to keep us connected and to improve our lifestyle. this huge amount of information, produced every day, needs to be treated, manipulated, and analysed, requiring convincing data structures to do so. dataframes, regularly used worldwide, are powerful data structures used to analyse and manipulate data of any kind. a dataframe organizes data into a 2-dimensional table of rows and columns, similar to sql tables or csv files. furthermore, it can span alongside thousands of computers or servers, making it easier to work with huge amounts of data, called big data, using distributed systems and parallel computing. this dataframe’s distributed nature led to the rise of distinct scalable and parallel dataframe tools. the most used dataframe tool, pandas, only performs on sequential execution and has some limitations when there is the need to handle huge volumes of data, and some tools such as modin, polars, rapids, and so forth, appeared in order to overcome those limitations. the vast offer of these scalable tools brought the need to make an analysis and comparison between these frameworks and pandas, studying their behaviour and results with different workflows. this comparison is not linear and there is a need to use a benchmarking tool, in order to produce a homogeneous and reliable evaluation of the different frameworks. to perform this analysis, we worked with several workflows, manipulating real and synthetically produced data on distributed and parallel environments and on different hardware configurations. we designed and developed a benchmarking tool that supports a set of dataframe frameworks, is flexible to the addition of new frameworks, and is able to perform micro-benchmarking evaluation with the analysis of a group of individual and common operations used on data science, and macro-benchmarking evaluation with the analysis of workflows that represent a set of chained operations. both of these evaluations aggregate performance and energy consumption results for each framework.",
      "hoje em dia, é possível extrair dados de onde quer que queiramos, mantendo-nos todos conectados e con tribuindo para um melhor estilo de vida. esta quantidade enorme de informação, produzida diariament, precisa de ser tratada, manipulata e analisada, precisando de recorrer a estruturas de dados capazes de fazê-lo. dataframes, utilizados globalmente, são uma poderosa estrutura de dados capaz de analisar e manipular qualquer tipo de dados. organiza-se numa tabela de 2 dimensões de columas e linhas, como uma tabela de sql ou um ficheiro csv. um dataframe consegue ser dividido por múltiplos servidores, facilitando o trabalho com enormes quantidades de dados, uma vez que é possível utilizar computação paralela. esta característica de paralelismo dos dataframes levou ao aparecimento de várias ferramentas escaláveis e distribuídas. a ferramenta de dataframes mais utilizada, pandas, apenas é capaz de executar sequen cialmente, tendo algumas limitação quando há a necessidade de trabalhar com enormes quantidades de dados, e algumas ferramentas como o modin, polars, rapids, entre outros, apareceram para superar essas mesmas limitações. a oferta vasta destas ferramentas escaláveis trouxe a necessidade de fazer uma análise e comparação entre estas ferramentas e o pandas, estudando o seu comportamento e re sultados com diferentes workdflows. esta comparação não é linear e existe a necessidade de utilizar uma ferramenta de benchmarking, para gerar uma avaliação homogénea e fiável. para fazer esta análise, trabalhamos com workflows de vários tipos, manipulatdos dados reais e sinteticamente produzidos em ambientes distribuídos e em diferentes configurações de hardware. prototipamos e desenvolvemos uma ferramenta de benchmarking, com suporte a várias ferramentas distribuídas, e flexível à adição de novas ferramentas, que é capaz de realizar avaliações de micro benchmarking, com a análise de operações individuais, e macro-benchmarking, com a análise de work flows que representam um conjunto de operações encadeadas. ambas as avaliações agregam resultados sobre a performance e o consumo energético de cada framework."
    ],
    0.0
  ],
  [
    [
      "in recent years, with the growth of energy consumption by computing devices, energy efficiency is a crucial concern in the it area due to its economics and environmental impact. the recent but widespread use of powerful computing devices, namely smartphones, which rely on \"the cloud\" to store large amounts of information (like, for example, photos and videos), is demanding the construction and maintenance of large data centers. such data centers run large-scale internet-based systems like cloud services. as a consequence, the energy consumed by data centers is growing fast, which is a crucial concern in the it area due to its economics and environmental impact. the growing reliance on cloud construction services is one of the main reasons for the rapid rise in research and development of energy efficient software and hardware for data centers. nowadays, the most popular usage of data centers is the database management systems (dbms) that, normally, are responsible for the access, management, manipulation, and organization of data. while there have been advances and studies in energy-awareness in this area, there isn't enough knowledge on the energy efficiency provided by different database systems. this master thesis intends to tackle this lack of knowledge by analyzing the energy consumption of dbms software. through benchmarks that simulate real usage environments, this research plays a key role in improving the knowledge on the energy efficiency of dbms. we analyze four systems, namely mysql, postgres, mariadb, and redis. moreover, we use the hammerdb benchmark framework for the simulation of dbms in a real environment. thus, to have a precise knowledge of the energy consumption of dbms, we analyze the energy consumption in various subsystems of the computer, namely like cpu, dram, gpu, and disk. moreover, we present further analysis of the energy consumption per performance ratio in all subsystems levels. our results show that, indeed, there are significant differences in the energy consumption of which dbms and that in some scenarios, the one with better run time performance is not what consumes more energy.",
      "nos últimos anos, com o crescimento do consumo de energia pelos dispositivos computacionais, a eficiência energética é uma preocupação crucial na área de ti devido ao seu impacto económico e ambiental. com a recente generalizada utilização de potentes dispositivos informáticos, nomeadamente smartphones, que dependem da \"cloud\" para armazenar grandes quantidades de informação (como por exemplo, fotos e vídeos) está a exigir a construção e manutenção de grandes centros de dados. esses centros de dados executam aplicações baseadas na internet em grande escala, como serviços em nuvem. como consequência, a energia consumida pela data centre está aumentar rapidamente, o que é uma preocupação crucial devido ao impacto económico e ambiental que estes trazem. o aumento da dependência destes serviços em nuvem é uma das principais razões para o interesse em estudos e desenvolvimento de software e hardware com baixo consumo de energia. hoje em dia, o uso mais popular dos data centre são os sistemas de gesto de base de dados (sgbd) que, normalmente, são responsáveis pelo acesso, gestão, manipulação e organização dos dados. embora tenha havido alguns avanços e estudos em eficiência energética nesta área, ainda existe falta de conhecimento nesta área. esta dissertação pretende reduzir a falta de conhecimento do consumo de energia do software dbms. ao usar ferramentas de benchmarks que simulam ambientes reais, este estudo desempenha um papel fundamental no aprimoramento do conhecimento sobre a eficiência energética de diferentes tipos sgbd. analisamos quatro sistemas, nomeadamente mysql, postgres, mariadb e redis. além disso, usamos o framework de benchmark hammerdb para a simulação da sgbd em um ambiente real. para ter um conhecimento aprefundado sobre o consumo de energia do sgbd, analisamos o consumo de energia em vários subsistemas do computador, nomeadamente como cpu, dram, gpu disco. além disso, apresentamos uma análise mais aprofundada do consumo de energia relacionada com o desempenho em todos os níveis dos subsistemas. esta tese apresenta resultados aonde pode ser verificado que existem diferenças significativas no consumo de energia das inerentes sgbd e em alguns cenarios, a base de dados com melhor desempenho de performance de execução não é o que consome mais energia."
    ],
    [
      "sendo certo que o recurso à tecnologia no ensino é cada vez mais notório, a utilização de sistemas informáticos de tutoria continua aquém do seu potencial, ainda que seja um tema abordado há já algumas décadas. assim, surgiu a iniciativa leonardo e o respetivo desenvolvimento de uma ferramenta computacional para sistemas de avaliação de conhecimento, com vista a ser aplicada, pelo menos, no suporte de processos de avaliação de alunos, na universidade do minho. de entre os módulos que caracterizam estes agentes de software, no contexto desta dissertação, destacam se a base de conhecimento, o mecanismo de raciocínio e o modelo do estudante. dado que o esforço maior recai em habilitar os tutores artificiais à adaptação, em tempo real, da avaliação ao nível de conhecimento atual dos alunos, surge a necessidade de desenvolvimento de um mecanismo de raciocínio, que seja capaz de determinar, criteriosamente, o que deve ser apresentado de seguida num dado momento avaliativo. o trabalho desta dissertação focou-se na conceção e implementação de um sistema de avaliação baseado em conhecimento para o sistema leonardo, com a capacidade de ajustar de forma dinâmica, à medida da perícia e conhecimento dos estudantes alvos do processo de avaliação, o seu comportamento, acompanhando de perto a evolução do processo de aprendizagem dos estudantes. essencialmente, neste trabalho implementou-se a “máquina” de raciocínio para o sistema leonardo poder sustentar de forma efetiva a avaliação de estudantes ao longo do tempo, numa ou mais áreas do conhecimento.",
      "despite the fact that the use of technology in education is progressively more noticeable, the use of computer tutoring systems stays below its potential, even though this is a subject that has been addressed for decades. thus emerged the leonardo initiative and the respective development of a computational tool for knowledge evaluation systems, aiming to be applied, at least, to support students’ assessment processes at the university of minho. among the models that usually constitute these software agents, in the context of this dissertation, we focus the knowledge base, the reasoning mechanism and the student model. as the major effort is to enable artificial tutors to adapt, in real time, the assessment to the students’ current level of knowledge, the need arises for the development of a reasoning mechanism capable of carefully decide what should be presented next at a certain evaluation moment. the work of this dissertation focused on the conception and implementation of a knowledge-based assessment system for the leonardo system, capable of dynamically adjusting its behavior, according to the expertise and knowledge of the target students of the assessment process, keeping track of the students’ learning process. mainly, in this work the reasoning “machine” was implemented for the leonardo system to effectively support student assessment over time in one or more areas of knowledge."
    ],
    0.3
  ],
  [
    [
      "formal methods are usually applied by specialists in the final phases of software development. they aim to identify programming errors, and through that reduce the probability of a future failure. usually, errors are more related with misinterpretation of requirements than with bad programming. more than ever, requirements documents deal with complex terms, which programmers aren’t familiar with, resulting in an increase of misinterpretation of requirements and increasing the costs of the execution of a software project. the use of formal methods could reduce these costs, if properly used to verify requirements and not source code. however, most companies avoid using formal methods due to high costs associated with formal methods application. programmers or requirements engineers can’t apply formal methods efficiently without previously having specific training, which implies hiring expensive specialists in formal methods. this dissertation presents methods which aim to bring formal methods closer to requirements descriptions. for such, formal modeling is used to verify and validate the descriptions of requirements, and not source code. initially it’s presented a standard to create formal models, which makes a direct correspondence between each requirement and its model. this standard is supported by a tool which, among other things, automatically generates graphics representations of requirements using its models. afterwards it’s presented a connection between requirements boilerplates and alloy models. this connection allows to generate formal models in an automatic fashion, without the need of a specialist. this drastically reduces the costs of using formal methods in software projects. it’s also presented the beginning of an algebra which allows to aggregate these templates. this aggregation allows one to write its requirements documents throught boilerplates and at the end have the complete model of all requirements, for free. when one is modeling a requirements document in alloy and at some point appears requirements with explicit temporal restrictions, it’s necessary to recreate the whole model in a tool which allows that kind of specification (eg. uppaal). this process is highly error prone, because it’s a manual transformation and highly dependent on the interpretation of who is modeling. in this dissertation it’s presented a method which allows to automatically generate an uppaal model from an alloy model. this transformation allows that at any point in the requirements document, the requirements engineer can generate the correspondent uppaal model and there specify the temporal properties.",
      "os métodos formais são normalmente aplicados por especialistas nas fases finais do desenvolvimento de software. a sua aplicação visa identificar erros de programação, reduzindo assim a probabilidade de uma falha futura. tipicamente, os erros que se encontram prendem-se com má interpretação de requisitos e não má programação. cada vez mais os documentos de requisitos tratam de termos complexos e fora do conhecimento do programador, o que leva a mais erros de interpretação e consequentemente a um aumento dos custos de execução de um projeto de software. a utilização de métodos formais poderia minimizar estes custos, caso eles fossem utilizados não para verificar código, mas sim para verificar requisitos. no entanto, muitas empresas evitam a utilização de métodos formais, devido ao custo elevado da sua aplicação. os programadores ou engenheiros de requisitos não conseguem aplicar métodos formais de forma eficiente sem terem formação prévia e específica na área, o que implica a contratação de especialistas em métodos formais. nesta dissertação são apresentados métodos que visam aproximar os métodos formais da escrita dos requisitos. para tal, a modelação formal é utilizada não para verificar código, mas para verificar a escrita de requisitos. inicialmente é apresentado um standard para a criação de modelos, que faz uma correspondência direta entre cada requisito e o seu modelo formal. este standard é suportado por uma ferramenta que, entre outras coisas, gera de forma automática representações gráficas dos requisitos através dos seus modelos. posteriormente é apresentada uma conexão entre templates de requisitos (requirements boilerplates) e modelos alloy. esta conexão permite a criação de modelos formais de forma automática, sem necessidade de um especialista. isto reduz drasticamente o custo de utilização de métodos formais. apresenta-se igualmente o começo de uma álgebra que permite agregar estes templates. esta agregação permite que um engenheiro de requisitos escreva o seu documento de requisitos através de templates e no fim tenha de forma automática o modelo formal de todos os requisitos. quando se está a modelar um documento de requisitos em alloy e a certo ponto aparecem requisitos com restrições temporais explícitas, é necessário recriar todo o modelo numa ferramenta que permita essa modelação (ex: uppaal). este processo está sujeito a erros, porque esta transformação é manual e altamente dependente da interpretação de quem está a modelar. nesta dissertação é apresentado um método que permite a geração automática de um modelo uppaal a partir de um modelo alloy. esta transformação permite que a qualquer ponto da modelação em alloy, se crie o modelo uppaal correspondente e se especifiquem as propriedades temporais."
    ],
    [
      "o presente trabalho de dissertação surgiu no contexto do projeto sensible car, uma parceria entre a bosch e a universidade do minho (um), onde se está a desenvolver um sensor da condição do piso (rcs) em que circula um veículo automóvel. com esta dissertação pretendia-se verificar se a aplicação de dados meteorológicos aliados à informação ótica apresenta vantagens na classificação da condição do piso, para além da utilização da informação ótica. também se pretendia demonstrar se, com recursos computacionais limitados, é possível implementar um classificador de piso com a fiabilidade e a capacidade de resposta exigidas. para atingir os objetivos propostos aplicou-se aprendizagem automática com supervisão e utilizaram-se dados de treino que combinam (i) os rácios da intensidade da luz recebida (após reflexão no piso) sobre a intensidade da luz emitida pelos dispositivos óticos do rcs, para quatro comprimentos de onda distintos, com (ii) dados meteorológicos. os dados óticos são essenciais para a circulação com segurança em veículos com condução autónoma. isto porque a deteção da condição do piso onde se circula permite ao veículo tomar melhores decisões em tempo real. para além de se comparar o desempenho de cada modelo treinado só com dados óticos, com o desempenho do mesmo modelo treinado com dados resultantes da fusão entre dados óticos e meteorológicos, testaram-se diversos modelos, para selecionar o que mais se adequa à classificação da condição do piso. numa fase inicial, selecionaram-se os modelos que apresentaram melhor desempenho, i.e. melhor, precisão e recall, na classificação de amostras dos vários tipos de piso. os modelos aqui selecionados foram svm gaussiano (0.96 de precisão e 0.93 de recall), regressão logística (0.91 e 0.88), árvore de decisão (0.91 e 0.85) e xgboost (0.94 e 0.94). posteriormente, implementaram-se e testaram-se os melhores modelos no dispositivo nvidia jetson nano. nesta fase, além de se confirmar as percentagens de acerto dos modelos a classificar a condição do piso, verificou-se se eram capazes de classificar as amostras ao ritmo a que o sensor de condição de piso gera os dados óticos. os resultados obtidos mostraram que os modelos desenvolvidos são capazes de acompanhar o ritmo de geração de dados do sensor da condição do piso, em que o modelo svm faz 1040 classificações por segundo, a regressão logística faz 2080, a árvore de decisão efetua 1950 e o xgboost faz 223. o modelo selecionado no fim foi o svm gaussiano, pois apesar de não ser o modelo com maior número de classificações por segundo, é o que possui o melhor desempenho geral na classificação da condição do piso.",
      "this dissertation arose in the context of the sensible car project, a partnership between bosch and university of minho, where a road condition sensor (rcs) is being developed. with this dissertation, it was intended to verify if the application of meteorological data allied to optical information presents advantages in the classification of the road condition, besides the use of optical information. it was also intended to demonstrate whether, with limited computing resources, it is possible to implement a road condition classifier with the required reliability and response capacity. to achieve the proposed objectives, supervised machine learning was applied and it was used training data that combines (i) the ratio of the received light intensity (after reflection on the pavement) over the light intensity emitted by the rcs optical device, for four different wavelengths, with (ii) meteorological data. optical data is essential for safe autonomous driving. this is because detecting the condition of the road surface enables the vehicle to make better decisions in real time. in addition to comparing the performance of each model trained with optical data only, with the performance of the same model trained with data resulting from the fusion between optical and meteorological data, several models have been tested in order to select the one that best fits the tread condition classification. initially, the models with the best perfor mance, in terms of precision and recall, were selected to classify samples of the different road conditions, namely the svm gaussian model (precision of 0.96 and recall of 0.93), the logistic regression (0.91 and 0.88), the decision tree (0.91 and 0.85) and xgboost (0.94 and 0.94). subsequently, the best models were implemented and tested on the nvidia jetson nano device. at this stage, in addition to confirming the models’ accuracy to classify the road condition, it was checked whether they were able to classify samples at the rate that the road condition sensor generates the optical data. the results showed that the developed models are capable of keeping up with the pace of data generation from the road condition sensor, where the svm model makes 1040 classifications per second, the logistic regression does 2080, the decision tree makes 1950 and the xgboost 223. the final model that was selected is the gaussian svm. although it is not the model with the highest number of clas sifications per second, it reached the best overall performance in road condition classification."
    ],
    0.0
  ],
  [
    [
      "in robotic applications, it is common to develop several variants of the same system (also known as a software product line), for example, to support different configurations of a robot. ros is the most popular framework for developing robotic applications, where each application is implemented as a distributed system of computation nodes that communicate through message passing. haros is a framework for static analysis of ros-based code. it can extract an abstract model of a ros system’s architecture (called the computation graph) and perform an analysis on that model. however, it can only analyse one configuration at a time. in this thesis, we present three different approaches for encoding various ros computation graphs in a single variational data structure, which contains the information related to the whole system and not just a configura tion. additionally, we also define a variational execution algorithm for each approach, along with a small query language, so that we can query and perform some analysis on said data structures. lastly, we evaluate these algorithms and data structures so that we can reach some conclusions on which approaches work best, and in what conditions.",
      "nas aplicações robóticas, é comum desenvolver diversas variantes do mesmo sistema (também conhecido como uma software product line) para, por exemplo, suportar diferentes configurações de um robot. o ros é a framework mais popular no que toca ao desenvolvimento de aplicações robóticas, onde cada aplicação é implementada como um sistema distribuído de nós de computação que comunicam entre si através do envio de mensagens. o haros é uma framework de análise estática de código ros. consegue extrair um modelo abstrato de uma arquitetura de um sistema ros (chamado grafo de computação) e executar nesse modelo uma análise. nesta tese, apresentamos três diferentes abordagens para codificar vários grafos de computação ros numa única estrutura de dados variacional, que contém a informação relativa a todo o sistema e não apenas a uma configuração. adicionalmente, também definimos um algoritmo de execução variacional para cada abordagem, juntamente com uma pequena linguagem de query, de forma a que possamos analisar e pesquisar nessas estruturas de dados. por fim, avaliámos estes algoritmos e estruturas de dados de modo a que possamos chegar a algumas conclusões sobre que abordagens funcionam melhor, e em que situações."
    ],
    [
      "today, there are many cities that offer to citizens smart solutions to make their daily lives easier so that the available resources can be better managed and the global quality of life improved. these solutions generally rely on a variety of wireless sensor networks (wsn), which are applied in a wide range of scenarios. most of these solutions work without human intervention, therefore, there has been a lot of interest in increasing the longevity of these sensor networks. in this context, the main purpose of this work is to study and optimize an adaptive, energy-aware sensing algorithm for wsns, e-litesense [11], wich is an algorithm capable of auto-regulate how data is sensed, adjusting it to each applicational scenario. this work, resorts to a simulation scenario representing a case in real life, namely, an intelligent irrigation system. in this study, cupcarbon is used as a simulation tool to implement wsn-based system and the e-litesense algorithm. the aim is to adapt the number of measurement events of environmental parameters so that the energy consumption of the different nodes of the network can be reduced while maintaining the correct evaluation of the measurement data and increasing the lifetime of the sensor network. the versatility of the algorithm in relation to its effectiveness and ability to self-configure in different types of sensing scenarios is also evaluated.",
      "atualmente, são várias as cidades que disponibilizam aos seus cidadãos soluções inteligentes para facilitar o seu dia a dia, de maneira a haver uma melhor gestão dos recursos existentes e uma melhoria da qualidade de vida proporcionada aos seus habitantes. muitas destas soluções recorrem geralmente a uma série de redes de sensores sem fios (rssf), sendo que estas são aplicadas a uma grande variedade de cenários. a maioria destes soluções funcionam sem intervenção humana, havendo assim cada vez mais interesse em aumentar a longevidade destas redes de sensores. neste contexto, o principal objetivo deste trabalho é estudar e otimizar uma solução de sensing adaptativo e de eficiência energética para rssf's, o e-litesense [11]. este algoritmo é capaz de regular automaticamente a forma como os dados são deteta-dos, adaptando-se a cada cenário aplicacional. este trabalho recorre a um cenário de simulação que representa um caso da vida real, nomeadamente, um sistema de rega inteligente. neste estudo, a ferramenta de simulação cupcarbon é usada para implementar esse sistema, baseado em rssf, e o algoritmo e-litesense. o objetivo é adaptar o número de eventos de medição de parâmetros ambientais para que o consumo energético dos diferentes nós da rede possa ser reduzido, mantendo a avaliação correta dos dados de medição e aumentando a vida útil da rede de sensores. a versatilidade do algoritmo relativamente à sua eficácia e capacidade de auto-configuração em diferentes tipos de sensing cenários também será avaliada."
    ],
    0.0
  ],
  [
    [
      "na administração pública portuguesa, existe uma preocupação recorrente em relação à in formação gerada e recebida pelas diferentes entidades produtoras, sendo o uso de papel um fator significativo na perda de informação. a clav é um projeto nacional, financiado pelo simplex, que visa a classificação e avalia ção de todos os documentos da administração pública portuguesa, com base em normas e orientações delineadas pela direção geral do livro, dos arquivos e das bibliotecas. a aplicação e o modelo de desenvolvimento já suportavam a criação e a manutenção dos instrumentos de classificação e avaliação, designados por lista consolidada e tabelas de seleção, onde já estavam definidos padrões de controlo, gestão e acesso à informação ou documentos arquivísticos. esta dissertação implementou a ação de avaliação, especificamente, a gestão e controlo dos procedimentos de eliminação. assim, foi necessário definir um modelo para as instruções de eliminação juntamente com todos os seus requisitos e invariantes. na plataforma clav foi crucial adicionar as interfaces que implementam os métodos e funcionalidades necessárias para criar ou importar as instruções de eliminação e todas as funcionalidades relacionadas ao seu processamento, nomeadamente, o registo da meta-informação num banco de dados estático, análise e consulta das declarações de eliminação, entre outros.",
      "at the portuguese public administration office, there is a recurring concern regarding the information generated and received by the different producing entities, being the use of paper a significant factor in the loss of information. thus, the clav, a national project, financed by simplex, which aims to classify and eva luate all documents of the portuguese public administration, based on rules and guidelines outlined by the direção geral do livro, dos arquivos e das bibliotecas. at the beginning, the application and the development model already supported the crea tion and maintenance of the classification and evaluation instruments, called lista consolidada and tabelas de seleção, where standards of control, management and access to information or archival documents were already defined. this dissertation implemented the evaluation action, specifically, the management and control of disposal procedures. therefore, it was necessary to define a model for the dis posal instructions together with all its requirements and invariants. in the clav platform it was crucial to add the interfaces that implement the necessary methods and functionali ties to create or import the elimination instructions and all the functionalities related to their processing, namely, the registration of meta-information in a static database, analysis and consultation of the elimination statements, among others."
    ],
    [
      "o objeto da nossa dissertação consiste na análise da adequação ou inadequação da atribuição de personalidade jurídica aos robots artificialmente inteligentes. no ordenamento jurídico português são atualmente admitidos dois tipos de pessoas jurídicas: as pessoas singulares e as pessoas coletivas. poderá considerar-se a criação de um novo tipo de pessoas jurídicas? o florescimento da inteligência artificial, i.e., da disciplina que procura construir máquinas que atuem de forma semelhante ao ser humano, capazes de executar tarefas ou funções e tomar decisões com uma eficácia semelhante ou superior àquele, levantou dúvidas relativamente à adequação dogmática da tradicional dicotomia entre a personalidade singular e a personalidade coletiva. a criação de robots artificialmente inteligentes capazes de receber e utilizar dados que circulam em rede, bem como de aprender e utilizar experiências anteriores, contribui em larga escala para a pertinência de uma análise cuidada do tema. tanto mais que a implementação de robots artificialmente inteligentes no quotidiano social e económico traz consigo uma panóplia de implicações éticas, as quais constituem desafios que cumpre teorizar e solucionar. a personalidade jurídica singular e a personalidade jurídica coletiva não assentam em considerandos axiológico-jurídicos e filosóficos iguais. a personalidade singular fundamenta-se no homem, enquanto sujeito dotado de uma dignidade originária e inviolável, pelo que tem de ser reconhecida a todos os seres humanos, em qualquer circunstância (nunca a pessoa humana poderá ser objeto de um direito). por sua vez, a personalidade coletiva assenta na necessidade de fornecer às pessoas singulares os instrumentos jurídicos adequados à prossecução dos seus propósitos ou objetivos. determinados os fundamentos axiológico-jurídicos que subjazem a este instituto jurídico no ordenamento jurídico português, a atribuição de personalidade jurídica aos robots artificialmente inteligentes pressuporá a verificação de um desses considerandos. haverá razões justificativas e legitimadoras da atribuição de personalidade jurídica aos robots artificialmente inteligentes? qual é o entendimento perfilado pela comissão europeia na proposta de regulamento publicada em 21 de abril de 2021? estas são algumas das questões que nos propusemos tratar.",
      "the object of our thesis consists in analyzing the adequacy or inadequacy of assigning legal personality to artificially intelligent robots. in the portuguese legal system, there are currently admitted two types of legal personality: the natural person and the legal person. should we consider a new type of juridical person? the flourishing of artificial intelligence, that is, the discipline that seeks to build machines that function in a way similar to a human, capable of performing tasks or assignments and making decisions in a comparable or superior way, has raised questions about the dogmatic sufficiency of the traditional partition between the natural person and legal person. the creation of artificially intelligent robots, capable of receiving and utilizing network data, as well as learning and applying previous experiences, contributes to the relevance of a careful analysis on the subject. moreover, the implementation of artificially intelligent robots in our economy and everyday lives, brings with it a panoply of ethical implications that constitute challenges that must be posited and taken into consideration when regulating artificial intelligence. the natural and legal personhoods do not rest in the same legal and philosophic principles. the natural personhood is substantiated by the human person, as a being gifted with an original and sacrosanct dignity, which mandates that it must be recognized to all human beings, under any circumstance (a human person must never be the subject matter of a right). on the other hand, the legal personhood rest in the need of offering to the natural person the legal instruments suited to the prosecution of their purposes or goals. thus, once established the axiological-legal motivations that underline this legal institute in the portuguese legal system, the assignment of a juridical personhood to the artificially intelligent robots will require the substantiation of one of those axiological-legal motivations. are there justifiably reasons to assign juridical personhood to artificially intelligent robots? what is the position shared by the european commission in the regulatory proposal published on the 21st of april 2021? these are some of the questions that we intend to address."
    ],
    0.0
  ],
  [
    [
      "somewhat worryingly, software is becoming increasingly complex with the passing of time. even though society has become completely dependent on it, there’s still not enough quality teaching and tooling to help software engineers verify the correctness of their solutions. furthermore, quickly put together solutions are often incentivized over a more rigorous approach. software is always bound to have bugs. however, formal specification languages allow the modeling of complex systems by specifying the relevant entities, how they interact, and testing the expected guarantees. hence, helping developers gain valuable understanding of the systems they work with. this approach has the drawbacks of not only being time costly, adding another step in the development process that requires deep understanding of the problem, but also being difficult to learn. the cause is due to the more abstract nature of specification compared to programming, paired with the need to be comfortable working with formal logic concepts. alloy is a formal specification language capable of structural and behavioral analysis. it is a popular framework for validating and verifying requirements, in part due to its expressiveness and flexibility. this makes it a prime candidate to develop and experiment new automatic repair techniques. they can help experienced developers speed up the process of writing specifications and new developers to learn quicker. with this in mind, some work has been done on repairing flawed structural alloy models, but none considering behavioral aspects. thus, this thesis presents an overview of the alloy language, along with previously proposed automatic repair techniques; it proposes the first mutation-based technique for the automatic repair of first-order temporal logic specifications using alloy6; also, it describes the integration of an automatic hint generation system for alloy4fun, an online platform for teaching alloy.",
      "de forma preocupante, o software está a tornar-se cada vez mais complexo com a passagem do tempo. a sociedade está completamente dependente dele. no entanto, mesmo nos dias que decorrem, não há ensino nem ferramentas suficientes que permitam aos engenheiros de software verificar a correção das suas soluções. para além disto, soluções construídas rapidamente e com negligência relativamente à qualidade são incentivadas em contraste a uma abordagem mais rigorosa. todo o tipo de software terá inevitavelmente defeitos. no entanto, as linguagens de especificação formal permitem modelar sistemas complexos através da especificação das entidades relevantes, como as mesmas interagem, e testes das garantias esperadas. consequentemente isto auxilia profissionais a compreender mais aprofundadamente os sistemas em que trabalham. esta abordagem tem algumas desvantagens, como ser custosa temporalmente, sendo que adiciona mais um passo no processo de desenvolvimento, para além de ser difícil de aprender. a causa disto vem da natureza abstrata de especificações quando comparadas a programação, em conjunto da necessidade de estar confortável a trabalhar com conceitos de lógica formal. alloy é uma linguagem de especificação formal, capaz de análise estrutural e também temporal. é uma framework popular para validar e verificar requisitos, em grande parte por ser bastante expressiva e flexível. isto torna-a uma excelente candidata para desenvolver e testar novas técnicas de reparação automática. estas podem ser capazes ajudar estudantes a aprender mais rapidamente, tal como acelerar o desenvolvimento para utilizadores experientes. com esta finalidade, algum trabalho já foi feito em relação à reparação de modelos estruturais em alloy; no entanto, nada se encontra feito quando se põe em consideração os aspetos temporais. sendo assim, este trabalho apresenta um resumo da linguagem alloy, em conjunto com as técnicas de reparação automática já propostas; propõe a primeira técnica baseada em mutações para reparação automática de especificações de lógica de primeira ordem em alloy 6; para alem disso, descreve a integração de um sistema de geração automática de dicas para a plataforma alloy4fun."
    ],
    [
      "nutrition is fundamental to human well-being and health, especially when applied to patients who need special health care. in these cases, it is crucial that each patient has adequate nutrition to meet their needs, in order to accelerate their recovery process. recommender systems make it possible to offer suggestions to users, adapted to their preferences and to previously obtained information about them. food recommender systems are recommender systems applied to nutrition and diet. they are usually implemented feeding plans recommendation platforms based on food and the person using it. in this sense, the existing gap in the use of these recommendation systems applied to nutrition in health care is notorious. this is mainly due to the difficulty in associating the nutritional value of each food with the needs of patients. the main objective of this project is to fill the existing void, through the development and implementation of a platform that will allow the planning of meals taking into account the nutritional plan of the food and the specific needs associated with the users of the vila verde social canteen. the use of machine learning algorithms will allow us to identify how the connection between food and patient requirements can be made, making this task possible, which is complex due to the wide domain associated with it. this platform will be used for the generation of kitchen meal plans, which shall be produced using the algorithms developed after a bibliographic study and an investigation of the existing work, in order to understand how they can be implemented and which are the most adequate to the nutritional recommendations system.",
      "a nutrição é fundamental no bem-estar e na saúde do ser humano, principalmente quando aplicada a pacientes que necessitam de cuidados de saúde especiais. nestes casos, é fulcral que cada paciente tenha uma nutrição adequada às suas necessidades, de forma a acelerar o seu processo de recuperação. os sistemas de recomendação permitem oferecer sugestões aos utilizadores, adequados às suas preferências e às informações previamente obtidas acerca dos mesmos. os sis-temas de recomendação de alimentos são sistemas de recomendação aplicados à nutrição e alimentação. estes são usualmente implementados em plataformas de recomendações de receitas e planos de alimentação tendo como base a comida e a pessoa. neste sentido, é notória a falha atual no que diz respeito à utilização destes sistemas de recomendação aplicados à nutrição em cuidados de saúde. isto deve-se maioritariamente à dificuldade na associação entre o valor nutricional de cada alimento e as necessidades dos pacientes. este projeto tem como principal objetivo preencher a lacuna existente, através do desen-volvimento e implementação de uma plataforma que irá permitir o planeamento de refeições tendo em conta o plano nutricional dos alimentos e as necessidades específicas associadas aos utentes da cantina social de vila verde. a utilização de algoritmos de machine learning permitirá perceber como pode ser feita a conexão entre os alimentos e os requisitos dos pacientes, tornando possível esta tarefa, que é complexa devido ao largo domínio associado à mesma. esta plataforma será utilizada para a geração de planos de refeições da cozinha, sendo estes produzidos utilizando os algoritmos desenvolvidos após um estudo bibliográfico e uma investigação ao trabalho existente com o objetivo de perceber como poderão ser implementados e quais os mais adequados ao sistema de recomendações nutricional."
    ],
    0.06666666666666667
  ],
  [
    [
      "as a compact representation of joint probability distributions over a dependence graph of random variables, and a tool for modeling and reasoning in the presence of uncertainty, bayesian networks are becoming increasingly relevant both for natural and social sciences, for example, to combine domain knowledge, capture causal relationships, or learn from in complete datasets. known as an np- hard problem in a classical setting, bayesian inference pops up as a class of algorithms worth to explore in a quantum framework. the present dissertation explores this research field and extends the previous algorithm by embedding them in decision-making processes. in this regard, several attempts were made in order to find new and enhanced ways to deal with these processes. in a first at tempt, the quantum device was considered to run a subprocess of the decision-making pro cess, resulting in a quadratic speed-up for that subprocess. afterward, “decision-networks” were taken into account and allowed a fully quantum implementation of a decision-making process, benefiting from a quadratic speed-up during the whole process. lastly, a solution was found. it differs from the existing ones by the judicious use of the utility function in an entangled configuration. this algorithm explores the structure of input data to efficiently compute a solution. in addition, for each one of the algorithms developed, their computa tional complexity was determined in order to provide the information necessary to choose the most efficient one for a concrete decision problem. a prototype implementation in qiskit (a python-based program development language for the ibm q machines) was developed as a proof-of-concept. if qiskit offered a simulation platform for the algorithm considered in this dissertation, string diagrams provided the verification framework for algorithmic proprieties. further, string diagrams were studied with the intention to obtain formal proofs about the algorithms developed. this framework provided relevant examples and the proof that two different implementations for the same algorithm are equivalent.",
      "as redes bayesianas tem-se tornado cada vez mais importantes no domínio das ciências naturais e sociais, na medida em que permitem inferir relações de causalidade entre variáveis e aprender através de conjuntos incompletos de dados. trata-se de uma representação compacta de distribuição de probabilidade conjunta feita sobre um grafo que representa dependências entre variáveis. num contexto clássico, inferência sobre estas estruturas é visto como um problema de complexidade np destacando-se como uma das classes de algoritmos a explorar num enquadramento quântico. esta dissertação explora este domínio de investigação e insere as redes bayesianas num processo de tomada de decisão. neste sentido, foram feitas várias tentativas para se encontrarem novas e melhores formas de lidar com estes processos. numa primeira tentativa, considerou-se que o dispositivo quântico executava um subprocesso do processo de tomada de decisão, resultando numa aceleração quadrática do mesmo. posteriormente, foram consideradas decision networks que permitiram uma implementação totalmente quântica de um processo de tomada de decisão. através desta implementação foi possível obter uma aceleração quadrática durante todo o processo. por fim, foi encontrada uma solução viável. difere das já existentes pelo uso criterioso da função de utilidade num estado emaranhado. este algoritmo explora a estrutura dos dados de entrada para calcular de forma eficaz uma solução. além disso, para cada um dos algoritmos desenvolvidos, foi determinada a respetiva complexidade computacional de modo a que fossem conhecidas todas as informações necessárias para escolher o algoritmo mais eficiente para um determinado problema de decisão. foi desenvolvida uma implementação inicial no qiskit (um software que permite o desenvolvimento de programas baseados em python para as máquinas ibm q) como prova de conceito. se o qiskit ofereceu uma plataforma de simulação para o algoritmo considerado nesta dissertação, os string diagrams forneceram a estrutura de verificação para propriedades algorítmicas. além disso, estes diagramas foram estudados com a intenção de se obter provas formais sobre os algoritmos desenvolvidos. esta estrutura forneceu exemplos relevantes e a prova de que duas implementações diferentes para o mesmo algoritmo são equivalentes."
    ],
    [
      "consensus is essential to the blockchain as it enables participants to share a consistent view of the underlying distributed ledger. currently existing protocols either rely on proof-of work or similar economic incentive schemes, with high transaction latency but that can handle thousands of participants or on classical byzantine fault tolerant consensus protocols, with low transaction latency but that do not scale well with the number of participants. in this work, one goal is to look at classical consensus protocols and assess the impact that protocol parameters can have on the behaviour of the system, considering different settings (e.g. network), scale (participants), load and trust assumptions, for example. furthermore, we propose an adaptive consensus protocol for the blockchain, using an optimization mechanism that configures the protocol automatically.",
      "o consenso é essencial para a blockchain, pois permite que os participantes compartilhem uma visão coerente do ledger distribuído subjacente. os protocolos actualmente existentes baseiam-se em esquemas de incentivo económico como o proof-of-work da bitcoin ou similares, com alta latência de transações, mas que podem lidar com milhares de participantes ou com protocolos clássicos de consenso tolerantes a falhas bizantinas, com baixa latência de transações, mas que não escalam bem com o número de participantes. nesta dissertação, um dos objetivos é analisar os protocolos de consenso clássicos e avaliar o impacto que os parâmetros do protocolo podem ter no comportamento do sistema, considerando, por exemplo, diferentes ambientes (por exemplo, rede), escala (participantes), carga e suposições de confiança. para além disso, nós propomos um protocolo de consenso adaptativo para a blockchain, usando um mecanismo de otimização que configura o protocolo automaticamente."
    ],
    0.06666666666666667
  ],
  [
    [
      "remote working is not a new concept, having become a more viable option with the advent of personal computers and high-speed internet connections. even so, it is safe to say that the percentage of profession als remotely working reached unprecedented proportions during the covid-19 pandemic. consequently, for many, this peculiar virus containment period meant their first contact with teleworking. however, the obligation to work from home eventually came to an end, meaning that employers and employees regained the autonomy to decide together whether or not to invest in teleworking. now, with a notable difference: both, with a few exceptions, are already familiarized with teleworking, its advantages and challenges, and the team dynamics adapted to allow for virtual communication. it is within this post-pandemic context that this dissertation provides a comprehensive view of the adoption of remote working among portuguese software development teams. therefore, it intends to study the current prevalence of teleworking, the challenges posed by the coexistence of remote and in office work, and how to make this symbiosis more effective and productive. to attain this, 175 valid testimonials were collected through a questionnaire distributed between march and june of 2023. analyzing the responses, it is possible to observe a significant migration from face-to face to remote work between the period before and after the pandemic. avoiding daily commuting and having more time for family and leisure activities were some of the primary motivations for this migration. it can be asserted that the coexistence of remote and face-to-face professionals induces a slight negative impact on team dynamics. lastly, and with the intention of optimizing the dynamics of teams that accept remote work, a set of recommendations is presented based on the participants’ testimonies.",
      "o trabalho remoto não é um novo conceito, tendo se tornado uma opção particularmente viável com o surgimento dos computadores pessoais e das conexões internet de alta velocidade. ainda assim, é seguro afirmar que a percentagem de profissionais a trabalharem à distância atingiu proporções inéditas no decurso da pandemia covid-19. consequentemente, para muitos este peculiar período de combate a um vírus significou o primeiro contacto com o teletrabalho. contudo, a obrigatoriedade de trabalhar a partir de casa acabou por cair, dando novamente liberdade aos empregadores e profissionais para, em conjunto, decidirem ou não apostarem no teletrabalho. agora com uma notória diferença: ambos, salvo raras exceções, estão já garantidamente familiarizados com o teletrabalho, com as suas vantagens e desafios, e as dinâmicas da equipa adaptadas à comunicação virtual. é inserida neste contexto pós-pandémico, que esta dissertação tem como principal objetivo traçar um retrato atual da adoção do trabalho remoto entre as equipas de desenvolvimento de software portuguesas. propõe-se, assim, a estudar qual a prevalência atual do teletrabalho, quais os desafios colocados pela coexistência do trabalho remoto e presencial, e, ainda, como tornar essa simbiose mais eficaz e produtiva. para tal, foram recolhidos 175 testemunhos através de um questionário distribuído entre março e junho de 2023. analisando as respostas, foi possível observar uma expressiva migração do trabalho presencial para o remoto entre o período anterior e posterior à pandemia. evitar viagens diárias de e para o escritório e ter mais tempo para a família e atividades de lazer foram algumas das principais motivações para essa migração. é possível afirmar que a coexistência de profissionais remotos e presenciais induz um ligeiro impacto negativo nas dinâmicas de equipa. por último, e com o intuito de otimizar as dinâmicas das equipas que acolhem o trabalho remoto, diversas recomendações são apresentadas com base nos testemunhos dos participantes."
    ],
    [
      "spreadsheets are the most used programming environment, mostly because they are very flexible. this is due to the lack of restrictions imposed on them which can lead to lots of errors. a first approach to model-driven engineering was already suggested to improve spreadsheets, providing them with specifications and checking tools. however, users have to learn how to use these tools on top of their existing spreadsheet host system. to remove that difficulty, the work for this thesis describes an embedding of spreadsheet models within spreadsheet themselves. this embedding enables users to create models in the same environment that they use for spreadsheet development and that they are familiar with. moreover, a set of operations that can be performed on these models and respective instances is defined. this way, users interact with models and spreadsheets in the same environment with the objective to improve work performance and reduce errors. resulting from this work, a prototype was created and is also discussed in this dissertation. this prototype can be used to validate the approach taken in this thesis and to provide a base framework for future developments.",
      "folhas de cálculo são provavelmente o ambiente de programação mais usado no mundo inteiro. a sua popularidade advém principalmente da facilidade com que se começa a usá-las, da sua disponibilidade em quase qualquer computador, da sua simples interface visual, mas principalmente da sua flexibilidade. isto deve-se à falta de restrições impostas por este tipo de sistema, o que pode levar a numerosos erros na maioria das folhas de cálculo, como indicado por numerosos estudos. o trabalho apresentado nesta tese visa combater o problema de erros em folhas de cálculo. a estratégia descrita baseia-se no uso de modelos e é alcançada embutindo modelos de folhas de cálculo dentro das folhas de cálculo em si. esta embutidura possibilita aos utilizadores criar modelos no mesmo ambiente em que desenvolvem as suas folhas de cálculo, com o qual já estão habituados. mais, um conjunto de operações sobre esses modelos e respectivas instâncias também foi definido. deste modo, utilizadores podem interagir com modelos e folhas de cálculo dentro do mesmo ambiente. isto facilita o estabelecimento e manutenção de uma relação de consistência entre modelos e dados durante o ciclo de vida de folhas de cálculo, esperando-se que se reduza o número de erros cometidos e que se aumente a produtividade usando folhas de cálculo. um protótipo foi criado como resultado deste trabalho, e também é discutido nesta dissertação. esta ferramenta pode ser usada para validar a abordagem escolhida nesta tese e também fornece uma base de trabalho para desenvolvimentos futuros."
    ],
    0.0
  ],
  [
    [
      "o desenvolvimento industrial, a constante inovação e a necessidade de melhoria contínua por parte das organizações originou um crescimento exponencial do volume de dados armazenados existindo, por isso, uma maior quantidade de informação relacionada com cada instituição. cada vez mais as instituições dependem do tipo de dados que armazenam e acumulam. atualmente, uma organização tem de fazer uma gestão eficiente das suas bases de dados de modo a extrair o máximo de conhecimento possível para apoiar no processo de tomada de decisão e assim garantir competitividade no mundo dos negócios e dos mercados. a necessidade de implementação de um sistema de apoio à decisão no seio de uma instituição fez emergir o conceito de business intelligence: processo responsável pela transformação dos dados em informação útil e organizada, e a subsequente conversão dessa informação em conhecimento valioso para a tomada de decisão. a área da saúde é bastante susceptível, tanto a nível clínico como administrativo, à qualidade e rapidez das decisões tomadas, uma vez que estas decisões colocam sempre em causa a vida humana. assim, é de extrema importância a utilização de sistemas de apoio à decisão nas unidades de saúde. o propósito deste projeto prende-se, essencialmente, com a exploração e aplicação de uma ferramenta de bi aplicada no contexto da saúde. por outro lado, pretende-se também avaliar a aplicabilidade de uma ferramenta open source em sistemas complexos e integrados como os das instituições hospitalares. neste sentido, a ferramenta explorada e avaliada foi o pentaho. foi realizada uma monitorização e simulação dos dados clínicos relativos às listas de espera para consulta e cirurgia e à ocupação das salas do bloco operatório de um hospital no norte de portugal. verificou-se que o pentaho, enquanto ferramenta open source, é inteiramente capaz de ser implementada e integrada numa instituição hospitalar, com a potencialidade de uma ferramenta proprietária. sendo assim conclui-se que o pentaho é uma ferramenta de bi bastante eficiente, capaz de apresentar soluções válidas e atrativas para a resolução de problemas e o para suporte à tomada de decisões.",
      "nowadays organizational environment is entirely dependent on several types of data such as, operational data, suppliers and customers. with the industrial development, the constant innovation and the need for continuous improvement by the organizations, the volume of stored data has grown exponentially, and therefore, there is a greater amount of information regarding each institution. presently, an organization has to manage efficiently its databases in order to turn all the available information into possible valuable knowledge to support the decision making process and to be competitive in the business and markets worlds. the need of a system to support decision making within an organization, created the concept of business intelligence (bi): process responsible for turning data into usefull and organized information, and convert it to valuable knowledge for decison making. healthcare is a field which is very susceptible to the speed and quality of decision making, both at the clinical and administrative levels, since this decisions often put human life at risk. therefore, it is of extreme importance the implementation of decision support systems in healthcare facilities. the main purpose of this project is implementing a bi tool in the healthcare context. moreover, it also aims to evaluate the performance of an open source tool when applied to complex and integrated systems, such as hospital units. for that, the tool explored and evaluated was pentaho. the clinical data analysed were the waiting lists for doctor’s appointments and surgery, and occupational rate of the operating room of an hospital in the north of portugal. pentaho, as an open source tool, is fully capable of being implemented and integrated in an hospital organization, with the potentiality of a proprietary tool. it was concluded that pentaho is a very efficient bi tool, capable of presenting valid and attractive solutions for problem solving and support of decision making."
    ],
    [
      "hoje, no turismo, a tendência é hoje cada vez mais voltada para a personalização, para as experiências locais, para a qualidade da viagem e não para a quantidade de cidades visitadas. para o sucesso de uma viagem personalizada é muito importante dois fatores, a agência, que planeia a viagem e o guia. a agência, para além de ter de conhecer o destino a ser visitado, tem de ser organizada, seja relativamente á equipa que a constitui como com os parceiros com que trabalha para construir as viagens. para além, disso esta também precisa de ter uma comunicação fácil com o cliente pois sem isso não tem como saber quais são as preferências e motivos da viagem que este quer fazer. o principal objetivo desta dissertação é fazer uma plataforma web que possa fornecer as ferramentas necessárias á agência para que esta possa construir viagens personalizadas com sucesso. para isso, o desenvolvimento desta foi contou com a orientação da equipa de backoffice da your tours e graças a isso foi possível detetar quais as maiores dificuldades que era necessário a plataforma resolver, relativamente a organização, comunicação e planejamento e perceber quais as features que faltavam nas plataformas utilizadas por estes. graças a isso, esta plataforma que possibilita fácil organização de trabalhos /tarefas e comunicação entre a equipa do backoffice, uma comunicação direta com o cliente para que este possa ter uma participação ativa na construção da sua viagem e possa acompanhar todo o processo em tempo real e um flow eficiente e otimizado de construção de viagens personalizadas com orçamentação automática, cronogramas dinâmicos e construção de itinerários digitais fácil e eficiente.",
      "today, in tourism, the trend is now more and more towards personalization, towards local experiences, towards the quality of the trip and not towards the quantity of cities visited. for a personalized trip to be successful, two factors are very important: the agency that plans the trip and the guide. the agency, besides having to know the destination to be visited, has to be organized, both regarding the team that makes it up as well as the partners it works with to build the trips. besides, it also needs to have an easy communication with the customer because without it there is no way to know what the preferences and motives of the trip are he wants to make. the main goal of this dissertation is to make a web platform that can provide the necessary tools to the agency so that it can build successful custom trips. for that, the development of this platform counted with the guidance of the backoffice team of your tours and thanks to that it was possible to detect which were the biggest difficulties that the platform needed to solve, regarding organization, communication, and planning and to realize which features were missing in the platforms used by them. thanks to this, this platform that allows easy organization of work/tasks and communication between the backoffice team, a direct communication with the client so that he can have an active participation in the construction of his trip and can follow the whole process in real time and an efficient and optimized flow of building customized trips with automatic budgeting, dynamic schedules, and easy and efficient construction of digital itineraries."
    ],
    0.0
  ],
  [
    [
      "the purpose of this dissertation is to analyse the impact of certain practices in long term power usage and expand on the concept of technical debt by introducing this aspect of energy consumption, dubbing the resulting notion as energy debt. this dissertation presents energy debt as a range of excess of energy required to execute code. it holds a minimum and maximum cost which depends on a set of factors during runtime. we analyse existing research regarding energy consumption to compile a detailed set of energy smells and the expected energy savings when they are eliminated via refactoring. then, we present the debt model that computes excessive energy expenditure of a software system. this debt model is based on the number and variety of occurrences of energy smells present on the software’s source code. lastly, we’ve developed a tool which we dubbed e-debitum, which extends the sonar qube framework to detect energy smells and compute energy debt.",
      "o objetivo desta dissertação é de analisar o impacto energético de certas práticas a longo termo e expandir o conceito de débito técnico, introduzindo o fator de consumo de energia, denominando o conceito resultante de débito energético. este relatório apresenta débito energético como um intervalo de valores representante do excesso energético necessário para executar código. este contém um custo mínimo e máximo dependente num conjunto de fatores no momento de execução. foi indagada pesquisa existente em consumo energético apresentado um conjunto detalhado de smells energéticos e as poupanças de energia projetadas quando estes são eliminados através de refactoring. é apresentado depois o modelo de débito que computa gasto energético excessivo de um sistema de software. este modelo de débito é baseado na quantidade e variedade de smells energéticos presentes no código. por fim, é delineada uma ferramenta denominada de e-debitum, que extende a framework sonarqube para melhor detetar e computar o débito energético."
    ],
    [
      "bacterial plasmids are mobile genetic structures capable of conferring selective advantages to their hosts, such as resistance to antibiotics, virulence genes and tolerance to pollutants. by associating with other genetic elements, like integrons and transposons, plasmids provide a platform for genetic recombination and for gene transfer between different bacterial species, allowing them to colonize multiple environments and guaranteeing their persistence. although there are over 4000 complete plasmid sequences available in genbank, most have absent or non-standardized (disorganized) information regarding their isolation source, environment and year and country of isolation. furthermore, a prediction about their mobility and incompability group is also lacking. the goals of this thesis are, besides completing the missing information about plasmid data, the development of a repository of fully sequenced plasmids and the development of easy-to-use web tools for the characterization of plasmid data regardless of their source environment and bacterial host. for the development of these tools, shiny was used, which is a package from the r scientific computing environment. the present work is organized as follows: the core concepts related to plasmids are described, their background is characterized and a critical analysis of the available web tools for plasmid classification is carried out. then, the adopted approach and the development (implementation, outcomes) of the database and web tool are explained. lastly, the main conclusions are highlighted.",
      "os plasmídeos bacterianos são estruturas genéticas móveis capazes de conferir vantagens seletivas ao seu hospedeiro, tais como resistência a antibióticos, genes de virulência e tolerância a poluentes. ao associar-se a outros elementos genéticos, como integrões e transposões, os plasmídeos constituem uma plataforma para a recombinação genética e transferência de genes entre diferentes espécies bacterianas, permitindo que colonizem múltiplos ambientes e garantindo a sua persistência. embora existam acima de 4000 sequencias completas de plasmídeos disponíveis no gen- bank, a maioria apresenta informação ausente ou não sistematizada (desorganizada) em relação à sua fonte de isolamento, ambiente e país e ano de isolamento. os objetivos desta tese são, para além de completar a informação em falta sobre plasmídeos, o desenvolvimento de um repositório de plasmídeos completamente sequenciados e a disponibilização de ferramentas online facilmente utilizáveis para a caracterização de plasmídeos independentemente da sua fonte de isolamento e hospedeiro bacteriano. para o desenvolvimento destas ferramentas, foi utilizado o shiny, que é um package do sistema de computação científica r. este trabalho está organizado da seguinte forma: os principais conceitos relacionados com os plasmídeos são apresentados, a sua história é caracterizada e é efetuada uma análise das ferramentas online existentes para a classificação de plasmídeos. depois, a abordagem utilizada e a ferramenta desenvolvida são explicadas. finalmente, as principais conclusões são destacadas."
    ],
    0.3
  ],
  [
    [
      "today, ai is very important in our lives as its used all around us without our knowledge. from simple things such as personal assistants like alexa and siri, and advertising algorithms focusing on our tastes - netflix on the recommendation of movies or, even more common, the presentation of advertising based on our search history -, to robots and to smart houses, cities or even vehicles. the presence of ai is increasing and even if we are still far away from our ’general ai’ ideology, a machine capable of anything autonomously, each day we get closer. in the last decade multiple applications of ai have been through breakthroughs. for example, the first implementations of autonomous vehicles were introduced by tesla and other companies. a number of discoveries must have been made to achieve this revolution of ai performance and, among them, is two of the most important developments: object detection and semantic segmentation, closely related to each other. these are responsible for understanding the environment so the machine can take actions, being the latter an improvement of the first in terms of sensibility error associated to each entity detected as well as being able to detect its corresponding type, in a pixel level. these machines require more and more data to analyse, having many types of sensors in order to collect information, such as radars, cameras, lidar, among others. this work falls in the study of the use of semantic segmentation techniques and its application on categorising data from image related sensors in order to explain its breakthroughs and challenges, as well as improving and overcoming such obstacles. data will consist mainly of scans from outdoor/self-driving cars pov (kitti360) with the ability to be used with other types of data such as indoor scans (coco), to explain both road and more day-to-day images semantic compositions, applied on a state-of-art solution. consecutively we will perform a process of optimisation in order to reduce computation costs. currently the works of deeplab (with the research of deeplabv3[1]) have achieved a high success on semantic segmentation overcoming previous problems such as handling component boundaries with more refined lines while keeping it fairly easy to run on more less powerful machines, being the start point for this work.",
      "hoje a inteligência artificial é uma ferramenta com grande presença nas nossas vidas, muitas vezes sem a nossa perceção. de utilizações simples como assistentes pessoais - alexa e siri -, e personalização de publicidade baseada nos nossos gostos - apresentação de publicidade relevante baseada no nosso histórico de pesquisas -, para casos mais complexos como robôs e casas, veículos e cidades autónomas. a presença da inteligência artificial cresce, e apesar de estarmos ainda bastante longe da nossa ideia de ’general ai’ - uma máquina capaz de executar qualquer função de forma autónoma -, cada dia nos encontrámos mais perto. na última década múltiplas aplicações da inteligência artificial realizaram avanços significativos como, por exemplo, as primeiras implementações de veículos autónomos que foram introduzidas por várias empresas, sendo a mais reconhecida a tesla. para chegar a este patamar nesta revolução da inteligência artificial foi necessário uma grande quantidade de estudo e descobertas onde, entre elas, duas se destacam com grande importância: deteção de objectos e segmentação semântica, ambas relacionadas. estas são responsáveis por recolher conhecimento sobre o ambiente em que se encontra a máquina de forma a esta executar ações. apesar de estarem ambas conectadas, a última pode ser considerada uma melhoria perante a primeira em relação á sensibilidade do erro associado a cada entidade detetada, assim como a nova capacidade de detetar todos os objetos e os seus respectivos tipos. para isso as máquinas necessitam cada vez mais de dados para analisar e treinar, obtidas de vários possíveis sensores como radares, diferentes tipos de câmaras, lidar, entre outros. esta dissertação tem então o objetivo de estudar o uso de técnicas de segmentação semântica e as suas aplicações na categorização de imagens provenientes de sensores do tipo câmara para explicar as suas descobertas e desafios, assim como melhorar e ultrapassar obstáculos existentes. os dados consistem de scans das estradas, capturadas do pov de um veículo (kitti360, sendo facilmente adaptada a utilizar scans de outros contextos para categorizar imagens mais ”comuns” (coco). atualmente os trabalhos do deeplab (com o desenvolvimento do modelo deeplabv3[1]) conseguiram resultados com bastante sucesso, ultrapassando desafios anteriormente existentes tal como o tratamento dos limites de cada entidade da imagem, com vizinhanças bem definidas, sendo ao mesmo tempo capaz de ser executada em máquinas com poucos recursos, sendo por isso o ponto de início para este trabalho."
    ],
    [
      "due to the high appearing of many bandwidth consumer services, as well as the increase of the bandwidth consumed by those services, on the last decade the pon (passive optical network) architectures and the fttx (fiber to the place) concept forcibly evolved, enabling larger disponibility on data rates to the costumer and to the services. there are many pon standards today, for example epon (ethernet passive optical network), bpon (broadband passive optical network), gpon (gigabit passive optical network) and xg-pon (10 gbps pon). although having these technologies, the idea that pon have unlimited capacity is erroneous, although, even limited, it can be much higher than the capacity available today by the copper network. given the architectural options, implementation schemes, performance of some equipment, gradual requirements by customers and services, the biggest problems will be spectral austerity/lack and the need to supply customers with an even broader data and service rate , higher than 10 gbps (provided by xg-pon), and as a consequence, these solutions can be seen as short to medium term. in the wake of these developments, a possible long-term solution to address these shortcomings is a development standard entitled ng-pon2 (next-generation pon 2). this work has as main objective the proposal of an architecture of implementation ng-pon2 oriented to the central office, being a solution with applicability in the access networks. in parallel, the ng-pon2 and previous ones will be studied. it will also make a characterization of current architectures and technologies. the work will also integrate the construction of a real ng-pon2 scenario, encompassing the necessary tests. all this work, from study, architecture design, implementation and testing will be developed at the company altice labs, s.a. in aveiro.",
      "devido à elevada multiplicação de diferenciados serviços de consumo de largura de banda, bem como o aumento de banda que estes serviços consomem, as arquiteturas de rede pon e o conceito de rtx evoluíram forçosamente na última década, facultando ao cliente e aos serviços maior disponibilidade em taxas de dados. existem várias normas pon em uso nos dias de hoje, como por exemplo: epon, bpon, gpon e xg-pon. ainda que dispondo destas tecnologias, a ideia que as pon têm capacidade ilimitada é errónea, apesar de que, mesmo limitada consegue ser muitíssimo superior à capacidade disponível atualmente pela rede de cobre. face a opções de arquitetura, esquemas de implementação, desempenho de alguns equipamentos, exigências graduais por parte dos doentes e serviços, os maiores problemas serão a austeridade/carência espectral e a necessidade de abastecer os clientes com uma taxa de dados e serviços ainda mais ampla, superior aos ao gbps (fornecido pelo xg-pon). como consequência, estas soluções podem ser encaradas como sendo de curto a médio prazo. no seguimento destes acontecimentos, uma possível solução a longo prazo que visa colmatar estas deficiências será uma norma em desenvolvimento, denominada por ng-pon2. este trabalho tem como principal objetivo a proposta de uma arquitetura de implementação ng-ponz orientado à central office, sendo uma solução com aplicabilidade nas redes de acesso. em paralelo será estudada a norma ng-ponz e outras anteriores. será também feita uma caraterização de arquiteturas e tecnologias atuais. o trabalho integrará igualmente a construção de um cenário real nc-ponz, englobando os testes necessários. todo este trabalho, desde estudo, especificação da arquitetura, implementação e testes será desenvolvido na empresa altice labs, s.a. em aveiro."
    ],
    0.3
  ],
  [
    [
      "in the last decades, the emergence and evolution of the next generation sequence technologies have revolutionised genomic research, leading to an exponential increase in the number of sequenced genomes. many of the sequenced genomes belong to bacteriophages (phages), mostly due to their therapeutic potential against bacterial infections. this abundance of genomic data demands the creation of user-friendly bioinformatics tools for performing genome annotation. the most challenging step in phage genome annotation is the identification of regulatory elements, primarily promoters, to understand phage transcription regulation mechanisms. thus, in this work, phagepromoter, a tool for promoter prediction in phage genomes, was developed, using machine learning methods. several models were created using different datasets and machine learning algorithms, such as support vector machines (svm), artificial neural networks (ann) and random forests (rf). all models were tested using a 5-fold cross-validation process. the datasets were composed by known phage promoter sequences, mainly retrieved from the phisite database, and by a different number of negative cases. after optimization, the performance was similar for all models and two were selected to be integrated in the tool: the ann model created with the dataset containing 1600 negative examples and the svm model created with the dataset containing 2400 negatives. the ann model presented 92% of accuracy, 89% of precision and 87% of recall, whereas the svm model presented 93% of accuracy, 91% of precision and 80% of recall. hence, the first model will predict more sequences as promoters and may lead to more false positives. the svm model will return few positive results, but most of them will be correct classified while some real promoters may not be identified by the model. phagepromoter was integrated in the widely used galaxy framework, available at https://galaxy.bio.di.uminho.pt/?tool_id=get_proms&version=0.1.0&__identifer=4u05obc3o5w, which provides a graphical user interface. this tool returns better results when compared to other tools, such as bprom, promoterhunter and cnnpromoter_e.",
      "nas últimas décadas, o surgimento e a evolução das tecnologias de nova geração de sequenciação revolucionaram a investigação genómica, levando a um aumento exponencial no número de genomas sequenciados. muitos destes genomas pertencem aos bacteriófagos (fagos), principalmente devido ao seu potencial terapêutico contra infeções bacterianas. esta abundância de dados genómicos requer a criação de ferramentas bioinformáticas intuitivas e fáceis de usar, para facilitar a anotação de genomas. o ponto mais difícil da anotação de genomas de fagos é a identificação de elementos reguladores, principalmente promotores, que irá permitir uma melhor compreensão dos mecanismos de regulação da transcrição nos fagos. assim, neste trabalho, foi desenvolvida uma ferramenta, phagepromoter, para prever promotores em genomas de fagos, usando métodos de aprendizagem automática. vários modelos foram desenvolvidos usando diferentes conjuntos de dados e algoritmos, como máquinas de vetor de suporte (svm), redes neuronais artificiais (ann) e random forests (rf). todos os modelos foram testados usando o processo de validação cruzada com 5 folds. os conjuntos de dados são constituídos por sequências de promotores de fagos conhecidas, retiradas maioritariamente da base de dados phisite, e por números diferentes de casos negativos. depois de otimizados, os modelos obtiveram resultados semelhantes e dois foram escolhidos para serem incorporados na ferramenta: o modelo ann treinado com o conjunto de dados com 1600 exemplos negativos e o modelo svm treinado com o conjunto de dados com 2400 exemplos negativos. o modelo ann apresentou 92% de exatidão, 89% de precisão e 87% de sensibilidade, enquanto que o modelo svm apresentou 93% de exatidão, 91% de precisão e 80% de sensibilidade. assim, o primeiro modelo irá prever mais sequências como promotoras, podendo originar mais falsos positivos. já o modelo svm irá prever poucas sequências como promotoras, mas a maioria estarão corretamente classificadas, enquanto que alguns promotores reais poderão não ser identificados. phagepromoter foi integrada no galaxy, uma framework amplamente usada, disponível em https://galaxy.bio.di.uminho.pt/?tool_id=get_proms&version=0.1.0&__identifer=4u05obc3o5w, que fornece uma interface gráfica para o utilizador. a ferramenta desenvolvida obtém melhores resultados quando comparada com outras ferramentas como brpom, promoterhunter e cnnpromoter_e."
    ],
    [
      "toposem is a software package with the aim of reconstructing a 3d surface topography of a microscopic sample from a set of 2d scanning electron microscopy (sem) images. toposem is also able to produce a stability report on the calibration of the sem hardware based solely on output images. one of the key steps in both of these workflows is the use of a digital image correlation (dic) algorithm, a no-contact imaging technique, to measure full-field displacements of an input image. a novel dic implementation fine-tuned for 3d reconstructions was originally developed in matlab to satisfy the feature requirement of this project. however, near real-time usability of the toposem is paramount for its users, and the main barrier towards this goal is the under-performing dic implementation. this dissertation work ported the original matlab implementation of toposem to sequential c++ and its performance was further optimised: (i) to improve memory accesses, (ii) to explore the available vector exten sions in each core of current multiprocessor chips processors to perform computationally intensive operations on vectors and matrices of single and double-precision floating point values, and (iii) to additionally improve the execution performance through parallelization on multi-core devices, by using multiple threads with a front wave propagation scheduler. the initial matlab implementation took 3279.4 seconds to compute the full-field displacement of a 2576 pixels by 2086 pixels image on a quad-core laptop. with all added improvements, the new parallel c++ version on the same laptop lowered the execution time to 1.52 seconds, achieving an overall speedup of 2158.",
      "toposem é um programa cujo objetivo é reconstruir em 3d a topografia de uma amostra capturada por um mi croscópio electrónico de varrimento. esta ferramenta é também capaz de gerar um relatório sobre a estabilidade da calibração do microscópio com base apenas em imagens capturadas. um dos passos chave para ambas as funcionalidades trata-se da utilização de um algoritmo de correlação digital de imagens (dic), uma técnica de visão por computador que não envolve contacto direto com a amostra e que permite medir deslocamentos e deformações entre imagens. criou-se uma nova implementação de dic em matlab especialmente formulada para reconstrução 3d. no entanto, a capacidade de utilizar o toposem em quase tempo real é fundamental para os seus utilizadores e a principal barreira para tal são os elevados tempos de execução da implementação em matlab. esta dissertação portou o código de matlab para código sequencial em c++ e a sua performance foi melho rada: (i) para otimizar acessos a memória, (ii) para explorar extensões de vetorização disponíveis em hardware moderno para otimizar operações sobre vetores e matrizes, e (iii) para através de paralelização em dispositivos multi-core melhorar ainda mais a performance utilizando para isso vários fios de execução com um escalonador de propagação em onda. a implementação inicial em matlab demorava 3279.4 segundos para computar uma imagem com resolução de 2576 pixels por 2086 pixels num portátil quad-core. com todas as melhorias de performance, a nova imple mentação paralela em c++ reduziu o tempo de execução para 1.52 segundos para as mesmas imagens no mesmo computador, atingindo um speedup de 2158."
    ],
    0.02727272727272727
  ],
  [
    [
      "a análise roc (receiver operating characteristic) tem vindo a ganhar muita popularidade, principalmente na área da medicina, dado que é uma ferramenta útil para avaliar e especificar problemas no desempenho de um indicador de diagnóstico. a área abaixo da curva roc (auc) é um indicador que pode ser utilizado para comparação de duas ou mais curvas roc. este trabalho, surgiu da necessidade de existência de softwares que permitem o cálculo das medidas necessárias para comparação de sistemas com base nas curvas roc. existem vários softwares que efetuam o cálculo de medidas associadas à análise roc, no entanto apresentam algumas lacunas, nomeadamente no que diz respeito à comparação para amostras independentes com diferentes dimensões e na comparação de duas curvas roc quando estas se intersetam. neste trabalho é apresentado uma nova aplicação que se designa por cercus. esta foi desenvolvida usando a linguagem de programação java e destaca-se pela possibilidade de comparar duas ou mais curvas roc. este programa tem como principal intuito o cálculo de várias estimativas roc, usando os diferentes métodos sugeridos no desenrolar do trabalho e fazer a comparação de curvas roc, mesmo que haja interseção, quer para amostras independentes ou amostras emparelhadas. permite ainda, a representação no plano unitário da curva roc empírica e a área entre as curvas.",
      "receiver operating characteristic (roc) analysis has gained much popularity, especially in the medical field, as it is a useful tool to assess and specify problems in the performance of a diagnostic indicator. the area below the roc curve (auc) is an indicator that can be used to compare two or more roc curves. this work emerged from the need for software to allow the calculation of the necessary measurements to compare systems based on roc curves. there are several software that perform the calculation of measures related to roc analysis, however they present some gaps, particularly as regards the comparison for independent samples with different dimensions and in comparing two roc curves where they intersect. in this work a new application is presented that is denominated by cercus. this was developed using the programming language java and stands out by the possibility of comparing two or more roc curves. the main purpose of this program is the calculation of several roc estimates, using the different methods suggested along in the dissertation and comparing roc curves, even if there is an intersection, for independent samples or paired samples. it also allows the representation in the unit plane of the empirical roc curve and the area between the curves."
    ],
    [
      "ray tracing is a rendering technique that allows simulating a wide range of light transport phenomena, resulting on highly realistic computer generated imaging. ray tracing is, however, computationally very demanding, compared to other techniques such as rasterization that achieves shorter rendering times by greatly simplifying the physics of light propagation, at the cost of less realistic images. the complexity of the ray tracing algorithm makes it unusable for interactive applications on machines without dedicated hardware, such as gpus. the extreme task independent nature of the algorithm offers great potential for parallel processing, increasing the available computational power by using additional resources. this thesis studies different approaches and enhancements on the decomposition of workload and load balancing in a distributed shared memory cluster in order to achieve interactive frame rates. this thesis also studies approaches to enhance the ray tracing algorithm, by reducing the computational demand without decreasing the quality of the results. to achieve this goal, optimizations that depend on the rays’ processing order were implemented. an alternative to the traditional image plan traversal order, scan line, is studied, using space-filling curves. results have shown linear speed-ups of the used ray tracer in a distributed shared memory cluster. they have also shown that spatial coherence can be used to increase the performance of the ray tracing algorithm and that the improvement depends of the traversal order of the image plane.",
      "o ray tracing é uma técnica de síntese de imagens que permite simular um vasto conjunto de fenómenos da luz, resultando em imagens geradas por computador altamente realistas. o ray tracing é, no entanto, computacionalmente muito exigente quando comparado com outras técnicas tais como a rasterização, a qual consegue tempos de síntese mais baixos mas com imagens menos realistas. a complexidade do algoritmo de ray tracing torna o seu uso impossível para aplicações interativas em máquinas que não disponham de hardware dedicado a esse tipo de processamento, como os gpus. no entanto, a natureza extremamente paralela do algoritmo oferece um grande potencial para o processamento paralelo. nesta tese são analisadas diferentes abordagens e optimizações da decomposição das tarefas e balanceamento da carga num cluster de memória distribuída, por forma a alcançar frame rates interativas. esta tese também estuda abordagens que melhoram o algoritmo de ray tracing, ao reduzir o esforço computacional sem perder qualidade nos resultados. para esse efeito, foram implementadas optimizações que dependem da ordem pela qual os raios são processados. foi estudada, nomeadamente, uma travessia do plano da imagem alternativa à tradicional, scan line, usando curvas de preenchimento espacial. os resultados obtidos mostraram aumento de desempenho linear do ray tracer utilizado num cluster de memória distribuída. demonstraram também que a coerência espacial pode ser usada para melhorar o desempenho do algoritmo de ray tracing e que estas melhorias dependem do algoritmo de travessia utilizado."
    ],
    0.0
  ],
  [
    [
      "os sistemas de localização e navegação indoor permitem determinar a posição de pessoas ou certos objetos em grandes edifícios, assim como ajudam na navegação e orientação dentro destes ambientes fechados. a sua aplicabilidade pode-se estender a diversas áreas, tais como: hospitalar, o turismo, e até infantaria militar ou um guia para invisuais. de acordo com as necessidades específicas de localização indoor, a abordagem clássica do gps é inadequada, uma vez que este é completamente inoperacional em espaços fechados. a navegação indoor levanta assim desafios adicionais quando comparada com o outdoor. o interesse no indoor tem-se expandido significativamente, e como tal, diversos métodos têm vindo a ser estudados e apresentados, com resultados, custos e contextos bastante diferentes entre si. dentro deste contexto, a presente dissertação tem como objetivo criar uma solução para a navegação no interior de edifícios, que resolva os problemas caraterísticos do indoor. para tal, foi necessário fazer um levantamento bibliográfico, principalmente na web devido a ser uma área muito recente. dessa pesquisa resultou uma avaliação dos métodos e soluções de indoor existentes. perante a dificuldade de encontrar uma forma consensual de representar espaços indoor, foi apresentado um possível modelo para a captação e representação destes espaços. assim, tendo como base os dados provenientes do open street map (osm) e um software open source de routing multi-modal - o open trip planner (otp), foi criada uma plataforma de routing adequada à perceção da informação indoor. esta plataforma - navios, encontra-se disponível na web. os objetivos deste projeto foram alcançados com sucesso, conseguindo-se uma representação simples dos espaços indoor, uma solução de routing para estes espaços, e que esta representação fosse percetível à solução criada. apenas não se conseguiram disponibilizar na plataforma os dados 3d, para visualização e navegação, apesar de estes dados terem sido previamente preparados. este será um passo futuro, na continuação deste projeto.",
      "the indoor navigation and localization systems allow to determine the position of people and objects within large buildings, as well as they help in the process of navigation and orientation in such closed environments. these systems can be applied in many different fields, such as the hospital, the turism and the military ones, as well as a guide for blind people. according to the specific needs of the indoor localization, the classic approach of gps turns out to be inadequate due to its inoperability in closed spaces. this way, the indoor navigation raises additional problems when compared to the outdoor navigation. the interest in such systems have been increasing over the past few years, leading to a wide number of studies concerning different methods, with different results, costs and contexts. in this context, the present work has the purpose of creating a solution for the navigation within buildings, to overcome the typical problems of indoor. it was necessary a literature review, mainly focused on web resources, since this is a very recent problem. an evaluation of the indoor methods and solutions that exist resulted from the review. facing the difficulty to find a consensual way to represent indoor spaces, it was presented a possible method to capture and represent information within these spaces. thus, relying on the data from open street map (osm) and a multimodal routing open-source software - open trip planner (otp), it was created a web-based routing platform - navios. this platform is able to perceive the indoor information. the objectives of this project were achieved successfully since it was obtained a simple way of indoor space representation, a routing solution for such spaces, and the perception from this solution to the way of representation created. the only objective we can consider as not concluded was the 3d visualization and navigation data in the platform, although this data had been prepared earlier. this represents the next step of the project, to be worked on in the future."
    ],
    [
      "o número de soluções utilizadas pelas empresas para fornecer serviços aos clientes e para fazer a gestão dos processos internos é cada vez maior. este aumento provocou o aparecimento de novos problemas no que diz respeito à manutenção e à integração de novos serviços, uma vez que grande parte das aplicações não conseguem viver isoladamente. com o objetivo de conseguir a integração de diferentes aplicações, surgiu o conceito de enterprise service bus (esb) — uma infraestrutura de conectividade que permite a comunicação entre aplicações, que podem ter diferenças a nível das plataformas em que são executadas, das linguagens de programação em que são escritas e dos modelos de dados que utilizam. a eurotux informática, s.a., é uma empresa especialista no planeamento, integração e implementação de sistemas informáticos, onde, devido à existência de diferentes interdependências entre aplicações de apoio ao negócio, surgiu a necessidade de implementar uma solução de integração utilizando um barramento de serviços. assim, nesta dissertação de mestrado, para além do estudo dos padrões de integração de aplicações, é apresentado o processo de planeamento e implementação de uma solução de integração de aplicações num contexto empresarial.",
      "the number of solutions that companies use to provide customer service and manage internal processes is growing. this increase has led to new problems in maintaining and integrating new services as most applications cannot live isolated. in order to achieve the integration of different applications, the concept of enterprise service bus emerged — a connectivity infrastructure that allows communication between applications, running on dif-ferent platforms, written in different programming languages and with different data models. eurotux informatica, s.a., is a company specialized in the planning, integration and implementation of computer systems. due to the existence of different interdependencies between applications that support the company's business, there is a need to implement an integration solution using an enterprise service bus. this master thesis aims to study application integration patterns and the planning and implementa-tion of an integration solution in a business context."
    ],
    0.0
  ],
  [
    [
      "a doença de parkinson (dp) é uma doença degenerativa do sistema nervoso central, geralmente caracterizada por prejudicar vários aspetos da marcha dos pacientes, como bradicinesia, comprimento do passo encurtado e congelamento da marcha. as escalas de avaliação clínica são tipicamente usadas com base em exames para monitorizar esses sintomas motores associados à marcha. além disso, estas avaliações são baseadas na memória dos pacientes e pesquisas subjetivas, fornecendo dados tendenciosos. assim, são necessários dados de longo prazo sobre as atividades motoras diárias do paciente. avanços tecnológicos forneceram dispositivos sensores pequenos e vestíveis capazes de capturar dados de longo prazo, podendo ser utilizados em ambientes domiciliares permitindo a captura de dados precisos. a combinação desses sensores com inteligência artificial (ia) produz modelos capazes de biomarcar os níveis de doença, condições motoras e bem-estar dos pacientes, e de fornecer dados não tendenciosos sobre os padrões de marcha dos pacientes. a integração destes modelos num aplicativo para médicos facilitará gerir o estado de dp e tratamentos mais personalizados serão alcançados. tendo isto em conta, esta tese tem como objetivo usar dados de pacientes que apresentam deficiências de marcha para treinar modelos baseados em ia que sejam capazes de classificar níveis de doença, condições motoras e qualidade de vida desses pacientes. para isso, foram adquiridos dados de 40 pacientes com dp, com o objetivo de desenvolver 3 modelos de ia diferentes, um usado para classificar o nível de doença de um paciente na escala updrs-iii, outro para classificar as condições motoras escala h&y e outro usado para classificar a qualidade de vida. esses modelos foram implementados numa app para auxiliar os médicos durante as suas consultas. os resultados obtidos foram positivos. o modelo updrs-iii conseguiu uma acurácia de 91,67%, uma sensibilidade de 90,43% e uma especificidade de 93,98%, enquanto o modelo h&y alcançou uma acurácia de 88,98%, uma sensibilidade de 88,71%, e especificidade de 92,79%, sendo que o modelo pdq-39 obteve acurácia de 84,19%, sensibilidade de 82,13% e especificidade de 90,24%.",
      "parkinson’s disease (pd) is a degenerative disease of the central nervous system, usually characterized by causing several gait impairment symptoms, such as bradykinesia, shortened stride length, shuffling gait and freezing of gait. clinical assessment scales are typically used based on observational examinations to monitor these motor symptoms associated with gait. further, these assessments are based on patients’ memory recall, subjective surveys, medication phase, and mood during the appointment, providing biased data. thus, long-term data regarding the patient’s daily motor activities is required. technological advancements provided small and wearable sensor devices able to capture long-term acquisitions of data. given their miniaturized size and portability, these sensors can be used in domiciliary environments enabling to capture accurate data. combining these sensors with artificial intelligence (ai) produces models able to biomark patients’ disease levels, motor conditions and well-being. these ai models can provide non-biased data about patients’ gait-associated patterns. integrating these ai-based solutions in a user-friendly clinic app for physicians will facilitate pd management, and more personalized treatments will be achieved. taking this in mind, this thesis aims to use data from patients who show developed gait impairments to train ai-based models that are able to classify disease levels, motor conditions and the quality of life of said patients. for that, data from 40 patients with pd was gathered. this data was then used to develop 3 different ai models, one used to classify a patient’s disease level on the unified parkinson’s disease rating scale (updrs-iii) scale, another to classify a patient’s motor conditions on the hoehn and yahr (h&y) scale, and another one used to classify a patient’s quality of life (qol). these models were then implemented in an easy to use app to help the physicians during their appointments with the patients. positive results were obtained, being observed that. the updrs-iii model manged to achieve achieve an accuracy of 91.67%, a sensitivity of 90.43%, and a specificity of 93.98%, while the h&y model achieved an an accuracy of 88.98%, a sensitivity of 88.71%, and a specificity of 92.79%, and the parkinson’s disease questionnaire (pdq-39) model achieved an accuracy of 84.19%, a sensitivity of 82.13%, and a specificity of 90.24%."
    ],
    [
      "o mundo ciber-físico deixou de optar por verificações manuais e promoveu a adoção de sistemas mais eficientes e fiavéis para detetar transações fraudulentas. estes sistemas visam otimizar e também melhorar a forma como estas transações são validadas. para atingir estes objetivos, foram criados ou adaptados modelos de aprendizagem automática para realizar estas tarefas. são cuidadosamente testados e desenvolvidos para atender às necessidades dos utilizadores para garantir que não se envolvem em negócios fraudulentos e para evitar tentativas maliciosas de roubar ou fazer qualquer dano ao utilizador final. nos últimos anos, o dtx tem vindo a desenvolver um sistema capaz de hospedar este tipo de algoritmos e disponibilizá-los para sistemas de produções em ambientes ciber-físicos. no início deste trabalho, a dtx propôs conceber e criar uma plataforma que pudesse ser implementada num ambiente em nuvem e também capaz de acolher um módulo de ia que esteja qualificado para prever entradas de churn em extratos de telecomunicações. nesta dissertação, o grande objetivo foi criar uma plataforma baseada numa arquitetura de microserviços, de forma a fornecer uma solução para os requisitos especificados pelo dtx e torná-la uma solução simples, mas eficiente. de forma abrangente, esta dissertação começa por expor um estudo profundo sobre o estado atual da arte dos sistemas ciber-físicos, ambientes em nuvem, algoritmos de aprendizagem automática e plataformas que podem acolher este tipo de sistemas. em seguida, apresenta-se então as especificações do sistema, a forma como foi implementado, os seus diversos serviços e, finalmente, uma análise dos resultados onde é possível ver que maior parte dos requisitos foram atingidos.",
      "the cyber-world has dropped manual checking and promoted the adoption of more efficient and relying systems to detect fraudulent transactions. these systems aim to optimize and also improve how these transactions are validated. for the sake of achieving these objectives, machine learning models were created or adapted to accomplish these tasks. they are carefully tested and developed to meet users needs to make a more secure way to guarantee that they do not fall into fraudulent businesses and to prevent malicious attempts to steal or do any arm to the end user. in the past few years, dtx has been developing a system that is capable of hosting these kinds of algorithms and making them available for cyber-environment productions systems. in the beginning of this work, dtx proposed to design and create a platform that could be deployed in a cloud environment and also capable of hosting an ai module that was qualified to predicting churn entries in telecom extracts. in this dissertation, the major goal was to create a platform based on a microservice architecture, in order to provide a solution to the requirements specified by dtx and making it a simple but efficient solution. comprehensively, this dissertation begins by exposing a deep study on the current state of the art of cyber-physical systems, cloud environments, machine learning algorithms and platforms that can host these types of systems. it is then presented the specifications of the system, how it was implemented, its various services, and finally a analysis of results where it is possible to see that the major needs were achieved."
    ],
    0.3
  ],
  [
    [
      "this document represents the study developed under the master’s thesis analysis of visualization of social networks, that overlaps two main scientific fields, sociology (more concisely social networks) and computer science, aiming at the design and implementation of a system for social network analysis. nowadays we face an age of massive internet usage, with online social networks we practically live this parallel reality where everything we do and everyone we met is exposed and shared through these online ”worlds”. today, being able to study and understand how information flows and how relationships are built within these online networks is of paramount importance for various reasons, these can be social, educational, political or economical. this master work studied sociology, social network analysis, and computer science to employ the researched material aiming at building a tool that allows users to explore their social structure in order to derive sophisticated conclusions, that wouldn’t normally come up when they are browsing through their online feeds, because we provide to the end user a personalized, macroscopic and objective perspective of their social network.",
      "o presente documento relata o estudo desenvolvido no âmbito do trabalho de mestrado do autor sobre análise e visualização de redes sociais dinâmicas conducente à tese que aqui se expõe e defende, trabalho esse que resulta sobretudo da intersecção de dois ramos científicos, a sociologia e as ciências da computação, com o objectivo de propor o desenho e implementação de um sistema de análise de redes sociais. vivemos atualmente numa era de uso massivo da internet. com as redes sociais online, acessíveis através da internet, experienciamos uma espécie de realidade paralela onde todas as pessoas com quem convivemos e praticamente tudo o que fazemos é exposto e partilhado através destes ”mundos” virtuais. na atualidade, a capacidade de estudar e compreender de que forma a informação flui e como se constroem novos relacionamentos dentro destas redes online é de extrema importância por diversos fatores, podendo estes ser de ordem social, educativa, política ou económica. no âmbito desta dissertação de mestrado estudamos sociologia, análise de redes sociais e ciências da computação com o objectivo de construir uma ferramenta que permita aos utilizadores explorar as suas estruturas sociais para que possam chegar a conclusões mais sofisticadas, conclusões que não surgiriam simplesmente por navegarem num perfil duma rede social online. com a nossa ferramente providenciamos ao utilizador final uma perspectiva personalizada, macroscópica e objectiva da sua rede social."
    ],
    [
      "current identity management systems rely on centralized databases to store user’s personal data, which poses a great risks for data security, as these infrastructure create a critical point of failure for the whole system. beside that service providers have to bear huge maintenance costs and comply with strict data protection regulations. self-sovereign identity (ssi) is a new identity management paradigm that tries to answer some of these problems by providing a decentralized user-centric identity management system that gives users full control of their personal data. some of its underlying concepts include decentralized identifiers (dids), verifiable claims and credentials. this approach does not rely on any central authority to enforce trust as it often uses blockchain or other decentralized ledger technologies (dlt) as the trust anchor of the system, although other decentralized network or databases could also be used for the same purpose. this thesis focuses on finding alternative solutions to dlt, in the context of ssi. despite being the most used solution some dlts are known to lack scalability and performance, and since a global identity management system heavily relies on these two requirements it might not be the best solution to the problem. this document provides an overview of the state of the art and main standards of ssi, and then focuses on a non-dlt approach to ssi, referencing non-dlt implementations and alternative decentralized infrastructures that can be used to replace dlts in ssi. it highlights some of the limitations associated with using dlts for identity management and presents a ssi framework based on decentralized names systems and networks. this framework couples all the main functionalities needed to create different ssi agents, which were showcased in a proof of concept application.",
      "actualmente os sistemas de gestão de identidade digital estão dependentes de bases de dados centralizadas para o armazenamento de dados pessoais dos seus utilizadores. isto representa um elevado risco de segurança, uma vez que estas infra-estruturas representam um ponto crítico de falha para todo o sistema. para além disso os service providers têm que suportam elevados custos de manutenção para armazenar toda esta informaçao e ainda são obrigados a cumprir as normas de protecção de dados existentes. self-sovereign identity (ssi) é um novo paradigma de identidade digital que tenta dar resposta a alguns destes problemas, criando um sistema focado no utilizador e totalmente descentralizado que oferece aos utilizadores total controlo sobre os seus dados pessoais. alguns dos conceitos subjacentes incluem decentalized identifiers (dids), verifiable credentials e presentations. esta abordagem não depende de qualquer autoridade central para estabelecer confiança, dado que utiliza blockchains ou outras decentralized ledger technilogies (dlt) como âncora de confiança do sistema. no entanto outras redes ou bases de dados descentralizadas podem também ser utilizadas para alcançar o mesmo objectivo. esta tese concentra-se em encontrar soluções alternativas para a dlt no âmbito da ssi. apesar de esta ser a solução mais utilizada, sabe-se que algumas dlts carecem de escalabilidade e desempenho. sendo que um sistema de identidade digital com abrangência global dependerá bastante destes dois requisitos, esta pode não ser a melhor solução. este documento fornece uma visão geral do estado da arte e principais standards da ssi, focando-se de seguida numa abordagem não dlt, que inclui uma breve referência a implementações não-dlt e tecnologias alternativas que poderão ser utilizadas para substituir as dlts na ssi. alem disso aborda algumas das principais limitações associadas ao uso de dlts na gestão de identidades digitais e apresenta uma framework baseada em name systems e redes descentralizadas. esta framework inclui as principais funcionalidades necessárias para implementar os diferentes agentes ssi, que foram demonstradas através de algumas aplicações proof of concept."
    ],
    0.3
  ],
  [
    [
      "na área de engenharia de software, a modelação de sistemas com recurso a diagramas, permite representar um sistema de forma padronizada, com o intuito de facilitar a compreensão da especificação, estrutura lógica, e documentação dos mesmos. hoje em dia, no mundo empresarial, a utilização de diagramas através de ferramentas próprias para o efeito tem como objetivo a comunicação entre equipas, inserindo-se na fase de modelação dos projetos. no entanto, a construção de aplicações com recurso a técnicas de low code, ou mesmo zero code, é uma realidade cada vez mais atual. a evolução natural deste conceito resultará na geração automática de código através de uma linguagem visual, como os diagramas, facilitando, assim, a produção de código, e ao mesmo tempo, conseguir-se-á uma poupança de tempo aproveitando o trabalho realizado numa fase mais precoce do projeto. posto isto, a utilização de modelos, mais ou menos standard, como forma de especificar e prototipar aplicações é e será, cada vez mais, uma realidade bem fundada e com sucesso assinalável, permitindo também gerir de forma mais eficaz questões de multi-plataforma, visto que a geração de código não é exclusiva a nenhum paradigma nem linguagem de programação específica. com esta dissertação pretende-se, então, utilizar modelos uml como mecanismo único de especificação de aplicações, automatizando o processo de construção do respetivo código e os aspetos tecnológicos relativos ao seu deployment e instalação, disponibilizando uma ferramenta que possibilite o processo de criação de aplicações web e android a partir de diagramas uml. assim, foi criada uma aplicação que, através da interação do utilizador, recebe diagramas de classe exportados em formato xml interpretando-os e gerando aplicações android e aplicações web. estas aplicações realizam as operações crud para cada entidade representada no diagrama de classe.",
      "in software engineering, modeling systems using diagrams, allows a system to be represented in a standardized way, in order to ease the understanding of its logical structure, specification and documentation. nowadays at the business world, the use of diagrams using the proper tools, aims communication between teams and is introduced at an early stage of the project as modeling it. however, the construction of applications using the low code techniques or even zero code is an increasing reality. the natural evolution of this concept will result in the automatic code generation through a visual language, such as diagrams, thus facilitating the code’s production, and in the meantime, time can be saved by having work done at an earlier stage of the project. having that said, the use of models, more or less standard as a way to specify and prototyping applications, are and will be a well established reality and with considerable success, also allowing to manage more effectively multiplatform’s issues, since code generation is not exclusive to any paradigm or specific programming language. with this dissertation is intended to use unified modeling language (uml) models as the only mechanism to specify applications, automating code’s construction process and technological aspects of its deployment and installation, providing a tool that enables the process of creating web applications and android from uml models. therefore, an application was created, through user interaction, which receives class diagrams exported in extensible markup language (xml) format that are interpreted and thereby generates web and android applications. these applications take under create read update and delete (crud) operations for each entity represented on the class diagram."
    ],
    [
      "technological progress and advancements are constant and natural. the extreme impact and the daily use of everything related to technology on everyone’s day to day life is irrefutable. the inclusion of robots and automated vehicles and machines is increasing because of the advantages linked to their use. this reality led to the beginning of a project, smart autonomous mobile units (samu), included in the ifactory program which comes from a partnership between bosch car multimedia (cm), and university of minho. the project consists of the automation of specific industrial vehicles used in the movement of materials and finished products that support the intralogistics processes. this dissertation presents the software process behind the transformation of a laboratory prototype to an industrial prototype. a laboratory prototype was the result of the previous stages of development since this prototype was created and tested only in a controlled environment. for the next step of the samu project, the proof of concept, it is mandatory to increase the maturity level of this prototype. the poc will be performed on a real industrial environment having a lot more adversities than the prior tests setting where everything was under control. on the bosch braga plant the autonomous vehicles have to interact with manually-driven vehicles as well. for this reason, the necessity of creating a system that could coordinate the allocation of logistic services to each vehicle had to emerge. a new solution was structured and developed following the basics of software design and architecture to modify and complement the former system with all the functions it is required to have. the final solution for the central platform of the industrial prototype is composed of seven microservices and each of them have a different purpose and business logic behind. the improvement of the central platform led to the development of new functionalities, such as the organization of the vehicles’ fleet and the visualization of the vehicles dynamically moving across their area of operation. the created solution was proven, by the results, to be functioning properly and as intended. the use of microservices reinforces their advantages for this specific context, since the deployment becomes easier and their maintenance is more straightforward. another important aspect is that the addition of new functionalities is also a more effortless process. considering that the previous solution was being developed with microservices the preservation of this type of architecture was the right answer for the continuation of development.",
      "o progresso a nível tecnológico é algo constante e natural. é evidente e irrefutável o ex-tremo impacto e uso diário das várias inovações nesta área no dia-a-dia de cada um. a inclusão de robôs e a automatização de máquinas é cada vez mais procurada devido às várias vantagens inerentes à sua utilização. esta realidade levou ao início de um projeto, smart autonomous mobile units (samu), insirido no programa ifactory proveniente de uma parceria entre a universidade do minho e a bosch car multimedia (cm). o projeto consiste na automatização de veículos indus-triais usados no movimento de materiais e produto acabado apoiando assim os processos intralogisticos. esta dissertação apresenta o processo por detrás da transformação de um protótipo de laboratório num protótipo industrial. um protótipo de laboratório foi o resultado obtido nas anteriores fases de desenvolvimento, uma vez que foi criado e testado apenas num ambiente controlado. para a próxima etapa, a prova de conceito, é obrigatório aumentar o nível de maturidade deste protótipo. a poc será realizado num ambiente industrial real, com muito mais adversidades do a localização dos testes anteriores, onde tudo estava sob controlo. na fábrica da bosch braga, os veículos autónomos também tém de interagir com veículos conduzidos manualmente. por este motivo, surgiu a necessidade de criar um sistema que pudesse coordenar a alocação de serviços de logística a cada veículo. uma nova solução foi estruturada e desenvolvida seguindo os conceitos básicos de design e arquitetura de scitzvare para modificar e complementar o sistema anterior com todas as funções necessárias. a solução final para a plataforma central do protótipo industrial é composta por sete microsserviços e cada um deles tem uma finalidade e lógica de negócio diferente. a melhoria da plataforma central leva ao desenvolvimento de novas funcionali-dades, como a organização da frota de veículos e a visualização dos veículos que se deslo-cam dinamicamente na sua área de operação. a solução criada provou, pelos resultados, estar a funcionar corretamente e conforme o planeado. o uso de microsserviços reforça as suas vantagens para este contexto específico, uma vez que o deployment se torna mais fácil e a manutenção é mais direta. outro aspecto importante é que a adição de novas funcionalidades também se torna num processo mais fácil. considerando que o sistema anterior estava a ser desenvolvido com microsserviços, a preservação deste tipo de arquitetura foi a solução certa a continuação do desenvolvimento."
    ],
    0.0
  ],
  [
    [
      "when assembling bottom terminated components in printed circuit boards, connectivity is extended through metallized terminals. to minimize thermal fatigue failure of the welds, software tools have been developed to model liquid surfaces shaped by various forces and constraints. surface evolver (se) is the software tool used by bosch in their media entertainment products to model liquid surfaces through the analysis of discrete parts of that surface. however, depending on the level of detail, this process may have long execution times, which is not consistent with the demand of industry and mainly in an interactive software where users expect the results to be obtained quickly. this dissertation aims to improve the efficiency of se, through the optimization of the total energy computation, taking advantage of vectorization, parallel computing and other high performance techniques. the analysis and profile of the current se version were crucial to support the decisions taken to improve the computational performance of the software. scalability tests, taking into account the amdahl’s law, call graphs and other profiling analysis helped to identify bottlenecks, where an effort should be invested to improve the software. one of the heaviest computations identified in se is the computation of the total energy of the configuration. se was identified to be a memory-bounded software, mainly due to its current mesh data structure, implemented with linked lists, which limits the use of the vectorization features on current cpu cores and also does not support data parallelization techniques and data locality. a new data structure was proposed to overcome these performance constraints, which led to a faster execution of se. the results showed an improvement on the total energy computation, an increase of vectorizable operations, software prefetching techniques and scheduling optimizations which, alongside the alternative data structure, increased the performance of the se.",
      "na montagem de bottom terminated components em printed circuit boards, a conetividade é expandida através de terminais metalizados. de modo a minimizar as falhas por fadiga térmica das soldaduras, ferramentas de software foram desenvolvidas para modelar superfícies líquidas condicionadas por várias forças e restrições. o surface evolver (se) é a ferramenta de software utilizada pela bosch nos seus produtos de media entertainment para modelar superfícies líquidas através da análise de partes discretas destas superfícies. no entanto, dependendo do nível de detalhe, este processo poderá ter longos tempos de execução, o que não é condizente com a exigência da indústria e principalmente num software interativo onde o utilizador espera que os seus resultados sejam obtidos rapidamente. esta dissertação tem como objetivo melhorar a eficiência do se, através da otimização do cálculo da energia total, tirando partido de vectorização, computação paralela e outras técnicas de computação de alta performance. a análise e o profiling da implementação ao atual do se foram cruciais para suportar as decisões tomadas para melhorar a eficiência computacional do software. testes de escalabilidade, tendo em consideração a lei de amdahl, call graphs e outras análises de profiling ajudaram a identificar bottlenecks, onde um esforço deverá ser investido de modo a melhorar a performance do software. um dos cálculos mais pesados identificado no se ´e o cálculo da energia total da configuração. o se foi identificado como sendo memory-bounded, principalmente devido à sua estrutura de dados atual, implementada com listas ligadas, o que limita o uso de funcionalidades de vetorizacao nos cores de cpu atuais e também não suporta técnicas de paralelização e localidade de dados. uma nova estrutura de dados foi proposta de forma a ultrapassar estas limitações, o que resultou numa execução mais rápida do se. os resultados mostraram uma melhoria no cálculo da energia total, um aumento de operações vetorizáveis, técnicas de software prefetching e otimizações de scheduling que, juntamente com a estrutura de dados alternativa, aumentaram a performance do se."
    ],
    [
      "this thesis explores the detection of impacts that cause damage based on data retrieved by an accelerometer placed inside a vehicle and subsequently classified by deep learning algorithms. the real world application of this work inserts itself in the car sharing market, by providing an automated service that allows constant monitoring on the vehicle status. the proposed solution was set as an alternative to the current machine learning algorithms in use. previous research showed that deep learning algorithms are achieving better performance results when compared to non deep learning algorithms. we use data retrieved from two types of events: normal driving and damage causing situations to test if the models are capable of generalising damage events. the approach to achieve this objective consisted in exploring and testing different algorithms: multi layer perceptron (mlp), convolutional neural network (cnn) and recurrent neural network (rnn). results revealed promising performance, with the mlp reaching a 82% true positive rate. despite not matching the result obtained by the current non deep learning algorithm allows us to assess that deep learning is a strong alternative in the long term as more data is collected.",
      "o principal objectivo desta tese foi a exploração e detecção de impactos que causam danos com base em dados recolhidos por um acelerómetro colocado no interior um veículo e posteriormente classificados por algoritmos de deep learning. a aplicação deste trabalho no mundo real insere-se no mercado de partilha de veículos, ao fornecer um serviço automático que permite uma monitorização constante do estado do veículo. a solução proposta foi definida como uma alternativa aos actuais algoritmos de machine learning em uso. a revisão de literatura revelou que algoritmos de deep learning estão a alcançar melhores resultados de desempenho quando comparados com algoritmos de machine learning. utilizamos dados recolhidos de dois tipos de eventos: condução normal e situações que causam dano e testar se os modelos são capazes de generalizar os eventos de danos. a abordagem para alcançar este objectivo consistiu em explorar e testar diferentes algoritmos: mlp, cnn e rnn. os resultados revelaram um desempenho promissor, com a mlp a atingir uma taxa de 82% de verdadeiros positivos. apesar de não corresponder ao melhor resultado obtido pelo actual algoritmo de machine learning em uso permite-nos avaliar que deep learning é uma forte alternativa a longo prazo à medida que mais dados forem recolhidos."
    ],
    0.06666666666666667
  ],
  [
    [
      "massificação dos sistemas de informação tem contribuído significativamente para a forma como os utilizadores interagem com as empresas e seus sistemas. esta nova relação entre cliente e fornecedor tem aumentado significativamente o volume de dados gerados pelas organizações, criando novas necessidades de como manter e gerir toda esta informação. assim, as empresas têm investido cada vez mais em soluções que permitam manter toda a informação tratada e consolidada num repositório único de dados. estes sistemas são vulgarmente designados por sistemas de data warehousing. tradicionalmente, estes sistemas são refrescados em modo offline, em períodos de tempo que podem ser diários ou semanais. contudo, o aumento da competitividade no mundo empresarial torna este tipo de refrescamentos desadequados, originando uma reação atrasada à ação que despoletou essa informação. na realidade, períodos longos de refrescamento tornam a informação desatualizada, diminuído consequentemente a sua importância e valor para a organização em causa. assim sendo, é cada vez mais necessário que a informação armazenada num sistema de data warehousing, seja a mais recente possível, evitando interrupções na disponibilização da informação. a necessidade de obter a informação em tempo real, coloca alguns desafios, tais como manter os dados acessíveis 24 horas por dia, 7 dias por semana, 365 dias por ano, reduzir o período de latência dos dados ou evitar estrangulamentos operacionais nos sistemas transacionais. assim, é imperativo a utilização de técnicas de coleta de dados não intrusivas, que atuem no momento em que determinado evento ocorreu num sistema operacional e reflitam a sua informação de forma imediata (ou quase imediata) num sistema de data warehousing. neste trabalho de dissertação pretendese estudar a problemática relacionada com a captura de dados em tempo real e conceber um componente que capaz de suportar um sistema de extração de dados em tempo real universal, que capture as mudanças ocorridas nos sistemas transacionais, de forma não intrusiva, e as comunique na altura certa ao seu sistema de data warehousing.",
      "the mass of information systems has contributed significantly to the way users interact with companies and their systems. this new relation between customer and supplier hassignificantly increased the amount of data generated by organizations, creating new needs to maintain and manage all this information. thus, companies haveincreasingly invested in solutions that allow them to maintain all the information processed and consolidated on a unique data repository. these systems are commonly called data warehousing systems. traditionally, these systems are refreshed in offline mode in periods of time that can be daily or weekly. although, the increase of the competitively in the business world, makes this kind of refreshments unsustainable, resulting in a delayed reaction to the action that triggered this information. in truth, long periods between refreshments make the information out-dated, consequently decreasing his importance and the value of the organization. . in that case, it is increasingly necessary that the information stored on the data warehousing systems, is the more recent possible, taking back interruption on the share of that information. the need of obtain information in real time, puts some challenges, as keep all the data accessible 24 hours a day, 7 day a week, 365 days a year, reducingthe periods of data latency or avoiding operational strangulations in transactional systems. thus, it is imperative the usage of techniques of data collection nonintrusive that can act when some particular event occurred on operational systems and reflect that information immediately (or almost immediately) on the data warehousing system.in this dissertation, we intend to study all the problematic related to real time change data capture, and conceiving a component capable to support an universal real time data extraction system, capable of capture the changes occurred on a transactional system, in a non-intrusive way and communicate with the data warehousing system in the right time."
    ],
    [
      "the constant growth of high-throughput data generation and omics approaches require informatics support and (semi) automated processes to be developed. with increasing number of sequenced genomes available, metabolic engineering processes will allow a rational alteration of the genetic architecture to achieve specific phenotypes. these alterations will allow to generate and optimize features of some organisms with economic and health interest. lactobacillus helveticus is an important industrial lactic-acid bacterium being used in the production of several types of cheese. the metabolic activities of the bacterium contribute to the cheese flavour and reduce bitterness. lb. helveticus is a growing body of literature on the health-promoting properties of its various strains and generally accepted as probiotic for its anti-mutagenic, immunomodulatory and anti-diarrheal effects. the aim of this project was to reconstruct a genome-scale metabolic network of lb. helveticus cnrz32, based on its genome sequence annotation as well as known biochemical and physiological characteristics. the generated model contained 790 reactions, 894 metabolites and 1687 genes. the growth rate predicted by the model on sugar was comparable to the reported in literature. this model provides the basis for a constraint-based mathematical model capable of simulating the phenotype of the organism under different growth conditions and guiding indepth physiological studies and hypothesis generation.",
      "o crescimento constante do volume de dados de alto rendimento gerados e de abordagens ómicas urgem de desenvolvimento de suporte informático e processos (semi) automatizados. o aumento do número de genomas sequenciados disponíveis, os processos de engenharia metabólica permitirá uma alteração racional da arquitetura genética para alcançar fenótipos específicos. estas alterações irão permitir gerar e otimizar características de organismos com interesse económico e de saúde. lactobacillus helveticus é uma bactéria lática com importância para o uso industrial e utilizada na produção de vários tipos de queijo. a atividade metabólicas da bactéria contribui para o sabor do queijo e para a redução da sua acidez. lb. helveticus é geralmente aceite como probiótico, com um crescente volume de literatura sobre as suas propriedades que contribuem positivamente para a saúde em várias das suass estirpes, assim como os seus efeitos antimutagénicos, imunomoduladores e antidiarreicos. o objetivo deste projeto é gerar uma reconstrução da rede metabólica à escala geneomica de lb. helveticus cnrz32 baseado na anotação de sequência do genoma, bem como das suas características bioquímicas e fisiológicas. o modelo gerado continha 790 reações, 894 reações e 1687 genes. a taxa de crescimento prevista pelo modelo sobre o açúcar é comparável ao relatado na literatura. a reconstrução deste modelo serve como base para a rescontrução de modelo matemático baseado em restrições capaz de simular o fenótipo do organismo sob diferentes condições de crescimento e orientar estudos fisiológicos em profundidade e geração de hipóteses."
    ],
    0.0
  ],
  [
    [
      "counterfeiting of luxury goods is becoming more of a problem, and manufacturers must battle it while also accepting the challenge of finding new ways to combat it. companies have spent a lot of money over the years to ensure that these products are authentic, and blockchain has given the luxury market fresh hope and potentially fuel a new process of authenticity. one of the technologies seeking to ensure the validity of these valuable products is blockchain technology, which has generated multiple start-ups, blockchain platforms, and protocols. with the explosive growth of blockchain technology, nfts (non-fungible tokens) are becoming increas ingly popular, even in sectors where there appears to be no benefit, such as in the postal service. its use is already standardized (for example, through erc-721, erc-1155, erc20). in the context of this work, it was necessary to analyze and evaluate existing nft platforms, with regard, among others, to costs, usability, ease of programming; design and implement a platform that is specifically tailored to the authenticity of luxury items, such as jewelry, art, and luxury clothes; and finally test all procedures and features that are appropriate for the authenticity of luxury products.",
      "a falsificação de objetos de luxo está se a tornar um problema cada vez maior, e os fabricantes devem lutar contra isso enquanto, também, devem encontrar novas maneiras de combatê-la. as empresas têm gasto muito dinheiro ao longo dos anos para garantir que estes produtos são autênticos, e a blockchain deu ao mercado de luxo uma nova esperança e, potencialmente acelerar um novo processo de auten ticidade. umas das tecnologias que visa procurar a garantia da validação destes produtos valiosos é a tecnologia blockchain, que já deu oportunidades à criação de múltiplas start-ups, plataformas de blockhain e protocolos. com o crescimento explosivo da tecnologia blockchain, os nfts (non fungible tokens) têm uma utili zação cada vez maior, estando até a ser utilizados em áreas em que aparentemente não haveria qualquer vantagem na sua utilização, como por exemplo na área postal.a sua utilização já está standardizada (entre outros, através do erc-721, assim como do erc-1155, erc-20). no contexto deste trabalho, foi necessário analisar e avaliar plataformas de nfts existentes, no que diz respeito, a custos usabilidade, facilidade de programação; desenhar e implementar uma plataforma costumizada especificamente para a autenticidade de objetos de luxo, como jóias, arte e roupas de luxo; e finalmente testar todos os procedimentos e recursos adequados para a autenticidade de luxo."
    ],
    [
      "driven by the pervasiveness of mobile devices, location based services are becoming increasingly popular. these services use information about the physical location of users, usually with commercial or informative purposes. however, and particularly for large scale scenarios, this type of services may pose a risk to the privacy of the users. by using location information either directly or indirectly (associated with other information), it is possible to expose personal information that users wish to keep private or even to uncover their identities. this may lead to the rejection of these types of technologies. there are however, non trivial ways to store information without compromising the users’ privacy. this dissertation presents two bluetooth scenarios where stochastic summarizing techniques are used as a solution to the privacy problem. in the first scenario, gate counting, the goal is to provide accurate counting for the number of unique devices sighted while trying to minimize the amount of collected information. for that purpose, we provide an analysis of several stochastic counting techniques that not only provide a sufficiently accurate count for the number of unique devices, but offer privacy guarantees as well, all in a space efficient way. for the second scenario, causality tracking, the objective is to study human mobility patterns, also while minimizing the quantity of data gathered. for this purpose, we developed precedence filters, a new technique, which is able to provide accurate results regarding the popularity of specific routes without compromising the individual privacy of the users. based on these scenarios, this dissertation demonstrates that stochastic summarizing techniques are viable means to the anonymization of location information.",
      "motivados pelo cada vez maior número de dispositivos móveis, os serviços baseados na localização estão a tornar-se cada vez mais populares. estes serviços utilizam informação acerca da localização física dos utilizadores, normalmente para fins comerciais ou informativos. contudo, e particularmente para cenários de larga escala, este tipo de serviços pode constituir um risco para a privacidade dos utilizadores. informação relacionada com localização dos utilizadores pode ser utilizada de forma direta ou indireta (associada com outra informação) para revelar informação privada acerca dos mesmos, podendo até ser suficiente para revelar as suas identidades. este facto pode levar à rejeição deste tipo de tecnologias. existem contudo, maneiras não triviais de guardar informação sem comprometer a privacidade dos utilizadores. nesta dissertação, apresentamos dois cenários bluetooth, onde o problema da privacidade é solucionado através do uso de técnicas de sumarização estocásticas. no primeiro cenário, gate counting, o objetivo é obter contagens precisas para o número de avistamentos de dispositivos distintos, tentando em simultâneo reduzir a quantidade de informação recolhida. para esse efeito, fazemos uma análise a várias técnicas de contagem estocásticas que não só fornecem contagens para o número de dispositivos únicos, com uma precisão adequada, como também garantias de privacidade, tudo de uma forma eficiente em termos de espaço. para o segundo cenário, causality tracking, o objetivo é estudar os padrões de mobilidade humanos, ao mesmo tempo que, também se tenta minimizar a quantidade de informação recolhida. com este propósito, desenvolvemos os filtros de precedência, uma nova técnica capaz de fornecer resultados precisos sobre a popularidade de determinados percursos/caminhos específicos, sem comprometer iii iv a privacidade individual dos utilizadores. com base nestes cenários, esta dissertação demonstra que as técnicas de sumarização estocásticas são meios viáveis para a anonimização de informação baseada na localização."
    ],
    0.0
  ],
  [
    "the rapid adoption of microservices and cloud-native architectures has revolutionized the way modern applications are developed and deployed. however, this shift has introduced new challenges in terms of ensuring the reliability and performance of these distributed systems. in response, observability is proposed as a new methodology to address these challenges. observability refers to the collection of telemetry data (including traces, metrics, and logs) from a system components in real time, allowing for a comprehensive understanding of its internal status and behavior. this capability is essential for troubleshooting, performance optimization, and enhancing system reliability by facilitating the detection of errors and anomalies. the main objective of this thesis is to implement an observability concept within a python flask based system. the system follows a cloud-native, microservices, and event-driven architecture. the main motivation for this study is the recent, but important development of observability and the culture of development and operations (devops). the chosen method for implementation is opentelemetry, a neutral and open-source approach to observability. this decision aims to avoid vendor lock-in, which can be a concern with vendor-specific agents. furthermore, a study is carried out to make a choice among the vendors considered which are compat ible with opentelemetry, e.g. jaeger, zipkin, prometheus, elastic search, new relic, datadog, dynatrace, grafana, splunk, and appdynamics. each vendor offers different approaches to observability and visual ization of the telemetry data. in addition, a weighted decision matrix is used to aid in the decision along with a decision criterion defined by the development team. the results of this study not only highlight the vendor selection process for telemetry data visualization but also emphasize that opentelemetry is a viable and standardized approach to observability, offering an effective means to prevent vendor lock-in.",
    [
      "the decision to drop out of school is one that carries profound implications, not only for the individual involved but also for the wider society. this complex issue intersects with various socioeconomic, cultural, and personal factors, leading to a range of long-term effects that extend far beyond the immediate impli cations of leaving school. for individual consequences, dropping out of school often results in reduced employment opportunities and earning potential. without the foundational education provided by a high school diploma or equivalent, individuals may find themselves limited to lower-wage jobs with fewer bene fits and job security. this, in turn, can lead to a lifetime of financial instability and limited social mobility. the repercussions of school dropouts ripple through society. economically, a lower-educated workforce can mean reduced productivity and innovation, impacting national competitiveness and economic growth. societally, there can be an increase in dependency on welfare systems and public assistance, as indivi duals without adequate education may struggle to support themselves and their families. education is closely linked to health outcomes. individuals who leave school prematurely are at a greater risk of en gaging in unhealthy behaviors such as substance abuse and may have limited access to healthcare and health information. this not only affects their personal health and well-being but also places a burden on public health systems. high dropout rates often disproportionately affect marginalized communities, exacerbating existing inequalities. this perpetuates a cycle of poverty and limited educational attainment, impacting generations. education plays a critical role in fostering civic awareness and participation. in dividuals who drop out of school may be less likely to engage in civic activities, vote, or participate in community organizations, leading to a less engaged and informed citizenry. in a global context, the edu cational attainment of a population is a key factor in a country’s ability to compete and cooperate on the international stage. higher dropout rates can impede a nation’s development and its ability to address global challenges effectively. evaluating user profiles plays a crucial role in anticipating risk situations. therefore, the focus of this master thesis is to process data using machine learning and deep learning methods. the aim is to develop detailed profiles that include information on the emotional state of individuals, identification of risk alerts predictions of potential warning situations, and strategies to be adopted. this master thesis aims to use these advanced technologies to create an effective risk prediction and response system. so, it’s present a study on the classification of middle and secondary school students based on their interactions with a bot and their school’s management system, in order to identify risk behaviors and predict success or failure in school. through the use of machine learning techniques, the answers given by students will be analyzed to extract relevant characteristics and predict their math and portuguese grades. the results of this study will provide insight into the potential use of student data from this age group as a means of identifying at-risk behaviors and predicting student success in school, and will inform the development of more effective interventions and support for at-risk students.",
      "a decisão de abandonar a escola tem implicações profundas, não só para o indivíduo em causa, mas também para a sociedade em geral. esta questão complexa cruza-se com vários factores socioeconómicos, culturais e pessoais, conduzindo a uma série de efeitos a longo prazo que vão muito além das implicações imediatas do abandono escolar. em termos de consequências individuais, o abandono escolar resulta frequentemente numa redução das oportunidades de emprego e do potencial de rendimento. sem a formação de base proporcionada por um diploma do ensino secundário ou equivalente, os indivíduos podem ver-se limitados a empregos com salários mais baixos, com menos benefícios e segurança no emprego. isto, por sua vez, pode levar a uma vida de instabilidade financeira e mobilidade social limitada. as repercussões do abandono escolar estendem-se a toda a sociedade. do ponto de vista económico, uma mão de obra menos instruída pode significar uma redução da produtividade e da inovação, com impacto na competitividade nacional e no crescimento económico. a nível social, pode haver um aumento da dependência dos sistemas de segurança social e de assistência pública, uma vez que os indivíduos sem educação adequada podem ter dificuldades em sustentar-se a si próprios e às suas famílias. a educação está intimamente ligada aos resultados no domínio da saúde. os indivíduos que abandonam a escola prematuramente correm um maior risco de se envolverem em comportamentos pouco saudáveis, como o abuso de substâncias, e podem ter um acesso limitado aos cuidados de saúde e à informação sobre saúde. isto não só afecta a sua saúde e bem-estar pessoais, como também sobrecarrega os sistemas de saúde pública. as elevadas taxas de abandono escolar afectam muitas vezes de forma desproporcionada as comunidades marginalizadas, exacerbando as desigualdades existentes. esta situação perpetua um ciclo de pobreza e de sucesso escolar limitado, afectando gerações. a educação desempenha um papel fundamental na promoção da consciência cívica e da participação. os indivíduos que abandonam a escola podem ter menos probabilidades de se envolver em actividades cívicas, votar ou participar em organizações comunitárias, o que conduz a uma cidadania menos empenhada e informada. num contexto global, o nível de escolaridade de uma população é um fator chave na capacidade de um país competir e cooperar na cena internacional. taxas de abandono escolar mais elevadas podem impedir o desenvolvimento de uma nação e a sua capacidade de enfrentar eficazmente os desafios globais. a avaliação dos perfis dos utilizadores desempenha um papel crucial na antecipação de situações de risco. por isso, o foco desta tese de mestrado é o tratamento de dados utilizando métodos de machine learning e deep learning. o objetivo é desenvolver perfis detalhados que incluam informações sobre o estado emocional dos indivíduos, identificação de alertas de risco, previsões de potenciais situações de alerta e estratégias a adotar. esta tese de mestrado tem como objetivo utilizar estas tecnologias avançadas para criar um sistema eficaz de previsão e resposta a riscos. portanto apresenta um estudo sobre a classificação de estudantes do ensino básico e do secundário com base nas suas interações com um chatbot e no sistema de gestão da sua escola, a fim de identificar comportamentos de risco e prever o sucesso ou insucesso na escola. através da utilização de técnicas de machine learning, as respostas dadas pelos estudantes serão analisadas para extrair características relevantes e prever as suas notas de matemática e português. os resultados deste estudo proporcionarão uma visão sobre o uso potencial de dados de alunos desta faixa etária como meio de identificar comportamentos de risco e prever o sucesso dos alunos na escola, e informarão o desenvolvimento de intervenções e apoio mais eficazes para os alunos em risco."
    ],
    0.3
  ],
  [
    [
      "the continuous social and economic development has led, over time, to an increase in consumption, as well as a greater demand from the consumer for what he buys. in this sense retailers have the need to respond to these challenges and explore new opportunities. naturally, the selling price of a product assumes a fundamental role in the purchase decision, and in that way the retailers must carefully analyze and define the best price for each product, based on several factors, such as: perceived value of the product, positioning of the product, the company strategy, competition. faced with all these challenges, the use of information systems is essential for retailers so that it can support them in the pricing decision. these information systems are becoming increasingly complex, including demand forecasts, and making recommendations based on balanced buying patterns due by the economic evolution of markets. in a first phase the ideia was to make a study on two main price recommendation systems: rules motors and price optimization. as the objective of the dissertation is to change the algorithm of regular price optimization of the software tool profimetrics, part of the study was conducted according to the methodology of the tool. after an analysis of the company’s current algorithm, the changes were made to perfect it. subsequently, we used the case study methodology, in the application of the algorithm developed to a retail company. through this case study it was possible to make a brief diagnosis in order to compare the current algorithm of the company with the developed algorithm.",
      "o contínuo desenvolvimento social e económico tem conduzido ao longo do tempo a um aumento do consumo, assim como uma maior exigência do consumidor relativamente ao que compra. neste sentido os retalhistas têm necessidade de dar respostas a estes desafios e explorar novas oportunidades. naturalmente, o preço de venda de um produto assume um papel fundamental na de-cisão de compra do consumidor e neste sentido os retalhistas necessitam de cuidadosa-mente analisar e definir qual o melhor preço para cada produto, tendo como base diversos fatores, tais como: valor recebido do produto, posicionamento do produto, estratégia da empresa, concorrência. face a todos estes desafios, torna-se essencial para os retalhistas o uso de sistemas de informação que os apoiem na tomada de decisão do preço de venda. estes sistemas de informação são cada vez mais complexos, contendo previsões da procura e efetuando recomendações com base em padrões de compra balanceado pela evolução económica. neste trabalho, numa primeira fase realiza-se um estudo sobre dois os principais sistemas de recomendação de preço: motores de regras e otimização de preço. como o objetivo principal da dissertação é a alteração do algoritmo de otimização do preço regular da ferramenta de software profimetrics, parte do estudo foi conduzido de acordo com metodo-logia da ferramenta. depois de unia análise ao algoritmo atual da empresa, foram feitas as alterações necessárias de forma aperfeiçoá-lo. posteriormente, recorreu-se à metodologia de estudo de caso, na aplicação do algoritmo desenvolvido a unia empresa retalhista. através deste caso de estudo realiza-se um breve diagnóstico de forma a comparar o algoritmo atual da empresa com o algoritmo desenvolvido."
    ],
    [
      "as ameaças cibernéticas constituem um grande problema para as empresas. desde phishing a ran somware, são distribuídas principalmente através de correio eletrónico, através do qual os atacantes fazem-se passar por fontes confiáveis, convencendo deste modo a vítima a clicar no link e/ou anexo malicioso. este problema agrava-se com a prática de spear-phishing, especialmente prevalente no con texto empresarial, que personaliza o e-mail com informações específicas relevantes para o(s) alvo(s), o que aumenta a eficácia do ataque. os objetivos do trabalho realizado foram a pesquisa e o desenvolvimento de ferramentas que permitam identificar estes e-mails maliciosos. as soluções propostas tiram partido de protocolos como o dkim, o spf e o dmarc para ajudar a classificar a confiabilidade de um e-mail suspeito. dado um e-mail, o objetivo é obter o resultado para cada um destes protocolos, que pode ter sido calculado por outros servidores de correio eletrónico antes de chegar ao destinatário ou é calculado após a sua receção. ambos os tipos de validações foram desenvolvidas e ficaram operacionais, com a exceção da validação “metódica” do dkim (feita in house), devido ao seu posicionamento limitado pela arquitetura imposta. os testes confirmaram o bom funcionamento e resiliência das soluções contra e-mails provenientes de domínios diferentes.",
      "cyber threats pose a significant problem for companies. from phishing to ransomware, they’re distributed mainly through e-mail, in which the attackers pose as reliable sources, convincing the victim to click on the malicious link/attachment. this is exarcebated by spear-phishing, especially prevalent in the corporate world, which tailors the e-mail with specific and relevant information for the target(s) that increases the attack’s rate of success. the objectives of this dissertation are the research and development of tools that can identify these malicious e-mails. the proposed solutions take advantage of protocolos like dkim, spf and dmarc to help score the legitimacy of a suspected e-mail. given an e-mail, the goal is to get the result for each of these protocols, which may have been calculated by other e-mail servers before reaching the recipient or is computed after it’s received. both types of validations were developed and are operational, except for the “methodic” validation of dkim (done in-house), because of its position limited by the imposed architecture. testing confirmed good behaviour and robustness of the solutions against e-mails from different domains."
    ],
    0.3
  ],
  [
    [
      "os dispositivos conectados pertencem à área de ”ambient intelligence” (ami) e são dispositivos inteligentes que podem fornecer diversos serviços ou através de comandos por voz ou de forma autónoma. estes dispositivos conseguem ser autónomos, devido ao facto de conseguirem capturar informação do ambiente através dos seus sensores e depois processá-la, de modo a que consigam ativar a ação necessária (”context-aware computing”). os assistentes digitais também pertencem à área de ami e são programas de software baseados em ”natural user interfaces”, o que significa que estes funcionam com recurso a comandos por voz para efetuar uma determinada ação [46]. os assistentes podem estar presentes em dispositivos conectados e foram desenvolvidos para ajudar as pessoas nas suas tarefas diárias. devido ao aumento no uso de assistentes digitais, surgiu a necessidade de atender às exigências de uma gama mais ampla de utilizadores, dado que as funcionalidades básicas, para as quais os assistentes haviam sido programados, já não eram suficientes. esta necessidade levou a uma nova abordagem em relação à expansão das funcionalidades dos assistentes digitais, que consistiu na criação de aplicações por voz. as aplicações por voz ainda são relativamente recentes e como tal ainda não existem muitas ferra mentas, padrões arquiteturais que tenham sido estabelecidos ou uma metodologia ”standard” que possa ser usada no processo de desenvolvimento. este problema é ainda maior se abordarmos as aplicações por voz ”cross-platform”, dado que hoje em dia existe uma abundância de diferentes assistentes digitais integrados. a inexistência de uma metodologia ”standard” significa que os programadores irão acabar por usar a(s) metodologia(s) que lhes pareçam as mais adequadas tendo em conta o seu objetivo de obter um produto estável. a falta de standardização e de suporte ao desenvolvimento ”cross-platform” de aplicações por voz é a motivação desta dissertação de mestrado. o objetivo desta dissertação é o desenvolvimento de um processo de construção independente de plataforma, que irá promover a criação de aplicações por voz ”cross-platform” e a automatização do mesmo. este processo vai estar disponível através de uma plataforma, com um editor visual incorporado, que irá permitir a criação de um template de modelo de linguagem que mais tarde irá ser usado para gerar modelos específicos a uma plataforma de modo a que se possa definir o ”frontend” e código ”boilerplate” para o desenvolvimento inicial da funcionalidade do ”backend”. ao usar esta plataforma, os programadores irão ser capazes de criar e fazer o ”deploy” de aplicações por voz para a amazon alexa e para o google assistant a partir de uma única fonte de informação, apesar das diferenças que existem entre os seus modelos aplicacionais e, mais importante, recorrendo principalmente aos requisitos pretendidos e não somente aos aspectos tecnológicos.",
      "connected devices belong in the ambient intelligence (ami) area, and are intelligent devices that can provide various services through voice commands or autonomously. these devices can be autonomous due to the fact that they can gather information from the environment through their sensors, and then process it in order to trigger the necessary action (context-aware computing). digital assistants also belong to the ami area and are software programs based on natural user interfaces, which means that they work via voice commands to perform a certain action [46]. the assistants can be present in connected devices and were developed in order to help people on their daily tasks. due to the growth in the usage of digital assistants there was a need to cater to a wider range of users and their necessities given that the basic functionalities, that the assistants had been programmed to, weren’t enough. this need led to a new approach regarding the expansion of the digital assistants functionalities, which consisted in the creation of voice applications. voice applications are still relatively new and as such there are still not that many tools, established architectural patterns or even a standard methodology that can be used in the development process. this problem is even bigger if we address cross-platform voice applications, given there is nowadays a plethora of different vendors of integrated digital assistants. the lack of a standardized methodology means that the developers will end up using the methodology(ies) that seems the most adequate concerning the purpose of obtaining a stable product. this lack of standardization and support to the cross-platform development of voice applications is the motivation for this master’s dissertation. the goal of this dissertation is the development of a platform independent construction process that promotes the creation of cross-platform voice applications and its automatization. this process will be made available via a platform, with an incorporated visual editor, that allows the creation of a language model template that will later be used to generate platform-specific models to define the frontend and boilerplate code for the initial development of the backend functionality. by using this platform, the developers will be able to create and deploy voice applications for amazon alexa and google assistant from a single source of information despite the differences in their application models and, most important, resorting primarily to the intended requirements and not only to technological aspects."
    ],
    [
      "the wine industry is facing challenging times due, mostly, to climate change and changing consumer demands. the urge to innovate stimulates r&d of new fermentation processes using non-conventional yeast species (e.g. non-cerevisiae saccharomyces species). while recent research approached the physiology of diverse non-conventional yeast species, little is known about their metabolism in different environmental conditions. in this work, a previously developed dynamic genome-scale model was adapted to study the metabolism of saccharomyces kudriavzevii in wine fermentation at two temperatures, 25ºc and 12ºc. adjustments included the addition of metabolic pathways and dynamic constraints. goodness-of-fit of the model to measurements of the extracellular compounds was satisfactory, i.e. the median values of r2 are 0.95 and 0.87 for 25ºc and 12ºc, respectively. the model was then used to explore the differences in the dynamics of metabolism between temperatures. the most significant differences appeared in the stationary phase: 1) the strain produces more mevalonate and succinate at 25ºc, probably due to a late response to stress and the maintenance of redox balance via the gaba shunt, respectively, 2) erythritol flux is higher at 12ºc, probably due to the conditions of formation lasting longer and 3) the production of higher alcohols, mostly de novo, is higher at 12ºc, due to the longer viability of the cells. the proposed model provided a comprehensive picture of the main steps occurring inside the cell during wine fermentation. model predictions are consistent with experimental data and previous findings, but it also brought novel results, such as the role of the gaba shunt or the production of mevalonate in the metabolism of s. kudriavzevii, worth being explored further.",
      "a indústria vinícola atravessa tempos desafiantes, sobretudo devido às alterações climáticas e às mudanças das exigências dos consumidores. a necessidade de inovar estimula a i&d de novos processos fermentativos usando espécies de leveduras não convencionais, (e.g. espécies saccharomyces não-cerevisiae). apesar de investigações recentes abordarem a fisiologia de diversas espécies de leveduras não convencionais, sabe-se pouco sobre o seu metabolismo em diferentes condições ambientais. neste trabalho, adaptou-se um modelo dinâmico à escala genômica para estudar o metabolismo da saccharomyces kudriavzevii cr85 durante a fermentação de vinho a 25ºc e 12ºc. os ajustes incluem a adição de vias metabólicas e restrições dinâmicas. a adequação do modelo às medições dos compostos extracelulares foi satisfatória, i.e., os valores medianos de r2 são de 0.95 e 0.87 para 25ºc e 12ºc, respectivamente. o modelo foi então utilizado para explorar as diferenças nas dinâmicas do metabolismo entre temperaturas. a maioria das diferenças significativas surgem na fase estacionária: 1) a estirpe produz mais mevalonato e succinato a 25ºc, provavelmente devido a uma resposta tardia ao stress e à manutenção do balanço redox através da via alternativa do gaba, respectivamente, 2) fluxo de eritritol é maior a 12ºc, provavelmente devido às condições da sua formação durarem mais e 3) a produção de álcoois superiores, sobretudo via de novo, é superior a 12ºc, devido à longa viabilidade das células. o modelo proposto permitiu um retrato completo dos principais passos ocorridos no interior da célula durante a fermentação de vinho. as previsões do modelo estão de acordo com os dados experimentais e descobertas anteriores, no entanto, também trouxe resultados inovadores, como o papel da via gaba ou a produção de mevalonato no metabolismo da s. kudriavzevii, que vale a pena ser mais explorado."
    ],
    0.02727272727272727
  ],
  [
    [
      "in the last few years, de novo molecular design has increasingly been using generative models, from the emergent field of deep learning (dl), to propose novel compounds that are likely to possess desired properties/activities, in areas such as drug discovery, materials sciences or biotechnology. a panoply of deep generative models, such as recurrent neural networks, variational autoencoders, adversarial autoencoders and generative adversarial networks, can be trained on existing datasets, and provide for the generation of novel compounds, typically with similar properties of interest. additionally, different optimization strategies, including transfer learning, bayesian optimization, reinforcement learning, and conditional generation, can be used to direct the generation process towards desired aims, regarding their biological activities, synthesis processes or chemical features. various instances of experimental validation of these emerging methods have surfaced, with de novo generated molecules being synthesized and proving successful in in vitro, and even in vivo, assays. these successful practical realizations encourage further research into this blooming field. this dissertation aims to explore the application of generative dl to the de novo molecular design, with a focus on the targeted generation of new compounds. two frameworks were developed to support this endeavor and stand as the main contributions of this work. the first, termed deepmolgen, standardizes the implementation and usage of various generative dl architectures for molecular design. the second, termed eamo, employs multi-objective evolutionary algorithms to navigate the latent space of autoencoder based models, optimizing the generation of molecules with desired characteristics. these frameworks were accompanied with a systematic and critical review on deep generative models, the related optimization methods for targeted compound design, and their applications. four state-of-the-art architectures were implemented, trained and evaluated under the deepmolgen framework using a standard dataset and common metrics such as validity, uniqueness, novelty and the moses benchmark. the results showed that deepmolgen was capable of performing the intended tasks and that most of the implemented models performed on par with their publications. similarly, four case studies from the literature were optimized with eamo and the results compared to previous works. these experiments showed that eamo could control abstract chemical properties and is competitive with other state-of-the-art methods. lastly, the three best performing models were combined with transfer learning and eamo within a pipeline for the generation of sweeteners. the resulting set of 102 promising molecules was reviewed by expert chemists and the pipeline improved with their feedback. a second set of 99 compounds was then generated and the preliminary observations pointed to significantly improved results.",
      "ao longo dos últimos anos, a criação de moléculas de novo tem vindo cada vez mais a utilizar modelos generativos, da área do deep learning (dl), para propor compostos com propriedades/atividades de interesse em áreas como descoberta de fármacos, ciências dos materiais ou biotecnologia. uma panó plia de modelos dl, que incluem arquiteturas como recurrent neural networks, variational autoencoders, adversarial autoencoders e generative adversarial networks, podem ser treinados com conjuntos de da dos existentes permitindo a geração de novos compostos, tipicamente com propriedades de interesse semelhantes. adicionalmente, várias estratégias de otimização, incluindo transfer learning, otimização bayesiana, aprendizagem por reforço e geração condicionada, podem ser utilizadas para guiar o pro cesso de geração em direção a propriedades de interesse como atividade biológica, processo de síntese ou características químicas. têm surgido ainda vários exemplos de validação experimental destes méto dos, nos quais moléculas geradas de novo são sintetizadas e demonstram sucesso em ensaios in vitro e in vivo. estes sucessos práticos encorajam investigações adicionais nesta área emergente. a presente dissertação pretende explorar a aplicação de dl generativo para o desenho de moléculas de novo, com um foco na geração direcionada de novos compostos. duas frameworks foram desenvolvidas para este propósito e constituem as principais contribuições deste trabalho. a primeira, deepmolgen, padroniza a implementação e utilização de variadas arquiteturas de dl para o desenho molecular. a segunda, eamo, aplica algoritmos evolucionários para navegar o espaço latente de modelos baseados em autoencoders, otimizando a geração de moléculas com características pretendidas. estas frameworks foram acompanhadas de uma revisão sistemática sobre modelos generativos de dl, métodos de otimiza ção para a geração direcionada de compostos, e as suas respetivas aplicações. quatro arquiteturas do estado-da-arte foram implementadas, treinadas e avaliadas com o deepmolgen, usando um conjunto de dados standard e métricas comuns como validade, unicidade, novidade e o conjunto de testes moses. os resultados mostraram que o deepmolgen conseguiu realizar as tarefas pretendidas e que a maioria dos modelos comportaram-se de forma semelhante às respetivas publicações. de forma semelhante, quatro casos de estudo da literatura foram otimizados com o eamo e os resulta comparados com publicações prévias. estas experiências mostraram que o eamo é capaz de controlar propriedades químicas abstratas e que é competitivo com outras abordagens do estado-da-arte. por fim, os três melhores modelos foram combinados com transfer learning e o eamo para abordar a geração de compostos adoçantes. o conjunto de 102 moléculas resultante foi revisto por especialistas em química e a metodologia melhorada com os comentários. um segundo conjunto de 99 compostos foi então gerado e os comentários preliminares apontaram para uma melhoria significativa dos resultados."
    ],
    [
      "nas estações de tratamento de águas residuais (etar), a exploração da composição taxonómica e da abundância de comunidades microbianas tornou-se um esforço indispensável. é essencial para otimizar os processos de tratamento, monitorizar a saúde do sistema e garantir a conformidade com as normas ambientais. os microrganismos desempenham um papel crucial na decomposição da matéria orgânica, na remoção de contaminantes e na recuperação de recursos. o conhecimento das comunidades microbianas leva a estações de tratamento de águas residuais (etars) mais eficientes e resilientes, promovendo a saúde pública e a sustentabilidade ambiental. a utilização do sequenciamento do gene 16s rrna para a análise de comunidades microbianas em digestores anaeróbicos em grande escala em estações de etars tem vindo a crescer. essa metodologia envolve uma série de processos computacionais, abrangendo avaliação da qualidade da sequência, remoção de ruído, classificação taxonômica, alinhamento e construção de árvores filogenéticas. notavelmente, o conjunto de software quantitative insights into microbial ecology versão 2 (qiime2) emergiu como uma ferramenta valiosa, simplificando a análise de dados do gene marcador 16s rrna. facilita a análise ponta a ponta de diversos conjuntos de dados de microbiomas e facilita estudos comparativos com dados disponíveis publicamente. qiime2 equipa os investigadores com ferramentas para selecionar profundidades de amostragem apropriadas para conduzir análises de diversidade alfa e beta. nesta tese, apresentamos uma análise comparativa abrangente das comunidades microbianas que habitam digestores anaeróbios de grande escala em etars. o estudo utiliza a sequenciação de nova geração, nomeadamente a sequenciação de alto rendimento do gene 16s rrna, e depois utiliza o qiime2 para determinar e analisar a taxonomia, a diversidade e a abundância relativa das comunidades de digestores em cada etar e entre elas. ion torrent foi a tecnologia utilizada para a sequenciação de alto rendimento de nova geração do gene 16s rrna. o fluxo de trabalho utilizado no qiime2 consistiu na importação de dados, no controlo de qualidade (denoising), clustering e só depois a análise da diversidade e da taxonomia para determinar a composição, a diversidade e a abundância das comunidades. os resultados mostraram que as comunidades microbianas permaneceram bastante estáveis durante diferentes pontos de amostragem, mas distantes quando comparadas entre diferentes digestores de lamas. porém, alguns dos microrganismos que compõem a comunidade metanogénica presentes nas comunidades foram methanobacteriales, methanomicrobiales, methanosarcinales. em relação à comunidade bacteriana, os microrganismos mais abundantes em um determinado digestor e presentes em todos os digestores foram atribuídos a sedimentibacter, phycicoccus, thermovirgae, cloacimonas e phycicoccus.",
      "in wastewater treatment plants (wwtps), exploring the taxonomic composition and abundance of microbial communities has become an indispensable effort. it is essential for optimizing treatment processes, monitoring system health and ensuring compliance with environmental standards. microorganisms play a crucial role in decomposing organic matter, removing contaminants and recovering resources. knowledge of microbial communities leads to more efficient and resilient wastewater treatment plants (wwtps), promoting public health and environmental sustainability. the use of 16s rrna gene sequencing to examine microbial communities in large-scale anaerobic digesters at wwtps has been growing. this methodology involves a series of computational processes, covering sequence quality assessment, noise removal, taxonomic classification, alignment and construction of phylogenetic trees. notably, the quantitative insights into microbial ecology version 2 (qiime2) software suite has emerged as a valuable tool, simplifying analysis of 16s rrna marker gene data. facilitates end-to-end analysis of diverse microbiome datasets and facilitates comparative studies with publicly available data. qiime2 equips researchers with tools to select appropriate sampling depths to conduct alpha and beta diversity analyses. in this thesis, we present a comprehensive comparative analysis of the microbial communities inhabiting large-scale anaerobic digesters within wwtps. the study uses next-generation sequencing, namely high throughput sequencing of the 16s rrna gene, and then uses qiime2 to determine and analyze the taxonomic, diversity and relative abundance of digester communities within each wwtp and between them. ion torrent was the technology used for next-generation high-throughput sequencing of the 16s rrna gene. the workflow used in qiime2 consisted of importing data, quality control (denoising), clustering and only then analyzing diversity and taxonomy to find out the composition, diversity and abundance of communities. the results showed that the microbial communities were quite stable during different sampling points, but were distant when compared between different sludge digesters. however, some of the microorganisms that make up the methanogenic community present in the communities were methanobacteriales, methanomicrobiales, methanosarcinales. regarding the bacterial community, the most abundant microorganisms in a given digester and present in all digesters were attributed to sedimentibacter, phycicoccus, thermovirgae, cloacimonas, and phycicoccus."
    ],
    0.03
  ],
  [
    [
      "diabetic retinopathy (dr) is a common complication of diabetes, which is among the major causes of vision loss in the world. an early detection of the disease is the key to avoid the patient’s blindness. however, at the initial phase of the disease, the vision impairment is not easily percieved by the patient. therefore, regular follow-up exams are recommended in order to detect anomalous patterns in the patient’s retina. exudates are one of the most prevalent signs during the early stage of dr and, therefore, its early detection is vital to prevent the patient’s blindness. however, the manual detection of exudates by experts is laborious and time-consuming. thus, automated screening techniques for exudate detection have great significance in saving cost, time and labor, allowing the ophthalmologists to make the treatment decision timely. in this sense, one of the main objectives of this thesis is to develop and compare different strategies to locally extract information of fundus images for detecting exudates. several methods related to the automatic detection of exudates have been proposed in the literature however, these methods focus their efforts in the segmentation of exudates or require the extraction of features from a lesion candidate map. on the other hand, in the methodologies proposed in this thesis, the characterization of healthy and damaged retinal areas is performed by applying image descriptors in a local way, avoiding the segmentation step and the generation of candidate maps. a system based on local feature extraction and support vector machine classification is used to develop and compare different strategies for automated detection of exudates. the main novelty of this work is allowing the detection of exudates using non-regular regions to perform the local feature extraction. to accomplish this objective, different methods for generating superpixels are applied to the fundus images of e-ophta database and texture and morphological features are extracted for each of the resulting regions. finally, each region is classified according to healthy and pathological classes, during the classification stage. the strategies proposed in order to generate superpixels rely on applying the marker-controlled watershed transformation to a spatially regularized gradient. from these strategies, two different types of superpixels are created: c-waterpixels and m-waterpixels. in the end, an elaborated comparison between the proposed methods for generating m and c-waterpixels and the state-of-the-art method for generating slic superpixels is performed. additionally, a system based on convolutional neural networks (cnn) is explored to discriminate between healthy and pathological regions in fundus images. transfer learning is applied to fine-tune some of the most important state-of-the-art cnn architectures. exudates usually represent less than one percent of the total number of pixels that compose the retinal image. this is the reason why, in both the systems presented in this thesis, the fundus images are divided in superpixels and the classification is performed for each of the regions. lastly, an exhaustive comparison between the two created systems to automatically detect exudates is performed. in other words, the classification results obtained through the system involving cnns are compared with the ones obtained by applying the approach based on feature extraction and subsequent classification using machine learning algorithms.",
      "a retinopatia diabética (rd) é uma complicação da diabetes, que está entre as maiores causas de perda de visão no mundo. a deteção da doença durante a sua fase inicial é a chave para evitar a cegueira do paciente. no entanto, a debilitação da visão é muitas das vezes impercetível para o doente durante esta fase. por este motivo, é recomendado que pacientes com diabetes realizem exames de acompanhamento regulares de forma a detetar possíveis anomalias nas suas retinas. os exsudatos são um dos sinais mais prevalentes durante a fase inicial da rd e, deste modo, a sua deteção precoce é vital para prevenir a cegueira do paciente. no entanto, a deteção manual destas lesões é trabalhosa e demorada. as técnicas que permitem a deteção automática de exsudatos desempenham então um papel importante na diminuição dos custos, tempo e trabalho, permitindo que os oftalmologistas atribuam o tratamento a tempo de evitar a cegueira do paciente. neste sentido, um dos principais objetivos desta tese é o desenvolvimento de diferentes estratégias que extraiam informação localmente de imagens fundus, com o intuito de detetar exsudatos. diversos métodos relacionados à deteção automática de exsudatos têm sido propostos na literatura. no entanto, esses métodos baseiam-se na segmentação de exsudatos ou exigem a extração de informação a partir de um mapa de candidatos. por outro lado, nas metodologias apresentadas neste trabalho, a caracterização do tecido saudável e patológico é feita aplicando descritores de imagem localmente. um sistema baseado numa extração local de características e posterior classificação é usado para desenvolver e comparar diferentes estratégias de deteção automatizada de exsudatos. o caracter inovador deste trabalho é permitir a deteção de exsudatos recorrendo a regiões não-regulares para realizar a extração local de características. para atingir este objetivo, diferentes métodos para gerar superpixels foram aplicados às imagens da base de dados e-ophta e características texturais e morfológicas foram extraídas para cada uma das regiões resultantes. por fim, cada região é classificada em patológica ou saudável durante o processo de classificação. as estratégias propostas para gerar superpixels consistem em aplicar a chamada watershed transformation a um gradiente espacialmente regularizado. a partir da referida estratégia, são obtidos dois tipos de superpixels: c-waterpixels e m-waterpixels. no final, é realizada uma comparação elaborada entre os métodos propostos para a geração de m e c-waterpixels e o método estabelecido na literatura para gerar superpixels slic. para além disso, é ainda explorado um método baseado em redes neurais convolucionais (cnn) para detetar regiões patológicas e saudáveis em imagens da retina. a técnica de transfer learning é usada para transferir conhecimento sobre o tecido da retina para alguns dos modelos mais relevantes pré-treinados com a base de dados imagenet. normalmente, os exsudatos representam menos de um porcento to número total de pixéis que compõem a imagem e este é o motivo pelo qual, em ambos os sistemas apresentados neste trabalho, as imagens são divididas em superpixeis e a classificação é feita para cada uma das regiões. por fim, uma comparação exaustiva entre os dois sistemas criados para detetar automaticamente exsudatos é realizada. isto é, os resultados de classificação obtidos através do sistema baseado em cnns são comparados com os obtidos através da aplicação da abordagem baseada na extração de características e subsequente classificação usando algoritmos de machine learning."
    ],
    "com a crescente utilização e adoção de dispositivos orientados à internet of things, especialmente dispositivos orientados ao espaço casa, crescem também o número de protocolos, apis e frameworks desenvolvidos pelos fabricantes para complementar os seus dispositivos. este elevado número de soluções, sem a adoção de um standard geral que facilita o desenvolvimento de aplicações para esta área, serve de motivação para esta dissertação. com isto em mente, esta dissertação tem como objetivo o desenvolvimento de uma camada de middleware, que faz a ligação entre os mais diversos dispositivos e as aplicações clientes, abstraindo os dispositivos e as funcionalidades, facilitando imenso o desenvolvimento destas aplicações. este middleware também deverá oferecer algumas comodidades a nível de utilização, sob a forma de mecanismos de automação e definição de cenários. a dissertação irá expor o estado da tecnologia nesta área, identificando as mais valias e também os possíveis problemas das mesmas. também será revisto algum trabalho de investigação na área, de modo a entender os esforços já feitos. feito o estudo sobre o estado de arte, iremos proceder ao desenho e conceção da arquitetura do middleware, passando para o desenvolvimento do mesmo. por fim, será feito um caso de estudo, sobre a forma de uma aplicação mobile, que faz uso do middleware, demonstrando todas as suas funcionalidades e capacidades de interoperabilidade.",
    0.12857142857142856
  ],
  [
    [
      "quantum simulation is one of the most relevant applications of quantum computation for the near future, due to its scientific impact and also because quantum simulation algorithms are typically less demanding than generalized quantum computations. ultimately, the success of a quantum simulation depends on the amount and reliability of information one is able to extract from the results. in such a context, this work reviews the theory behind quantum simulation, with a focus on digital quantum simulation. the concepts of efficiency and reliability in quantum simulations are discussed, particularly for implementations of digital simulation algorithms in state-of-the-art quantum computers. a review of approaches for quantum characterization, verification and validation techniques (qcvv) is also presented. a digital quantum simulation of the schrödinger equation for a single particle in 1 spatial dimension was experimentally implemented and analyzed, along with a quantum state tomography procedure for characterization of the final quantum state and evaluation of simulation reliability. from the literature, it is shown that digital quantum simulation is theoretically sound and experimentally feasible, with several applications in a wide range of physics-related fields. nonetheless, a number of conditions arise that must be observed for a truly efficient implementation of a digital quantum simulation, from theoretical conception to experimental circuit design. the review of qcvv techniques highlights the need for characterization and validation techniques that could be efficiently implemented for current models of quantum computation, particularly in instances where classical verification is not tractable. however, there are proposals for efficient verification procedures when a set of parameters defining the final result of the simulation is known. the experimental simulation demonstrated partial success in comparison with an ideal quantum simulation. from the results it is apparent that better coherence times, better reliability and finer control are as decisive for the advancement of quantum computing power as the more-publicized number of qubits of a given device.",
      "a simulação quântica é uma das aplicações mais relevantes da computação quântica num futuro próximo, não só devido ao seu impacto científico como também porque os algoritmos de simulação quântica são tipicamente menos exigentes do que algoritmos quânticos numéricos. em última análise, o sucesso de uma simulação quântica depende da quantidade e fiabilidade das informações que é possível extrair dos resultados. neste contexto, este trabalho apresenta uma revisão da teoria da simulação quântica, com ênfase na simulação quântica digital. os conceitos de eficiência e fiabilidade em simulações quânticas são discutidos, particularmente para implementações de algoritmos de simulação digital. uma revisão de técnicas de caracterização, verificação e validação de sistemas quânticos (qcvv) é também apresentada. uma simulação quântica digital da equação de schrödinger para uma única partícula a uma dimensão espacial foi implementada experimentalmente e analisada, juntamente com um método de tomografia de estado quântico para a caracterização do estado quântico final e avaliação da fiabilidade da simulação. a partir da literatura, é demonstrado que a simulação quântica digital é teoricamente sólida e experimentalmente viável, com várias aplicações em diversas áreas da física. no entanto, existem várias condições a ter em conta para uma implementação verdadeiramente eficiente de uma simulação quântica digital, da sua concepção teórica até à implementação experimental de circuitos. a revisão de técnicas qcvv destaca a necessidade de técnicas de caracterização e validação que possam ser eficientemente implementadas para modelos atuais de computação quântica, particularmente em instâncias em que a verificação clássica não é possível ou desejável. no entanto, existem propostas para técnicas de verificação que são eficientes quando se conhece, a priori, um conjunto de parâmetros característicos do resultado final da simulação. a simulação experimental demonstrou sucesso parcial relativamente a uma simulação quântica ideal. a partir dos resultados, evidencia-se que melhores tempos de coerência, maior fiabilidade e controlo mais refinado são tão decisivos para o avanço da computação quântica quanto o número de qubits de um dispositivo."
    ],
    [
      "devido à actual conjunctura económica e social, é necessário adoptar medidas ao nível das instituições de saúde que visem uma redução de custos. estas medidas devem fazer-se acompanhar de uma política de qualidade na prestação de cuidados de saúde. tem de se verificar o cumprimento dos requisitos necessários para um tratamento adequado dos pacientes e uma diminuição dos erros cometidos pelos profissionais de saúde. deslocações regulares às unidades de saúde traduzem-se num maior congestionamento dos serviços das várias unidades, conduzindo a uma menor qualidade do serviço prestado ao paciente. os últimos desenvolvimentos nos campos da tecnologia de sensores biomédicos e dispositivos móveis, combinados com maiores capacidades das comunicações sem fios, têm tornado possível o surgimento de novos paradigmas de monitorização de saúde. este tipo de paradigmas visa uma melhoria da qualidade de vida dos pacientes e uma diminuição da afluência às unidades de saúde. as funcionalidades cada vez maiores dos dispositivos móveis como os personal digital assistants (pdas) e os smartphones fazem deles um componente vital em sistemas de monitorização de saúde. neste trabalho, define-se uma arquitectura de um sistema de monitorização remota do sinal de electrocardiograma (ecg) de pacientes em mobilidade. para este projecto recorre-se ao bitalino como plataforma de aquisição de sinais vitais. a utilização das tecnologias no contexto da saúde implica o cumprimento de elevados padrões de segurança. deve ser possível a transmissão de informação dos vários utentes sem que esta esteja sujeita a possíveis ataques informáticos (ataques do tipo man in the middle e dos (denial of service) ou ddos (distributed denial of service) ao servidor).",
      "due to the current economic and social context, it is necessary to adopt measures at the healthcare providers’ level which presuppose a cost reduction. this leverage in costs ought to be accompanied with a healthcare quality policy. this policy determines the accomplishment of the necessary requirements for a proper treatment of patients and a reduction of the errors committed by the healthcare personnel. regular commuting to the healthcare units corresponds to a major congestion of the services of the several units which yields to a decrease in the quality of the service provided to the patient. the most recent developments in the core fields of biomedical sensor technology and handheld devices, associated with greater capacities of wireless communications, have made possible the appearance of new healthcare monitoring paradigms whose purpose is to improve the quality of life of the patients and to reduce the overall congestion of the healthcare units. the increased functionalities of the mobile devices such as personal digital assistants (pdas) and smartphones make themselves a vital component in innovative systems of healthcare units. in this thesis, an architecture of a remote monitoring system of the electrocardiogram signal (ecg) of patients in mobility is defined, using bitalino as the vital sign acquisition platform. the recurrent use of technologies in the healthcare context implies the fulfillment of high security standards. this way data transmission of the several patients is possible without being too susceptible to man in the middle attacks or denial of service (dos) and distributed denial of service (ddos) attacks at the server side."
    ],
    0.0
  ],
  [
    [
      "there is a plethora of information inside the web. even the most famous commercial search engines cannot download and index all available information. for this reason, from the last years until now, there are several research works on the design and implementation of focused crawlers in a particular topic, and also on geographic scope crawlers. those who follow carefully the research on the area of web crawling are witnessing that the temporal dimension has not the importance it deserves in the literature.in the opposite direction, there is an increasing interest on time dimension in other areas of information retrieval namely retrieval models, result sets presentation, clustering, classification, and others. therefore, the challenge we have set ourselves in this work, was to develop a crawler whose purpose is to deal with time constraints. the importance of this dimension is certainly quite amplified when combined with the topic or geography, but now we wanted to study it in isolation. the used approach is quite direct. it is based on an algorithm for temporal segmentation ofweb pages and follows links only in segments within the temporal scope of the restriction. this system is designed forweb pages written in portuguese though its design philosophy can be applied to other languages. in addition and for increase results effectiveness, the used algorithm prioritized the downloading of pages with more links within the temporal scope. the precision of results is around 75%.",
      "existe uma infinidade de informações dentro da web. até mesmo os motores de busca mais famosos não podem descarregar e indexar toda a informação disponível. por esta razão, desde há já alguns anos que há vários trabalhos de investigação sobre o desenho e implementação de robôs focados num tópico em particular mas também em robôs de âmbito geográfico. aqueles que seguem com atenção a investigação na área de descargas web podem constatar que a dimensão temporal não tem a importância que merece na literatura. na direcção oposta, há um interesse crescente sobre a dimensão temporal em outras áreas da recolha de informação, nomeadamente modelos de recolha, apresentação de conjuntos de resultados, agrupamento, classificação entre outros. o desafio para que este trabalho aponta é desenvolver um robô cujo propósito seja lidar com as restrições temporais. a importância desta dimensão é certamente amplificada quando combinada com o tópico ou a geografia, mas agora apenas a iremos estudar isoladamente. a abordagem aplicada é muito directa. é baseada num algoritmo de segmentação temporal de textos e segue apenas as ligações em segmentos dentro do âmbito temporal imposto pela restrição. este sistema está concebido para páginas web em português, embora a sua filosofia possa ser aplicada a outras línguas. além disso, e para melhorar os resultados, o algoritmo utilizado prioriza o descarregamento de páginas com mais ligações dentro do âmbito temporal. a precisão dos resultados ronda os 75%."
    ],
    [
      "a presença ubíqua das redes sociais faz com que estas sejam plataformas ideais para a disseminação de notícias, mas quando o conteúdo destas notícias é falso estas podem pôr em risco a privacidade digital de um utilizador. a disseminação de notícias nas redes sociais é principalmente feita com base em relações de confiança e confiabilidade entre os utilizadores, estas relações estão então diretamente ligadas com o poder de disseminação de um utilizador. a partilha e compartilha das notícias nas redes sociais é atualmente feita por utilizadores e contas bot, a disseminação das notícias falsas por bots põem a privacidade digital dos utilizadores das redes sociais em risco. de forma a ter a recolher dados que pudessem ser livremente manipulados foi utilizado a ferramenta ”fakenewsnet”, esta permitiu que fosse criado um dataset com os dados recolhidos da rede social twitter. de forma a melhor compreender as redes de disseminação das notícias, os dados recolhidos foram aplicados ao ”community health assessment model”, este modelo é baseado nos modelos epidemiológicos, e que permite obter dados sobre a disseminação das notícias nas redes sociais entre utilizadores e dentro de comunidade ou ”echo chambers”. os dados contidos no dataset foram também aplicados ao ”botometer”, um modelo supervisionado de deteção de contas bot no twitter, em que as contas dos disseminadores de notícias foram analisadas. com os resultados obtidos dos modelos aplicados, é medido o impacto das contas bot na disseminação das notícias. o impacto das contas bot para a disseminação das notícias é entre 2,9% a 6,9% na disseminação de notícias dentro de comunidades e nas notícias falsas contribuem entre 2,8% a 0,7% na disseminação entre utilizadores, sendo que estas contas correspondem entre 0,272% a 0,296% da população que dissemina as notícias. utilizando os dados dos dataset foi também feita uma análise de como as notícias são disseminadas. a análise foi feita em duas partes, uma parte foi dedicada às publicações e as relações que os utilizadores têm com contas bot e a outra parte foi dedicada ao poder de disseminação dos utilizadores. a partir da análise foi identificado que as contas bot desempenham um papel maior na disseminação das notícias falsas. utilizando esses dados são propostas medidas de modo a que os utilizadores consigam se proteger da influência das contas bot.",
      "the ubiquitous presence of social networks makes them ideal platforms for spreading news, but when the content of this news is fake, it can put a user’s digital privacy at risk. the dissemination of news on social networks is mainly done based on trust and trustworthiness relationships between users; these relationships are then directly linked with the dissemination power of a user. the bot users and accounts currently do the sharing and resharing of news on social networks, and the dissemination of fake news by bots puts the digital privacy of social network users at risk. to collect data that could be freely manipulated, the tool ”fakenewsnet” was used, which allowed the creation of a dataset with the data collected from the social network twitter. to better understand the news dissemination networks, the collected data was applied to the ”community health assessment model”, which is based on epidemiological models and allows to obtain data about the dissemination of news on social networks between users and within communities or ”echo chambers”. the data contained in the dataset was also applied to the ”botometer”, a supervised twitter bot account detection model, in which the accounts of news disseminators were analyzed. with the results obtained from the applied models, the impact of bot accounts on news dissemination is measured. the effect of bot accounts on news dissemination is between 2.9% and 6.9%on news dissemination within communities. on fake news, they contribute between 2.8% and 0.7% to dissemination among users, and these accounts correspond between 0.272% and 0.296% of the news disseminating population. an analysis of how the news is spread was also performed using the dataset data. the study was done in two parts. one was dedicated to the publications and the relationships that users have with bot accounts, and the other was to the users dissemination power. from the analysis, it was identified that bot accounts play a more significant role in disseminating fake news. using this data, measures are proposed so that users can protect themselves from the influence of bot accounts."
    ],
    0.0
  ],
  [
    [
      "currently, the chronic diseases of hypertension and diabetes have a huge prevalence not only in portugal but also in europe, which leads to the need to develop mechanisms that allow greater control of the health condition of chronic patients with hypertension or diabetes. with this need to increase management control, mobile applications are presented as a solution to improve the management and monitoring of the health condition of chronically ill patients. due to their mobility and connectivity with medical devices, they are increasingly positioned as the most complete answer that users can turn to better manage their health condition. however, existing applications are mostly manual control or are dependent on a specific medical device. it was in this context that altice labs proposed this dissertation. thus, the purpose of this dissertation is to develop a mobile application for the management and monitoring of hypertensive and diabetic patients, in order to allow greater efficiency in the management and telemonitoring of the chronic diseases of hypertension and diabetes. the application must be integrated with the smartal platform by altice labs and allow the user to easily and intuitively record vital signs and daily activity, as well as food. after carrying out studies on hypertension and diabetes and carrying out an extensive analysis of the market for mobile applications, the application was developed according to the proposed objectives, having been validated through a pilot test with an end user. in the conclusion, a reflection is carried out on the work carried out, the results obtained and the future work.",
      "atualmente as doenças crónicas da hipertensão e diabetes tem uma prevalência enorme não só em portugal como na europa, o que leva a que seja necessário desenvolver mecanismos que permitam um maior controlo da condição de saúde do doente crónico com hipertensão ou diabetes. com essa necessidade de aumentar o controlo da gestão, as aplicações móveis apresentam-se como solução para melhorar a gestão e monitorização da condição de saúde de doentes crónicos. devido à sua mobilidade e conetividade com dispositivos médicos cada vez mais se posicionam como a resposta mais completa que os utilizadores podem recorrer para melhorar a gestão da sua condição de saúde. no entanto, as aplicações existentes apresentam um controlo principalmente manual ou estão dependentes de um dispositivo médico específico. foi neste âmbito que a altice labs propôs a presente dissertação. deste modo, a proposta da presente dissertação tem como objetivo o desenvolvimento de uma aplicação mobile para a gestão e monitorização de hipertensos e diabéticos, de forma a permitir uma maior eficácia na gestão e telemonitorização das doenças crónicas da hipertensão e diabetes. a aplicação deve ser integrada com a plataforma smartal da altice labs e permitir que o utilizador registe de forma fácil e intuitiva os sinais vitais e a sua atividade diária assim como a sua alimentação. após efetuar estudos sobre a hipertensão e diabetes e realizar uma análise extensiva ao mercado de aplicações mobile, desenvolveu-se a aplicação de acordo com os objetivos propostos, tendo sido validada através de um teste piloto com um utilizador final. na conclusão, é realizada uma reflexão sobre o trabalho realizado, os resultados obtidos e o trabalho futuro."
    ],
    [
      "in the constantly evolving realms of deep learning and machine learning, the ascent of intricate and powerful neural network models has pushed the frontiers of computational intelligence to new heights. nevertheless, the conspicuous absence of an accessible and user-friendly visualization tool has presented an imposing hurdle in the endeavor to comprehend and dissect these intricate and multifaceted architectures. in direct response to this critical challenge, this dissertation unveils a network visualizer platform: neural network explorer. this platform has been developed in an attempt to offer a comprehensive solution to these complex problems. through an exhaustive and in-depth analysis of existing tools and apis, the neural network explorer platform provides a streamlined and intuitive methodology for visualizing a diverse and extensive array of neural networks. this innovative tool empowers users to seamlessly unravel and decode the intricacies of the various layers and structures that compose these intricate model architectures, thereby fostering a deeper and more insightful understanding of the fundamental mechanisms that govern their functioning. by delving into the realm of cutting-edge network visualization techniques, the platform not only facilitates the seamless export of illustrative images but also grants access to a wealth of detailed and layer-specific information. this abundant and comprehensive data resource acts as a dynamic catalyst for researchers and developers, equipping them with the essential insights and understanding needed to adeptly navigate the landscape of deep learning models and harness the potential within. through its robust exploration and innovative methodologies, this work provides researchers and practitioners with an indispensable and user-friendly tool for unraveling and exploring the complex and architectures that underscore the foundation of advanced computational intelligence.",
      "nos domínios em constante evolução do deep learning e machine learning, a ascensão de modelos de redes neuronais e poderosos impulsionou os limites da inteligência computacional a novas alturas. no entanto, a ausência notória de uma ferramenta de visualização acessível e fácil de usar tem representado um obstáculo imponente na busca por compreender e desvendar essas arquiteturas complexas e multifacetadas. em resposta direta a este desafio crítico, esta dissertação revela com orgulho a plataforma neural network explorer, elaborada para preencher essa lacuna e oferecer uma solução abrangente para esses problemas complexos. através de uma análise exaustiva e aprofundada das ferramentas e apis existentes, a plataforma neural network explorer oferece uma metodologia simplificada e intuitiva para visualizar uma gama diversificada e extensa de redes neurais. esta ferramenta inovadora capacita os utilizadores a desvendar e decifrar sem esforço as complexidades das várias camadas e estruturas que compõem essas arquiteturas de modelos, promovendo assim uma compreensão mais profunda e perspicaz dos mecanismos fundamentais que regem o seu funcionamento. através da exploração de técnicas de visualização de redes de ponta, a plataforma não só facilita a exportação imediata de imagens ilustrativas, mas também proporciona acesso a um conjunto de informações detalhadas e específicas de layers. este recurso de dados abrangente e abundante atua como um catalisador dinâmico para investigadores e desenvolvedores, dotando-os dos conhecimentos e compreensão essenciais necessários para navegar habilmente pela paisagem dos modelos de aprendizagem profunda e aproveitar o seu potencial. através da sua exploração robusta e metodologias inovadoras, este trabalho marca um marco fundamental no campo da visualização de redes neurais, fornecendo aos investigadores e profissionais uma ferramenta indispensável e fácil de usar para desvendar e explorar as arquiteturas complexas que sustentam a base da inteligência computacional avançada."
    ],
    0.0
  ],
  [
    [
      "a avaliação da idade óssea (a maturação esquelética) é uma prática clínica comum para investigar doenças endocrinológicas, genéticas e de crescimento em crianças. geralmente é realizada por exame radiológico da mão esquerda usando o método greulich e pyle (g & p) ou o tanner whitehouse (tw). no entanto, ambos procedimentos clínicos demonstraram várias limitações, desde o esforço do exame que tem que ser feito pelos radiologistas até a significativa variabilidade intra e inter-operador. para resolver este problema, várias abordagens com recurso a sistemas de apoio ao diagnóstico médico (especialmente tomando como base o método tw) foram propostas. nenhum deles demonstrou capacidades de generalização para diferentes raças, faixas etárias e géneros. a avaliação de exames radiológicos requer a análise de um profissional com a máxima atenção. no caso do método de greulich e pyle a radiografia da mão do paciente é comparada com um atlas padrão sendo possível observar deficiências no crescimento dos pacientes. este é um trabalho exaustivo e sujeito a erros devido ao nível de atenção que é necessário durante o diagnóstico. os métodos de deep learning têm sido aplicados a diversas tarefas de análise de imagem médica como, por exemplo, classificação de lesões e segmentação de tecidos. o principal objectivo deste trabalho e desenvolver um modelo capaz de automaticamente determinar a idade óssea. neste trabalho foram primeiramente testadas várias arquitecturas de redes neuronais convolucionais na determinação da idade óssea que mostraram bons resultados em tarefas comuns de visão por computador. baseado nos resultados obtidos foi desenvolvido/optimizado um novo modelo que é apresentado neste documento. foi usado transfer learning e o treino de raiz nas redes neuronais seleccionadas obtendo uma taxa de erro de 7.89 meses na determinação da idade óssea em pacientes do sexo feminino e uma taxa de erro de 8,28 meses ao executar esta tarefa em homens.",
      "bone age assessment is a common clinical practice to detect endocrinological, genetic and growth diseases. usually it’s performed using a x-ray image of the non dominant hand applying the greulich and pyle or the tanner whitehouse (tw) method. however both procedures showed to possess several limitations since the effort deman ded to radiologists to the significant intra- and inter-operator variability. to address these problems, several automated approaches (especially relying on the tw method) have been proposed. none of them showed to be able to generalize to different races, age ranges and genders. the evaluation of x-ray images requires the analysis of a professional with maximum levels of attention. the greulich and pyle method consists in comparing the image with an atlas being able to observe disabilities in the growing of pacients. this is an intensive job and error-prone due to the level of attention that is needed during the diagnosis. deep learning methods have been applied to different medical imaging analysis tasks like, for e.g., lesion classification and tissue segmentation. the main objective of this dis sertation is to develop a model capable of automatically assess bone age. in this work, we first have tested several state-of-the-art convolution neural networks models for assessing bone age that previously has shown great results in general computer vision tasks. ba sed on these results, we have developed/optimized a new model, which is presented here. for this purpose, we used transfer learning methods and trained the selected networks from scratch achieving a 7.89-month error rate when assessing bone age in females and 8.28-month error rate when performing this task on men."
    ],
    [
      "the need to ensure the confidentiality and integrity of data generated in industrial systems and applications has been increasingly highlighted over the years, due to the clear and urgent requirements of not disclosing sensible proprietary information and ensuring that data is kept immutable since it is generated until it is permanently stored. it is from these two main ideas that this dissertation is created, framed in a project that is being developed at the digital transformation colab with bilanciai and cachapuz. these are the industrial partners and key stakeholders of this project, having identified the requirements for the weight measurement process that occurs in the weighing stations that are placed in their customers. this dissertation essentially consists on the definition of a secure internet of things (iot) communication system between the devices that operate on the weighing stations of the customers and on top of that, develop a smart contract application using blockchain technology capable of: i) automating the process of verifying the correct application of weighing guidelines; and ii) registering and storing ”receipts” of weighings that take place in the customers’ weighing stations. in this dissertation, a revision of the state of the art is made with the goal to perceive the most secure and current technologies capable of providing the required functionalities, which are the fuel for the identification of the problems and challenges that such a project might face, ultimately leading to the design of a solution that can both: i) mitigate the aforementioned problems and challenges; and ii) comply with the goals defined for the dissertation. additionally, in this document, the development of such a solution is also explored by providing clear insights into the decisions that were made and the reasoning behind them and by implementing components that are able to provide registration and rich querying of weighing tickets (receipts), weighing ticket building and secure communication as well as the enforcing of a blockchain network structure that fosters data confidentiality. ultimately, results are shown, collected from a proof of concept, which essentially provide evidence on the functional correctness of the system that was built, i.e., its ability to grant the retainment of weighing ticket characteristics and the capabilities of the communication system, which demonstrates to be able to securely build and transmit weighing tickets, with fault tolerance. the outcomes of this project can be integrated into existing systems of the industrial partners to increase efficiency, security and business innovation.",
      "a necessidade de assegurar a confidencialidade e a integridade dos dados gerados em sistemas e aplicações industriais tem sido cada vez mais destacada ao longo dos últimos anos, devido a claros e urgentes requisitos de não divulgar informação proprietária e de garantir que essa informação permanece imutável desde o momento em que é gerada até ao ponto em que é guardada permanentemente. é a partir destas duas ideias principais que esta dissertação é criada, enquadrada num projeto que esta a ser desenvolvido no digital transformation colab com a bilanciai e a cachapuz. estes são os parceiros industriais e stakeholders do projeto, tendo identificado os requisitos para o processo contínuo de medição de pesagens que ocorre nas estações de pesagem dos seus clientes. esta dissertação consiste, essencialmente, na definição de uma comunicação segura em iot entre os dispositivos que operam nas estações de pesagem dos clientes e, complementarmente, desenvolver uma aplicação baseada em smart contracts utilizando tecnologia blockchain com o intuito de: i) automatizar o processo de verificação da aplicação correta de diretrizes de pesagem; e, ii) registar e armazenar ”recibos” de pesagem ˜ que são efetuadas nas estações de pesagem dos clientes. nesta dissertação, a revisão do estado da arte é feita com o objetivo de entender as tecnologias mais atuais e seguras capazes de providenciar as funcionalidades adjacentes aos requisitos, o que se torna na base para a identificação dos problemas e desafios que um projeto desta natureza pode enfrentar, resultando, em última instância, no desenho de uma solução que consiga: i) mitigar os problemas e desafios anteriormente mencionados; e, ii) cumprir com os objetivos definidos para esta dissertação. adicionalmente, neste documento, o desenvolvimento da solução é explorado, ao fornecer informações claras sobre as decisões que foram tomadas e o raciocínio por trás das mesmas e ao implementar componentes capazes de fornecer o registo e consulta avançada de recibos de pesagem, construção e transmissão segura dos mesmos, como também a capacidade de estruturar e assegurar uma organização da rede blockchain que promove a confidencialidade de dados. finalmente, resultados são ilustrados, extraídos de uma prova de conceito, fornecendo provas da correção funcional do sistema construído, isto e, a sua capacidade para garantir a manutenção das características dos recibos de pesagem e, além disso demonstra a capacidade do sistema de comunicação em transmitir, de forma segura, os recibos de pesagem, com tolerância a falhas. o resultado obtido neste projeto tem a possibilidade de ser integrados em sistemas existentes dos parceiros industriais com o objetivo de aumentar a eficiência, segurança e inovação nos seus modelos de negócio."
    ],
    0.0
  ],
  [
    [
      "in modern days, it becomes more and more common for software solutions to focus on mobile and web technologies, therefore the current desktop market has been shrinking. due to the big impact that web technologies are having on the market and user’s daily basis it has become impossible for developers to neglect this evolution. nevertheless, in some cases it is difficult to justify the development of some web applications since the benefits are to small and the costs to high. due to this problem and some other small inconveniences, there are some emerging technologies that try to close the gap between desktop and web applications by trying to combine the best of both worlds. there are some well-known technologies such as java applets, which are mainly java applications that can be executed on the browser. even though these technologies are very interesting and in some specific cases very useful, companies avoid taking this path since this kind of software applications raise some problems, which are making sure that these new technologies are abandoned and forgotten (e.g. some security problems with regard to plugin installation). with this project, we intend to create a framework, which main goal is to ease the hybrid application development. this framework allows users to develop native java se applications, that can be accessed as normal desktop applications, but at the same time it is possible to access the same content through a regular web browser, using common well-known technologies such as html, javascript and css. with this solution, it is possible to avoid high costs on web application development, and avoid other small problems such as security problems when installing plugins that can be found in the current existing solutions. this way it is possible to develop a single desktop application that is reusable on the browser if needed. the idea is not to allow the user to create a new application that can be accessed on both platforms, but on the contrary it aims java applications that have already been developed or that will be developed with no intention of making them accessible on the web, but at some point the urge to port the application appears and the user won’t need to rebuild everything from scratch, but he will simply need to invest some time developing the new user interface for the web version that he wants to provide.",
      "nos dias de hoje é cada vez mais comum as soluções de software que se encontram no mercado serem feitas à volta de tecnologias mobile ou web, o que tem criado uma diminuição no mercado de aplicações nativas desktop. devido ao grande impacto que as tecnologias web têm tido no mercado e nos utilizadores, tem sido impossível para os desenvolvedores de aplicações, negligenciar esta evolução. mesmo assim, em alguns casos específicos torna-se muito difícil justificar o desenvolvimento de aplicações web, sendo que os benefícios obtidos são muito baixos e os gastos de produção muito altos. devido a este tipo de problemas entre outros pequenos detalhes, tem surgido novas tecnologias que tentam encurtar a diferença entre aplicações desktop e web, tentando combinar o melhor dos dois mundos. existem algumas tecnologias muito conhecias tais como java applets, que são no fundo aplicações java que podem ser executadas dentro de um browser. mesmo estas tecnologias sendo muito interessantes e até mesmo muito úteis em certos casos, grandes companhias tendem a evitar o uso destas devido a alguns problemas que têm aparecido ao longo do tempo e que podem por em jogo a segurança e duração de vida dessas aplicações. um grande problema encontrado neste ramo, são falhas de segurança na instalação de plugins, que esta a fazer com que este tipo de tecnologias esteja a ser abandonada. com este projeto criamos uma framework que permite e facilita o desenvolvimento de aplicações híbridas. esta framework permite que os utilizadores desenvolvam aplicações em java se, que podem ser acedidas como aplicações normais, desktop, mas ao mesmo tempo é possível aceder a algum do conteúdo dessas aplicações através de um simples navegador de internet. isto tudo criando uma nova camada composta de tecnologias web tais como html5, css e javascript sem ter de recriar a aplicação. com esta solução é possível evitar grandes custos no desenvolvimento de novas aplicações web, e ao mesmo tempo evitar pequenos problemas de segurança como os que já foram mencionados antes em soluções já existentes. desta forma é possível criar aplicações nativas que podem ser reutilizadas como aplicações web caso necessário. a ideia por trás deste projeto não se concentra em permitir criar novas aplicações híbridas, sendo que já se encontram soluções para este tipo de problemas. pelo contrário o principal problema que tentamos resolver com esta solução é permitir a aplicações java que já existem ou que estão a ser desenvolvidas como simples aplicações nativas, possam no futuro ser portadas para o domínio web sem que seja necessário a recriação de uma versão completa web, e ao mesmo tempo limitar o custo desse processo a um mínimo, requerendo apenas que uma nova ui para a versão web seja desenvolvida."
    ],
    [
      "distanced learning has a rich history and its evolution is closely connected to technology. it started with hand-written letters, later through radios, cd-roms, and now with the internet. with these advancements, the definition of e-learning appears - learning through the use of electronic devices. this new learning methodology comes with its own list of advantages (accessibility, lower cost, flexibility, etc.) and disad vantages (low motivation, social isolation, effectiveness, etc.). larger institutions, such as universities and organisations, find e-learning very appealing mostly due to its long-term cost-efficiency. web accessibility and usability are important aspects of e-learning, rooted in the fact that the web is one of the most used means for sharing content. ensuring that educational materials are accessible to all learners, regardless of their abilities is paramount to e-learning. developers need to understand how can they make websites more accessible and usable, considering aspects such as design and implementation. e-learning is now widely used, therefore a market appeared around it, paving the way for platforms such as blackboard and moodle. these platforms have carved their niche in the realm of digital education, each with its own strengths and unique features. by looking at what they offer we can deepen our understanding of what e-learning tools have to offer and what functionalities users value most. formare is an e-learning web platform created by altice labs. the company feels the platform is built using obsolete technologies (web forms) and wants to modernize the front-end layer of the application. connecting formare’s functionalities to a new web interface (using react) is not a straightforward process, so an api that implements the back office and connects it to the front end was introduced. this dissertation documents the implementation process of modernizing an e-learning platform with a new user interface, going in-depth about the problems and solutions found throughout development.",
      "o ensino à distância possui uma rica história e a sua evolução está ligada à tecnologia. começou com cartas, passou pelos rádios, cd-roms e agora a internet. com estes avanços, surge a definição de e learning - ensino através de dispositivos eletrónicos. este novo tipo de aprendizagem traz consigo tanto vantagens (acessibilidade, custos mais baixos, flexibilidade, etc.) como desvantagens (baixa motivação, isolamento social, eficácia, etc.). instituições, como universidades e empresas, encontram no e-learning uma metodologia de ensino atraente principalmente devido ao seu custo e eficiência a longo prazo. a acessibilidade e usabilidade na web são aspetos importantes do e-learning, devido ao facto de a web ser um dos meios mais utilizados para partilhar conteúdos. garantir que os materiais educativos são acessíveis a todos os alunos, independentemente das suas capacidades, é fundamental para o e-learning. os desenvolvedores precisam de compreender como podem desenvolver websites mais acessíveis, con siderando aspetos como o design e a implementação. o e-learning é agora amplamente utilizado, o que deu origem a um novo mercado, abrindo caminho para plataformas como blackboard e moodle. estas plataformas criaram o seu espaço no campo da educação digital, cada uma com as suas próprias vantagens e características. ao analisarmos o que elas oferecem, podemos aprofundar a nossa compreensão das ferramentas e-learning e das funcionalidades mais valorizadas pelos utilizadores. o formare é uma plataforma de e-learning criada pela altice labs. a empresa considera que a plataforma foi construída utilizando tecnologias obsoletas (web forms) e pretende modernizar a camada front-end da aplicação. a conexão das funcionalidades do formare a uma nova interface web (usando react) não é um processo simples, por isso foi introduzida uma api que implementa o back office e a conecta à camada front-end. esta dissertação documenta o processo de implementação da modernização de uma plataforma de e-learning com uma nova interface, aprofundando os problemas e soluções encontrados ao longo do desenvolvimento."
    ],
    0.3
  ],
  [
    [
      "critical software can be potentially dangerous if not well verified, leading to serious failures. accordingly, there is a need for improved validation and verification methods in order to have guarantees about the software final product. the aim of this project is to define a more linear and organized verification and validation plan to, formally, verify the most critical parts of the obdh (on-board data handling) subsystem of itasat, supported by the alloy formal language. alloy supports the description of systems whose state involves complex relational structure. the application of alloy and alloy analyzer was motivated by the need for a formal specification that is more closely tailored to state-machines, and more amenable to automatic analysis. structural and behavioural properties are described declaratively, by conjoining relations and constrains, making it possible to develop and analyze a model incrementally. due to the high cost of using these methods, they are mainly used in the development of high-critical software where safety and security are crucial. this dissertation presents a set of guidelines for analysis and modelling of software systems which support the creation of a formal model and allow some extra behaviours such as synchronization, interruptions and flags. a new tool, modelmaker, was developed in order to create models using these guidelines in a more interactive way.",
      "o software crítico torna-se potencialmente perigoso quando não cuidadosamente verificado, podendo resultar em falhas graves. por consequência, existe uma necessidade de melhorar os métodos de validação e verificação, oferecendo garantias sobre o produto final. o objetivo do presente projeto e a definição de um plano de verificação e validação linear e organizado para verificar formalmente as partes mais críticas do subsistema obdh (on- board data handling) do satélite do ita (instituto de tecnologia aeronáutica), através da linguagem formal alloy. alloy permite a descrição de sistemas cujo estado envolve estruturas relacionais complexas. a sua aplicação e a do alloy analyzer justificou-se pela necessidade de uma especificação formal que fosse mais adaptada a máquinas de estado e mais recetiva a análises automáticas. as características estruturais e comportamentais sao descritas declarativamente, através da juncão de relações e restrições, permitindo desenvolver e analisar um modelo de forma iterativa. devido ao seu custo elevado, estes métodos são principalmente usados no desenvolvimento de softwares críticos, para os quais a segurança e um pressuposto fundamental. esta dissertação apresenta um conjunto de diretrizes para a análise e modelação de sistemas. estas suportam a criação de um modelo formal e permitem alguns comportamentos adicionais como a sincronização, a interrupção e ags. e também introduzido o modelmaker, uma ferramenta que, fazendo uso das directrizes, ajuda numa construção interativa e automática de modelos."
    ],
    [
      "nitrogen is one of the four most common elements in any cell and thus, it is needed to sustain all kinds of life, making the nitrogen cycle crucial to life on earth. however human activities have doubled the transfer of the reactive nitrogen into the biosphere, largely through the excessive use of fertilizers. this lead to eutrophication of aquatic systems, a negative ecosystem response usually associated with reduction of the biodiversity in it. this work is set to improve the removal technique of reactive nitrogen by transforming it into non-reactive nitrogen - through nitrosomonas europaea, an essential and ubiquitous bacteria in the nitrogen cycle. by using it in wastewater treatment plants, it is possible to overcome a limiting step of this transformation, which ultimately helps to stop eutrophication. n. europaea is the most studied ammonia-oxidizing bacteria to date and has various pathways that involve different compounds of nitrogen, making it metabolically versatile and, therefore, suitable for wastewater treatments. in this work, it was reconstructed a genome-scale metabolic model of n. europaea, using merlin (a specialized software for this task), to allow performing in silico simulations with different environmental conditions, providing knowledge of its underlying metabolic fluxes. this reconstruction was made through computational means (including several iterative steps such as automatic and manual annotation of the genome, curation of the metabolic pathways, among others), was validated through laboratorial means (by growing the organism in a chemostat and quantifying the compounds of its biomass), and was supported by literature in many cases. this validation was represented by the accuracy of the model (a comparison between the in vivo with the in silico data), and was equal to 98, 36 %. now, with a metabolic model of the organism, a guided approach may be developed to optimize the conversion of ammonia into nitrite, to be later metabolized by other organisms to produce molecular diatomic nitrogen (inactive nitrogen), thus providing a solution to eutrophication.",
      "o azoto é um dos quarto elementos mais comuns na célula, e por isso é necessário para sustentar qualquer tipo de vida, tornando o ciclo do azoto crucial para a vida na terra. mas as actividades humanas duplicaram a transferência de azoto reactivo para a biosfera, maioritariamente através do uso excessivo de fertilizantes. isto conduziu à eutrofização de sistemas aquáticos, uma resposta negativa do ecossistema normalmente associada à sua redução da sua biodiversidade. este trabalho está focado em melhorar a técnica de remoção de azoto reactivo ao transformá-lo na sua forma inactiva - através da nitrosomonas europaea, uma bactéria essencial e ubíqua no ciclo do azoto. ao usá-la em plantas de águas residuais, é possível ultrapassar um passo limitante desta conversão, que por sua vez ajuda a parar a eutrofização. n. europaea é a bactéria oxidante de amoníaco mais estudada e esta contém várias vias metabólicas que envolvem diferentes compostos de azoto, tornando-a metabolicamente versátil, e assim é adequada para tratamento de águas residuais. foi reconstruído, neste trabalho, um modelo metabólico à escala genómica da n. europaea, usando merlin (um software especializado para esta tarefa), para permitir a realização de simulações in silico sujeitadas a diferentes condições ambientais, fornecendo informação sobre os seus fluxos metabólicos. esta reconstrução foi feita através de meios computacionais (incluindo vários passos iterativos, como a anotação automática e manual do genoma, a curação das vias metabólicas, entre outros), foi validada através de meios laboratoriais (ao crescer este organismo num quimiostato e ao quantificar os compostos da sua biomassa), e foi apoiada e justificada através da literatura, em muitos casos. esta validação foi representada através da exatidão do modelo (uma comparação entre informação in vivo e in silico), e foi igual a 98, 36 %. agora, com o modelo metabólico deste organismo, uma abordagem orientada para a otimização da conversão de amoníaco para nitrito poderá ser desenvolvida, para este composto ser metabolizado por outros organismos para ser produzido azoto diatómico molecular (azoto na sua forma inactiva), e assim, fornecer uma solução para a eutrofização."
    ],
    0.0
  ],
  [
    [
      "one of the major challenges in machine learning is to investigate the capabilities and lim itations of the existing algorithms to identify when one algorithm is more adequate than another to solve particular problems. traditional approaches to predicting the performance of algorithms often involve costly trial-and-error procedures or expert knowledge, which is not always straightforward to acquire. thus, the main goal of this dissertation is to support beginners or even experienced data scientists by automatically indicating which classifica tion algorithm is most suitable for their datasets. this dissertation proposes the use of meta-learning as a possible solution to the above mentioned problem. in this respect, we introduced a novel framework for the automatic generation of meta-datasets. taking advantage of the developed framework, several clas sification datasets from public sources were used. the result is the meta-dataset for the experiment of this research project. concerning the goal of forecasting the best model for a classification dataset, two different solutions are presented: the first toward binary classification and the second on multiclass classification. a variety of machine learning algorithms are tested and compared through cross-validation. the experiment confirms the feasibility of applying meta-learning to select the algorithm that is expected to obtain the best performance for classification problems.",
      "um dos principais desafios do machine learning passa por investigar os recursos e as limitações dos algoritmos existentes para identificar quando é que um algoritmo é mais adequado do que outro para resolver um determinado problema. por norma, as abordagens tradicionais envolvem procedimentos de tentativa e erro, que requerem muito tempo ou conhecimento especializado, o que nem sempre e fácil de adquirir. assim, a presente dissertação pretende auxiliar iniciantes, indivíduos que não são cientistas de dados e até cientistas de dados experientes, indicando automaticamente qual o algoritmo que é mais vantajoso para os seus conjuntos de dados de classificação. o presente trabalho propõe a utilização de meta-learning como uma possível solução para o problema acima mencionado. numa primeira etapa é apresentada uma framework para extração automática de meta-características informativas. tirando recurso da framework desenvolvida, foram utilizados vários conjuntos de dados de classificação de fontes públicas, gerando assim o meta conjunto de dados para o experimento desta dissertação. relativamente a meta previsão do melhor modelo a utilizar, foram abordadas duas soluções: uma primeira focada em classificação binária e a segunda em classificação com múltiplas classes. em ambas foram testados e comparados vários algoritmos de machine learning através de validação cruzada. o experimento confirmou a viabilidade da aplicação de meta-learning para a seleção de algoritmos com melhor desempenho em problemas de classificação."
    ],
    [
      "productivity in an office space is directly affected by atmospheric conditions. with the capabilities of internet of things appliances, it’s possible to automate the surrounding environment and maintain optimal work conditions while, at the same time, integrating with the team’s workflow. the direct control of the environment then becomes part of any office management application used by the teams. this dissertation addresses the creation of a prototype capable of doing so. starting with a single board computer packed with atmospheric sensors, it describes the building blocks for office automation, creating a new architecture and communication protocol. instead of implementing the code to interact with physical appliances and their own intrinsic behaviour and interfaces, this protocol is easily extensible, allowing consumers to add custom nodes responsible for bridging that gap. these custom nodes can themselves produce readings and automate physical appliances regardless of the nodes that are already taking part in the protocol. having a protocol with those properties, the prototype to be developed can produce relevant information about the surrounding environment while leaving complex computations to a third party, which can be technology enthusiasts or even appliance vendors.",
      "a procura por sistemas capazes de interagir com o ambiente levou a um investimento crescente em internet of things (iot). a maioria destas soluções consiste num único dispositivo que comunica com um servidor centralizado. técnicas distribuídas ainda não se provaram como uma alternativa popular a arquiteturas centralizadas. a coleção e agregação de dados, manutenção de um estado consistente e redução do consumo energético são alguns dos principais obstáculos à implementação de sistemas distribuídos para os chamados ambientes wireless sensor network (wsn). surgem problemas similares a implementar protocolos complexos que garantem integrações com dispositivos de diferentes vendors. esta proposta foca-se principalmente no desenvolvimento de um protótipo baseado numa arquitetura distribuída para recolha de dados. o protocolo de comunicação deve permitir automação do ambiente em que está inserido e suportar ligações com nodos personalizados, responsáveis por interagir com vendors específicos. este projeto foi realizado em parceria com a subvisual, que forneceu o hardware necessário e serviu como local de testes da solução final."
    ],
    0.3
  ],
  [
    [
      "an optimization of irrigation systems and better management of green spaces is essential nowadays, as one of our main resources, water, is often wasted and the soil does not contain the necessary nutrients, which can lead to death of vegetation. this work presents a solution where, using a set of public apis through which the environment data is collected, it is possible to intelligently and autonomously activate or deactivate the irrigation system, taking into account a group of previously defined metrics. the system is also prepared to receive real data from sensors implemented in the field. a web application is also developed so that these data are presented in a clear and intuitive way, in order to support decision-making by the owner of a particular land. finally, a machine learning algorithm was created that, based on the history of rain occurrence, tries to predict the occurrence of precipitation for a particular day, thus contributing for a more efficient solution.",
      "uma otimização dos sistemas de rega e uma melhor gestão dos espaços verdes é imprescindível nos dias de hoje, pois um dos nossos principais recursos, a água, é muitas vezes desperdiçado e o solo acaba por não conter os nutrientes necessários o que pode levar à morte da vegetação. neste trabalho é apresentada uma solução onde, usando um conjunto de apis públicas através das quais é feita a recolha de dados do ambiente, é possível ativar ou desativar de forma inteligente e autónoma o sistema de rega tendo em consideração um grupo de métricas previamente definidas. o sistema está ainda preparado para receber dados reais de sensores implementados no terreno. é também desenvolvida uma aplicação web para que estes dados sejam apresenta dos de uma forma clara e intuitiva com o objetivo de suportar a tomada de decisão por parte do proprietário de um determinado terreno. por fim foi criado um algoritmo de machine learning que, tendo como base o histórico de ocorrência de chuva tenta prever a ocorrência de precipitação para um determinado dia, por forma a tornar o sistema desenvolvido mais eficiente."
    ],
    [
      "hoje em dia, graças à existência de várias aplicações em grande escala com acesso a grandes quantidades de informação, bases de dados monolíticas não são capazes de satisfazer as suas necessidades, quer a nível de disponibilidade, de escalabilidade ou de performance. deste modo, necessitamos de sistemas distribuídos de gestão de bases de dados para conseguir satisfazer estas aplicações. destes sistemas, são particularmente interessantes aqueles que se destinam a um grande número de servidores espalhados por diferentes zonas geográficas, devido à urgência de os aproximar das populações para obter uma melhor escalabilidade do sistema e uma melhor performance. estes sistemas estão geralmente divididos em duas famílias: uma que dá prioridade à coerência dos dados e uma que dá prioridade à disponibilidade do serviço. apesar do interesse que estes sistemas despertam, existe um grande custo associado ao seu teste no mundo real, sendo necessário recorrer a modelos de simulação para reproduzir o seu comportamento. além disso, estes sistemas contém bastantes diferenças entre eles, sendo muitas vezes difícil de comparar as suas vantagens e desvantagens em contexto real. nesta tese desenvolvemos o sageo, um simulador de bases de dados geo-replicadas configurável, capaz de avaliar e comparar o desempenho relativo de diversas bases de dados distribuídas. para além disso, configuramos este simulador para três algoritmos de bases de dados diferentes e apresentamos comparações de resultados de diversas simulações realizadas.",
      "now a days, thanks to the existence of various large scale applications with access to large amounts of information, monolithic databases can not satisfy their needs in terms of availability, scalability and performance. in that way, distribuited database systems are needed to solve those application issues. from those systems, those who have a large number of servers in different geographic locations are particularly intersting, due to the urge to get them closer to the populations in order to obtain a better scalability and a better performance. these systems are usually divided in two families: one that prioritizes data consistency and one that prioritizes service availability. despite the interest that these systems arouse, there is a large temporal and monetary cost associated with testing them in the real world, and it is necessary to resort to simulation models to reproduce their behavior. futhermore, these systems are very different from each other, making it often difficult to compare their advantages and disadvantages in a real context. in this thesis we developed sageo, a configurable simulator for geo-replicated databases, capable of evaluating and comparing the relative performance of different distributed databases. moreover, we configured these simulator to three different database algoritms and presented comparisons between several simulation results."
    ],
    0.3
  ],
  [
    [
      "o principal componente de um sistema blockchain e o protocolo de acordo distribuído que tem de ser capaz de tolerar faltas bizantinas na chegada a decisões. existem muitas implementações de blockchain, cada uma utilizando diferentes protocolos de acordo, porém todos eles revelam limitações. implementações cujo protocolo é da categoria proof of, apesar de escalarem, implicam compromissos entre desempenho e coerência. protocolos ditos tradicionais (e.g. pbft) são muito restritos na escalabilidade que oferecem, não conseguindo manter o desempenho ao aumentar o numero de participantes. para além disso, cada protocolo foca-se em características particulares com padrões de comunicação específicos, pelo que para alterar algum destes aspetos e necessário substituir o protocolo de acordo. neste trabalho propõe-se um protocolo que combina a tolerância a faltas bizantinas com as características do protocolo mutable consensus que admite diferentes padrões de comunicação aplicáveis a diferentes ambientes. adicionalmente, um desses padrões que privilegia uma comunicação por difusão epidémica (gossip) oferece grande escalabilidade, permitindo assim construir um protocolo que também possa escalar.",
      "the main component of a blockchain system is the consensus protocolo that must tolerate byzantine faults. there are many blockchain implementations, each one using a distinct consensus protocol, though all of them have limitations. some use a protocol from the proof of family, that exhibit tradeoffs regarding consistency and peformance. others rely in more traditional protocols (e.g. pbft), whose biggest disadvantage is its poor scalability. additionally, protocols have their own communication patterns and properties, and to change any of those it is necessary to replace the whole protocol. this dissertation aims to build a protocol that combines byzantine fault tolerance with the features of the mutable consensus protocol which allows to build multiple communications patterns adaptable to different requirements. moreover, one of those patterns, that spreads messages in an epidemic manner (gossip), offers great scalability, thus allowing to create a scalable protocol."
    ],
    [
      "a displasia congénita da anca é uma doença esquelética congénita comum em recém nascidos. o seu diagnóstico é importante para evitar complicações tardias no crescimento e locomoção. por ser um exame tão complexo e de grande responsabilidade, os diagnósticos feitos pelos profissionais são muitas vezes associados a um grau elevado de incerteza na decisão, provocando receio na realização de exames do género. os atos complementares de diagnóstico, neste caso a construção de ferramentas de apoio, são sem dúvida o maior passo para reduzir ou eliminar este problema. desta forma, com profissionais mais instruídos, consegue-se um diagnóstico mais seguro e fiável. são apresentadas recomendações para a realização do exame, englobando parâmetros como a realização do exame clínico, do exame de ecografia e da leitura de imagens de ecografia. as imagens de ecografia têm imenso ruído e para permitir um melhor processamento foram experimentadas operações básicas de processamento de imagem. é também proposto um relatório normalizado para este exame. o benefício da implementação do relatório é a sua ligação ao sistema de machine learning em que informações colocadas nos campos de preenchimento do relatório seriam transformadas em metainformação das imagens de ecografia guardadas também no relatório, funcionando como a alimentação do sistema. este sistema permitiria avaliar e classificar imagens de ecografia de um exame às articulações coxo-femorais. para além destas ferramentas descritas, é proposto uma para otimizar em termos práticos o exame - um sistema de comandos por voz com ligação ao ecógrafo para que o profissional não tenha de desviar a atenção para carregar num simples botão do ecógrafo para assinalar frames essenciais para o diagnóstico. a adoção de ferramentas de apoio ao diagnóstico da displasia congénita da anca que permitam melhorar a prestação dos cuidados de saúde é uma necessidade. as ferramentas apresentadas são um contributo e representam o início de novas abordagens ao despiste desta anomalia.",
      "congenital hip dysplasia is a common skeletal congenital disease in newborns. its diagnosis is important to avoid late complications in growth and mobility. due to such complexity and responsibility demand in the examination, diagnoses made by professionals are often associated with a high level of uncertainty in decision, causing apprehension in performing examinations of this kind. complementary acts of diagnosis, in this case the development of aiding tools, are undoubtedly the biggest step to reduce or eliminate this problem. this way, with more instructed professionals, it is possible to achieve a safer and more reliable diagnosis. recommendations to the examination are suggested, which enclose factors as clinical examination, hip ultrasound and the interpretation of the ultrasound images. the creation of standardized reports for this examination is also suggested. considering high presence of noise in ultrasound scans, and in order to attain better processing, basic image processing operations were tested. the benefit of using a report is its connection to the machine learning system, in which data and images in the report fields would be used as the entrance instances of a machine learning system using supervised learning. this would allow the evaluation and classification of ultrasound images in an examination of coxofemoral joints. in addition to the described tools, one is suggested to optimize the examination in practical terms – a voice command system connected to the ultrasound machine, allowing the professional not to divert the attention pressing a button located in the machine to capture the essential frames to the diagnosis. adopting congenital hip dysplasia diagnosis support tools that provide healthcare improvement is a need. the presented tools are a contribution and represent the beginning of a new approach to the screening of this anomaly."
    ],
    0.0857142857142857
  ]
]